
            <!DOCTYPE html>
            <html lang="en">
            <head>
                <meta charset="UTF-8">
                <meta name="viewport" content="width=device-width, initial-scale=1.0">
                <title>Recurrent Neural Networks (RNNs)</title>
                <style>
                    body { font-family: Arial, sans-serif; margin: 40px; padding: 20px; line-height: 1.6; }
                    .container { max-width: 800px; margin: auto; }
                    h1, h2 { color: #333; }
                    .box { background: #f4f4f4; padding: 15px; margin-bottom: 20px; border-radius: 5px; }
                    .category { font-weight: bold; color: #0056b3; }
                </style>
            </head>
            <body>
                <div class="container">
                    <h1>Recurrent Neural Networks (RNNs)</h1>
                    <p><strong>Instance Code:</strong> T9</p>
                    <p><strong>Original Category:</strong> CS</p>
                    <p><strong>Target Category:</strong> CL</p>
                    <div class="box">
                        <h2>Original Text</h2>
                        <p>A recurrent neural network (RNN) is any network that contains a cycle within its network connections, meaning that the value of some unit is directly, or indirectly, dependent on its own earlier outputs as an input. [...] As with ordinary feedforward networks, an input vector representing the current input, xt, is multiplied by a weight matrix and then passed through a non-linear activation function to compute the values for a layer of hidden units. This hidden layer is then used to calculate a corresponding output, yt. [...] The key difference from a feedforward network lies in the recurrent link [...] [which] augments the input to the computation at the hidden layer with the value of the hidden layer from the preceding point in time. The hidden layer from the previous time step provides a form of memory, or context, that encodes earlier processing and informs the decisions to be made at later points in time. [...] the context embodied in the previous hidden layer can include information extending back to the beginning of the sequence. Adding this temporal dimension makes RNNs appear to be more complex than non-recurrent architectures. [...] we're still performing the standard feedforward calculation [...]. The most significant change lies in the new set of weights, U, that connect the hidden layer from the previous time step to the current hidden layer. [...] the [tailored] backpropagation algorithm [...] [is] referred to as backpropagation through time [...].</p>
                    </div>
                    <h2>CL Tailored Explanations</h2>
            
                    <div class="box">
                        <p class="category">mistral - CL (CL_mistral_prompt1)</p>
                        <p>Recurrent Neural Networks (RNNs) are a type of network that includes loops in their connections, allowing the present output to be influenced by past outputs. In simple terms, RNNs remember their previous computations, which can be useful for understanding sequences of data like sentences or speech.

    Just like traditional Feed-forward Neural Networks (FNNs), RNNs take an input vector, x, and multiply it by a weight matrix before passing it through an activation function to produce a set of hidden unit values. These hidden unit values are then used to calculate the output, y.

    The main difference between RNNs and FNNs is the recurrent link, which incorporates the hidden unit values from the previous time step into the current hidden layer computation. This previous hidden layer serves as a kind of memory or context that informs the decisions made at later time steps. The context can include information going back to the start of the sequence, giving RNNs a temporal dimension that makes them seem more complex than non-recurrent architectures.

    However, the basic feedforward calculation remains the same. The main change is in the weights, U, connecting the hidden layer from the previous time step to the current hidden layer.

    To train RNNs, we use a specialized version of backpropagation called backpropagation through time (BPTT), which accounts for the temporal dimension and allows us to adjust the weights effectively. In essence, BPTT allows the network to learn from sequences of data, rather than just individual examples.</p>
                    </div>
                    
                    <div class="box">
                        <p class="category">llama - CL (CL_llama_prompt5)</p>
                        <p>Imagine you're training a language model to predict the next word in a sentence. The model needs to understand the context of the previous words to make an accurate prediction. A Recurrent Neural Network (RNN) is designed to handle this type of task. It's a network that uses feedback connections to keep track of information from previous time steps, allowing it to capture temporal relationships in sequential data, like the order of words in a sentence. Think of it like trying to understand a conversation - you need to remember what was said earlier to respond appropriately. In a feedforward network, the input is processed and then discarded, but in an RNN, the hidden layer's output from the previous time step is fed back into the network, providing a form of memory that informs the model's decisions. This memory can stretch back to the beginning of the sequence, allowing the model to capture long-range dependencies. To train an RNN, we use a variation of the backpropagation algorithm, called backpropagation through time, which takes into account the temporal relationships between the inputs and outputs. This allows the model to learn from the entire sequence, not just individual time steps. In essence, RNNs are like conversational AI models that can remember and understand the context of a conversation, making them well-suited for tasks like language translation, text summarization, and chatbots.</p>
                    </div>
                    
                    <div class="box">
                        <p class="category">gpt4o - CL (CL_gpt4o_prompt5)</p>
                        <p>### Computational Linguistics (CL) Audience

A recurrent neural network (RNN) is a type of neural network designed for handling sequential data, such as text or speech. Unlike feedforward networks, which process inputs in a single pass, RNNs have loops in their structure. This means that the output from a previous time step can be used as an input in the current step, allowing the network to maintain a form of memory.

In practical terms, when an RNN processes an input sequence, each element, represented as a vector \( x_t \), is multiplied by a weight matrix and passed through a non-linear function to produce hidden layer values. These values then help produce an output \( y_t \). The twist with RNNs is that the hidden layer also incorporates information from the previous time step via weights \( U \), making it context-aware. This memory enables the RNN to remember crucial details throughout the sequence, much like how you remember the plot of a story as you read it.

The process of enhancing this sequence memory is similar to standard backpropagation used in training feedforward networks, but involves a technique called backpropagation through time. This method updates the weights based on how the network performs over a sequence, rather than a single snapshot, allowing RNNs to learn longer dependencies effectively.</p>
                    </div>
                    
                    <div class="box">
                        <p class="category">claude - CL (CL_claude_prompt5)</p>
                        <p>For CL Audience:

Recurrent Neural Networks (RNNs) are like language processing systems with memory, similar to how humans use context from previous words to understand the next ones in a sentence. While traditional neural networks process each input independently (like treating each word in isolation), RNNs maintain a "memory" of what came before.

Think of it like parsing a sentence: when you process the word "it" in "The book fell off the shelf and it broke," you need to remember "the book" from earlier to resolve this reference. RNNs work similarly - they have a special connection (the recurrent link) that passes information from previous processing steps to the current one.

The network processes input sequences (like words in a sentence) one element at a time, but unlike standard feedforward networks that you might know from basic NLP tasks, RNNs have an additional set of weights (U) that connect the previous state to the current one. This is analogous to how anaphora resolution requires maintaining and accessing previous discourse information.

The learning process, called backpropagation through time, is similar to regular backpropagation used in other neural networks you've encountered in NLP tasks, but it accounts for these temporal connections. Just as you might analyze a parse tree both forward and backward to check for consistency, this algorithm propagates errors backward through the temporal sequence to adjust the weights.

This architecture has been particularly useful in sequence-to-sequence tasks like machine translation, where maintaining context across the input sequence is crucial for accurate translation.</p>
                    </div>
                    
                    <div class="box">
                        <p class="category">deepseek - CL (CL_deepseek_prompt4)</p>
                        <p>An RNN processes sequences by dynamically updating a hidden state that serves as a compressed memory of previous inputs. Unlike feedforward networks (e.g., standard LLM layers), each step combines the current input (e.g., a word embedding) with the prior hidden state via learned weights (matrix **U**), enabling context-aware predictionsâ€”like using preceding words to resolve ambiguous token meanings in a sentence. Training unrolls these steps into a computational graph (**backpropagation through time**), optimizing how past context influences current decisions. For NLP tasks like POS tagging, this mirrors tracking grammatical agreement across a sentence, where verb tense depends on a subject mentioned earlier.</p>
                    </div>
                    </div></body></html>