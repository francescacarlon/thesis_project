
            <!DOCTYPE html>
            <html lang="en">
            <head>
                <meta charset="UTF-8">
                <meta name="viewport" content="width=device-width, initial-scale=1.0">
                <title>Automatic Speech Recognition (ASR)</title>
                <style>
                    body { font-family: Arial, sans-serif; margin: 40px; padding: 20px; line-height: 1.6; }
                    .container { max-width: 800px; margin: auto; }
                    h1, h2 { color: #333; }
                    .box { background: #f4f4f4; padding: 15px; margin-bottom: 20px; border-radius: 5px; }
                    .category { font-weight: bold; color: #0056b3; }
                </style>
            </head>
            <body>
                <div class="container">
                    <h1>Automatic Speech Recognition (ASR)</h1>
                    <p><strong>Instance Code:</strong> T15</p>
                    <p><strong>Original Category:</strong> CL</p>
                    <p><strong>Target Category:</strong> L</p>
                    <div class="box">
                        <h2>Original Text</h2>
                        <p>The first step in ASR is to transform the input waveform into a sequence of acoustic feature vectors, […]. The input to a speech recognizer is a complex series of changes in air pressure […]. We represent sound waves by plotting the change in air pressure over time. [...] The first step in digitizing a sound wave […]. This analog-to-digital conversion has two steps: sampling and quantization. To sample a signal, we measure its amplitude at a particular time; the sampling rate is the number of samples taken per second. [...] This maximum frequency for a given sampling rate is called the Nyquist frequency. [...] Amplitude measurements are stored as integers, […]. This process of representing real-valued quantization numbers as integers is called quantization; [...]. From the digitized, quantized representation of the waveform, we need to extract spectral features from a small window of speech that characterizes part of a particular phoneme. The speech extracted from each window is called a frame. The windowing is characterized by three parameters: the window size or frame size of the window (its width in milliseconds), the frame stride, (also called shift or offset) between successive windows, and the shape of the window. [...] The tool for extracting spectral information […] is the discrete Fourier transform or DFT. [...] If we plot the magnitude against the frequency, we can visualize the spectrum. [...]</p>
                    </div>
                    <h2>L Tailored Explanations</h2>
            
                    <div class="box">
                        <p class="category">mistral - L (L_mistral_prompt3)</p>
                        <p>1. Convert the audio input into a sequence of acoustic parameters, such as pitch and volume, for the speech recognizer to process.

    2. The audio input is a sequence of pressure changes in the air. We represent these pressure changes visually as a graph over time.

    3. To convert an analog audio wave into digital form, we first sample it, measuring its amplitude at specific intervals (sampling rate).

    4. The maximum frequency that a certain sampling rate can capture is known as the Nyquist frequency.

    5. We store the sampling results as integers (quantization).

    6. From the digitized, quantized representation of the waveform, we extract spectral features that define parts of specific phonemes (speech sounds). We call each of these chunks of speech a frame.

    7. The size, spacing, and shape of the frames are defined by three parameters: frame size (width in milliseconds), frame stride (offset between frames), and window shape.

    8. We use the Discrete Fourier Transform (DFT) to extract spectral information from each frame.

    9. If we plot the magnitude against the frequency, we can visualize the spectrum.</p>
                    </div>
                    
                    <div class="box">
                        <p class="category">llama - L (L_llama_prompt4)</p>
                        <p>Consider sound as a sequence of phonetic segments, similar to a spoken sentence composed of distinct words and phonemes. In speech recognition, the initial step involves capturing the acoustic properties of this sound sequence by converting it into a digital format, much like a linguistic analysis of phonetic transcriptions. This process entails digitizing the sound wave, which is analogous to translating a spoken language into a written, symbolic representation. The two primary steps in this translation are sampling and quantization, comparable to the way a phonetician might measure the acoustic characteristics of speech sounds at regular intervals and then categorize them based on their intensity. Sampling can be thought of as taking snapshots of the sound wave at consistent intervals, with the frequency of these snapshots determining the level of detail in the analysis. Quantization then assigns a numerical value to each snapshot, similar to how a linguist might assign phonetic symbols to represent distinct sounds. The resulting digital representation is composed of a series of frames, each representing a brief window of speech, akin to a single phoneme or syllable. To extract meaningful information from these frames, we employ a mathematical tool known as the discrete Fourier transform (DFT), which is comparable to a spectrogram used in phonetic analysis to visualize the acoustic properties of speech sounds. By applying the DFT, we can visualize the sound wave's spectral characteristics, plotting its magnitude against frequency, much like a linguist might analyze the formant structure of vowels.</p>
                    </div>
                    
                    <div class="box">
                        <p class="category">gpt4o - L (L_gpt4o_prompt4)</p>
                        <p>To understand how Automatic Speech Recognition (ASR) works, imagine we're trying to convert spoken words into something a computer can process. We start by capturing the sound waves, which are essentially changes in air pressure over time. These sound waves need to be turned into a digital format the computer can understand. This involves two main steps: sampling and quantization.

Sampling is like taking snapshots of the sound wave at regular intervals, similar to how a movie is made up of individual frames. The rate at which we take these snapshots is called the sampling rate. The highest pitch we can accurately capture with a given sampling rate is known as the Nyquist frequency. Once we've sampled the sound wave, we need to convert these snapshots into numbers. This conversion process is called quantization, where we represent the sound's intensity levels as whole numbers.

Once we have this digital representation, we focus on small segments of the sound wave, called frames, to analyze specific sounds or phonemes. Each frame is extracted from a window of time, and three factors define this window: its size (how many milliseconds it covers), how much it shifts forward for each new frame, and its shape.

To understand the sound's characteristics in each frame, we use a mathematical tool called the Discrete Fourier Transform (DFT). The DFT helps us see the different frequencies present in the sound by plotting their magnitudes. This visualization is known as the sound spectrum. By analyzing these spectra, we can identify and differentiate between various phonemes, helping the ASR system understand what was said.</p>
                    </div>
                    
                    <div class="box">
                        <p class="category">claude - L (L_claude_prompt5)</p>
                        <p>Here's the linguistics-oriented explanation:

Speech sounds, which linguists analyze through phonetics, can be captured and processed similarly to how we break down language into discrete units. Just as we segment continuous speech into phonemes, the digital processing of speech involves breaking down sound waves into measurable pieces.

Think of it like analyzing a stream of speech: just as we take samples of language at different points to study its features, a digital system samples the sound wave at regular intervals - similar to how we might mark specific points in continuous speech to study intonation patterns.

The process mirrors phonetic transcription in many ways. Just as we convert continuous speech into discrete phonetic symbols (like IPA), the digital system converts continuous sound waves into discrete numerical values. This is comparable to how we distinguish between gradient phonetic features by categorizing them into distinct phonological units.

The analysis then moves to examining short segments of speech (called frames), much like how we analyze syllables or phonemes in a utterance. These frames overlap slightly, similar to how coarticulation affects adjacent sounds in natural speech. From these frames, we extract frequency information - analogous to how we analyze formants in acoustic phonetics to distinguish between vowels.

The resulting frequency analysis (spectrum) is comparable to a spectrogram used in phonetic analysis, showing the distribution of acoustic energy across different frequencies - just like how we visualize formant patterns in vowel analysis.

This whole process is essentially creating a digital equivalent of what phoneticians do when breaking down speech into analyzable components for linguistic study.</p>
                    </div>
                    
                    <div class="box">
                        <p class="category">deepseek - L (L_deepseek_prompt5)</p>
                        <p>In speech recognition, we first convert spoken input into a structured form for analysis, much like phonologists decompose continuous speech into discrete phonemes. The raw input—air pressure fluctuations—is analogous to a dynamic prosodic contour. To digitize this:  

1. **Sampling**: Capture "snapshots" of the sound wave at regular intervals (e.g., 16,000 snapshots/second). This mirrors using a narrow transcription rate to avoid aliasing, where undersampling misrepresents high-frequency components (like failing to notate a rapid [ɾ] vs. [d] distinction). The **Nyquist limit** ensures our sampling rate is twice the highest frequency present, akin to needing sufficient phonetic detail to distinguish [s] (high-frequency) from [ʃ].  

2. **Quantization**: Assign integer values to amplitude measurements, similar to categorizing gradient vowel formants into discrete phonemic symbols (e.g., reducing a continuum of [æ~ɛ] to /æ/).  

Next, we divide the digitized signal into **frames**—brief, overlapping temporal windows (e.g., 25ms), like isolating a syllable nucleus or measuring formant stability in a vowel. Each frame’s **spectral features** are extracted via a mathematical tool (DFT), which decomposes the sound into its frequency components, much like a spectrogram visualizes formants (F1/F2) for vowel identification. These features act as acoustic "distinctive features," encoding cues relevant to phoneme identity (e.g., burst frequency for [k] vs. [t]).  

This process transforms raw speech into a phonetically interpretable sequence, analogous to mapping a phonetic signal to a phonological representation.</p>
                    </div>
                    </div></body></html>