{
  "T2": {
    "topic": "Constituency and Context-Free Grammars",
    "instance_code": "T2",
    "original_text_2": "Syntactic constituency is the idea that groups of words can behave as single units, or constituents. Consider the noun phrase, a sequence of words surrounding at least one noun. [...] they can all appear in similar syntactic environments, for example, before a verb: 'three parties from Brooklyn arrive'. [...] A widely used formal system for modeling constituent structure in natural language is the context-free grammar (CFG). [...] A context-free grammar consists of a set of rules or productions, each of which expresses the ways that symbols of the language can be grouped and ordered together, and a lexicon of words and symbols. [...] an NP (or noun phrase) can be composed of either a ProperNoun or a determiner (Det) followed by a Nominal; [...]. Context-free rules can be hierarchically  embedded, [...]: Det → a, Det → the, Noun → flight. [...] Thus, a CFG can be used to generate a set of strings. This sequence of rule expansions is called a derivation of the string of words. It is common to represent a derivation by a parse tree (commonly shown inverted with the root at the top). [...]. A CFG like that of L0 defines a formal language. Sentences (strings of words) that can be derived by a grammar are in the formal language defined by that grammar, and are called grammatical sentences. Sentences that cannot be derived by a given formal grammar are not in the language defined by that grammar and are  referred to as ungrammatical. [...] In linguistics, the use of formal languages to model natural languages is called generative grammar [...].",
    "token_count": 248,
    "readability": {
      "flesch_reading_ease": 62.88,
      "flesch_kincaid_grade": 8.7,
      "smog_index": 10.9
    },
    "pos": {
      "JJ": 0.07563025210084033,
      "NN": 0.2647058823529412,
      "VB": 0.10084033613445378,
      "DT": 0.15546218487394958,
      "IN": 0.15546218487394958,
      "MD": 0.03361344537815126,
      "CC": 0.04201680672268908,
      "VBG": 0.008403361344537815,
      "CD": 0.004201680672268907,
      "NNP": 0.0546218487394958,
      "RB": 0.029411764705882353,
      "VBN": 0.07563025210084033
    },
    "selected_texts": {
      "CS": {
        "CS_gpt4o_prompt3_2": {
          "text": "In computer science terms, syntactic constituency is like treating certain groups of words as single \"modules\" or \"units\" in a program. Take a noun phrase, for example: it’s a sequence of words that includes at least one noun. These noun phrases can be used in similar positions in a sentence, much like how functions can be called in similar contexts within different parts of your code. For instance, in the sentence \"three parties from Brooklyn arrive,\" the noun phrase \"three parties from Brooklyn\" acts as a single unit that appears before the verb \"arrive.\"\n\nTo formally model how these word groups work, we use something similar to what you might recognize as context-free grammars (CFGs) in programming languages. A CFG is a set of rules that describe how words and symbols can be combined, much like syntax rules in a programming language. For example, a noun phrase (NP) might be made up of a ProperNoun or a determiner (Det) followed by a Nominal. These rules can be layered, similar to how functions can call other functions: Det → a, Det → the, Noun → flight.\n\nUsing these rules, a CFG can generate sequences of words, known as a derivation. Think of it like tracing the execution of a program through a call stack. This derivation can be visualized using a parse tree, where the tree's root is at the top and branches show how the sentence is built, much like a decision tree in algorithms.\n\nA CFG defines a formal language, where sentences that can be \"derived\" or generated by the grammar are considered \"grammatical,\" like valid inputs that your code can process. Sentences that can't be derived are \"ungrammatical,\" similar to invalid inputs that would cause your program to throw an error.\n\nIn the field of linguistics, using formal languages to model natural language structures is called generative grammar, akin to how we use formal languages to define the syntax rules in programming languages.",
          "cosine_similarity": 0.8995142579078674,
          "hallucination_avg": 4.7,
          "token_count": 322,
          "readability": {
            "flesch_reading_ease": 65.05,
            "flesch_kincaid_grade": 9.9,
            "smog_index": 11.6
          },
          "pos": {
            "IN": 0.15120274914089346,
            "NN": 0.29553264604810997,
            "JJ": 0.09965635738831616,
            "VB": 0.12027491408934708,
            "VBG": 0.024054982817869417,
            "CC": 0.01718213058419244,
            "DT": 0.13058419243986255,
            "CD": 0.010309278350515464,
            "MD": 0.044673539518900345,
            "VBN": 0.048109965635738834,
            "RB": 0.010309278350515464,
            "NNP": 0.044673539518900345,
            "FW": 0.003436426116838488
          },
          "bleu_score": {
            "bleu_score": 0.5219638242894057
          },
          "rouge_scores": {
            "rouge_1": 0.6402753872633391,
            "rouge_2": 0.2867012089810017,
            "rouge_L": 0.43029259896729777
          },
          "bertscore": {
            "bertscore_precision": 0.3646497130393982,
            "bertscore_recall": 0.44694414734840393,
            "bertscore_f1": 0.40703248977661133
          },
          "LLMs_judge": 33,
          "humans_judge": 14
        },
        "CS_claude_prompt4_2": {
          "text": "Here's my adaptation for CS practitioners:\n\nIn programming languages, syntactic constituency is like nested object composition, where smaller components combine to form larger functional units. Think of it as a tree data structure where each node represents a language component.\n\nContext-Free Grammar (CFG) is similar to the compilation process's parsing phase, where code is broken down into a hierarchical structure. Just as programming languages have syntax rules (like how to form a valid if-statement or function declaration), natural language has production rules that define valid word combinations.\n\nConsider this analogy: A noun phrase is like a custom object class, where different instances (three parties from Brooklyn, the big computer, my red car) can all serve as the subject in a statement, just as different objects of the same class can be used interchangeably in your code.\n\nThe CFG rules work like a recursive function: starting from a root node (like the main() function), it expands into smaller components following predefined patterns. For example:NP -> Det Nominal  // Parent class definition\nDet -> \"the\"       // Terminal value assignment\nNominal -> \"computer\" // Terminal value assignmentThe parse tree generated is essentially an Abstract Syntax Tree (AST), similar to what compilers create when parsing source code. Valid sentences are those that can be generated by following these grammar rules, just as valid programs must follow the programming language's syntax rules.\n\nThink of generative grammar as the BNF (Backus-Naur Form) notation used to define programming language syntax, but for natural language instead.",
          "cosine_similarity": 0.8151059150695801,
          "hallucination_avg": 4.3,
          "token_count": 240,
          "readability": {
            "flesch_reading_ease": 35.71,
            "flesch_kincaid_grade": 15.0,
            "smog_index": 16.5
          },
          "pos": {
            "RB": 0.027149321266968326,
            "NN": 0.3393665158371041,
            "IN": 0.11312217194570136,
            "NNP": 0.08597285067873303,
            "VBG": 0.02262443438914027,
            "JJ": 0.13574660633484162,
            "VB": 0.10407239819004525,
            "DT": 0.1085972850678733,
            "VBN": 0.027149321266968326,
            "RP": 0.004524886877828055,
            "CC": 0.00904977375565611,
            "CD": 0.004524886877828055,
            "MD": 0.01809954751131222
          },
          "bleu_score": {
            "bleu_score": 0.36838806309423616
          },
          "rouge_scores": {
            "rouge_1": 0.4126984126984127,
            "rouge_2": 0.09163346613545818,
            "rouge_L": 0.19444444444444448
          },
          "bertscore": {
            "bertscore_precision": 0.09143423289060593,
            "bertscore_recall": 0.10574717074632645,
            "bertscore_f1": 0.10216652601957321
          },
          "LLMs_judge": 36,
          "humans_judge": 13
        }
      }
    },
    "original_text_2_score": {
      "LLMs_judge": 21,
      "humans_judge": 9
    }
  },
  "T3": {
    "topic": "Semantic Roles and Diathesis Alternations",
    "instance_code": "T3",
    "original_text_3": "Consider the meanings of the arguments Sasha, Pat, the window, and the door in these two sentences. Sasha broke the window. Pat opened the door. The subjects Sasha and Pat, what we might call the breaker of the window breaking event and the opener of the door-opening event, have something in common. They are both volitional actors, often animate, and they have direct causal responsibility for their events. Thematic roles are a way to capture this semantic commonality between breakers and openers. We say that the subjects of both these verbs are agents. Thus, AGENT is the thematic role that represents an abstract idea such as volitional causation. Similarly, the direct objects of both these verbs, the BrokenThing and OpenedThing, are both prototypically inanimate objects that are affected in some way by the action. The semantic role for these participants is theme. [...] Semantic roles thus help generalize over different surface realizations of predicate arguments. For example, while the AGENT is often realized as the subject of the sentence, in other cases the THEME can be the subject. [...] John (AGENT) broke the window (THEME). John (AGENT) broke the window (THEME) with a rock (INSTRUMENT). The rock (INSTRUMENT) broke the window (THEME). The window (THEME) broke. The window (THEME) was broken by John (AGENT). These examples suggest that break has (at least) the possible arguments AGENT, THEME, and INSTRUMENT. [...] These multiple argument structure realizations [...] are called verb alternations or diathesis alternations.",
    "token_count": 238,
    "readability": {
      "flesch_reading_ease": 59.3,
      "flesch_kincaid_grade": 8.0,
      "smog_index": 11.0
    },
    "pos": {
      "VB": 0.09170305676855896,
      "DT": 0.19213973799126638,
      "NN": 0.2663755458515284,
      "IN": 0.1091703056768559,
      "NNP": 0.11790393013100436,
      "CC": 0.034934497816593885,
      "CD": 0.004366812227074236,
      "VBN": 0.048034934497816595,
      "MD": 0.008733624454148471,
      "VBG": 0.004366812227074236,
      "JJ": 0.09606986899563319,
      "RB": 0.026200873362445413
    },
    "selected_texts": {
      "CS": {
        "CS_mistral_prompt4_3": {
          "text": "1. Distinguish the functions of Sasha, Pat, window, and door in these statements: Sasha shattered the window, Pat opened the door. Sasha and Pat, the instigators of the window-shattering and door-opening incidents, share a commonality. They are active agents who have direct control over their actions. In CS, we label these subjects as agents. The thematic role term for this concept is AGENT. On the other hand, the window and door are passive entities that are subject to change due to the actions of these agents. The semantic role for them is theme.\n\n    2. For example, in the sentences \"John shattered the window\" and \"John shattered the window with a hammer\", the AGENT is John, while the THEME is the broken window, and the hammer acts as the INSTRUMENT in the second sentence. In \"The window shattered\", the window is still the theme, but without an explicit agent. In \"The window was shattered by John\", the agent is John, and the window remains the theme. These examples demonstrate that the verb \"shatter\" can take the arguments AGENT, THEME, and INSTRUMENT.\n\n    3. These diverse argument structures, such as those illustrated in the examples, are known as verb alternations or diathesis alternations. They provide a means to generalize over various forms of predicate arguments. For instance, while the AGENT is often the subject of the sentence, in some cases, the THEME can take on this role.\n\n    By using familiar CS terminology and providing examples that the jury can relate to, the explanation becomes more understandable and accurate.",
          "cosine_similarity": 0.8980931639671326,
          "hallucination_avg": 4.2,
          "token_count": 250,
          "readability": {
            "flesch_reading_ease": 62.68,
            "flesch_kincaid_grade": 8.7,
            "smog_index": 11.2
          },
          "pos": {
            "VB": 0.100418410041841,
            "DT": 0.200836820083682,
            "NN": 0.2719665271966527,
            "IN": 0.13389121338912133,
            "NNP": 0.09205020920502092,
            "CC": 0.0502092050209205,
            "VBN": 0.03765690376569038,
            "JJ": 0.07531380753138076,
            "RB": 0.012552301255230125,
            "MD": 0.012552301255230125,
            "RP": 0.0041841004184100415,
            "VBG": 0.008368200836820083
          },
          "bleu_score": {
            "bleu_score": 0.5616883116883117
          },
          "rouge_scores": {
            "rouge_1": 0.6156941649899397,
            "rouge_2": 0.2828282828282828,
            "rouge_L": 0.3822937625754528
          },
          "bertscore": {
            "bertscore_precision": 0.3525541424751282,
            "bertscore_recall": 0.37998685240745544,
            "bertscore_f1": 0.36868295073509216
          },
          "LLMs_judge": 27,
          "humans_judge": 12
        },
        "CS_gpt4o_prompt1_3": {
          "text": "In computer science, we often deal with structured representations of data and the relationships between different entities. When analyzing language, we can think of sentences as structured data, where each part has a specific role. Let's break down the concepts using this analogy.\n\nConsider these sentences: \"Sasha broke the window\" and \"Pat opened the door.\" Here, Sasha and Pat are the subjects. In our CS terms, think of them as the main actors or agents in these events. An agent, in linguistic terms, is like the main thread in a program that initiates an action. Sasha and Pat are the ones causing something to happen, just like a function call might initiate a process.\n\nThe window and the door are the objects affected by these actions. In programming, you might think of these as the data structures that are modified by a function. These objects have a thematic role called THEME, which represents the items being acted upon.\n\nNow, let's discuss the flexibility of these roles. In different contexts, the same verb can involve various arguments. For example, in \"John broke the window with a rock,\" John is the AGENT, the window is the THEME, and the rock is the INSTRUMENT. Here, the rock is like a tool or parameter used by the function (John) to achieve an effect (breaking the window).\n\nSometimes, the structure changes, but the roles can be understood by the effect they have on the sentence's meaning. For instance:\n- \"The rock (INSTRUMENT) broke the window (THEME).\" \n- \"The window (THEME) broke.\"\n- \"The window (THEME) was broken by John (AGENT).\"\n\nThese examples show that the same verb \"break\" can have different argument structures, known as verb alternations or diathesis alternations. This is similar to how a function might be overloaded to handle different types or numbers of parameters in programming.\n\nUnderstanding these roles and alternations helps us abstract the core semantics of sentences, similar to how we abstract data types and functions in computer science to handle various situations effectively.",
          "cosine_similarity": 0.7376251220703125,
          "hallucination_avg": 4.75,
          "token_count": 332,
          "readability": {
            "flesch_reading_ease": 64.61,
            "flesch_kincaid_grade": 8.0,
            "smog_index": 11.5
          },
          "pos": {
            "IN": 0.1189710610932476,
            "NN": 0.3054662379421222,
            "RB": 0.022508038585209004,
            "VB": 0.1157556270096463,
            "JJ": 0.05466237942122187,
            "CC": 0.04180064308681672,
            "DT": 0.1832797427652733,
            "VBG": 0.022508038585209004,
            "MD": 0.022508038585209004,
            "RP": 0.003215434083601286,
            "NNP": 0.06109324758842444,
            "VBN": 0.04823151125401929
          },
          "bleu_score": {
            "bleu_score": 0.49148418491484186
          },
          "rouge_scores": {
            "rouge_1": 0.5669565217391305,
            "rouge_2": 0.21291448516579406,
            "rouge_L": 0.29913043478260865
          },
          "bertscore": {
            "bertscore_precision": 0.19560104608535767,
            "bertscore_recall": 0.2893739938735962,
            "bertscore_f1": 0.24385039508342743
          },
          "LLMs_judge": 43,
          "humans_judge": 14
        }
      }
    },
    "original_text_3_score": {
      "LLMs_judge": 20,
      "humans_judge": 10
    }
  },
  "T4": {
    "topic": "Word Senses and Relations Between Senses",
    "instance_code": "T4",
    "original_text_4": "A sense (or word sense) is a discrete representation of one aspect of the meaning of a word. [...] In context, it's easy to see the different meanings: mouse1: a mouse controlling a computer system in 1968. mouse2: a quiet animal like a mouse. bank1: a bank can hold the investments in a custodial account. bank2: as agriculture burgeons on the east bank, the river...[...] we need to consider the alternative ways that dictionaries and thesauruses offer for defining senses. One is based on the fact that dictionaries or thesauruses give textual definitions for each sense called glosses. [...] bank: 1. financial institution that accepts deposits and channels the money into lending activities 2. sloping land (especially the slope beside a body of water). [...] when two senses of two different words (lemmas) are identical, or nearly identical, we say the two senses are synonyms [...] couch/sofa vomit/throw up filbert/hazelnut car/automobile. [...] antonyms are words with an opposite meaning, like: long/short big/little fast/slow cold/hot dark/light. [...] [a] hyponym of [a] word if the [other word] is more specific, denoting a subclass of the other. For example, car is a hyponym of vehicle, dog is a hyponym of animal, [...]. Conversely, we say that vehicle is a hypernym of car, and animal is a hypernym of dog. [...] meronymy, the part-whole relation. [...] wheel is a meronym of car, and car is a holonym of wheel.",
    "token_count": 209,
    "readability": {
      "flesch_reading_ease": 57.98,
      "flesch_kincaid_grade": 8.5,
      "smog_index": 11.8
    },
    "pos": {
      "DT": 0.1691542288557214,
      "NN": 0.3681592039800995,
      "CC": 0.03482587064676617,
      "VB": 0.10945273631840796,
      "JJ": 0.07960199004975124,
      "IN": 0.15422885572139303,
      "CD": 0.024875621890547265,
      "VBG": 0.01990049751243781,
      "MD": 0.004975124378109453,
      "VBN": 0.009950248756218905,
      "RB": 0.01990049751243781,
      "RP": 0.004975124378109453
    },
    "selected_texts": {
      "CS": {
        "CS_claude_prompt4_4": {
          "text": "Here's my adapted explanation for CS practitioners:\n\nThink of word senses like different object instances of the same class name in programming. Just as \"Mouse\" could be instantiated as either a ComputerPeripheral or a Mammal class, words can have multiple distinct implementations of meaning.\n\nDictionary definitions (glosses) work like documentation strings or comments in code, explaining the specific implementation of each sense. For example, \"bank\" could be instantiated as either a FinancialInstitution or a GeographicalFeature class.\n\nThe relationships between words mirror object-oriented programming concepts:\n- Synonyms are like aliases or multiple references pointing to the same object implementation (car/automobile)\n- Antonyms are like boolean opposites (hot/cold)\n- Hyponymy/hypernymy reflects inheritance hierarchies: Vehicle is the parent class, while Car is a child class inheriting from Vehicle\n- Meronymy represents composition relationships, similar to how a Car class contains Wheel objects as component parts\n\nJust as we use type systems to organize code, these semantic relationships create a structured hierarchy for organizing word meanings in computational systems.",
          "cosine_similarity": 0.611548662185669,
          "hallucination_avg": 4.8,
          "token_count": 157,
          "readability": {
            "flesch_reading_ease": 4.95,
            "flesch_kincaid_grade": 20.6,
            "smog_index": 19.9
          },
          "pos": {
            "RB": 0.013245033112582781,
            "JJ": 0.1390728476821192,
            "NN": 0.3509933774834437,
            "IN": 0.1390728476821192,
            "NNP": 0.052980132450331126,
            "DT": 0.09271523178807947,
            "VBG": 0.033112582781456956,
            "MD": 0.019867549668874173,
            "VB": 0.09933774834437085,
            "VBN": 0.013245033112582781,
            "CC": 0.039735099337748346,
            "$": 0.006622516556291391
          },
          "bleu_score": {
            "bleu_score": 0.2061644686516962
          },
          "rouge_scores": {
            "rouge_1": 0.34739454094292804,
            "rouge_2": 0.054862842892768084,
            "rouge_L": 0.17369727047146402
          },
          "bertscore": {
            "bertscore_precision": 0.06583473086357117,
            "bertscore_recall": 0.06621532887220383,
            "bertscore_f1": 0.06977730989456177
          },
          "LLMs_judge": 42,
          "humans_judge": 14
        },
        "CS_mistral_prompt5_4": {
          "text": "1. Word Senses:\n       - A word sense is a separate definition of a word's meaning in a specific context. For instance, the word 'bank' can refer to a financial institution (bank1), or the sloping land beside a body of water (bank2).\n\n    2. Defining Word Senses:\n       - Dictionaries and thesauruses provide definitions, known as glosses, for each distinct sense.\n\n    3. Synonyms:\n       - Synonyms are words with identical or nearly identical meanings, such as couch and sofa, filbert and hazelnut, or car and automobile.\n\n    4. Antonyms:\n       - Antonyms are words with opposite meanings, like big and small, fast and slow, hot and cold, light and dark.\n\n    5. Hyponymy:\n       - A hyponym is a word that is more specific and denotes a subclass of another word. For example, car is a hyponym of vehicle, and dog is a hyponym of animal.\n\n    6. Hypernymy:\n       - The opposite of hyponymy, a hypernym is a more general word that encompasses a specific word. For instance, vehicle is a hypernym of car, and animal is a hypernym of dog.\n\n    7. Meronymy:\n       - Meronymy refers to the part-whole relationship, such as wheel being a part (meronym) of a car, and a car being the whole (holonym) that contains the wheel.",
          "cosine_similarity": 0.8777247667312622,
          "hallucination_avg": 4.8,
          "token_count": 184,
          "readability": {
            "flesch_reading_ease": 60.14,
            "flesch_kincaid_grade": 9.7
          },
          "pos": {
            "JJ": 0.12849162011173185,
            "VB": 0.09497206703910614,
            "DT": 0.15083798882681565,
            "NN": 0.3743016759776536,
            "IN": 0.11731843575418995,
            "MD": 0.00558659217877095,
            "CC": 0.08379888268156424,
            "VBG": 0.01675977653631285,
            "NNP": 0.00558659217877095,
            "VBN": 0.00558659217877095,
            "RB": 0.01675977653631285
          },
          "bleu_score": {
            "bleu_score": 0.49667979734497714
          },
          "rouge_scores": {
            "rouge_1": 0.6589861751152074,
            "rouge_2": 0.2916666666666667,
            "rouge_L": 0.4654377880184332
          },
          "bertscore": {
            "bertscore_precision": 0.24799351394176483,
            "bertscore_recall": 0.3140544891357422,
            "bertscore_f1": 0.2830999493598938
          },
          "LLMs_judge": 28,
          "humans_judge": 14
        }
      }
    },
    "original_text_4_score": {
      "LLMs_judge": 20,
      "humans_judge": 8
    }
  },
  "T7": {
    "topic": " Training FFNNs (Loss Function and Gradient)",
    "instance_code": "T7",
    "original_text_7": "A feedforward neural net is an instance of supervised machine learning in which we know the correct output y for each observation x. What the system produces is ˆy, the system's estimate of the true y. The goal of the training procedure is to learn parameters W[i] and b[i] for each layer i that make ˆy for each training observation as close as possible to the true y. [...] First, we'll need a loss function that models the distance between the system output and the gold output, and it's common to use the loss function used for logistic regression, the cross-entropy loss. Second, to find the parameters that minimize this loss function, we'll use the gradient descent optimization algorithm [...]. Third, gradient descent requires knowing the gradient of the loss function, the vector that contains the partial derivative of the loss function with respect to each of the parameters. [...] for neural networks, with millions of parameters in many layers, it's much harder to see how to compute the partial derivative of some weight in layer 1 when the loss is attached to some much later layer. How do we partial out the loss over all those intermediate layers? The answer is the algorithm called error backpropagation or backward differentiation. [...] Computing the gradient requires the partial derivative of the loss function with respect to each parameter. [...] The solution to computing this gradient is an algorithm called error backpropagation.",
    "token_count": 235,
    "readability": {
      "flesch_reading_ease": 49.75,
      "flesch_kincaid_grade": 11.6,
      "smog_index": 13.2
    },
    "pos": {
      "DT": 0.20574162679425836,
      "JJ": 0.10526315789473684,
      "NN": 0.3827751196172249,
      "VB": 0.10526315789473684,
      "IN": 0.11004784688995216,
      "NNP": 0.014354066985645933,
      "CC": 0.019138755980861243,
      "RB": 0.019138755980861243,
      "VBN": 0.019138755980861243,
      "VBG": 0.014354066985645933,
      "RP": 0.004784688995215311
    },
    "selected_texts": {
      "L": {
        "L_mistral_prompt1_7": {
          "text": "Neural networks are a type of machine learning where we train the system to predict the correct outcome (y) for given input (x). To do this, we adjust weights (W[i]) and biases (b[i]) in each layer (i) to make our prediction (ˆy) as close as possible to the actual outcome (y).\n\n     To measure how close we are, we use a loss function, similar to the one used in logistic regression, called cross-entropy loss. This function tells us how far our prediction is from the correct answer.\n\n     To find the best weights and biases, we use a method called gradient descent. However, this method requires knowing the gradient, or the rate of change, of the loss function for each parameter.\n\n     In a neural network with many layers and millions of parameters, it can be challenging to compute the gradient for a single weight in the first layer when the loss is attached to a much later layer. This is where backpropagation, or error backpropagation, comes in.\n\n     Backpropagation helps us calculate the gradient by partially deriving the loss function over all intermediate layers. In other words, it helps us trace the error back from the last layer to the very first one.\n\n     To compute the gradient using backpropagation, we need to calculate the partial derivative of the loss function with respect to each parameter. This is essential for the gradient descent algorithm to find the best weights and biases that minimize the loss function.",
          "cosine_similarity": 0.7685374617576599,
          "hallucination_avg": 5.0,
          "token_count": 243,
          "readability": {
            "flesch_reading_ease": 59.43,
            "flesch_kincaid_grade": 10.0,
            "smog_index": 12.7
          },
          "pos": {
            "JJ": 0.10900473933649289,
            "NN": 0.3080568720379147,
            "VB": 0.15165876777251186,
            "DT": 0.1848341232227488,
            "IN": 0.10900473933649289,
            "VBG": 0.023696682464454975,
            "VBN": 0.023696682464454975,
            "NNP": 0.014218009478672985,
            "CC": 0.02843601895734597,
            "RB": 0.04265402843601896,
            "MD": 0.004739336492890996
          },
          "bleu_score": {
            "bleu_score": 0.6062717770034843
          },
          "rouge_scores": {
            "rouge_1": 0.6557377049180327,
            "rouge_2": 0.29218106995884774,
            "rouge_L": 0.4385245901639344
          },
          "bertscore": {
            "bertscore_precision": 0.3468829393386841,
            "bertscore_recall": 0.39278966188430786,
            "bertscore_f1": 0.3719981908798218
          },
          "LLMs_judge": 30,
          "humans_judge": 18
        },
        "L_deepseek_prompt3_7": {
          "text": "**Adapted Explanation:**  \nA feedforward neural network works like a language model that learns by correction. Imagine teaching a student to transcribe phonetics: you show them a sound (input **x**) and the correct symbol (target **y**). The student’s guess (**ŷ**) is compared to **y**, and we adjust their \"mental rules\" (parameters) to minimize errors.  \n\n**Training involves three linguistic parallels:**  \n1. **Loss Function**: Like measuring phonetic dissimilarity, we use *cross-entropy loss*—a tool from logistic regression—to quantify how far **ŷ** is from **y**.  \n2. **Gradient Descent**: Think of tuning a synthesizer’s pitch. To fix errors, you adjust dials (parameters) incrementally, guided by *which changes most reduce distortion* (the gradient).  \n3. **Backpropagation**: Syntax trees show how sentence errors trace back to individual words. Similarly, backpropagation calculates how each \"rule\" (weight) in early network layers contributed to the final error, working backward layer-by-layer—like diagnosing mispronunciation by retracing articulatory steps.  \n\nThis mirrors language acquisition: systematic feedback refines internal rules until performance aligns with target patterns.",
          "cosine_similarity": 0.5533915162086487,
          "hallucination_avg": 4.3,
          "token_count": 155,
          "readability": {
            "flesch_reading_ease": 29.96,
            "flesch_kincaid_grade": 13.0,
            "smog_index": 13.8
          },
          "pos": {
            "NNP": 0.08695652173913043,
            "NN": 0.37681159420289856,
            "DT": 0.07971014492753623,
            "JJ": 0.12318840579710146,
            "IN": 0.07971014492753623,
            "VB": 0.12318840579710146,
            "VBG": 0.043478260869565216,
            "CC": 0.014492753623188406,
            "VBN": 0.021739130434782608,
            "CD": 0.007246376811594203,
            "RB": 0.043478260869565216
          },
          "bleu_score": {
            "bleu_score": 0.2151817984583036
          },
          "rouge_scores": {
            "rouge_1": 0.31862745098039214,
            "rouge_2": 0.06896551724137932,
            "rouge_L": 0.18137254901960786
          },
          "bertscore": {
            "bertscore_precision": 0.042795635759830475,
            "bertscore_recall": 0.0653061494231224,
            "bertscore_f1": 0.057735588401556015
          },
          "LLMs_judge": 44,
          "humans_judge": 8
        }
      }
    },
    "original_text_7_score": {
      "LLMs_judge": 16,
      "humans_judge": 10
    }
  },
  "T8": {
    "topic": "Backward Pass",
    "instance_code": "T8",
    "original_text_8": "The importance of the computation graph comes from the backward pass, which is used to compute the derivatives that we'll need for the weight update. [...] Backwards differentiation makes use of the chain rule [...]. Suppose we are computing the derivative of a composite function f(x) = u(v(x)). The derivative of f(x) is the derivative of u(x) with respect to v(x) times the derivative of v(x) with respect to x. [...] The chain rule extends to more than two functions. If computing the derivative of a composite function f(x) = u(v(w(x))), the derivative of f(x) is: df/dx = du/dv·dv/dw·dw/dx. The intuition of backward differentiation is to pass gradients back from the final node to all the nodes in the graph. [...] Each node takes an upstream gradient that is passed in from its parent node to the right, and for each of its inputs computes a local gradient (the gradient of its output with respect to its input), and uses the chain rule to multiply these two to compute a downstream gradient to be passed on to the next earlier node. Let's now compute the 3 derivatives we need. Since in the computation graph L = ce, we can directly compute the derivative ∂L/∂c: ∂L/∂c = e. For the other two, we'll need to use the chain rule: ∂L/∂a=∂L/∂e*∂e/∂a [and] ∂L/∂b=∂L/∂e*∂e/∂d*∂d/∂b.",
    "token_count": 216,
    "readability": {
      "flesch_reading_ease": 60.35,
      "flesch_kincaid_grade": 9.6,
      "smog_index": 12.2
    },
    "pos": {
      "DT": 0.17801047120418848,
      "NN": 0.32460732984293195,
      "IN": 0.14659685863874344,
      "VB": 0.14136125654450263,
      "VBN": 0.015706806282722512,
      "NNP": 0.041884816753926704,
      "VBG": 0.010471204188481676,
      "JJ": 0.07329842931937172,
      "FW": 0.010471204188481676,
      "RB": 0.020942408376963352,
      "CD": 0.015706806282722512,
      "CC": 0.015706806282722512,
      "MD": 0.005235602094240838
    },
    "selected_texts": {
      "L": {
        "L_mistral_prompt5_8": {
          "text": "1. The computation graph in machine learning is crucial for its backward pass, which calculates the needed derivatives for updating the weights during the learning process. The principal tool used in this process is chain rule from calculus.\n\n    2. Let's imagine a complex function f(x) that is the result of multiple operations u(v(x)). To calculate the derivative of this function, we use the chain rule. Specifically, the derivative of f(x) is the derivative of u(x) with respect to v(x) times the derivative of v(x) with respect to x.\n\n    3. Consider a simple example of the chain rule applied to a composite function f(x) = u(v(w(x))). The derivative of f(x) can be calculated as follows: df/dx = du/dv·dv/dw·dw/dx.\n\n    4. The chain rule's fundamental principle is that it allows us to \"pass\" the gradients from the final node backward through the entire graph. Each node in the graph receives an upstream gradient from its parent node, computes local gradients (gradients of the output with respect to its input), and then passes downstream gradients to the next earlier node using the chain rule.\n\n    5. Let's calculate the derivatives we need for our example. Suppose our computation graph is L = ce, the derivative of L with respect to c is straightforward: ∂L/∂c = e. To find the derivatives with respect to a and b, we'll use the chain rule: ∂L/∂a = ∂L/∂e * ∂e/∂a and ∂L/∂b = ∂L/∂e * ∂e/∂d * ∂d/∂b.",
          "cosine_similarity": 0.9102436304092407,
          "hallucination_avg": 5.0,
          "token_count": 226,
          "readability": {
            "flesch_reading_ease": 60.45,
            "flesch_kincaid_grade": 9.6,
            "smog_index": 12.9
          },
          "pos": {
            "DT": 0.16176470588235295,
            "NN": 0.35784313725490197,
            "IN": 0.1323529411764706,
            "VB": 0.14705882352941177,
            "JJ": 0.09803921568627451,
            "VBG": 0.00980392156862745,
            "VBN": 0.014705882352941176,
            "FW": 0.00980392156862745,
            "NNP": 0.03431372549019608,
            "RB": 0.014705882352941176,
            "MD": 0.004901960784313725,
            "CC": 0.014705882352941176
          },
          "bleu_score": {
            "bleu_score": 0.7087378640776699
          },
          "rouge_scores": {
            "rouge_1": 0.7850098619329389,
            "rouge_2": 0.499009900990099,
            "rouge_L": 0.5996055226824457
          },
          "bertscore": {
            "bertscore_precision": 0.5684764385223389,
            "bertscore_recall": 0.6069672107696533,
            "bertscore_f1": 0.5891520380973816
          },
          "LLMs_judge": 26,
          "humans_judge": 13
        },
        "L_gpt4o_prompt3_8": {
          "text": "In the world of computation, we often deal with functions that depend on other functions, much like how words in a sentence depend on each other for meaning. Here, we're focusing on something called a \"computation graph,\" which is crucial for a process called \"backward differentiation.\" This process helps us figure out how to adjust the 'weights' in a system, similar to editing a sentence to improve its clarity or meaning.\n\nLet’s break down the key idea using concepts you're familiar with. Backward differentiation uses a mathematical principle called the chain rule, which you can think of as understanding how changes in one part of a sentence affect the whole sentence's meaning. Imagine you have a function f(x) that is made up of two smaller functions, u and v, like a sentence made up of phrases. The overall change in f(x) is influenced by changes in both u and v, just as changing one phrase affects the entire sentence.\n\nThe chain rule tells us that to find the change in f(x), we multiply the change in u with respect to v by the change in v with respect to x. This concept extends to more complex functions, like adding more phrases to a sentence. For example, if f(x) is made up of three functions u(v(w(x))), the change in f(x) involves multiplying the changes across all functions, much like understanding how each phrase in a compound sentence contributes to the overall message.\n\nIn backward differentiation, we trace these changes back through the computation graph, starting from the end and moving to the beginning, similar to proofreading a sentence backwards to see how each word contributes to the sentence's meaning.\n\nNow, let's see this in action with an example. Suppose in our computation graph, we have L = ce. Here, the derivative (or change) of L with respect to c is simply e, much like how changing a single word affects its immediate meaning. For changes involving more parts, like ∂L/∂a, we apply the chain rule: ∂L/∂a=∂L/∂e*∂e/∂a. Similarly, for ∂L/∂b, the chain rule helps us: ∂L/∂b=∂L/∂e*∂e/∂d*∂d/∂b.\n\nBy understanding how each part of the function affects the whole, we can adjust the 'weights' effectively, much as you would revise a complex sentence to ensure every word is contributing to your intended meaning.",
          "cosine_similarity": 0.6825222969055176,
          "hallucination_avg": 4.9,
          "token_count": 381,
          "readability": {
            "flesch_reading_ease": 65.86,
            "flesch_kincaid_grade": 9.6
          },
          "pos": {
            "IN": 0.16666666666666666,
            "DT": 0.1488095238095238,
            "NN": 0.28869047619047616,
            "RB": 0.044642857142857144,
            "VB": 0.125,
            "JJ": 0.09226190476190477,
            "VBG": 0.044642857142857144,
            "VBN": 0.026785714285714284,
            "RP": 0.005952380952380952,
            "CC": 0.01488095238095238,
            "NNP": 0.017857142857142856,
            "MD": 0.008928571428571428,
            "CD": 0.011904761904761904,
            "FW": 0.002976190476190476
          },
          "bleu_score": {
            "bleu_score": 0.3893617021276596
          },
          "rouge_scores": {
            "rouge_1": 0.5022970903522206,
            "rouge_2": 0.18433179723502305,
            "rouge_L": 0.30321592649310875
          },
          "bertscore": {
            "bertscore_precision": 0.21649083495140076,
            "bertscore_recall": 0.3311777412891388,
            "bertscore_f1": 0.2743002474308014
          },
          "LLMs_judge": 41,
          "humans_judge": 10
        }
      }
    },
    "original_text_8_score": {
      "LLMs_judge": 23,
      "humans_judge": 13
    }
  },
  "T10": {
    "topic": "Long Short-Term Memory (LSTMs)",
    "instance_code": "T10",
    "original_text_10": "[...] it is quite difficult to train RNNs for tasks that require a network to make use of information distant from the current point of processing. [...] the information encoded in hidden states tends to be fairly local, more relevant to the most recent parts of the input sequence and recent decisions. [...] the gradients are eventually driven to zero, a situation called the vanishing gradients problem. To address these issues, more complex network architectures have been designed to explicitly manage the task of maintaining relevant context over time, by enabling the network to learn to forget information that is no longer needed and to remember information required for decisions still to come. The most commonly used such extension to RNNs is the long short-term memory (LSTM) network. LSTMs [...] remove information no longer needed from the context, and adding information likely to be needed for later decision making. [...] LSTMs accomplish this by first adding an explicit context layer to the architecture [...], and control the flow of information into and out of the units [...]. These gates are implemented through the use of additional weights that operate sequentially on the input, and previous hidden layer, and previous context layers. [...] The purpose of [the forget gate] is to delete information from the context that is no longer needed. [...] the add gate to select the information to add to the current context. [...] The final gate is the output gate which is used to decide what information is required for the current hidden state. [...]",
    "token_count": 245,
    "readability": {
      "flesch_reading_ease": 50.67,
      "flesch_kincaid_grade": 11.3,
      "smog_index": 13.4
    },
    "pos": {
      "VB": 0.1415525114155251,
      "RB": 0.0684931506849315,
      "JJ": 0.0958904109589041,
      "NNP": 0.0228310502283105,
      "IN": 0.1050228310502283,
      "NN": 0.2968036529680365,
      "DT": 0.1506849315068493,
      "VBN": 0.0639269406392694,
      "CC": 0.0319634703196347,
      "CD": 0.0045662100456621,
      "VBG": 0.0182648401826484
    },
    "selected_texts": {
      "L": {
        "L_gpt4o_prompt2_10": {
          "text": "In the world of neural networks, particularly when dealing with Recurrent Neural Networks (RNNs), we encounter challenges when trying to process information that is not immediately nearby in the sequence of data. Imagine you're reading a sentence and trying to remember a word from the beginning while you're at the end. RNNs struggle with this because they tend to focus more on recent words, much like how we might remember the last word of a sentence more clearly than the first.\n\nThis issue is known as the \"vanishing gradients problem,\" where the important information from earlier parts of the sequence fades away as the network processes more data. To tackle this, a more sophisticated type of RNN called Long Short-Term Memory (LSTM) networks was developed. Think of LSTMs as a more advanced way to handle context, similar to how we might use a mental note to keep track of key points while reading a long text.\n\nLSTMs have special mechanisms, called \"gates,\" which function like filters. These gates help the network decide what information to discard, what to keep, and what to focus on at any given moment. For example, the \"forget gate\" acts like a mental eraser, removing details that are no longer relevant. The \"add gate\" is like highlighting important notes, ensuring that essential information is retained. Finally, the \"output gate\" helps determine what information should be actively used to make the next decision, much like choosing the right words to form a coherent sentence.\n\nIn essence, LSTMs are designed to help neural networks remember crucial information over longer sequences, allowing them to process language more effectively and make better decisions, much like how linguists analyze language structure to understand meaning and context.",
          "cosine_similarity": 0.8235943913459778,
          "hallucination_avg": 4.9,
          "token_count": 284,
          "readability": {
            "flesch_reading_ease": 55.78,
            "flesch_kincaid_grade": 11.4,
            "smog_index": 13.6
          },
          "pos": {
            "IN": 0.13654618473895583,
            "DT": 0.12449799196787148,
            "NN": 0.24497991967871485,
            "JJ": 0.11646586345381527,
            "RB": 0.0642570281124498,
            "VBG": 0.04417670682730924,
            "NNP": 0.05622489959839357,
            "VB": 0.14859437751004015,
            "CC": 0.01606425702811245,
            "MD": 0.012048192771084338,
            "VBN": 0.03614457831325301
          },
          "bleu_score": {
            "bleu_score": 0.39274924471299094
          },
          "rouge_scores": {
            "rouge_1": 0.46355140186915883,
            "rouge_2": 0.1275797373358349,
            "rouge_L": 0.205607476635514
          },
          "bertscore": {
            "bertscore_precision": 0.20870621502399445,
            "bertscore_recall": 0.23907874524593353,
            "bertscore_f1": 0.22682897746562958
          },
          "LLMs_judge": 34,
          "humans_judge": 18
        },
        "L_claude_prompt5_10": {
          "text": "Here's the linguistics-oriented explanation:\n\nThink of how we process complex sentences across discourse. Just as we need to track grammatical dependencies and referents across long stretches of text, artificial neural networks need to handle information across sequences. However, like how working memory has limitations in processing center-embedded clauses, basic neural networks struggle with long-distance dependencies.\n\nImagine parsing a long sentence - we naturally forget irrelevant earlier parts while maintaining important elements like subjects or antecedents for later reference. Long Short-Term Memory (LSTM) networks mirror this cognitive process. Like how we use discourse markers and cohesive devices to manage information flow in text, LSTMs have special mechanisms called \"gates\" that control information:\n\n- The \"forget gate\" acts like pronoun resolution - it clears out old referents that are no longer needed\n- The \"add gate\" works similarly to introducing new discourse topics or referents\n- The \"output gate\" functions like selecting relevant information for the current clause being processed\n\nJust as we maintain a mental discourse model while processing text, LSTMs maintain a \"context layer\" that stores important information. Think of it as keeping track of topic continuity and coherence relations across sentences. The gates work together like the cognitive mechanisms we use to build and update our mental representation of discourse structure.",
          "cosine_similarity": 0.5818450450897217,
          "hallucination_avg": 4.2,
          "token_count": 203,
          "readability": {
            "flesch_reading_ease": 28.27,
            "flesch_kincaid_grade": 15.8,
            "smog_index": 16.5
          },
          "pos": {
            "RB": 0.04371584699453552,
            "DT": 0.06557377049180328,
            "NN": 0.3551912568306011,
            "IN": 0.1366120218579235,
            "VB": 0.12568306010928962,
            "JJ": 0.15300546448087432,
            "CC": 0.03278688524590164,
            "VBG": 0.04371584699453552,
            "NNP": 0.02185792349726776,
            "VBN": 0.01639344262295082,
            "RP": 0.00546448087431694
          },
          "bleu_score": {
            "bleu_score": 0.2703182101480283
          },
          "rouge_scores": {
            "rouge_1": 0.35729847494553374,
            "rouge_2": 0.08315098468271334,
            "rouge_L": 0.187363834422658
          },
          "bertscore": {
            "bertscore_precision": 0.10483010113239288,
            "bertscore_recall": 0.1307893991470337,
            "bertscore_f1": 0.12120893597602844
          },
          "LLMs_judge": 40,
          "humans_judge": 9
        }
      }
    },
    "original_text_10_score": {
      "LLMs_judge": 16,
      "humans_judge": 9
    }
  }
}