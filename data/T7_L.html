
            <!DOCTYPE html>
            <html lang="en">
            <head>
                <meta charset="UTF-8">
                <meta name="viewport" content="width=device-width, initial-scale=1.0">
                <title> Training FFNNs (Loss Function and Gradient)</title>
                <style>
                    body { font-family: Arial, sans-serif; margin: 40px; padding: 20px; line-height: 1.6; }
                    .container { max-width: 800px; margin: auto; }
                    h1, h2 { color: #333; }
                    .box { background: #f4f4f4; padding: 15px; margin-bottom: 20px; border-radius: 5px; }
                    .category { font-weight: bold; color: #0056b3; }
                </style>
            </head>
            <body>
                <div class="container">
                    <h1> Training FFNNs (Loss Function and Gradient)</h1>
                    <p><strong>Instance Code:</strong> T7</p>
                    <p><strong>Original Category:</strong> CS</p>
                    <p><strong>Target Category:</strong> L</p>
                    <div class="box">
                        <h2>Original Text</h2>
                        <p>A feedforward neural net is an instance of supervised machine learning in which we know the correct output y for each observation x. What the system produces is ˆy, the system's estimate of the true y. The goal of the training procedure is to learn parameters W[i] and b[i] for each layer i that make ˆy for each training observation as close as possible to the true y. [...] First, we'll need a loss function that models the distance between the system output and the gold output, and it's common to use the loss function used for logistic regression, the cross-entropy loss. Second, to find the parameters that minimize this loss function, we'll use the gradient descent optimization algorithm [...]. Third, gradient descent requires knowing the gradient of the loss function, the vector that contains the partial derivative of the loss function with respect to each of the parameters. [...] for neural networks, with millions of parameters in many layers, it's much harder to see how to compute the partial derivative of some weight in layer 1 when the loss is attached to some much later layer. How do we partial out the loss over all those intermediate layers? The answer is the algorithm called error backpropagation or backward differentiation. [...] Computing the gradient requires the partial derivative of the loss function with respect to each parameter. [...] The solution to computing this gradient is an algorithm called error backpropagation.</p>
                    </div>
                    <h2>L Tailored Explanations</h2>
            
                <div class="box">
                    <p class="category">L_gpt4o_prompt5</p>
                    <p>In linguistics terms, a feedforward neural network can be thought of as a system designed to predict an outcome based on a given input. Imagine it as a complex language model trying to translate a sentence. For every input sentence (x), we know the correct translation (y) and the model tries to produce its best guess (ŷ) of that translation.

The goal is to train this model so that its guess (ŷ) is as close as possible to the correct translation (y). To achieve this, it adjusts certain internal elements (let's call them parameters, similar to phonetic rules in a language) over several layers, each layer refining the guess based on the previous one.

First, we need a way to measure the difference between the model's guess and the correct translation, much like evaluating how well a student has translated a sentence. We use a method called cross-entropy loss for this purpose, which is similar to assessing the semantic accuracy in translation.

Next, to minimize this difference, we use an optimization technique called gradient descent. This is akin to gradually fine-tuning a grammar rule by observing many instances of its application.

However, this process involves many parameters, like adjusting various phonetic and syntactic rules simultaneously, which can become complex in a large system. To effectively adjust these, we use a technique called error backpropagation. It's like retracing steps in a derivational tree to see how a change at one point affects the overall structure.

In essence, error backpropagation helps us understand how to tweak the phonetic and syntactic rules at various layers to improve the overall translation accuracy of the neural network.</p>
                </div>
                
                <div class="box">
                    <p class="category">L_deepseek_prompt5</p>
                    <p>A feedforward neural network operates like a supervised language learning task, where the system is trained on input-output pairs (x, y), akin to teaching a learner to map phonetic inputs (e.g., acoustic signals) to semantic outputs (e.g., word meanings). The network generates an estimate (ŷ) for each input, analogous to a language model predicting a syntactic structure for a sentence. Training adjusts "weights" (W[i]) and "biases" (b[i])—parameters acting like language-specific rules at different layers (phonological, morphological, etc.)—to minimize the mismatch between ŷ and y.  

The **loss function** (e.g., cross-entropy) quantifies this mismatch, much like measuring the divergence between a predicted phoneme sequence and its attested form. **Gradient descent** iteratively refines parameters, akin to adjusting phonological or syntactic rules to reduce errors in a language learner’s output. Crucially, computing how early-layer parameters (e.g., phonetic feature weights) contribute to later errors (e.g., semantic misinterpretations) requires **backpropagation**. This mirrors tracing a semantic error back through syntactic, morphological, and phonological layers to identify where the representation diverged, adjusting each layer’s "rules" accordingly. The process ensures coordinated learning across hierarchical linguistic levels, optimizing the entire system’s performance.</p>
                </div>
                </div></body></html>