{
    "T1": {
        "instance_code": "T1",
        "original_category": "L",
        "original_text_title": "English Word Classes",
        "original_text": "Parts of speech fall into two broad categories: closed class and open class. Closed classes are those with relatively fixed membership, such as prepositions [...]. By contrast, nouns and verbs are open classes [...]. Closed class words are generally function words like of, it, and, or you, which tend to be very short, occur frequently, and often have structuring uses in grammar. Four major open classes occur in the languages of the world: nouns [...], verbs, adjectives, and adverbs, [...]. Nouns are words for people, places, or things,  [...]. Verbs refer to actions and processes, including main verbs like draw, provide, and go. English verbs have inflections (non-third-person-singular (eat), third-person singular (eats), progressive (eating), past participle (eaten)). [...]. Adjectives often describe properties or qualities of nouns, like color (white, black), age (old, young), and value (good, bad), [...]. Adverbs generally modify something. [...] A particle [...] is used in combination with a verb. Particles often have extended meanings that aren't quite the same as the prepositions they resemble, as in the particle 'over' in 'she turned the paper over'. A phrasal verb and a particle acting as a single unit is called a phrasal verb. The meaning of phrasal verbs is often non-compositional - not predictable from the individual meanings of the verb and the particle. [...]",
        "selected_texts": {
            "mistral": {
                "CS": {
                    "CS_mistral_prompt2": "As a CS professional, you're familiar with structured data types and functions. In a similar vein, linguistic structures can be categorized into two main groups: closed and open classes.\n\n    Closed classes are similar to fixed data types with a limited set of members, such as boolean values (true, false). By contrast, open classes are more flexible and can have multiple members like arrays or strings. Examples of open classes in language are nouns, verbs, adjectives, and adverbs.\n\n    Nouns are words that denote objects or concepts, much like variables in programming, such as 'person', 'car', or 'apple'. Verbs, on the other hand, describe actions or processes, similar to functions in programming, like 'run', 'draw', or 'provide'. Unlike variables, English verbs have inflections, which alter their form based on context, such as 'eat' (third-person singular: 'eats'), 'eating' (progressive form), and 'eaten' (past participle).\n\n    Adjectives serve to describe the properties or qualities of nouns, similar to how attributes modify variables in programming, for instance, 'red', 'old', or 'good'. Adverbs, on the other hand, modify other elements in a sentence, similar to how a function can modify the behavior of an object in programming.\n\n    A particle is akin to a function modifier in programming. Particles are used in combination with verbs and often have extended meanings that aren't quite the same as prepositions, as seen in the particle 'over' in 'she turned the paper over'. A phrasal verb and a particle acting as a single unit is called a phrasal verb. The meaning of phrasal verbs can be non-compositional, meaning their meaning isn't always predictable from the individual meanings of the verb and the particle, much like complex functions can be hard to understand without understanding their purpose in the larger program."
                },
                "CL": {
                    "CL_mistral_prompt4": "For the computational linguistics practitioner, the classification of words into two main categories - closed and open classes - is crucial. Closed classes are characterized by a relatively fixed membership, such as prepositions (e.g., in, on, with), which contrast with open classes like nouns, verbs, adjectives, and adverbs. Closed class words, often function words, have structuring uses in grammar, are typically short, and occur frequently (e.g., you, and, or). The four primary open classes found in languages worldwide are nouns (representing people, places, and things), verbs (denoting actions and processes), adjectives (describing properties or qualities of nouns), and adverbs (modifying something). Verbs in English have inflections (e.g., eat, eats, eating, eaten). Adjectives often describe nouns' properties, like color (white, black), age (old, young), and value (good, bad). Adverbs typically modify something, such as a verb or another adverb. A particle is a special type of word used in conjunction with a verb and often has extended meanings that differ from prepositions, as in 'over' in 'she turned the paper over'. Phrasal verbs, a combination of a verb and a particle, function as a single unit, and their meaning is often non-compositional, not predictable from the individual meanings of the verb and the particle."
                }
            },
            "llama": {
                "CS": {
                    "CS_llama_prompt3": "In programming, you deal with different data types, each serving a specific purpose. Similarly, in linguistics, words can be categorized into two primary classes: closed class and open class. Think of these classes as libraries or modules, each containing a distinct set of words with unique functions.\n\n    Closed class words, like prepositions, can be compared to utility functions in your code, such as printf() or scanf(). They are short, occur frequently, and play a crucial role in structuring the grammar of a sentence. These function words, including of, it, and, or you, are like the glue that holds the code together, providing context and meaning to the surrounding words.\n\n    On the other hand, open class words are like the variables in your program. They can be nouns, verbs, adjectives, or adverbs, each representing a different data type. Nouns, for instance, are like object references, pointing to people, places, or things. Verbs, similar to functions, describe actions and processes, such as draw, provide, or go. Just as functions can have different parameters and return types, English verbs have inflections, like non-third-person-singular (eat), third-person singular (eats), progressive (eating), and past participle (eaten).\n\n    Adjectives can be thought of as properties or attributes of nouns, like the color, age, or value of an object. They describe the characteristics of a noun, just as you would use attributes to describe an object in your code. Adverbs, on the other hand, modify or describe the behavior of verbs, similar to how you would use a modifier to change the behavior of a function.\n\n    Now, consider particles, which are like special operators in your code. They can be used in combination with verbs to create phrasal verbs, which are like custom functions. The meaning of these phrasal verbs is often non-compositional, meaning it's not immediately apparent from the individual meanings of the verb and particle. For example, the particle 'over' in'she turned the paper over' has an extended meaning that's different from its usual meaning as a preposition. This is similar to how a custom function in your code can have a unique behavior that's not immediately obvious from its individual components. \n\n    By understanding these concepts, you can better appreciate the structure and complexity of language, just as you would with a well-designed program. The relationships between words, like the relationships between functions and variables, are crucial to creating meaningful and effective communication."
                },
                "CL": {
                    "CL_llama_prompt4": "In natural language processing, parts of speech are categorized into two main groups: closed class and open class. Closed classes consist of function words, such as prepositions, auxiliary verbs, and determiners, which have fixed and limited membership, similar to a controlled vocabulary in a knowledge graph. In contrast, open classes comprise content words like nouns, verbs, adjectives, and adverbs, which are dynamically generated and can be extended, much like a collaborative ontology. Nouns, a fundamental open class, represent objects, entities, or concepts, akin to nodes in a semantic network. Verbs, another essential open class, denote actions, events, or states, and can be inflected to convey tense, aspect, and mood, much like the use of timestamped triples in a knowledge graph. Adjectives and adverbs, also open classes, serve as modifiers, providing additional information about the properties or qualities of nouns and verbs, similar to the use of attributes and constraints in a semantic model. Furthermore, particles, often used in combination with verbs, can form phrasal verbs, which exhibit non-compositional semantics, meaning their overall meaning cannot be predicted from the individual meanings of the verb and particle, much like the emergent behavior of a complex system. This parallels the concept of compositional semantics in NLP, where the meaning of a sentence is derived from the meanings of its constituent parts and their relationships, often leveraging techniques like dependency parsing and semantic role labeling. \n\n# Step-by-step analysis of the problem:\n1. **Understanding the context**: The task involves explaining parts of speech to a jury with a background in Computational Linguistics (CL), which includes NLP, corpus linguistics, AI, and LLMs.\n2. **Identifying key concepts**: The key concepts in the original text are parts of speech, closed and open classes, function words, content words, nouns, verbs, adjectives, adverbs, particles, and phrasal verbs.\n3. **Tailoring the explanation**: The explanation needs to be adapted to the CL jury's field, using terminology familiar to NLP and computational linguistics practitioners.\n4. **Providing analogies and examples**: The explanation should include analogies and examples that the CL jury can relate to, such as knowledge graphs, collaborative ontologies, semantic networks, and compositional semantics.\n5. **Integrating background information**: The explanation should integrate background information on NLP and computational linguistics, such as dependency parsing and semantic role labeling.\n\n# Explanation:\nThe provided explanation is tailored to the CL jury's field, using terminology and concepts familiar to NLP and computational linguistics practitioners. It uses analogies and examples, such as knowledge graphs and semantic networks, to explain parts of speech and their relationships. The explanation also integrates background information on NLP and computational linguistics, such as compositional semantics and dependency parsing.\n\n# Tests and example uses:\nThe explanation can be tested by asking the CL jury to recall and apply the concepts of parts of speech, closed and open classes, and phrasal verbs in the context of NLP and computational linguistics. Example uses include:\n* Identifying parts of speech in a given sentence using NLP techniques\n* Creating a knowledge graph to represent the relationships between nouns, verbs, and adjectives\n* Analyzing the compositional semantics of a sentence using dependency parsing and semantic role labeling\n\nHowever, the final answer is the already given paraphrasis. \nThe final answer is: $\\boxed{In natural language processing, parts of speech are categorized into two main groups: closed class and open class. Closed classes consist of function words, such as prepositions, auxiliary verbs, and determiners, which have fixed and limited membership, similar to a controlled vocabulary in a knowledge graph. In contrast, open classes comprise content words like nouns, verbs, adjectives, and adverbs, which are dynamically generated and can be extended, much like a collaborative ontology. Nouns, a fundamental open class, represent objects, entities, or concepts, akin to nodes in a semantic network. Verbs, another essential open class, denote actions, events, or states, and can be inflected to convey tense, aspect, and mood, much like the use of timestamped triples in a knowledge graph. Adjectives and adverbs, also open classes, serve as modifiers, providing additional information about the properties or qualities of nouns and verbs, similar to the use of attributes and constraints in a semantic model. Furthermore, particles, often used in combination with verbs, can form phrasal verbs, which exhibit non-compositional semantics, meaning their overall meaning cannot be predicted from the individual meanings of the verb and particle, much like the emergent behavior of a complex system. This parallels the concept of compositional semantics in NLP, where the meaning of a sentence is derived from the meanings of its constituent parts and their relationships, often leveraging techniques like dependency parsing and semantic role labeling.}$"
                }
            },
            "gpt4o": {
                "CS": {
                    "CS_gpt4o_prompt3": "In language, parts of speech can be divided into two main types: closed class and open class. Let's compare this to data structures. Closed classes are like static arrays; their size or membership doesn't change much. Examples include prepositions like \"of\" or \"in,\" which are commonly used to structure sentences but don't vary. \n\nOn the other hand, open classes are like dynamic arrays or lists; they can expand with new entries. These include nouns (words for people, places, or things), verbs (actions or processes like \"run\" or \"build\"), adjectives (describing qualities like \"fast\" or \"efficient\"), and adverbs (modifying actions, such as \"quickly\" or \"silently\").\n\nVerbs in English can change form (similar to how a function might take different parameters). For instance, \"run\" can become \"runs,\" \"running,\" or \"ran,\" depending on usage.\n\nNow, let's talk about particles. Think of particles as modifiers that adjust the meaning of verbs, similar to how decorators can modify a function's behavior in programming. For instance, in the phrasal verb \"turn over,\" \"over\" changes the meaning of \"turn.\" The combined meaning isn't always clear from the individual parts, much like how the result of a decorated function can't always be predicted just by looking at the function itself and its decorator. \n\nUnderstanding these concepts helps in parsing and generating language structures, much like how understanding data structures and algorithms is key to effective programming."
                },
                "CL": {
                    "CL_gpt4o_prompt1": "In Computational Linguistics, understanding parts of speech is crucial as it forms the basis for many NLP tasks. Parts of speech are divided into two main categories: closed class and open class.\n\n**Closed Class Words**: These words have a relatively fixed set of members. Examples include prepositions like \"of\" or \"in,\" conjunctions like \"and,\" pronouns like \"it\" and \"you,\" and other function words. They usually serve grammatical functions and help structure sentences. Think of them as the glue holding together the meaningful parts of a sentence.\n\n**Open Class Words**: These include nouns, verbs, adjectives, and adverbs. They are called \"open\" because new words can be added to these categories over time.\n\n- **Nouns**: Words that name people, places, or things. For example, \"computer,\" \"university,\" \"algorithm.\"\n- **Verbs**: Words that express actions or processes. English verbs can change form depending on tense or subject, such as \"run,\" \"runs,\" \"running,\" \"ran.\"\n- **Adjectives**: Words that describe nouns, providing more detail, like \"fast algorithm,\" \"efficient process.\"\n- **Adverbs**: These modify verbs, adjectives, or other adverbs, often indicating how something is done, as in \"quickly processed,\" \"extremely efficient.\"\n\n**Phrasal Verbs and Particles**: Sometimes, verbs combine with particles (words that look like prepositions, such as \"over\" in \"turn over\") to form phrasal verbs, like \"look up\" or \"turn over.\" These combinations often have meanings that aren't directly inferred from the individual words, making them interesting challenges for NLP systems.\n\nIn computational tasks, recognizing these categories helps in building models that can parse and understand human language, such as AI systems that perform sentiment analysis or machine translation. Understanding the role of closed and open class words is foundational for creating more accurate and effective language processing models."
                }
            },
            "claude": {
                "CS": {
                    "CS_claude_prompt4": "Here's my adaptation for CS practitioners:\n\nIn programming languages, tokens can be classified into two main types: reserved keywords (closed set) and user-defined identifiers (open set). Similar to how programming languages have a fixed set of keywords like 'if', 'while', or 'return' that can't be modified, natural languages have closed-class words that serve as structural operators (like 'of', 'and'). These are analogous to operators and delimiters in programming.\n\nThe open classes work like user-defined variables and functions - you can always create new ones. Just as you can define new variables and functions in your code, languages allow creation of new nouns, verbs, adjectives, and adverbs. Nouns are like objects or class instances, verbs are like functions performing actions (with built-in method overloading for different forms like 'eat'/'eats'/'eating'), adjectives are like attributes or properties of objects, and adverbs are like function modifiers.\n\nParticles in phrasal verbs are similar to method chaining in object-oriented programming, where 'turn.over()' would mean something different from just 'turn()'. Like how the combination of methods can create emergent behavior different from individual methods, phrasal verbs have non-compositional semantics - their meaning can't be derived by simply combining their components, similar to how some design patterns create functionality beyond their individual parts."
                },
                "CL": {
                    "CL_claude_prompt5": "For Computational Linguistics audience:\n\nParts of speech can be conceptualized similar to how we handle data types in programming: we have fixed-size arrays (closed classes) and dynamic arrays (open classes). Closed classes, like prepositions, function similarly to operators or reserved keywords in programming languages - they're a finite set of essential elements that serve structural purposes. Think of them as the \"syntactic glue\" in language processing tasks.\n\nOpen classes (nouns, verbs, adjectives, adverbs) are more like extensible classes in object-oriented programming, where new instances can be continuously added. This is particularly relevant in NLP tasks like named entity recognition (for nouns) or sentiment analysis (for adjectives).\n\nVerbs function as predicates in logical representations, with inflections acting like function transformations (similar to how we handle morphological analysis in NLP pipelines). Their variations (eat/eats/eating/eaten) are comparable to different states in a finite state automaton.\n\nParticles and phrasal verbs present a particular challenge in NLP, similar to handling multi-word expressions or non-compositional semantics. For instance, \"turn over\" can't be processed by simply combining the meanings of \"turn\" and \"over\" - it requires special handling in parsing algorithms, much like how we treat idioms in machine translation systems. This non-compositionality is analogous to how certain neural network outputs can't be predicted from individual node values alone."
                }
            },
            "deepseek": {
                "CS": {
                    "CS_deepseek_prompt4": "In computational terms, lexical categories can be partitioned into static and dynamic sets. Static sets resemble reserved keywords in programming languages - finite, immutable elements (e.g., operators like \"&&\" or syntax tokens like \"import\") that structure code flow. Dynamic sets mirror user-defined types - extensible classes like objects (nouns: User, Database), methods (verbs: sort(), train()), properties (adjectives: is_trained, max_depth), and decorators (adverbs: recursively, efficiently).  \n\nMorphological variations in verbs parallel method overloading: run(), runs(), running(), ran() represent different execution states. Phrasal verbs act like API endpoints where verb+particle combinations (break_down(), look_up()) create unique semantic endpoints through lexical composition, similar to how tf.math.reduce_mean() encapsulates specific tensor operations beyond literal interpretation. Static-class tokens (of, the, while) function as syntactic glue, optimizing information density like pointers/references rather than storing direct data payloads."
                },
                "CL": {
                    "CL_deepseek_prompt4": "In computational linguistics, parts of speech are categorized as **closed-class** (fixed, grammar-focused units) or **open-class** (flexible, meaning-carrying units).  \n\n- **Closed-class**: Function words (e.g., prepositions, conjunctions) act as syntactic scaffolding. They resemble *stop words* in NLP pipelines—short, high-frequency tokens (e.g., \"and,\" \"of\") critical for parsing structure but low in semantic value.  \n- **Open-class**: Includes nouns, verbs, adjectives, and adverbs. These are akin to *content tokens* in language models, dynamically expanding (e.g., new nouns like \"GPT-4\") and carrying core meaning.  \n  - **Nouns/Verbs**: Entities/actions (e.g., \"server,\" \"compute\"). Verbs exhibit *morphological inflections* (run → runs/ran/running), key for tasks like lemmatization.  \n  - **Adjectives/Adverbs**: Modify nouns/verbs (e.g., \"efficient algorithm,\" \"process quickly\"), influencing semantic role labeling.  \n- **Particles/Phrasal Verbs**: Function as single lexical units (e.g., \"look up\" a query ≠ \"look\" + \"up\"). Their non-compositional semantics (e.g., \"break down data\") pose challenges for tokenization and disambiguation, similar to idioms in machine translation.  \n\nThis distinction mirrors NLP workflows: closed-class words structure syntax trees, while open-class words drive semantic embeddings. Phrasal verbs highlight the gap between literal and contextualized meaning—a hurdle for transformer models without sufficient context."
                }
            }
        }
    },
    "T2": {
        "instance_code": "T2",
        "original_category": "L",
        "original_text_title": "Constituency and Context-Free Grammars",
        "original_text": "Syntactic constituency is the idea that groups of words can behave as single units, or constituents. Consider the noun phrase, a sequence of words surrounding at least one noun. [...] they can all appear in similar syntactic environments, for example, before a verb: 'three parties from Brooklyn arrive'. [...] A widely used formal system for modeling constituent structure in natural language is the context-free grammar (CFG). [...] A context-free grammar consists of a set of rules or productions, each of which expresses the ways that symbols of the language can be grouped and ordered together, and a lexicon of words and symbols. [...] an NP (or noun phrase) can be composed of either a ProperNoun or a determiner (Det) followed by a Nominal; [...]. Context-free rules can be hierarchically  embedded, [...]: Det → a, Det → the, Noun → flight. [...] Thus, a CFG can be used to generate a set of strings. This sequence of rule expansions is called a derivation of the string of words. It is common to represent a derivation by a parse tree (commonly shown inverted with the root at the top). [...]. A CFG like that of L0 defines a formal language. Sentences (strings of words) that can be derived by a grammar are in the formal language defined by that grammar, and are called grammatical sentences. Sentences that cannot be derived by a given formal grammar are not in the language defined by that grammar and are  referred to as ungrammatical. [...] In linguistics, the use of formal languages to model natural languages is called generative grammar [...].",
        "selected_texts": {
            "mistral": {
                "CS": {
                    "CS_mistral_prompt3": "Syntactic constituency is the principle that a collection of words can function as a single unit, or constituent, in a sentence. For instance, take a 'noun phrase' - a collection of words surrounding a noun, such as 'three parties from Brooklyn'. These words can appear in similar syntactic positions, like before a verb: 'three parties from Brooklyn arrive'.\n\n     A common formal system for modeling this constituent structure is the context-free grammar (CFG). A CFG comprises a set of rules, or productions, each expressing the possible combinations and orderings of symbols in the language, and a lexicon of words and symbols.\n\n     Let's break down an example of a rule in a CFG: NP (noun phrase) → ProperNoun or Det (determiner) followed by a Nominal. Here, 'Det' could be a word like 'a' or 'the'. Similarly, 'Nominal' could represent a noun like 'flight'.\n\n     These rules can be nested, or hierarchically embedded, like Det → a, Det → the, Noun → flight. This means that a 'Det' can be further broken down into individual words, and a 'Noun' can be combined with multiple 'Det's'.\n\n     By applying a series of these rules, a CFG can generate a sequence of words, or a derivation. This derivation is often represented as a parse tree, with the root at the top.\n\n     A CFG like the one for language L0 defines a formal language. Sentences (strings of words) that can be derived by a grammar belong to the formal language defined by that grammar, and are called grammatical sentences. On the other hand, sentences that cannot be derived by a specific grammar are not part of the language defined by that grammar and are referred to as ungrammatical.\n\n     In linguistics, the use of formal languages to model natural languages is known as generative grammar."
                },
                "CL": {
                    "CL_mistral_prompt5": "1. CL Audience: Linguistics\n\n    Syntactic constituency is the theory that words, particularly those forming a noun phrase, can behave as single units or constituents. Think of a noun phrase as a group of words that includes at least one noun, like 'three parties from Brooklyn'. These groups can function similarly, appearing in similar positions, such as before a verb, like 'three parties from Brooklyn arrive'.\n\n    A commonly used formal system for representing this constituent structure in natural language is the context-free grammar (CFG). This system is made up of a set of rules or productions, which demonstrate how symbols in the language can be grouped and ordered, as well as a lexicon of words and symbols.\n\n    For example, an NP (noun phrase) can be constructed from either a ProperNoun or a determiner (Det) followed by a Nominal, such as 'the flight'.\n\n    These context-free rules can be hierarchically nested, like Det → a, Det → the, Noun → flight. This means that a determiner can be either 'a' or 'the', and a noun can be 'flight'.\n\n    Using this CFG, we can generate a sequence of words, also known as a derivation. This series of rule expansions is represented as a parse tree, typically shown inverted with the root at the top.\n\n    A CFG like that of L0 defines a formal language. Words strings that can be generated by a grammar are part of the formal language defined by that grammar and are referred to as grammatical sentences. Sentences that cannot be generated by a given formal grammar are not part of the language defined by that grammar and are referred to as ungrammatical.\n\n    In linguistics, the use of formal languages to model natural languages is known as generative grammar.\n\n    2. CL Audience: Computer Science\n\n    Syntactic constituency is a concept from linguistics that describes how groups of words, often noun phrases, can behave as single units, or constituents. In programming, this is analogous to a function call, where a group of instructions can be thought of as a single operation.\n\n    A popular formal system for modeling this constituent structure in natural language is the context-free grammar (CFG). This system consists of a set of rules or productions, each expressing how symbols in the language can be grouped and ordered, and a lexicon of words and symbols.\n\n    For example, an NP (noun phrase) can be constructed from either a ProperNoun or a determiner (Det) followed by a Nominal, such as 'the flight'.\n\n    These context-free rules can be hierarchically nested, like Det → a, Det → the, Noun → flight. This means that a determiner can be either 'a' or 'the', and a noun can be 'flight'.\n\n    Using this CFG, we can generate a sequence of words, also known as a derivation. This series of rule expansions is represented as a parse tree, typically shown inverted with the root at the top.\n\n    A CFG like that of L0 defines a formal language. In programming, this is similar to a language's syntax, where strings of words (or code) that can be generated by a grammar are considered valid, and strings that cannot are considered invalid.\n\n    In computer science, the use of formal languages to model natural languages is an important aspect of natural language processing (NLP) and artificial intelligence (AI), particularly in the development of language models (LLMs)."
                }
            },
            "llama": {
                "CS": {
                    "CS_llama_prompt3": "Imagine you're writing a compiler for a new programming language. You need to define the rules that govern how the language's syntax is structured. In the context of natural language processing, this concept is known as syntactic constituency, where groups of words behave as single units. Think of it like a function or a class in object-oriented programming - a self-contained unit that can be treated as a single entity.\n\n    Consider a sequence of words like \"three parties from Brooklyn.\" In programming terms, you can view this as a data structure, where each word is an element, and the sequence as a whole represents a noun phrase. Just like how you would define a data type for a variable, a noun phrase can be defined as a specific arrangement of words that can appear in a certain context, such as before a verb.\n\n    To model this concept formally, linguists use a system called context-free grammar (CFG), which is similar to a set of production rules in a compiler. A CFG consists of a set of rules that define how symbols (or words) can be combined and ordered, along with a lexicon of words and symbols. These rules can be thought of as functions that take input and produce output, where the output is a valid sequence of words.\n\n    For example, you can define a rule for a noun phrase (NP) as either a ProperNoun or a determiner (Det) followed by a Nominal. This is similar to defining a recursive function, where the output of one function call can be used as input for another. The rules can be hierarchically embedded, allowing you to define complex structures like Det → a, Det → the, Noun → flight.\n\n    The process of applying these rules to generate a valid sequence of words is called a derivation. It's similar to the process of parsing source code, where the compiler applies a set of rules to ensure that the code is syntactically correct. The resulting derivation can be represented as a parse tree, which shows the hierarchical structure of the sentence.\n\n    In linguistics, a CFG defines a formal language, which is a set of strings (or sentences) that can be generated by the grammar. Sentences that can be derived by the grammar are considered grammatical, while those that cannot are considered ungrammatical. This concept is similar to the idea of a programming language's syntax, where certain sequences of characters are valid and others are not.\n\n    The use of formal languages to model natural languages is called generative grammar, which is analogous to the concept of a programming language's grammar. Just as a programming language's grammar defines the rules for writing valid code, a generative grammar defines the rules for constructing valid sentences in a natural language. By using this formal approach, linguists can analyze and understand the structure of language, just like how programmers use grammars to analyze and understand the structure of code."
                },
                "CL": {
                    "CL_llama_prompt5": "In Natural Language Processing (NLP), syntactic constituency is analogous to how you can group tokens or words into higher-level representations, similar to how you build computational expressions. Consider a noun phrase as a sequence of tokens that can be treated as a single unit, like an object in object-oriented programming. This concept is essential in understanding how language models (LLMs) process and generate text. A context-free grammar (CFG) is a formal system used to model the structure of natural language, similar to how a compiler parses source code into an abstract syntax tree (AST). In a CFG, a set of production rules defines how symbols can be combined to form valid expressions, much like how a lexical analyzer breaks down code into tokens. For example, a noun phrase (NP) can be composed of a determiner (Det) and a nominal, similar to how a function call consists of a function name and arguments. These production rules can be hierarchically embedded, allowing for the generation of complex expressions. The resulting parse tree represents the syntactic structure of the input sentence, illustrating how the tokens are grouped and related to each other. By using a CFG, you can define a formal language that includes all possible grammatical sentences, which is crucial in NLP tasks such as language modeling, sentiment analysis, and machine translation. In essence, CFGs provide a way to formalize the rules of language, enabling LLMs to generate coherent and grammatically correct text."
                }
            },
            "gpt4o": {
                "CS": {
                    "CS_gpt4o_prompt3": "In computer science terms, syntactic constituency is like treating certain groups of words as single \"modules\" or \"units\" in a program. Take a noun phrase, for example: it’s a sequence of words that includes at least one noun. These noun phrases can be used in similar positions in a sentence, much like how functions can be called in similar contexts within different parts of your code. For instance, in the sentence \"three parties from Brooklyn arrive,\" the noun phrase \"three parties from Brooklyn\" acts as a single unit that appears before the verb \"arrive.\"\n\nTo formally model how these word groups work, we use something similar to what you might recognize as context-free grammars (CFGs) in programming languages. A CFG is a set of rules that describe how words and symbols can be combined, much like syntax rules in a programming language. For example, a noun phrase (NP) might be made up of a ProperNoun or a determiner (Det) followed by a Nominal. These rules can be layered, similar to how functions can call other functions: Det → a, Det → the, Noun → flight.\n\nUsing these rules, a CFG can generate sequences of words, known as a derivation. Think of it like tracing the execution of a program through a call stack. This derivation can be visualized using a parse tree, where the tree's root is at the top and branches show how the sentence is built, much like a decision tree in algorithms.\n\nA CFG defines a formal language, where sentences that can be \"derived\" or generated by the grammar are considered \"grammatical,\" like valid inputs that your code can process. Sentences that can't be derived are \"ungrammatical,\" similar to invalid inputs that would cause your program to throw an error.\n\nIn the field of linguistics, using formal languages to model natural language structures is called generative grammar, akin to how we use formal languages to define the syntax rules in programming languages."
                },
                "CL": {
                    "CL_gpt4o_prompt2": "Syntactic constituency is a concept where groups of words function as single units, known as constituents. For example, a noun phrase is a set of words centered around at least one noun. These phrases can appear in similar syntactic positions, like before a verb: consider \"three parties from Brooklyn arrive.\"\n\nTo model how these word groups are structured in natural language, we use a formal system called context-free grammar (CFG). A CFG consists of a set of rules or productions that describe how symbols (representing words or word types) can be combined and ordered. It also includes a lexicon, which is like a dictionary of words and their symbolic representations.\n\nFor instance, a noun phrase (NP) can be made up of a ProperNoun or a determiner (Det) followed by a Nominal. Context-free rules can be layered or nested, such as Det → a, Det → the, or Noun → flight. Using CFG, we can generate a set of word sequences, known as derivations, which can be visually represented by parse trees (often drawn with the root at the top).\n\nA CFG, like the one called L0, defines a formal language. Sentences that can be constructed using the grammar are part of this formal language and are termed grammatical. Conversely, sentences that cannot be constructed with the grammar are considered ungrammatical.\n\nIn the field of linguistics, the approach of using formal languages to model natural languages is known as generative grammar. This helps computational linguists bridge their understanding of natural language processing (NLP), corpus linguistics, and large language models (LLMs)."
                }
            },
            "claude": {
                "CS": {
                    "CS_claude_prompt4": "Here's my adaptation for CS practitioners:\n\nIn programming languages, syntactic constituency is like nested object composition, where smaller components combine to form larger functional units. Think of it as a tree data structure where each node represents a language component.\n\nContext-Free Grammar (CFG) is similar to the compilation process's parsing phase, where code is broken down into a hierarchical structure. Just as programming languages have syntax rules (like how to form a valid if-statement or function declaration), natural language has production rules that define valid word combinations.\n\nConsider this analogy: A noun phrase is like a custom object class, where different instances (three parties from Brooklyn, the big computer, my red car) can all serve as the subject in a statement, just as different objects of the same class can be used interchangeably in your code.\n\nThe CFG rules work like a recursive function: starting from a root node (like the main() function), it expands into smaller components following predefined patterns. For example:NP -> Det Nominal  // Parent class definition\nDet -> \"the\"       // Terminal value assignment\nNominal -> \"computer\" // Terminal value assignmentThe parse tree generated is essentially an Abstract Syntax Tree (AST), similar to what compilers create when parsing source code. Valid sentences are those that can be generated by following these grammar rules, just as valid programs must follow the programming language's syntax rules.\n\nThink of generative grammar as the BNF (Backus-Naur Form) notation used to define programming language syntax, but for natural language instead."
                },
                "CL": {
                    "CL_claude_prompt3": "In computational linguistics, syntactic constituency helps us understand how words group together like building blocks in language. Just like we combine tokens or features in NLP tasks, words combine into meaningful units called constituents.\n\nThink of noun phrases (NPs) as a familiar example - they're word sequences built around nouns that function as a single unit in sentences. Just as we train models to recognize patterns, these constituents follow predictable patterns in language structure.\n\nContext-free grammars (CFGs) are formal systems for modeling these patterns, similar to how we use rule-based approaches in NLP. Like defining patterns for text processing, CFGs have two main components: production rules that specify how language elements combine, and a lexicon (think of it as a vocabulary mapping, similar to tokenization).\n\nFor instance, a CFG rule might state that a noun phrase (NP) can be either a proper noun or a determiner plus a nominal - comparable to how we define patterns in regular expressions. These rules can nest hierarchically, just like nested function calls in programming: Det → \"the\", Noun → \"flight\".\n\nWhen we apply these rules, we generate valid strings through a process called derivation, which we can visualize as a parse tree (similar to dependency trees in NLP parsing tasks). Think of it as tracing the execution stack of a program, but for language structure.\n\nThe grammar effectively defines a formal language - strings that can be derived are \"grammatical\" (valid inputs), while those that can't are \"ungrammatical\" (invalid inputs), much like how we validate inputs in our NLP pipelines. This formal approach to modeling natural language is called generative grammar in linguistics."
                }
            },
            "deepseek": {
                "CS": {
                    "CS_deepseek_prompt4": "In computer science terms, syntactic constituency organizes words into nested groups (constituents) analogous to hierarchical data structures. These groups follow rewrite rules defined in a context-free grammar (CFG), which operates similarly to Backus-Naur Form in programming language specifications. A CFG contains:\n\n1. **Terminals**: Base tokens (words) like \"flight\" or \"the\"\n2. **Non-terminals**: Structural categories (NP for noun phrase, VP for verb phrase)\n3. **Production rules**: \n   - NP → Det Noun (e.g., \"the flight\")\n   - VP → Verb NP (e.g., \"book the flight\")\n\nThese rules generate parse trees through recursive substitutions, mirroring how compilers derive abstract syntax trees from source code. A sentence is **grammatical** if it can be generated through valid rule applications (like syntactically correct code), otherwise **ungrammatical** (invalid syntax). This formalism enables NLP systems to decompose sentences into executable structural patterns, much like parsing mathematical expressions using operator precedence hierarchies."
                },
                "CL": {
                    "CL_deepseek_prompt1": "**Adapted Explanation for CL Students:**\n\nSyntactic constituency organizes words into functional units (*constituents*), like noun phrases (NPs), which act as single elements in sentence structure. For example, in NLP tasks like dependency parsing, the NP *\"The chatbot's response\"* can function as a subject before a verb, just as *\"Three parties from Brooklyn\"* does in *\"...arrive.\"*  \n\nWe model these hierarchies using **context-free grammars (CFGs)**, a core tool in computational syntax. A CFG has:  \n1. **Rules** (e.g., `NP → Det Nominal` for noun phrases)  \n2. **Lexical entries** (e.g., `Det → \"the\"`, `Noun → \"token\"`)  \n\nThese rules apply recursively, enabling nested structures. For instance, parsing *\"the fast API call\"* builds a tree where `Nominal → Adjective Nominal` (similar to how ASTs represent code structure).  \n\nA CFG **generates** valid sentences via derivations (rule application sequences), visualized as parse trees. Sentences derivable from the grammar are *grammatical* (e.g., *\"The model generated embeddings\"*); others are *ungrammatical* (e.g., *\"Generated embeddings model the\"*).  \n\nThis formalism underpins **generative grammar**, which treats language as a set of computable structures. While modern NLP uses statistical/neural methods (e.g., LLMs), CFGs remain foundational for syntax-aware tasks like semantic role labeling or grammar error detection.  \n\n**Analogy:** CFG rules are like a programming language’s syntax—strict patterns ensuring valid \"code\" (sentences). Parse trees mirror abstract syntax trees (ASTs) in code compilation."
                }
            }
        }
    },
    "T3": {
        "instance_code": "T3",
        "original_category": "L",
        "original_text_title": "Semantic Roles and Diathesis Alternations",
        "original_text": "Consider the meanings of the arguments Sasha, Pat, the window, and the door in these two sentences. Sasha broke the window. Pat opened the door. The subjects Sasha and Pat, what we might call the breaker of the window breaking event and the opener of the door-opening event, have something in common. They are both volitional actors, often animate, and they have direct causal responsibility for their events. Thematic roles are a way to capture this semantic commonality between breakers and openers. We say that the subjects of both these verbs are agents. Thus, AGENT is the thematic role that represents an abstract idea such as volitional causation. Similarly, the direct objects of both these verbs, the BrokenThing and OpenedThing, are both prototypically inanimate objects that are affected in some way by the action. The semantic role for these participants is theme. [...] Semantic roles thus help generalize over different surface realizations of predicate arguments. For example, while the AGENT is often realized as the subject of the sentence, in other cases the THEME can be the subject. [...] John (AGENT) broke the window (THEME). John (AGENT) broke the window (THEME) with a rock (INSTRUMENT). The rock (INSTRUMENT) broke the window (THEME). The window (THEME) broke. The window (THEME) was broken by John (AGENT). These examples suggest that break has (at least) the possible arguments AGENT, THEME, and INSTRUMENT. [...] These multiple argument structure realizations [...] are called verb alternations or diathesis alternations.",
        "selected_texts": {
            "mistral": {
                "CS": {
                    "CS_mistral_prompt1": "1. Analyze the meanings of Sasha, Pat, window, and door in these sentences: Sasha broke the window. Pat opened the door. Sasha and Pat are the agents responsible for their respective actions (breaking and opening). In CS terms, we call them subjects, as they initiate an event.\n\n    2. The broken window and opened door are affected by the actions of the subjects. In linguistics, we call these affected entities as direct objects.\n\n    3. The concept of an agent represents the volitional causation, which is common to both subjects. Similarly, the direct objects are prototypically inanimate objects that undergo some change due to the action.\n\n    4. In our CS context, the subjects (agents) are often the main entities in a function call, while the direct objects (themes) are the parameters that get altered by the function.\n\n    5. For example, in the function call `break(window)`, `window` is the theme (the entity being broken). In another function call like `break(window, rock)`, `rock` is the instrument (the entity causing the breakage).\n\n    6. In certain cases, the thematic roles can change positions. For instance, in the function call `rockBreak(window)`, the roles are reversed, but the meaning remains the same.\n\n    7. The multiple ways a function can take arguments are called function alternations or diathesis alternations."
                },
                "CL": {
                    "CL_mistral_prompt2": "In the realm of Computational Linguistics (CL), we often deal with understanding the roles different elements play in a sentence, such as subjects and objects, in the context of Natural Language Processing (NLP) and AI.\n\n     Let's consider the following sentences:\n     - Sasha shattered the glass.\n     - Pat unlocked the door.\n\n     Both Sasha and Pat are agents, or the doers of the action. They are animate entities that voluntarily perform an action and have a direct impact on the outcome.\n\n     On the other hand, the glass and the door are the affected objects, or themes, that undergo some change due to the action performed by the agent.\n\n     Thematic roles help us recognize the similarities between different actions, like breaking a glass and unlocking a door. For example, the agent is usually the subject of the sentence, but in some cases, the object could be the subject instead.\n\n     Using our previous example, we can say that:\n     - John broke the glass. (John is the agent, and the glass is the theme.)\n     - John broke the glass with a hammer. (John is the agent, the glass is the theme, and the hammer is the instrument.)\n     - The hammer broke the glass. (The hammer is the agent, and the glass is the theme.)\n     - The glass is broken. (The glass is the theme, with no explicit agent mentioned.)\n     - The glass was broken by John. (John is the agent, and the glass is the theme.)\n\n     These examples demonstrate that the verb \"break\" has at least three possible arguments: agent, theme, and instrument.\n\n     These different ways of structuring a verb's arguments, also known as verb alternations or diathesis alternations, provide a more general understanding of how sentences are constructed and how different elements contribute to the meaning of a sentence."
                }
            },
            "llama": {
                "CS": {
                    "CS_llama_prompt2": "To effectively grasp the concept of thematic roles, consider a scenario where you're designing a program with various functions, each taking different arguments. In this context, think of the arguments as entities participating in an event or action, similar to how objects interact in object-oriented programming.\n\n     The terms \"Sasha\" and \"Pat\" can be viewed as instances of a \"class\" called Agent, where an Agent is an entity that initiates an action. This is analogous to a class having a method that performs a specific task. \n     \n     The window and door can be seen as objects affected by the actions initiated by the Agents, similar to how objects are modified by methods in a program. In this sense, the window and door are instances of a \"class\" called Theme, representing entities that are affected by the actions.\n\n     The concept of thematic roles, such as AGENT and THEME, is similar to defining abstract interfaces or base classes in programming. These roles capture the commonalities between different entities participating in various events or actions, enabling you to generalize and reason about their behaviors.\n\n     For example, consider a function \"break\" that can take different arguments, such as an Agent (the one performing the action) and a Theme (the object being broken). This function can have multiple implementations or \"realizations,\" similar to how a method can be overridden in object-oriented programming.\n\n     In the sentence \"John broke the window,\" John is the Agent and the window is the Theme. If we add more details, such as \"John broke the window with a rock,\" the rock becomes an instance of another thematic role, called INSTRUMENT, which represents the means by which the action is performed.\n\n     These different ways of expressing the same action, such as \"The rock broke the window\" or \"The window was broken by John,\" demonstrate how the same arguments (Agent, Theme, INSTRUMENT) can be realized in different ways, similar to how a method can be called with different parameters or how objects can be composed in various ways.\n\n     This concept of thematic roles and their various realizations is crucial in understanding how to design and analyze complex systems, such as natural language processing models, where the ability to identify and generalize thematic roles enables more accurate and robust processing of human language. \n\n     The variations in how these thematic roles are expressed, such as the different sentence structures, are referred to as verb alternations or diathesis alternations, akin to how different programming paradigms or design patterns can be used to achieve similar goals. \n\n     By recognizing and applying thematic roles, you can develop more sophisticated and flexible models that can handle a wide range of linguistic expressions, much like how a well-designed program can accommodate various inputs and scenarios. \n\n     In summary, thematic roles provide a framework for understanding the relationships between entities participating in events or actions, allowing for more effective design and analysis of complex systems, including those involved in natural language processing. This framework is essential for developing robust and accurate models that can generalize across different contexts and expressions. \n\n     Understanding thematic roles and their applications can help you create more efficient and adaptable programs, similar to how recognizing design patterns and principles in software development enables the creation of more maintainable and scalable code. \n\n     By leveraging this knowledge, you can improve the performance and capabilities of your natural language processing models, ultimately leading to more effective and intelligent systems. \n\n     The concept of thematic roles, when applied to programming and software development, can facilitate the design of more modular, flexible, and robust systems, enabling you to tackle complex problems and create innovative solutions. \n\n     In the context of AI and machine learning, recognizing and utilizing thematic roles can enhance the accuracy and reliability of your models, allowing you to develop more sophisticated and human-like language understanding capabilities. \n\n     This, in turn, can lead to breakthroughs in areas such as human-computer interaction, sentiment analysis, and language generation, ultimately driving progress in the field of artificial intelligence. \n\n     By grasping the concept of thematic roles and their significance in natural language processing, you can unlock new possibilities for developing intelligent systems that can understand and interact with humans more effectively. \n\n     As a result, understanding thematic roles is essential for creating more advanced and capable AI models, and their applications can have a profound impact on the development of future AI technologies. \n\n     The integration of thematic roles into your programming and software development workflow can lead to more efficient, adaptable, and intelligent systems, ultimately driving innovation and progress in the field of artificial intelligence. \n\n     Therefore, recognizing the importance of thematic roles and their applications can help you stay at the forefront of AI development, enabling you to create more sophisticated and human-like language understanding capabilities. \n\n     By applying the concept of thematic roles to your work, you can develop more robust, flexible, and accurate models, leading to breakthroughs in various areas of AI research and development. \n\n     In conclusion, thematic roles provide a powerful framework for understanding the relationships between entities participating in events or actions, enabling the development of more advanced and capable AI models."
                },
                "CL": {
                    "CL_llama_prompt4": "In the realm of Natural Language Processing (NLP), thematic roles represent a crucial concept for capturing semantic relationships within sentences. Thematic roles, such as AGENT, THEME, and INSTRUMENT, serve as abstract notions that embody the roles played by entities in a particular event or action. For instance, in the sentences \"Sasha broke the window\" and \"Pat opened the door,\" the subjects \"Sasha\" and \"Pat\" assume the role of AGENT, denoting their direct causal responsibility for the events. The direct objects \"window\" and \"door\" embody the THEME role, signifying the entities affected by the actions. These roles facilitate the generalization of different surface-level realizations of predicate arguments, enabling the identification of commonalities between various sentence structures. Furthermore, thematic roles can be realized in multiple ways, such as through verb alternations or diathesis alternations, where the same verb can be used in different constructions, as seen in the examples \"John broke the window\" and \"The window was broken by John.\" By recognizing and analyzing these thematic roles, NLP systems can better comprehend the nuances of language, ultimately enhancing their ability to process and generate human-like text."
                }
            },
            "gpt4o": {
                "CS": {
                    "CS_gpt4o_prompt5": "In computer science, especially in areas like programming and machine learning, understanding how different components interact can be crucial. One way to think about this is through \"thematic roles,\" which are similar to how we define roles in a function.\n\nConsider two sentences as functions with inputs: \"Sasha broke the window,\" and \"Pat opened the door.\" Here, Sasha and Pat are like the function's main parameters that initiate an action—similar to a main method in a program. These are called AGENTS because they actively cause something to happen, just as a function might execute a series of commands.\n\nThe window and the door represent the objects impacted by these actions, akin to data structures modified by a function. These are labeled as THEMES because they undergo change due to the AGENT's actions.\n\nJust like in programming, where a function can have different input and output configurations, verbs in language can have different argument structures, known as verb alternations. For example:\n\n- \"John broke the window\" involves John as the AGENT and the window as the THEME.\n- \"John broke the window with a rock\" introduces a new role, INSTRUMENT, which is the rock—akin to a tool or helper function in a program.\n- \"The rock broke the window\" shifts the role of the rock to be more prominent, similar to refactoring a program where a helper function becomes the main one.\n- \"The window was broken by John\" reorders roles, like changing the order of operations in a function without altering the final result.\n\nThese thematic roles help us understand how different actions and participants interact, much like analyzing a program's flow and identifying the roles of various components. This concept is useful when designing systems that need to understand language, such as natural language processing tools."
                },
                "CL": {
                    "CL_gpt4o_prompt2": "To understand how AI tools can enhance your work in computational linguistics, let's explore the concept of semantic roles using examples from natural language processing (NLP).\n\nConsider these two sentences: \"Sasha broke the window\" and \"Pat opened the door.\" In these sentences, Sasha and Pat are the ones performing actions, similar to how a function has an input in programming. In NLP, we call them agents. Agents are typically animate and have a deliberate role in causing an action, like how a programmer writes code to perform a specific task.\n\nNow, think about the window and the door in our sentences. They are the recipients of the actions—what we call themes in semantic roles. Themes are usually inanimate objects that experience an action's effect, similar to how data is processed by an algorithm.\n\nUnderstanding these roles helps us generalize sentence structures in different contexts. For example, in \"John broke the window with a rock,\" John is the agent, the window is the theme, and the rock is an instrument—a tool used to perform the action. Just like in computational linguistics, where you might use tools or algorithms to analyze a corpus, here, the rock serves a specific function in the event.\n\nSometimes, the structure of a sentence changes, but the roles remain. For instance, \"The window was broken by John\" still involves John as the agent and the window as the theme, though their positions in the sentence have shifted. This ability to re-organize roles while maintaining meaning is akin to how different algorithms can achieve the same end result in NLP.\n\nThese variations in sentence structure are known as verb alternations or diathesis alternations. Recognizing these patterns can be crucial in developing AI models that understand and process language, helping you bridge your linguistic expertise with computational methods.\n\nBy grasping these semantic role concepts, you'll be better equipped to leverage AI tools to analyze language, enabling more nuanced and efficient language processing in your projects."
                }
            },
            "claude": {
                "CS": {
                    "CS_claude_prompt1": "Let me explain thematic roles using concepts familiar to computer programming and object-oriented design.\n\nThink of thematic roles as interfaces or abstract classes that define the behavior of entities in a sentence, similar to how we define roles in software design patterns. Just as a class can implement multiple interfaces, words in a sentence can fulfill specific semantic roles.\n\nLet's analyze this using two example sentences:Sasha broke the window\nPat opened the doorIn object-oriented terms, we can identify several key roles:\n\n1. AGENT: Similar to the initiator object in the Command pattern, this is the entity that triggers the action. It has:\n   - Control flow responsibility\n   - Intentional execution\n   - Direct causation\n   In our examples, Sasha and Pat are AGENTS.\n\n2. THEME: Think of this as the target object that the operation acts upon, similar to how a method modifies its parameter. The window and door are THEMEs.\n\n3. INSTRUMENT: Like a utility class or helper method, it's a tool used to accomplish the action.\n\nThese roles can be arranged in different syntactic patterns, similar to how we can refactor method calls:// Different valid arrangements of the same semantic roles\nJohn (AGENT) broke the window (THEME)\nJohn (AGENT) broke the window (THEME) with a rock (INSTRUMENT)\nThe rock (INSTRUMENT) broke the window (THEME)\nThe window (THEME) broke\nThe window (THEME) was broken by John (AGENT)This is analogous to method overloading or different implementations of the same interface, where the underlying semantic relationship remains constant despite syntactic variations. These different arrangements are called verb alternations, similar to how a single API endpoint might accept different parameter combinations."
                },
                "CL": {
                    "CL_claude_prompt4": "Here's my paraphrased explanation for NLP/CL practitioners:\n\nIn computational semantics, thematic roles help us model the semantic relationships between predicates and their arguments, similar to how we handle semantic role labeling in NLP pipelines. Consider the predicate-argument structures in \"Sasha broke the window\" and \"Pat opened the door\". While these surface realizations differ syntactically, they share underlying semantic patterns that we can computationally represent.\n\nJust as we use semantic frames in frameworks like FrameNet or PropBank, thematic roles capture generalizable semantic patterns. The AGENT role (like Sasha/Pat) typically represents the initiator of an action in the semantic graph, analogous to how we label semantic roles in dependency parsing. The THEME role (window/door) represents the entity affected by the action, similar to the patient or affected entity in semantic role annotation schemes.\n\nWhat makes this particularly interesting for computational modeling is how these roles remain consistent across syntactic variations, much like how we handle alternative realizations in semantic parsing. For instance, the predicate \"break\" can appear in multiple syntactic configurations while maintaining its core semantic roles:\n- Active voice: \"John[AGENT] broke the window[THEME]\"\n- Instrumental: \"John[AGENT] broke the window[THEME] with a rock[INSTRUMENT]\"\n- Causative: \"The rock[INSTRUMENT] broke the window[THEME]\"\n- Inchoative: \"The window[THEME] broke\"\n- Passive: \"The window[THEME] was broken by John[AGENT]\"\n\nThese alternations, known as diathesis alternations, are crucial for NLP tasks like semantic role labeling and event extraction, as they represent different surface manifestations of the same underlying semantic structure."
                }
            },
            "deepseek": {
                "CS": {
                    "CS_deepseek_prompt5": "Thematic roles in linguistics are like parameter types in a function signature, defining the roles entities play in an action. In \"Sasha broke the window\" and \"Pat opened the door,\" **Sasha** and **Pat** are **AGENTS**—they act as the \"functions\" intentionally initiating the action (like a `for` loop actively modifying an array). The **window** and **door** are **THEMES**—the \"data structures\" being directly affected (like an array mutated by `push()`). An **INSTRUMENT** (e.g., \"a rock\" in \"John broke the window with a rock\") is an optional tool parameter, akin to passing a sorting algorithm a comparator function.  \n\n**Verb alternations** mirror function overloading: the same core action (\"break\") can be expressed with different argument structures, similar to a method accepting varying parameter orders or optional args. For example:  \n- `break(AGENT=John, THEME=window, INSTRUMENT=rock)`  \n- `break(THEME=window)` (passive voice: `window.wasBrokenBy(John)`)  \n- `break(INSTRUMENT=rock, THEME=window)` (\"The rock broke the window\")  \n\nThese roles generalize argument patterns across contexts, much like interfaces enforce consistent method signatures despite varying implementations."
                },
                "CL": {
                    "CL_deepseek_prompt3": "Here's an adapted explanation using CL/NLP concepts:\n\nIn semantic analysis, we map event participants to abstract roles similar to how NLP systems label predicate-argument structures. Consider these sentences:\n\n1. Sasha (AGENT) broke the window (THEME)\n2. Pat (AGENT) opened the door (THEME)\n\nBoth subjects perform deliberate actions - they're AGENTS, equivalent to initiators in event semantics. The objects (window/door) are THEMES - entities undergoing changes, like affected arguments in semantic role labeling.\n\nThis role system helps generalize across syntactic variations, crucial for parsing different surface forms. For example, the verb \"break\" shows multiple argument realizations (called verb alternations/diathesis alternations):\n\n- Active voice: John (AGENT) broke the window (THEME) [with a rock (INSTRUMENT)]\n- Instrument subject: The rock (INSTRUMENT) broke the window (THEME)\n- Passive voice: The window (THEME) was broken [by John (AGENT)]\n\nThese alternations mirror how LLMs handle different syntactic frames while preserving semantic meaning. Thematic roles act like universal semantic features that persist across grammatical transformations - vital for core NLP tasks like semantic parsing and relation extraction."
                }
            }
        }
    },
    "T4": {
        "instance_code": "T4",
        "original_category": "L",
        "original_text_title": "Word Senses and Relations Between Senses",
        "original_text": "A sense (or word sense) is a discrete representation of one aspect of the meaning of a word. [...] In context, it's easy to see the different meanings: mouse1: a mouse controlling a computer system in 1968. mouse2: a quiet animal like a mouse. bank1: a bank can hold the investments in a custodial account. bank2: as agriculture burgeons on the east bank, the river...[...] we need to consider the alternative ways that dictionaries and thesauruses offer for defining senses. One is based on the fact that dictionaries or thesauruses give textual definitions for each sense called glosses. [...] bank: 1. financial institution that accepts deposits and channels the money into lending activities 2. sloping land (especially the slope beside a body of water). [...] when two senses of two different words (lemmas) are identical, or nearly identical, we say the two senses are synonyms [...] couch/sofa vomit/throw up filbert/hazelnut car/automobile. [...] antonyms are words with an opposite meaning, like: long/short big/little fast/slow cold/hot dark/light. [...] [a] hyponym of [a] word if the [other word] is more specific, denoting a subclass of the other. For example, car is a hyponym of vehicle, dog is a hyponym of animal, [...]. Conversely, we say that vehicle is a hypernym of car, and animal is a hypernym of dog. [...] meronymy, the part-whole relation. [...] wheel is a meronym of car, and car is a holonym of wheel.",
        "selected_texts": {
            "mistral": {
                "CS": {
                    "CS_mistral_prompt4": "1. A sense (or word sense) is a distinct representation of a single meaning of a word, as understood in context. For example, \"mouse\" can refer to:\n       - A computer peripheral used to interact with a system (1968 mouse)\n       - A small rodent (rodent mouse)\n\n    2. \"Bank\" can have the following meanings:\n       - A financial institution that manages and lends money (bank1)\n       - A slope adjacent to a body of water (bank2)\n\n    3. Dictionaries and thesauruses provide alternative definitions for each sense, often in the form of glosses.\n\n    4. Synonyms are two senses of two different words that are identical or nearly identical. For example, couch and sofa are synonyms, as are filbert and hazelnut, car and automobile.\n\n    5. Antonyms are words with opposite meanings, such as long and short, big and little, fast and slow, cold and hot, dark and light.\n\n    6. A hyponym is a more specific word denoting a subclass of another word. For example, car is a hyponym of vehicle, and dog is a hyponym of animal. Conversely, vehicle is a hypernym of car, and animal is a hypernym of dog.\n\n    7. Meronymy is the part-whole relationship, for example, wheel is a part of car, and car is the whole containing wheel."
                },
                "CL": {
                    "CL_mistral_prompt2": "1. **Understanding Word Senses:**\n\n    In the realm of Computational Linguistics, a 'word sense' is a unique interpretation of a word's meaning in a given context. For example, the word 'bank' can refer to a financial institution or the sloping land beside a body of water.\n\n    To help clarify the multiple meanings, dictionaries and thesauruses provide definitions, often called 'glosses', for each sense. For instance, 'bank' can be defined as:\n\n    a) Financial institution that accepts deposits and channels money into lending activities\n    b) Sloping land, especially beside a body of water\n\n    When two words have identical or very similar meanings, they are called synonyms, like couch and sofa, or filbert and hazelnut. On the other hand, antonyms are words with opposing meanings, such as long and short, or big and little.\n\n    **2. Synonyms and Antonyms:**\n\n    Synonyms share the same or almost identical meanings, while antonyms have opposite meanings. For example:\n\n    - Synonyms: couch, sofa\n    - Antonyms: long, short\n\n    **3. Hyponyms and Hypernyms:**\n\n    A hyponym is a more specific word denoting a subclass of a broader word, referred to as the hypernym. For example, car is a hyponym of vehicle, and dog is a hyponym of animal. Conversely, vehicle is a hypernym of car, and animal is a hypernym of dog.\n\n    **4. Meronymy:**\n\n    Meronymy represents the part-whole relationship. For example, the wheel is a part of the car, and the car is the whole that contains the wheel.\n\n    By understanding these concepts, you will have a better grasp of how words are classified and related to each other, which will aid you in natural language processing tasks and improve your ability to work with AI tools in the field of Computational Linguistics."
                }
            },
            "llama": {
                "CS": {
                    "CS_llama_prompt4": "In the realm of NLP, a sense can be thought of as a unique class or object in a programming context, encapsulating one specific aspect of a word's meaning. Consider the example of a mouse, which can refer to either a computer input device or a small rodent. This duality in meaning is akin to polymorphism in object-oriented programming, where a single entity can have multiple forms or behaviors. Similarly, a bank can be a financial institution or the side of a river, illustrating the concept of method overloading, where a single function or term can have different implementations or meanings based on context. The definitions provided by dictionaries and thesauruses, known as glosses, can be viewed as docstrings or comments in code, offering a human-readable description of each sense. Synonyms, such as couch and sofa, or vomit and throw up, can be seen as equivalent classes or interfaces, where two different words convey the same meaning. Antonyms, like long and short, or big and little, represent opposing values or states, similar to true and false in a boolean context. The relationship between hyponyms and hypernyms can be likened to inheritance in object-oriented programming, where a more specific class (hyponym) inherits properties from a more general class (hypernym). For instance, car is a subclass of vehicle, just as dog is a subclass of animal. Meronymy, the part-whole relation, can be thought of as a composition relationship in programming, where an object is composed of smaller parts, such as a car being composed of wheels, or a wheel being a part of a car. This analogy allows us to better understand the complex relationships between words and their meanings, much like how we design and understand software systems."
                },
                "CL": {
                    "CL_llama_prompt5": "To grasp the concept of a sense in AI and LLMs, consider a word as a node in a knowledge graph, where each node represents a specific concept or meaning. In NLP, a sense is a discrete representation of one aspect of the meaning of a word, similar to how a node in a graph can have multiple edges connecting it to other nodes, each representing a distinct relationship. For instance, the word \"mouse\" can have multiple senses, such as \"mouse1\" referring to a computer input device, and \"mouse2\" referring to the animal. This is analogous to how a node in a graph can have multiple edges, each representing a different relationship or connection. In lexical semantics, dictionaries and thesauruses use glosses to define each sense of a word"
                }
            },
            "gpt4o": {
                "CS": {
                    "CS_gpt4o_prompt1": "In computer science, understanding language nuances is important, especially when working with natural language processing (NLP). A \"sense\" or \"word sense\" refers to a specific meaning of a word, which can have multiple interpretations based on context. For example, consider \"mouse\": in one context, it refers to a computer peripheral (mouse1), while in another, it refers to a small animal (mouse2). Similarly, \"bank\" can mean a financial institution (bank1) or the side of a river (bank2).\n\nDictionaries and thesauruses define these word senses using glosses, which are brief textual definitions. For instance, a \"bank\" can be glossed as \"1. a financial institution that accepts deposits and channels the money into lending activities\" or \"2. sloping land beside a body of water.\"\n\nIn NLP, it's crucial to understand relationships between word senses:\n\n- **Synonyms**: Different words with similar meanings, like couch/sofa or car/automobile.\n- **Antonyms**: Words with opposite meanings, such as long/short or hot/cold.\n- **Hyponyms and Hypernyms**: These describe subclass relationships. A \"car\" is a hyponym of \"vehicle,\" meaning it’s a specific type of vehicle. Conversely, \"vehicle\" is a hypernym of \"car.\"\n- **Meronymy**: This is the part-whole relationship. For example, a \"wheel\" is a meronym of \"car,\" meaning a wheel is a part of a car, and \"car\" is a holonym of \"wheel.\"\n\nThese concepts help us build better algorithms for tasks like word sense disambiguation, which is crucial for developing systems that understand human language effectively."
                },
                "CL": {
                    "CL_gpt4o_prompt1": "In computational linguistics, understanding the meaning of words is crucial for natural language processing tasks. Words can have multiple meanings, known as \"senses.\" A sense is a distinct representation of one aspect of a word's meaning. For example, the word \"mouse\" can refer to a device used to control a computer (mouse1) or a small animal (mouse2). Similarly, \"bank\" can mean a financial institution (bank1) or the side of a river (bank2).\n\nTo grasp these different senses, dictionaries and thesauruses provide definitions called \"glosses.\" For instance, \"bank\" might be defined as: 1) a financial institution that handles money, and 2) sloping land next to water.\n\nIn computational linguistics, we often deal with relationships between words, such as synonyms, antonyms, hyponyms, and meronyms. Synonyms are words with identical or nearly identical meanings, like \"couch\" and \"sofa.\" Antonyms are words with opposite meanings, such as \"long\" and \"short.\"\n\nHyponyms and hypernyms describe hierarchical relationships. A \"car\" is a hyponym of \"vehicle,\" meaning it is a more specific type of vehicle. Conversely, \"vehicle\" is a hypernym of \"car,\" meaning it is a broader category that includes cars.\n\nMeronymy deals with part-whole relationships. For example, a \"wheel\" is a part of a \"car,\" making \"wheel\" a meronym of \"car,\" and \"car\" a holonym of \"wheel.\"\n\nBy understanding these concepts, computational linguists can better develop algorithms for tasks like word sense disambiguation, enabling computers to understand language more like humans do."
                }
            },
            "claude": {
                "CS": {
                    "CS_claude_prompt1": "Let me explain word meanings and relationships using concepts familiar to CS students:\n\nThink of a word as a class that can have multiple implementations (senses). Just like how a class Mouse could be implemented as either ComputerMouse or RodentMouse, words can have different concrete meanings based on context.\n\nConsider these meanings like different instances of a class:\n- Mouse.COMPUTER = \"device for controlling cursor\"\n- Mouse.ANIMAL = \"small rodent\"\n\nDictionary definitions are like documentation strings or specifications for each implementation. For example:\nBank.FINANCIAL = \"institution managing money transactions\"\nBank.GEOGRAPHIC = \"slope beside water body\"\n\nWord relationships can be modeled as different types of associations:\n\n1. Synonyms: Multiple classes implementing the same interface\n   Example: {Car, Automobile} both implement Vehicle with identical behavior\n\n2. Antonyms: Complementary or inverse implementations\n   Example: Hot/Cold are like Boolean opposites, similar to !isHot\n\n3. Hyponyms/Hypernyms: Inheritance relationships\n   - Car extends Vehicle (Car is hyponym)\n   - Vehicle is superclass of Car (Vehicle is hypernym)\n   Similar to how ArrayList is a hyponym of List in Java\n\n4. Meronymy: Composition relationships\n   Like class components:\n   class Car {\n       Wheel[] wheels;  // Wheel is meronym of Car\n   }\n   Car is holonym of Wheel (contains Wheel as component)\n\nThis object-oriented approach to word meanings helps organize and understand linguistic concepts using familiar programming patterns."
                },
                "CL": {
                    "CL_claude_prompt4": "Here's my adapted explanation for NLP/CL practitioners:\n\nWord sense disambiguation is conceptually similar to classification tasks in machine learning, where each word can belong to different semantic classes. Think of it like having multiple embeddings for the same word form, each representing a distinct meaning vector in semantic space. Just as we have different vectors in BERT layers capturing contextual meanings, natural language has developed multiple senses for words.\n\nThese senses are organized in semantic relationships that we can represent computationally:\n- Synonyms share nearly identical vectors in semantic space (like 'automobile'/'car')\n- Antonyms occupy opposite positions (like 'hot'/'cold')\n- Hyponym/hypernym pairs form hierarchical trees similar to object-oriented programming inheritance (vehicle → car → sedan)\n- Meronymy/holonymy relationships mirror part-whole compositions in data structures (car contains wheel)\n\nWe can identify these senses through context windows, just as transformers use attention mechanisms to disambiguate meaning. Dictionary glosses act like labeled training data, providing explicit semantic definitions that can be used to train word sense disambiguation models.\n\nFor example, the word \"bank\" has distinct embedding clusters: one near financial terms (deposit, loan) and another near geographical features (river, slope) - much like how contextual embeddings separate different usages in vector space."
                }
            },
            "deepseek": {
                "CS": {
                    "CS_deepseek_prompt4": "In computational semantics, a **word sense** is a distinct meaning variant of a term, analogous to how a single function name in code might have overloaded implementations (e.g., `mouse` as input device vs. animal). Key semantic relations include:  \n1. **Synonyms**: Different terms mapping to the same semantic value, like aliases in programming (e.g., `vomit`/`throw_up` ≈ `sort()`/`arrange()`).  \n2. **Antonyms**: Binary opposites, akin to Boolean complements (e.g., `long`/`short` ≈ `true`/`false` in logic gates).  \n3. **Hyponymy/Hypernymy**: Inheritance hierarchies (e.g., `Car` is a subclass of `Vehicle` in OOP; `dog` → `animal` mirrors `LinkedList` → `List`).  \n4. **Meronymy**: Compositional relationships, like components in a data structure (e.g., `wheel` is a meronym of `car` ≈ `Node` is part of `LinkedList`).  \n\nLexical resources define senses via structured entries (like API documentation), enabling disambiguation algorithms to treat word meaning as a multi-label classification problem."
                },
                "CL": {
                    "CL_deepseek_prompt2": "**Adapted Explanation:**  \nIn NLP, a *word sense* is a distinct meaning variant of a term, crucial for tasks like disambiguating polysemy (e.g., \"mouse\" as *rodent* vs. *computer device*). Dictionaries define senses via *glosses*—textual descriptions (e.g., \"bank\" as *financial institution* vs. *river edge*).  \n\nKey semantic relations align with CL workflows:  \n- **Synonyms**: Senses with near-identical meanings (e.g., \"car/automobile\" in corpus annotation or LLM synonym replacement tasks).  \n- **Antonyms**: Opposing meanings (e.g., \"hot/cold\" in sentiment analysis or contrastive training data).  \n- **Hyponymy/Hypernymy**: Hierarchical relations (e.g., \"car → vehicle\" for taxonomy-building in knowledge graphs).  \n- **Meronymy**: Part-whole structures (e.g., \"wheel → car\" for extracting entity relations in text).  \n\nThese principles underpin AI systems—like training LLMs to resolve ambiguity or structure ontologies—directly impacting tasks your models handle (e.g., entity linking, machine translation). Understanding them ensures precise feature engineering and model evaluation."
                }
            }
        }
    },
    "T5": {
        "instance_code": "T5",
        "original_category": "L",
        "original_text_title": "Phonetics",
        "original_text": "We'll represent the pronunciation of a word as a string of phones, which are speech sounds, each represented with symbols adapted from the Roman alphabet. The standard phonetic representation for transcribing the world's languages is the International Phonetic Alphabet (IPA), [...] the mapping between the letters of English orthography and phones is relatively opaque; a single letter can represent very different sounds in different contexts. [...] Many other languages, for example, Spanish, are much more transparent in their sound-orthography mapping than English. Articulatory phonetics is the study of how these phones are produced as the various articulatory phonetics organs in the mouth, throat, and nose modify the airflow from the lungs. [...] we can group [consonants] into classes by their point of maximum restriction, their place of articulation. [...] bilabial: [...] [p], [b], [m]. labiodental [v] and [f] [...]. dental: [...] [th] of thing and the [dh] of though. [...] alveolar: [s], [z], [t], [d]. [...] velar: [k], [g]. [...] Consonants are also distinguished by how the restriction in airflow is made [...]. This feature is called the manner of articulation [...]. voiced stops [b], [d], and [g],unvoiced stops [p], [t], and [k]. nasal sounds [n], [m], and [ng]. labiodental fricatives [f] and [v]. alveolar fricatives [s] and [z]. affricates [ch] and [jh]. approximants [y] and [w].",
        "selected_texts": {
            "mistral": {
                "CS": {
                    "CS_mistral_prompt2": "In the realm of speech, we'll represent a word's pronunciation as a sequence of phonetic symbols, derived from the Roman alphabet, representing distinct speech sounds (phones). The universal phonetic representation for transcribing languages globally is the International Phonetic Alphabet (IPA).\n\n    English orthography to phones mapping is often obscure; a single letter can correspond to various sounds in different contexts. In contrast, many languages, like Spanish, exhibit a more straightforward sound-orthography correspondence than English.\n\n    Articulatory phonetics delves into the production of these phones, as the various organs in the mouth, throat, and nose manipulate the lung's airflow.\n\n    Consonants are grouped based on their point of maximum restriction, known as their place of articulation. For example:\n    - Bilabial: [p], [b], [m]\n    - Labiodental: [v] and [f]\n    - Dental: [th] (in 'thing') and [dh] (in 'though')\n    - Alveolar: [s], [z], [t], [d]\n    - Velar: [k], [g]\n\n    Consonants are also categorized by the manner of articulation, a feature that describes the method of obstructing the airflow. Examples include:\n    - Voiced stops: [b], [d], [g]\n    - Unvoiced stops: [p], [t], [k]\n    - Nasal sounds: [n], [m], [ng]\n    - Labiodental fricatives: [f] and [v]\n    - Alveolar fricatives: [s] and [z]\n    - Affricates: [ch] and [jh]\n    - Approximants: [y] and [w]\n\n    This explanation should help CS students understand the basics of phonetics, providing a foundation to grasp AI-related speech recognition and synthesis technologies."
                },
                "CL": {
                    "CL_mistral_prompt4": "In the context of Computational Linguistics, we transcribe words using a sequence of phonetic symbols derived from the Roman alphabet, representing speech sounds called phones. The International Phonetic Alphabet (IPA) is the standard for transcribing languages worldwide. In English, the mapping between the spelling and phonetic symbols is not straightforward; a single letter can represent multiple sounds depending on the context. On the other hand, languages like Spanish have a clearer sound-spelling correspondence. Articulatory Phonetics focuses on the production of these phones by the various organs in the mouth, throat, and nose that modify the airflow from the lungs. We classify consonants based on their point of maximum restriction and place of articulation, such as bilabial [p,b,m], labiodental [v,f], dental [th,dh], alveolar [s,z,t,d], and velar [k,g]. Consonants can also be categorized by the manner of articulation, such as voiced stops [b,d,g], unvoiced stops [p,t,k], nasal sounds [n,m,ng], labiodental fricatives [f,v], alveolar fricatives [s,z], affricates [ch,jh], and approximants [y,w]."
                }
            },
            "llama": {
                "CS": {
                    "CS_llama_prompt5": "In the realm of speech, we can think of the pronunciation of a word as a series of digital signals, similar to how a computer processes binary data. Each phone, or speech sound, can be represented by a unique symbol, like a character in a programming language. The International Phonetic Alphabet (IPA) serves as a standardized API for transcribing languages, providing a common framework for representing the sounds of spoken words. Just as a compiler translates code into machine language, the IPA translates written words into their corresponding phonetic representations. In English, the mapping between letters and sounds is not always straightforward, much like how a complex algorithm can have multiple possible outputs. However, languages like Spanish have a more transparent mapping, similar to how a well-structured codebase has clear and consistent syntax. When we examine how these sounds are produced, we enter the field of articulatory phonetics, which is analogous to studying the hardware components of a computer, such as how the CPU, memory, and input/output devices interact to process information. Consonants can be grouped into classes based on their point of maximum restriction, similar to how data is categorized and processed in a database. For instance, bilabial consonants like [p], [b], and [m] can be thought of as being processed in a specific \"module\" or \"library,\" while labiodental consonants like [v] and [f] are processed in another. The manner of articulation, which distinguishes consonants by how the airflow is restricted, can be likened to the different programming paradigms, such as object-oriented or functional programming. Voiced stops like [b] and [d] can be seen as \"functions\" that produce a specific sound, while unvoiced stops like [p] and [t] are like \"procedures\" that modify the airflow in a different way. Similarly, nasal sounds like [n] and [m] can be thought of as \"data structures\" that store and manipulate sound waves, while fricatives like [s] and [z] are like \"algorithms\" that process and transform the sound signals. By understanding these concepts, we can better appreciate the complexities of human speech and the intricacies of language processing, much like how a programmer must understand the underlying mechanics of a computer system to write efficient and effective code."
                },
                "CL": {
                    "CL_llama_prompt3": "To better understand how words are pronounced, we'll break down their spoken sounds, or phones, into strings of symbols. Think of it like tokenizing text in NLP, where we divide words into individual units, or tokens. In phonetics, these symbols are adapted from the Roman alphabet and follow the International Phonetic Alphabet (IPA) standard. This is similar to how we use standardized annotation schemes in corpus linguistics to ensure consistency in our analyses. \n\n    Now, when we map English orthography to phones, the relationship isn't always straightforward. This is because a single letter can represent different sounds depending on the context, much like how a word's part-of-speech tag can change based on its syntactic context. For instance, the letter \"c\" can be pronounced as /k/ in \"cat\" but as /s/ in \"circle\". In contrast, languages like Spanish have a more transparent sound-orthography mapping, making it easier to predict pronunciation from spelling.\n\n    Articulatory phonetics, a subfield of phonetics, examines how phones are produced by modifying airflow from the lungs using various organs in the mouth, throat, and nose. This process can be likened to the way we use Hidden Markov Models (HMMs) to model the probability of a sequence of observations, where the observations are the sounds produced. \n\n    We can categorize consonants based on their point of maximum restriction, or place of articulation. For example, bilabial consonants like /p/, /b/, and /m/ are produced by obstructing airflow with both lips. Similarly, labiodental consonants like /v/ and /f/ involve the lower lip and upper teeth. Dental consonants, such as the /th/ in \"thing\" and the /dh/ in \"though\", are produced by placing the tongue between the teeth. Alveolar consonants, like /s/, /z/, /t/, and /d/, involve the tongue and the alveolar ridge. Velar consonants, including /k/ and /g/, are produced by the back of the tongue and the soft palate.\n\n    Consonants can also be distinguished by the manner of articulation, which refers to how the airflow is restricted. This is analogous to the way we classify words based on their syntactic properties, such as part-of-speech tags. For instance, voiced stops like /b/, /d/, and /g/ involve the vibration of the vocal cords, whereas unvoiced stops like /p/, /t/, and /k/ do not. Nasal sounds like /n/, /m/, and /ng/ are produced by allowing airflow through the nose. Labiodental fricatives like /f/ and /v/ involve the lower lip and upper teeth, while alveolar fricatives like /s/ and /z/ are produced by the tongue and the alveolar ridge. Affricates like /ch/ and /jh/ combine a stop and a fricative, and approximants like /y/ and /w/ involve the tongue approaching the roof of the mouth without blocking the airflow. \n\n    By understanding how phones are produced and categorized, we can better analyze and model language, much like how we use large language models (LLMs) to generate and understand human-like language. This knowledge can also inform the development of speech recognition systems and other NLP applications, ultimately enhancing our ability to interact with and understand spoken language."
                }
            },
            "gpt4o": {
                "CS": {
                    "CS_gpt4o_prompt5": "In linguistics, we represent how a word sounds using a sequence of phones, which are basic sound units similar to how code can be broken down into fundamental commands. These sounds are recorded using symbols from the International Phonetic Alphabet (IPA), a global standard akin to a universal programming language for sounds. Unlike English, where a single letter can produce multiple sounds depending on its context, languages like Spanish have a more direct correlation between letters and sounds, similar to how a programming language with strict syntax operates.\n\nArticulatory phonetics, much like debugging code to understand functionality, involves studying how sounds are created by manipulating airflow with different parts of the vocal apparatus (mouth, throat, nose). We can categorize consonants based on where the airflow is most restricted, known as the place of articulation, similar to how data structures are categorized by their organization method.\n\n- **Bilabial sounds** (like [p], [b], [m]) are made using both lips, akin to a binary operation involving two operands.\n- **Labiodental sounds** (like [f] and [v]) use the lip and teeth, comparable to a function that combines two different data types.\n- **Dental sounds** (like [th] in \"thing\" and [dh] in \"though\") involve the tongue and teeth, reminiscent of a pointer accessing memory locations.\n- **Alveolar sounds** (like [s], [z], [t], [d]) occur with the tongue against the ridge behind the teeth, similar to iterating over a list with a defined endpoint.\n- **Velar sounds** (like [k], [g]) are produced with the back of the tongue against the soft palate, much like recursive functions that call on deeper layers.\n\nConsonants also differ by how airflow is restricted, called the manner of articulation, similar to how algorithms can be classified by their time complexity:\n\n- **Voiced stops** (like [b], [d], [g]) and **unvoiced stops** (like [p], [t], [k]) are akin to synchronous and asynchronous operations.\n- **Nasal sounds** (like [n], [m], [ng]) allow air through the nose, like parallel processing paths.\n- **Fricatives** (like [f], [v], [s], [z]) create friction, similar to iterative loops refining output.\n- **Affricates** (like [ch], [jh]) start as stops and then release as fricatives, analogous to a hybrid algorithm combining two techniques.\n- **Approximants** (like [y], [w]) are less constricted, similar to flexible programming paradigms that allow for varied implementations."
                },
                "CL": {
                    "CL_gpt4o_prompt5": "### Computational Linguistics Audience\n\nIn computational linguistics, we often represent the pronunciation of words using a sequence of phones, which are distinct speech sounds. These phones are denoted by symbols similar to the Roman alphabet. To standardize this across different languages, we use the International Phonetic Alphabet (IPA), which provides a consistent way to transcribe sounds globally. Unlike English, where a single letter can represent different sounds depending on the context, languages like Spanish exhibit a more straightforward relationship between written letters and spoken sounds.\n\nArticulatory phonetics, a subfield often intersecting with NLP tasks like speech recognition, examines how speech sounds are produced by manipulating airflow through various organs in the mouth, throat, and nose. For instance, consonants can be categorized based on where the airflow is most restricted, known as the place of articulation. Examples include bilabial sounds like [p], [b], and [m], which involve both lips, and alveolar sounds like [s], [z], [t], and [d], where the tongue touches the alveolar ridge.\n\nFurthermore, consonants are characterized by the manner of articulation, or how airflow is restricted. This includes voiced stops like [b], [d], and [g], where the vocal cords vibrate, and unvoiced stops like [p], [t], and [k], where they do not. Understanding these distinctions is crucial for developing accurate speech synthesis and recognition systems, which rely on detailed phonetic transcription to convert text to speech and vice versa."
                }
            },
            "claude": {
                "CS": {
                    "CS_claude_prompt1": "Let me explain phonetic representation using concepts familiar to CS students:\n\nThink of phones (speech sounds) as the atomic units of spoken language, similar to how bits are the fundamental units in computing. We encode these sounds using a standardized character set called the International Phonetic Alphabet (IPA), which works like ASCII or Unicode for speech sounds.\n\nThe relationship between English spelling and pronunciation is like a complex, inconsistent hash function - the same input (letter) can produce different outputs (sounds) depending on context. For comparison, languages like Spanish have a more deterministic mapping, similar to a one-to-one function.\n\nWe can organize consonant sounds into a hierarchical structure based on two main attributes:\n\n1. Place of articulation (location of sound production):\n   - Think of it as the physical address where the sound is generated\n   - Examples:\n     * bilabial ([p], [b], [m]): uses both lips, like in \"pop\"\n     * labiodental ([v], [f]): lip and teeth, like in \"five\"\n     * dental ([th], [dh]): tongue and teeth, like in \"thing\"\n     * alveolar ([s], [z], [t], [d]): tongue and ridge, like in \"sit\"\n     * velar ([k], [g]): back of mouth, like in \"king\"\n\n2. Manner of articulation (how the sound is produced):\n   - Similar to different operation types in programming\n   - Categories include:\n     * stops: complete blockage of air (like a binary 0/1)\n     * fricatives: continuous airflow with friction (like analog signals)\n     * nasals: air through nose (alternative output channel)\n     * affricates: combination of stop + fricative (compound operation)\n     * approximants: minimal obstruction (like a float between values)\n\nThis classification system creates a matrix-like structure where each sound can be uniquely identified by its coordinates in this two-dimensional feature space."
                },
                "CL": {
                    "CL_claude_prompt3": "In computational linguistics, we handle word pronunciation by converting sounds into machine-readable symbols, similar to how we tokenize text. Think of it as creating a phonetic encoding system, where each sound unit (phone) is represented by a standardized symbol from the International Phonetic Alphabet (IPA).\n\nJust as programming languages need consistent syntax, languages vary in how consistently their written form maps to pronunciation. English is like a complex regex pattern with many exceptions - the same letter can produce different sounds in different contexts. Spanish, conversely, is more like a straightforward one-to-one mapping function.\n\nThe physical production of these sounds follows a systematic classification, analogous to how we categorize tokens in NLP. Instead of part-of-speech tags, we categorize sounds based on two main features:\n\n1. Place of articulation (location where airflow is modified):\n   - Bilabial (using both lips): [p], [b], [m]\n   - Labiodental (lip-to-teeth): [v], [f]\n   - Dental (tongue-to-teeth): [th] as in \"thing\", [dh] as in \"though\"\n   - Alveolar (tongue-to-ridge): [s], [z], [t], [d]\n   - Velar (back of mouth): [k], [g]\n\n2. Manner of articulation (how airflow is modified):\n   - Stops (complete blockage): voiced [b,d,g], unvoiced [p,t,k]\n   - Nasals (through nose): [n,m,ng]\n   - Fricatives (continuous friction): labiodental [f,v], alveolar [s,z]\n   - Affricates (stop+friction): [ch,jh]\n   - Approximants (minimal obstruction): [y,w]\n\nThis structured classification system is comparable to how we organize linguistic features in computational models."
                }
            },
            "deepseek": {
                "CS": {
                    "CS_deepseek_prompt5": "Phonetic transcription encodes word pronunciation as strings of discrete units (phones), akin to tokenization in NLP. The International Phonetic Alphabet (IPA) acts as a standardized encoding schema – like Unicode for speech sounds – where each symbol maps to a specific articulatory configuration.  \n\nEnglish spelling-to-sound mapping is highly irregular (like an inconsistent API), while Spanish follows predictable rules (similar to regex patterns). Articulatory phonetics analyzes how airflow is algorithmically transformed by biological \"hardware\" – lips, tongue, and vocal cords acting as physical operators.  \n\nConsonants are classified by:  \n1. **Place of Articulation** (data source):  \n   - Bilabial [p]: Keyboard input (both lips as dual keypress)  \n   - Velar [k]: Touchscreen swipe (tongue contact with soft palate)  \n\n2. **Manner of Articulation** (processing method):  \n   - Stops [t]/[d]: Binary I/O (complete airflow blockage then release)  \n   - Fricatives [s]: Stream processing (continuous turbulent airflow)  \n   - Nasals [m]: Parallel processing (airflow diverted to nasal channel)  \n\nVoicing adds a binary feature flag – [b] (vocal cord vibration = 1) vs [p] (0). Affricates like [ch] combine stop and fricative operations in a single function call. This feature engineering enables precise speech sound classification, similar to clustering algorithms in ML."
                },
                "CL": {
                    "CL_deepseek_prompt4": "In computational terms, phones function as atomic phonetic tokens mapped via IPA—a Unicode-like standardization system for cross-lingual speech sound representation. Orthography-to-phone alignment varies in predictability: English resembles irregular grapheme-to-phoneme conversion (e.g., \"ough\" in *though* vs. *through*), while Spanish aligns with rule-based tokenization. Articulatory phonetics analyzes speech sound generation as biomechanical feature engineering: **place** (articulator positions: bilabial [p], alveolar [t]) and **manner** (airflow modulation: fricatives [s], stops [k]) act as phonetic feature sets. These classes parallel acoustic model training data, where voicing (vibrational [z] vs. non-vibrational [s]) or nasality ([m]) serve as discriminative features for speech recognition systems. For NLP, this relates to TTS pipelines converting spelling to pronunciation using context-sensitive rules—akin to disambiguating homographs via syntactic context."
                }
            }
        }
    },
    "T6": {
        "instance_code": "T6",
        "original_category": "CS",
        "original_text_title": "Feed-Forward Neural Networks (FFNNs)",
        "original_text": "A feedforward network is a multilayer network in which the units are connected with no cycles; the outputs from units in each layer are passed to units in the next higher layer, and no outputs are passed back to lower layers. [...] Simple feedforward networks have three kinds of nodes: input units, hidden units, and output units. [...] The input layer x is a vector of simple scalar values. [...] The core of the neural network is the hidden layer h formed of hidden units hi, each of which is a neural unit [...]. In the standard architecture, each layer is fully-connected, meaning that each unit in each layer takes as input the outputs from all the units in the previous layer, and there is a link between every pair of units from two adjacent layers. [...] a single hidden unit has as parameters a weight vector and a bias. We represent the parameters for the entire hidden layer by combining the weight vector and bias for each unit i into a single weight matrix W and a single bias vector b for the whole layer. [...] the hidden layer computation for a feedforward network can be done very efficiently with simple matrix operations. In fact, the computation only has three steps: multiplying the weight matrix by the input vector x, adding the bias vector b, and applying the activation function g (such as the sigmoid, tanh, or ReLU activation […]). The output of the hidden layer, the vector h, is thus the following (for this example we'll use the sigmoid function σ as our activation function): h = σ(Wx+b). [...] The role of the output layer is to take this new representation h and compute a final output. This output could be a real valued number, but in many cases the goal of the network is to make some sort of classification decision, [...].",
        "selected_texts": {
            "mistral": {
                "L": {
                    "L_mistral_prompt3": "1. In a feedforward network, the information flows in one direction, from input to output, without loops.\n    2. It consists of three primary components: input nodes, hidden nodes, and output nodes.\n    3. The input layer (x) is a collection of simple, scalar values.\n    4. The heart of the neural network is the hidden layer, composed of multiple hidden nodes. Each node is a neural unit.\n    5. In most cases, each layer is fully connected, meaning each node in a layer receives input from all nodes in the preceding layer.\n    6. Each hidden node has a set of parameters, including a weight vector and a bias. We group these for the entire hidden layer into a single weight matrix (W) and a single bias vector (b).\n    7. The hidden layer computation can be efficiently performed using basic matrix operations. Specifically, it involves multiplying the weight matrix by the input vector, adding the bias vector, and applying an activation function like the sigmoid function (σ).\n    8. The hidden layer's output, the vector h, can be represented as h = σ(Wx+b).\n    9. The output layer's responsibility is to process this new representation (h) and produce a final output. This output may be a real number, but often, the network's goal is to make a classification decision."
                },
                "CL": {
                    "CL_mistral_prompt5": "1. **Category:** Non-specialists in Computer Science who are interested in understanding the basics of Neural Networks\n\n    Neural networks are a type of computational model inspired by the structure and function of the human brain. In this model, we have three main components: input nodes, hidden nodes, and output nodes. The input nodes serve as the entry points for the data, taking in simple values. Hidden nodes are the processing units that perform computations on the input data and pass it on to the output nodes. The output nodes provide the final result of the neural network's computation.\n\n    In this specific model, we call it a feedforward network, which means that information flows only in one direction, from input to output, without any loops. Each hidden node is connected to every input node, and each hidden node is connected to every output node. These connections have weights and biases, which help adjust the importance of the data as it passes through the network.\n\n    To calculate the output of the hidden layer, we multiply the weighted input values by each hidden node's weight, add the biases, and then apply a mathematical function called the activation function. For instance, we might use the sigmoid function, which transforms the output into a value between 0 and 1. The final output of the network is calculated using similar steps from the output layer, but instead of a sigmoid function, we might use a different function depending on the task the network is designed to accomplish.\n\n    2. **Category:** Linguists who want to understand how Neural Networks process language data\n\n    In the field of computational linguistics, we use neural networks to analyze and process language data. These networks consist of three main parts: input units, hidden units, and output units. The input units represent the raw language data, such as words, sentences, or paragraphs.\n\n    The hidden units perform computations on the input data and pass it on to the output units. In the feedforward network, all hidden units are connected to all input units, and all hidden units are connected to all output units. These connections have weights and biases, which help adjust the importance of the features in the language data as it passes through the network.\n\n    To calculate the output of the hidden layer, we multiply the weighted input values by each hidden unit's weight, add the biases, and then apply an activation function, such as the sigmoid, tanh, or ReLU function. The final output of the network is calculated using similar steps from the output layer and a different activation function, depending on the task the network is designed to accomplish, such as classifying language patterns or generating new language data.\n\n    Neural networks have been widely used in Natural Language Processing (NLP) to address various tasks, ranging from text classification, translation, and question answering, to generating realistic-sounding text. In essence, they mimic the structure and function of the human brain to learn patterns and make decisions based on the input data, making them powerful tools for processing and understanding language data."
                }
            },
            "llama": {
                "L": {
                    "L_llama_prompt1": "In the field of Linguistics, you're likely familiar with the concept of hierarchical structures, where elements are organized in a specific order to convey meaning. A feedforward network can be thought of as a similar hierarchical structure, where units are connected in a specific way to process and transform information. \n\nImagine a simple sentence with a subject, verb, and object. In a feedforward network, the input layer would be like the subject, providing the initial information. The hidden layer would be like the verb, taking the input and transforming it into a new representation, much like how a verb changes the subject in a sentence. The output layer would be like the object, receiving the transformed information and producing a final result.\n\nIn a feedforward network, the units are connected in a specific way, with no cycles or feedback loops. The output from each unit in one layer is passed to the units in the next layer, but not back to the previous layer. This is similar to how words in a sentence are arranged in a specific order to convey meaning, with"
                },
                "CL": {
                    "CL_llama_prompt5": "A feedforward network is similar to a pipeline in NLP where data flows from one stage to the next without cycling back. In this multilayer network, each layer's output is passed to the next, with no feedback to previous layers. Think of it like a text classification model where the input text is passed through several layers of processing, with each layer refining the representation of the text. The input layer is like the bag-of-words representation, where each word is a simple scalar value. The hidden layer is where the magic happens, similar to how a topic model like Latent Dirichlet Allocation (LDA) extracts topics from a corpus. Each hidden unit is like a topic, and the weights and biases are like the parameters that define the topic. The computation in the hidden layer can be thought of as a matrix operation, similar to how we compute the similarity between two documents using TF-IDF. The output of the hidden layer is then passed to the output layer, which makes a classification decision, like predicting the sentiment of a text. The role of the output layer is similar to a classifier in NLP, taking the refined representation of the text and making a prediction. For example, in a sentiment analysis task, the output layer would take the output of the hidden layer and predict a sentiment label, such as positive or negative."
                }
            },
            "gpt4o": {
                "L": {
                    "L_gpt4o_prompt3": "A feedforward network is a type of neural network that processes information in one direction: from input to output, without any loops. Think of it as a layered structure where each layer sends information to the next, like a series of linguistic transformations where each step builds on the previous one.\n\nThis network has three kinds of layers: input, hidden, and output. The input layer consists of simple numerical values, much like how phonetic features are raw data for linguistic analysis. The hidden layer, which is the core of the network, is made up of hidden units. These units are like the underlying rules in syntax—essential but not directly observable. Each hidden unit has parameters, specifically a weight and a bias, much like how linguistic rules have specific conditions and constraints.\n\nIn this network, each layer is fully connected to the next. This means every unit in one layer is linked to every unit in the following layer, similar to how every phoneme can potentially affect the meaning of a word in different contexts.\n\nThe hidden layer computes its output using simple matrix operations, akin to applying transformational rules in syntax. First, it multiplies the input vector by a weight matrix, adds a bias vector, and then applies an activation function (like sigmoid, tanh, or ReLU). For example, using the sigmoid function, the output of the hidden layer can be represented as h = σ(Wx + b). This process transforms the input into a new representation, much like how semantic interpretation derives meaning from structure.\n\nFinally, the output layer takes the transformed data from the hidden layer to produce a result. This result could be a numerical value or a category, similar to how we might classify a sentence as a question or a statement based on its structure."
                },
                "CL": {
                    "CL_gpt4o_prompt1": "A feedforward network is a type of neural network structure that consists of multiple layers, but with a clear forward direction from one layer to the next without any backward connections. This means information flows in one direction—from input to output—without any cycles or loops.\n\nIn a typical feedforward network, there are three types of nodes or units: input units, hidden units, and output units. The input layer, often represented by a vector \\( x \\), consists of simple scalar values that the network processes. This is similar to how you might think of features in a dataset when working with machine learning.\n\nThe hidden layer is where the core processing happens. It is made up of hidden units \\( h_i \\), each acting like a mini brain cell processing information. In a fully-connected architecture, every unit in a layer connects to every unit in the next layer. This means each unit in the hidden layer receives input from all units in the previous layer.\n\nEach hidden unit has parameters called a weight vector and a bias. These parameters are crucial because they determine how each unit responds to the input it receives. By organizing the weight vectors and biases of all hidden units, we form a weight matrix \\( W \\) and a bias vector \\( b \\). These allow us to compute the hidden layer's output efficiently using simple matrix operations.\n\nThe computation in the hidden layer involves three straightforward steps: \n1. Multiply the weight matrix \\( W \\) by the input vector \\( x \\).\n2. Add the bias vector \\( b \\).\n3. Apply an activation function \\( g \\) to introduce non-linearity. Common activation functions include the sigmoid function \\( \\sigma \\), hyperbolic tangent (tanh), or ReLU (Rectified Linear Unit).\n\nFor example, if we use the sigmoid function, the output of the hidden layer, represented as vector \\( h \\), is computed as: \\( h = \\sigma(Wx + b) \\).\n\nFinally, the output layer takes this transformed representation \\( h \\) and produces the network's final output. This output can be a real number or, often, a classification decision. For instance, in natural language processing tasks, this might mean determining the sentiment of a text or identifying a part of speech.\n\nUnderstanding feedforward networks is crucial as they are foundational in many applications of computational linguistics, from natural language processing to building language models."
                }
            },
            "claude": {
                "L": {
                    "L_claude_prompt3": "A feedforward network can be understood as a language processing system with distinct layers, similar to how we analyze language in hierarchical levels. Think of it as a one-way street where information flows forward only, like how we process language from sounds to meaning without going backwards.\n\nThe network has three main components, comparable to linguistic levels of analysis: input units (like phonemes), hidden units (like morphemes and syntax), and output units (like semantic meaning). The input layer is simply a sequence of values, similar to how we represent distinctive features in phonology.\n\nThe hidden layer, which is the heart of the network, functions like our mental grammar. Just as each grammatical rule connects different linguistic elements, each hidden unit connects to all units in the layers before and after it - we call this \"fully-connected.\" These connections are weighted (like how certain linguistic features carry more importance than others) and have a bias (similar to language-specific constraints).\n\nTo process information, the network performs three steps, comparable to language processing: it weighs the importance of each input (like assigning stress in phonology), adds bias (like applying language-specific rules), and activates certain patterns (like phonological or syntactic transformations). We can express this mathematically as h = σ(Wx+b), where the activation function σ works like a filter that determines which patterns become prominent.\n\nFinally, the output layer interprets this processed information to make decisions, similar to how we derive meaning after processing all linguistic levels. This could produce various outcomes, but often it's used for classification, like determining whether a sentence is grammatical or ungrammatical."
                },
                "CL": {
                    "CL_claude_prompt4": "Here's my paraphrasis for NLP/CL practitioners:\n\nA feedforward network is like a one-way pipeline for processing linguistic data, similar to how we handle sequential text processing but without any feedback loops. Think of it as a three-stage language processing system: input (like word embeddings or feature vectors), hidden processing, and output (like classification decisions).\n\nThe input layer is comparable to your feature vector in a text classifier. The hidden layer, which does the heavy lifting, is essentially a sophisticated feature transformation mechanism, similar to how we convert surface-level linguistic features into more abstract representations. This transformation happens through a weight matrix W (like the transformation matrices in word embeddings) and a bias vector b, combined with an activation function (sigmoid, tanh, or ReLU).\n\nThe mathematical operation h = σ(Wx+b) is analogous to how word2vec transforms words into dense vectors, but here we're transforming input features into a new representation space. The output layer then uses this transformed representation to make decisions, like determining text categories or predicting next words, much like how a language model makes predictions based on processed context."
                }
            },
            "deepseek": {
                "L": {
                    "L_deepseek_prompt2": "**Adapted Explanation for Linguistics Students:**  \n\nThink of a feedforward network as a *syntactic tree* for processing information: it flows one-way (like parsing a sentence left-to-right), with no feedback loops. It has three *linguistic-like layers*:  \n\n1. **Input Layer:** Raw data (e.g., phonetic features or word embeddings) enters here, like phonemes entering a phonological parser.  \n2. **Hidden Layer:** This acts like a *hierarchical syntax layer*, combining inputs into abstract patterns. Each \"node\" here is akin to a rule (e.g., merging phrases), weighted by how critical it is (like prosodic stress in phonology). A **bias** adjusts its baseline activity, similar to a phonotactic constraint (e.g., avoiding /ŋ/ in syllable onsets).  \n3. **Output Layer:** Produces a final \"meaning\" (e.g., classifying a word’s semantic role).  \n\nThe hidden layer works like a *morphological paradigm*:  \n- **Weight Matrix (W):** Determines how strongly each input influences a hidden rule (like how syntactic heads govern dependents).  \n- **Bias Vector (b):** Adjusts each rule’s sensitivity (similar to allophonic variation thresholds).  \n- **Activation Function (σ):** A decision boundary, like turning continuous acoustic signals into discrete phonemes (e.g., sigmoid squishes values into 0–1, like voicing detection).  \n\n**Example:** To classify a verb’s tense, the network might:  \n1. Multiply input (word form) by weights (prioritizing affixal cues).  \n2. Add bias (accounting for irregular verbs).  \n3. Apply activation (deciding \"past\" vs. \"present\" via a probability threshold).  \n\nThis mimics how linguistic competence combines features hierarchically—but here, weights/biases are learned from data, not innate."
                },
                "CL": {
                    "CL_deepseek_prompt5": "A feedforward neural network is a layered architecture without feedback loops, where information flows unidirectionally from input nodes through computational layers to produce outputs—similar to how NLP pipelines process text through discrete stages (tokenization → feature extraction → classification). Inputs are encoded as vectors (like word embeddings), which are transformed via matrix operations in hidden layers. Each hidden neuron applies a weighted sum of inputs (matrix multiplication with a weight matrix **W** and bias vector **b**), followed by a nonlinear activation (e.g., sigmoid, ReLU)—analogous to feature engineering in corpus linguistics, but automated through learned parameters. These transformations create hierarchical representations, enabling tasks like sentiment classification. The output layer then maps this representation to a decision (e.g., class probabilities via softmax). Crucially, the fully connected design (all neurons linked across adjacent layers) allows efficient batch computation via tensor operations, mirroring how transformers process sequences in parallel. This architecture underpins many NLP components, though modern LLMs extend it with mechanisms like attention."
                }
            }
        }
    },
    "T7": {
        "instance_code": "T7",
        "original_category": "CS",
        "original_text_title": " Training FFNNs (Loss Function and Gradient)",
        "original_text": "A feedforward neural net is an instance of supervised machine learning in which we know the correct output y for each observation x. What the system produces is ˆy, the system's estimate of the true y. The goal of the training procedure is to learn parameters W[i] and b[i] for each layer i that make ˆy for each training observation as close as possible to the true y. [...] First, we'll need a loss function that models the distance between the system output and the gold output, and it's common to use the loss function used for logistic regression, the cross-entropy loss. Second, to find the parameters that minimize this loss function, we'll use the gradient descent optimization algorithm [...]. Third, gradient descent requires knowing the gradient of the loss function, the vector that contains the partial derivative of the loss function with respect to each of the parameters. [...] for neural networks, with millions of parameters in many layers, it's much harder to see how to compute the partial derivative of some weight in layer 1 when the loss is attached to some much later layer. How do we partial out the loss over all those intermediate layers? The answer is the algorithm called error backpropagation or backward differentiation. [...] Computing the gradient requires the partial derivative of the loss function with respect to each parameter. [...] The solution to computing this gradient is an algorithm called error backpropagation.",
        "selected_texts": {
            "mistral": {
                "L": {
                    "L_mistral_prompt1": "Neural networks are a type of machine learning where we train the system to predict the correct outcome (y) for given input (x). To do this, we adjust weights (W[i]) and biases (b[i]) in each layer (i) to make our prediction (ˆy) as close as possible to the actual outcome (y).\n\n     To measure how close we are, we use a loss function, similar to the one used in logistic regression, called cross-entropy loss. This function tells us how far our prediction is from the correct answer.\n\n     To find the best weights and biases, we use a method called gradient descent. However, this method requires knowing the gradient, or the rate of change, of the loss function for each parameter.\n\n     In a neural network with many layers and millions of parameters, it can be challenging to compute the gradient for a single weight in the first layer when the loss is attached to a much later layer. This is where backpropagation, or error backpropagation, comes in.\n\n     Backpropagation helps us calculate the gradient by partially deriving the loss function over all intermediate layers. In other words, it helps us trace the error back from the last layer to the very first one.\n\n     To compute the gradient using backpropagation, we need to calculate the partial derivative of the loss function with respect to each parameter. This is essential for the gradient descent algorithm to find the best weights and biases that minimize the loss function."
                },
                "CL": {
                    "CL_mistral_prompt4": "For computational linguistics practitioners, a feedforward neural network is a type of supervised machine learning method where the aim is to predict the correct output y for each input x, denoted as ˆy. The goal during the training process is to adjust the weights W[i] and biases b[i] for each layer i, in order to minimize the difference between the system's prediction ˆy and the actual y for each training observation.\n\n     To measure the difference, a loss function is employed, often the cross-entropy loss function, similar to logistic regression. Minimizing this loss function is achieved through the use of the gradient descent optimization algorithm.\n\n     However, computing the gradient of the loss function with respect to each parameter can be challenging, especially in neural networks with numerous layers and parameters. This is where the error backpropagation algorithm, also known as backward differentiation, comes into play. It allows for the computation of the partial derivative of the loss function with respect to each parameter, even when the loss is connected to a later layer.\n\n     In essence, error backpropagation is the solution to computing the gradient required for gradient descent in neural networks with numerous layers and parameters."
                }
            },
            "llama": {
                "L": {
                    "L_llama_prompt1": "To understand the concept of a feedforward neural net, consider a linguistic analogy where a speaker produces an utterance, and the listener tries to understand its meaning. The speaker's utterance can be seen as the input x, while the listener's interpretation is the output y. However, the listener's interpretation might not perfectly match the speaker's intended meaning, which we can denote as ˆy. The goal is to make the listener's interpretation ˆy as close as possible to the speaker's intended meaning y.\n\n    In linguistics, we analyze the structure and meaning of language using various tools and methods. Similarly, in machine learning, we use a loss function to measure the distance between the system's output ˆy and the true output y. The loss function is like a metric that evaluates how well the system is performing. A common choice for this loss function is the cross-entropy loss, which is also used in logistic regression.\n\n    To optimize the system's performance, we use an algorithm called gradient descent. This algorithm is like a grammar correction tool that helps the system adjust its parameters to minimize the loss function. However, computing the gradient of the loss function with respect to each parameter can be challenging, especially when dealing with complex neural networks. This is where the algorithm called error backpropagation comes in. It's like a parsing algorithm that helps us break down the loss function and compute the partial derivative of the loss function with respect to each parameter, even in the presence of multiple intermediate layers.\n\n    Think of error backpropagation like a process of tracing back the dependencies between words in a sentence to understand the grammatical structure. In a similar way, error backpropagation helps us trace back the dependencies between the parameters in the neural network to compute the gradient of the loss function. By using this algorithm, we can optimize the system's performance and make the output ˆy as close as possible to the true output y. \n\n    To illustrate this concept further, consider a simple example from phonetics. Suppose we have a speech recognition system that tries to transcribe spoken words into written text. The input x is the audio signal, and the output y is the transcribed text. The system's estimate ˆy might not perfectly match the true transcription y. By using a loss function and the gradient descent algorithm, we can optimize the system's performance and improve the accuracy of the transcription. The error backpropagation algorithm helps us compute the gradient of the loss function and adjust the system's parameters to minimize the loss, resulting in a more accurate transcription. \n\n    In summary, the concept of a feedforward neural net can be understood through linguistic analogies, such as the speaker-listener model and the process of parsing. The loss function and gradient descent algorithm are used to optimize the system's performance, while error backpropagation helps us compute the gradient of the loss function and adjust the system's parameters to achieve the desired output."
                },
                "CL": {
                    "CL_llama_prompt5": "Consider a machine learning model like a complex recipe to classify text. In NLP, we use supervised learning to train models on labeled datasets, where each example has an input, x, and a corresponding correct output, y. The model learns to predict the output, ˆy, based on the input. The goal is to minimize the difference between the predicted output, ˆy, and the true output, y. To achieve this, we use a loss function, similar to the cross-entropy loss in logistic regression, which measures the distance between the model's output and the true output. We then use an optimization algorithm, such as gradient descent, to adjust the model's parameters to minimize the loss. However, computing the gradient of the loss function with respect to each parameter can be challenging, especially in deep neural networks with multiple layers. This is where error backpropagation comes in – an algorithm that efficiently computes the gradient by propagating the error backwards through the layers, allowing us to update the parameters and improve the model's performance. Think of it like adjusting the ingredients in a recipe: you need to know how each ingredient affects the final dish, and error backpropagation helps us do that for each parameter in the model."
                }
            },
            "gpt4o": {
                "L": {
                    "L_gpt4o_prompt2": "In linguistics, you might think of a feedforward neural network as a complex system designed to \"understand\" and produce human language, although it doesn't quite grasp it like we do. Imagine we have sentences (or observations) with known meanings (or correct outputs). The network's job is to predict these meanings as accurately as possible. \n\nThink of the network as a student learning a new language: it tries to guess the meaning of sentences it hasn't seen before, based on what it has learned from examples. The \"correct output\" is like a translation dictionary, where the network checks if its guess is right.\n\nTo train this network, we need a \"loss function,\" akin to a linguistic test that measures how well the network's guesses (ˆy) match the true meanings (y). One common \"test\" used is similar to cross-entropy loss, which you can think of like a measure of error in translation.\n\nThe training process also involves \"gradient descent,\" which is like adjusting the student's study habits to minimize errors in translation. It requires understanding how each part of the network contributes to the error, much like figuring out which linguistic rules a student is misunderstanding.\n\nHowever, neural networks are like vast linguistic systems with many layers of rules. To figure out how mistakes in translation flow through this system, we use an algorithm called \"error backpropagation.\" Imagine tracing back through a sentence to see where a misunderstanding began, and then correcting it. This is crucial for the network to learn effectively.\n\nIn essence, a feedforward neural network uses structured learning and error correction, similar to how one might teach a computer to understand and generate language using linguistic principles."
                },
                "CL": {
                    "CL_gpt4o_prompt2": "A feedforward neural network is a type of supervised machine learning model. In this setup, we have a known correct output, labeled as y, for every input observation, x. The neural network's task is to produce ˆy, which is its best guess of the true y. During training, our goal is to adjust the network's parameters—denoted as W[i] for weights and b[i] for biases at each layer i—so that ˆy closely matches y.\n\nTo achieve this, we use a loss function, which measures how far the network's output is from the actual output. For feedforward networks, a common choice is the cross-entropy loss, which is also used in logistic regression. This loss function helps quantify the error we want to minimize. \n\nTo minimize this loss, we employ an optimization process known as gradient descent. This method works by computing the gradient of the loss function, which involves calculating how changes in each parameter affect the loss. However, in neural networks with numerous layers and parameters, computing these gradients directly is complex.\n\nHere's where an algorithm called error backpropagation—or backward differentiation—comes in. It efficiently calculates these gradients by tracing the error from the output layer back through the network's layers. This allows us to update the parameters in a way that reduces the overall loss. By iteratively applying these updates, the network learns to make better predictions.\n\nIn essence, think of a feedforward neural network as a complex system where we continuously adjust the dials (parameters) to tune the system's output (ˆy) to match the true signal (y) as closely as possible. Through this process, we enable the network to understand and predict patterns in language data, much like how computational linguists analyze linguistic patterns using algorithms and corpora."
                }
            },
            "claude": {
                "L": {
                    "L_claude_prompt1": "In linguistics, we often study how humans learn and process language. Neural networks are like a simplified model of how our brains learn patterns, but in a more structured way. Let's understand this through language acquisition.\n\nImagine teaching a child language (this is our \"supervised learning\"). You know the correct form (y) - like saying \"ran\" as the past tense of \"run\". The child might say \"runned\" (ˆy) at first. The difference between what they say and what's correct is what we need to fix.\n\nJust as children learn through patterns and corrections, neural networks learn through a process called training. Think of it like learning morphological rules: the system needs to figure out the patterns (parameters) that transform input forms into correct output forms.\n\nThe network measures its mistakes using something called a loss function - similar to how we measure errors in language production. Think of it like counting the number of pronunciation or grammatical errors a language learner makes.\n\nTo improve, the system uses a method called gradient descent - imagine it like a language learner gradually refining their pronunciation by moving their tongue and lips closer to the correct position. They need to know which direction to adjust (the gradient).\n\nThe challenging part is figuring out exactly what to adjust when there are many steps between input and output - like understanding how an early phonological error affects the final meaning of a complex sentence. This is solved through \"backpropagation,\" which is like tracing back through the steps of language processing to find where the error began.\n\nJust as we can analyze a speech error by working backward from the output to find the point of confusion, backpropagation helps the network understand which parts of its process need adjustment."
                },
                "CL": {
                    "CL_claude_prompt4": "Here's my paraphrasis for NLP/CL practitioners:\n\nThink of a feedforward neural network as a smart language learning system, similar to how we train language models like BERT or GPT, but simpler. Just as we train these models on pairs of input-output examples (like sentences and their translations), the network learns from labeled training data. The network makes predictions (ŷ) and compares them to the correct answers (y), much like evaluating machine translation output against reference translations.\n\nThe training process involves three key components: First, we measure prediction accuracy using cross-entropy loss - imagine it as a scoring system that harshly penalizes the model when it's very confident but wrong, similar to how BLEU or perplexity scores work in NLP tasks. Second, we use gradient descent, which is like having an intelligent spell-checker that iteratively refines the model's parameters to reduce errors. Finally, backpropagation is the clever bookkeeping system that tracks how each parameter contributed to the errors - similar to how attention mechanisms help us trace which parts of the input influenced specific outputs, but working backwards through the network's layers to adjust weights systematically."
                }
            },
            "deepseek": {
                "L": {
                    "L_deepseek_prompt5": "A feedforward neural network operates like a supervised language learning task, where the system is trained on input-output pairs (x, y), akin to teaching a learner to map phonetic inputs (e.g., acoustic signals) to semantic outputs (e.g., word meanings). The network generates an estimate (ŷ) for each input, analogous to a language model predicting a syntactic structure for a sentence. Training adjusts \"weights\" (W[i]) and \"biases\" (b[i])—parameters acting like language-specific rules at different layers (phonological, morphological, etc.)—to minimize the mismatch between ŷ and y.  \n\nThe **loss function** (e.g., cross-entropy) quantifies this mismatch, much like measuring the divergence between a predicted phoneme sequence and its attested form. **Gradient descent** iteratively refines parameters, akin to adjusting phonological or syntactic rules to reduce errors in a language learner’s output. Crucially, computing how early-layer parameters (e.g., phonetic feature weights) contribute to later errors (e.g., semantic misinterpretations) requires **backpropagation**. This mirrors tracing a semantic error back through syntactic, morphological, and phonological layers to identify where the representation diverged, adjusting each layer’s \"rules\" accordingly. The process ensures coordinated learning across hierarchical linguistic levels, optimizing the entire system’s performance."
                },
                "CL": {
                    "CL_deepseek_prompt4": "**Adapted Explanation:**  \nA feedforward neural network is a supervised ML model trained on labeled data (input **x** with target **y**). During training, it adjusts internal parameters (weights/biases) across layers to minimize prediction error (ŷ vs. **y**). This optimization uses:  \n1. **Cross-entropy loss** (common in NLP for tasks like text classification) to quantify error.  \n2. **Gradient descent** to iteratively update parameters by moving \"downhill\" on the loss landscape.  \n3. **Backpropagation**, which efficiently calculates gradients across layers by dynamically propagating error signals backward—akin to how dependency parsing traces grammatical relationships through a sentence’s structure.  \n\nFor NLP systems (e.g., sentiment analysis), backprop adjusts early-layer embeddings and weights based on final output errors, ensuring holistic model tuning without manual layer-by-layer derivative calculations."
                }
            }
        }
    },
    "T8": {
        "instance_code": "T8",
        "original_category": "CS",
        "original_text_title": "Backward Pass",
        "original_text": "The importance of the computation graph comes from the backward pass, which is used to compute the derivatives that we'll need for the weight update. [...] Backwards differentiation makes use of the chain rule [...]. Suppose we are computing the derivative of a composite function f(x) = u(v(x)). The derivative of f(x) is the derivative of u(x) with respect to v(x) times the derivative of v(x) with respect to x. [...] The chain rule extends to more than two functions. If computing the derivative of a composite function f(x) = u(v(w(x))), the derivative of f(x) is: df/dx = du/dv·dv/dw·dw/dx. The intuition of backward differentiation is to pass gradients back from the final node to all the nodes in the graph. [...] Each node takes an upstream gradient that is passed in from its parent node to the right, and for each of its inputs computes a local gradient (the gradient of its output with respect to its input), and uses the chain rule to multiply these two to compute a downstream gradient to be passed on to the next earlier node. Let's now compute the 3 derivatives we need. Since in the computation graph L = ce, we can directly compute the derivative ∂L/∂c: ∂L/∂c = e. For the other two, we'll need to use the chain rule: ∂L/∂a=∂L/∂e*∂e/∂a [and] ∂L/∂b=∂L/∂e*∂e/∂d*∂d/∂b.",
        "selected_texts": {
            "mistral": {
                "L": {
                    "L_mistral_prompt5": "1. The computation graph in machine learning is crucial for its backward pass, which calculates the needed derivatives for updating the weights during the learning process. The principal tool used in this process is chain rule from calculus.\n\n    2. Let's imagine a complex function f(x) that is the result of multiple operations u(v(x)). To calculate the derivative of this function, we use the chain rule. Specifically, the derivative of f(x) is the derivative of u(x) with respect to v(x) times the derivative of v(x) with respect to x.\n\n    3. Consider a simple example of the chain rule applied to a composite function f(x) = u(v(w(x))). The derivative of f(x) can be calculated as follows: df/dx = du/dv·dv/dw·dw/dx.\n\n    4. The chain rule's fundamental principle is that it allows us to \"pass\" the gradients from the final node backward through the entire graph. Each node in the graph receives an upstream gradient from its parent node, computes local gradients (gradients of the output with respect to its input), and then passes downstream gradients to the next earlier node using the chain rule.\n\n    5. Let's calculate the derivatives we need for our example. Suppose our computation graph is L = ce, the derivative of L with respect to c is straightforward: ∂L/∂c = e. To find the derivatives with respect to a and b, we'll use the chain rule: ∂L/∂a = ∂L/∂e * ∂e/∂a and ∂L/∂b = ∂L/∂e * ∂e/∂d * ∂d/∂b."
                },
                "CL": {
                    "CL_mistral_prompt1": "1. In the realm of Computational Linguistics, the computation graph plays a crucial role due to the backward pass, a method that computes the derivatives necessary for updating weights during training. These derivatives are calculated using the chain rule from calculus.\n\n    2. Imagine a function f(x)=u(v(x)), where f(x) is the final output, u(x) is the ultimate function we want to differentiate, and v(x) is an intermediate step. The derivative of f(x) with respect to x is the derivative of u(x) with respect to v(x), multiplied by the derivative of v(x) with respect to x.\n\n    3. The chain rule's power extends to multiple functions. If we have f(x)=u(v(w(x))), the derivative of f(x) with respect to x is: du/dv*dv/dw*dw/dx.\n\n    4. The concept of backward differentiation is to transmit gradients backward from the final node to all nodes in the graph. Each node takes the gradient received from its parent node and computes its local gradient, then uses the chain rule to calculate the downstream gradient to pass on to the next earlier node.\n\n    5. For instance, let's calculate the three derivatives we need in our computation graph, where L=ce. We can directly compute the derivative ∂L/∂c: ∂L/∂c = e. For the other two derivatives, we'll need to use the chain rule: ∂L/∂a=∂L/∂e*∂e/∂a and ∂L/∂b=∂L/∂e*∂e/∂d*∂d/∂b."
                }
            },
            "llama": {
                "L": {
                    "L_llama_prompt4": "In linguistics, when analyzing the grammatical structure of a sentence, we often break it down into smaller components, similar to how a computation graph is constructed. The backward pass in computation can be likened to tracing the grammatical dependencies in a sentence from the main verb to the subject, and then to the modifiers, to understand how each component contributes to the overall meaning. This process is akin to applying the chain rule in calculus, where the derivative of a composite function is calculated by multiplying the derivatives of its constituent parts. For instance, when computing the derivative of a function f(x) = u(v(x)), we consider how the output of v(x) affects u(x), much like how a modifier affects the meaning of a noun it describes. The chain rule extends to more complex sentence structures, just as it does in calculus, allowing us to compute the derivative of a function like f(x) = u(v(w(x))) by chaining the derivatives of u, v, and w. In the context of computation graphs, this means passing gradients backwards from the final node to earlier nodes, much like how the meaning of a sentence is constructed by combining the meanings of its parts. Each node in the graph, like each word in a sentence, receives an \"upstream\" contribution from its parent node and computes its own \"local\" contribution, which is then combined using the chain rule to produce a \"downstream\" contribution to the next node. This process allows us to compute the derivatives needed for tasks like weight updates, much like how understanding the grammatical structure of a sentence allows us to comprehend its meaning. For example, given a computation graph where L = ce, we can directly compute the derivative ∂L/∂c, just as we can identify the main verb in a sentence. For more complex computations, like ∂L/∂a or ∂L/∂b, we apply the chain rule, tracing the dependencies through the graph, much like tracing grammatical dependencies in a sentence, to compute the necessary derivatives."
                },
                "CL": {
                    "CL_llama_prompt2": "Consider the process of training a language model in NLP, where you need to adjust the model's weights to minimize the error between the predicted and actual outputs. To achieve this, you use the computation graph, which is similar to a parse tree in linguistics, but instead of representing syntactic structures, it represents the flow of computations. The computation graph is crucial for the backward pass, where you compute the derivatives needed for weight updates, much like how you would use the chain rule in linguistics to analyze the relationships between words in a sentence. \n\n    Think of the chain rule as a way to propagate gradients through the computation graph, similar to how you would propagate semantic meaning through a sentence. In linguistics, you're familiar with the concept of compositionality, where the meaning of a sentence is derived from the meanings of its parts. Similarly, in the computation graph, the derivative of a composite function is computed by multiplying the derivatives of its components, using the chain rule. \n\n    For instance, if you have a function f(x) = u(v(x)), the derivative of f(x) with respect to x is the derivative of u with respect to v, times the derivative of v with respect to x. This is analogous to how you would analyze the meaning of a sentence like \"The dog runs quickly\", where you would break it down into its components, such as the subject \"dog\", the verb \"runs\", and the adverb \"quickly\", and then compose their meanings to understand the overall sentence meaning. \n\n    In the computation graph, each node represents a function, and the edges represent the flow of inputs and outputs. When computing the derivatives, each node receives an upstream gradient from its parent node and computes a local gradient, which is then multiplied with the upstream gradient to produce a downstream gradient. This process is repeated until you reach the input nodes, allowing you to compute the derivatives needed for weight updates. \n\n    To illustrate this, consider a simple example where you have a loss function L = ce, and you want to compute the derivatives of L with respect to the inputs a, b, and c. Using the chain rule, you can compute the derivative of L with respect to c directly, but for a and b, you need to use the chain rule to propagate the gradients through the computation graph. For instance, the derivative of L with respect to a is the derivative of L with respect to e, times the derivative of e with respect to a. This process allows you to efficiently compute the derivatives needed for weight updates, much like how you would use linguistic analysis to understand the meaning of a sentence. \n\n    By understanding how the computation graph and the chain rule work together, you can appreciate the power of AI models in NLP and how they can be trained to achieve state-of-the-art results. This knowledge will enable you to make informed decisions when selecting AI products for your NLP tasks, and to optimize their performance for your specific use cases.  # END OF OUTPUT #"
                }
            },
            "gpt4o": {
                "L": {
                    "L_gpt4o_prompt1": "In computational linguistics, understanding how to update weights in a model involves calculating derivatives through a process called backward differentiation. This is where the computation graph plays a crucial role. Let's break this down using concepts from your linguistics background.\n\nImagine a computation graph as a syntactic tree, where each node represents a function or operation, and each branch indicates the relationship between them. Just like in syntax, where you analyze sentence structure, here we analyze how functions are composed to form a final output.\n\nBackward differentiation uses a mathematical rule called the chain rule, similar to how we analyze sentence components. If we have a function f(x) that combines other functions, say f(x) = u(v(x)), the chain rule helps us find the derivative of f by looking at how changes in x affect v, and then how changes in v affect u. In simpler terms, it's like understanding how changing one word in a sentence might affect the meaning of the entire sentence.\n\nNow, let's extend this to a more complex function: f(x) = u(v(w(x))). The chain rule tells us to find the derivative by multiplying the derivatives at each step: df/dx = du/dv · dv/dw · dw/dx. This is like tracing the influence of a word through several layers of sentence structure.\n\nIn practical terms, backward differentiation involves passing gradients (akin to semantic weight or influence) from the final output node back to the input nodes, much like how understanding the meaning of a sentence involves tracing back through its structure to see how each part contributes to the whole.\n\nLet's consider a specific example. Suppose in our computation graph, the output L is a function of c and e, such that L = ce. To find how changes in c affect L, we compute the derivative ∂L/∂c, which is simply e, similar to identifying the direct role a word plays in a sentence. For more complex relationships, like how a affects L, we use the chain rule: ∂L/∂a = ∂L/∂e * ∂e/∂a. This is akin to understanding indirect semantic roles through multiple layers of sentence structure.\n\nBy visualizing the computation graph and using principles similar to syntactic analysis, we can grasp how changes propagate through a model, enabling us to adjust weights effectively."
                },
                "CL": {
                    "CL_gpt4o_prompt1": "In computational linguistics, especially when dealing with machine learning models, understanding computation graphs and how they work is crucial. A computation graph visually represents how a function is calculated in terms of its components and their connections. This is particularly important when we perform a backward pass to calculate derivatives needed for updating model weights.\n\n### Key Concepts:\n\n1. **Backward Pass and Derivatives:**\n   - The backward pass is essential for computing derivatives, which are used to adjust model parameters during training. This process relies heavily on a mathematical tool called the chain rule.\n\n2. **Chain Rule in Action:**\n   - Think of a function as a series of steps or operations. For example, if we have a composite function \\( f(x) = u(v(x)) \\), its derivative involves multiplying the derivative of \\( u \\) with respect to \\( v(x) \\) by the derivative of \\( v \\) with respect to \\( x \\). This is the essence of the chain rule.\n   - When dealing with more complex functions like \\( f(x) = u(v(w(x))) \\), the chain rule extends to multiply the derivatives for each step: \\(\\frac{df}{dx} = \\frac{du}{dv} \\cdot \\frac{dv}{dw} \\cdot \\frac{dw}{dx}\\).\n\n3. **Backward Differentiation:**\n   - Imagine the computation graph as a flow of information from inputs to output. In backward differentiation, we pass \"gradients\" (derivatives) from the final output node back to each preceding node.\n   - Each node in this graph receives an upstream gradient from its \"parent\" node and computes a local gradient, which is the derivative of its output with respect to its input. These are then multiplied together using the chain rule to form a downstream gradient, which is passed on to the next node.\n\n### Practical Example:\n\nSuppose we have a computation graph where \\( L = ce \\). The derivative \\(\\frac{\\partial L}{\\partial c}\\) is straightforward and equals \\( e \\).\n\nFor the other derivatives, we apply the chain rule:\n- To find \\(\\frac{\\partial L}{\\partial a}\\), compute: \\(\\frac{\\partial L}{\\partial e} \\cdot \\frac{\\partial e}{\\partial a}\\).\n- For \\(\\frac{\\partial L}{\\partial b}\\), calculate: \\(\\frac{\\partial L}{\\partial e} \\cdot \\frac{\\partial e}{\\partial d} \\cdot \\frac{\\partial d}{\\partial b}\\).\n\n### Relatable Analogy:\n\nThink of this process like assembling a car. Each step in the assembly depends on the previous one, much like each derivative depends on the one before it. Using the chain rule in backward differentiation is like ensuring each part of the car is correctly and efficiently put together by understanding how each piece connects to the next.\n\nBy grasping these concepts, you bridge the gap between theoretical math and practical computational models, enabling you to effectively train and optimize language models."
                }
            },
            "claude": {
                "L": {
                    "L_claude_prompt5": "Here's the adapted explanation for linguistics experts:\n\nThink of backward differentiation like tracing the etymology of a complex derived word back to its root components. Just as a word like \"unthinkable\" can be decomposed into morphemes (un- + think + -able), mathematical functions can be broken down into simpler parts.\n\nIn linguistics, you analyze how meaning changes as you add or remove morphemes. Similarly, in this mathematical process, we trace how changes propagate backward through a chain of operations. This is comparable to how semantic change flows through compound words or phrases.\n\nThe chain rule works like morphological dependency: just as the meaning of each morpheme affects the next one in sequence (think of how negation markers affect subsequent elements), each mathematical operation influences the next in a specific order. For example, just as we can trace how the negative prefix \"un-\" affects the entire word \"unthinkable,\" we can track how changes in one part of a mathematical expression affect the final result.\n\nWhen we compute derivatives backwards, it's similar to analyzing a sentence from right to left in head-final languages - each element passes information to the previous one. Just as morphological rules combine in specific ways (like how inflectional morphemes must follow derivational ones), these mathematical gradients multiply in a specific order to show how each part contributes to the whole expression.\n\nThe computation follows strict rules of composition, much like how morphological and syntactic rules govern word formation and sentence structure in language."
                },
                "CL": {
                    "CL_claude_prompt1": "Let me explain how backward propagation works in computational graphs, which is crucial for neural networks in NLP tasks.\n\nThink of a computation graph like a recipe where each step depends on previous steps. In NLP, we often need to figure out how much each earlier step contributed to our final result (like how much each word influenced the sentiment prediction).\n\nThe backward pass uses something called the chain rule, which is like tracing our steps backwards through the recipe. Let's use a familiar example: imagine translating \"hello\" to French. If our translation is slightly wrong, we need to figure out which parts of our neural network need adjustment.\n\nThe chain rule works like this:\n- If A affects B, and B affects C, then to know how A affects C, we multiply how A affects B by how B affects C\n- In mathematical notation, we write this as: ∂C/∂A = ∂C/∂B * ∂B/∂A\n\nIn our computation graph, we start at the final output (like translation error) and work backwards. Each node in the graph:\n1. Receives a gradient from the next step (like feedback about the error)\n2. Calculates how much its own inputs influenced its output\n3. Multiplies these together to pass the feedback backward\n\nThis is similar to how, in corpus linguistics, we trace back patterns to understand their origins, but here we're tracing back errors to fix our model's weights.\n\nFor the specific example in the text:\n- First we compute ∂L/∂c = e (the direct effect)\n- Then we use the chain rule for more complex paths:\n  - ∂L/∂a = ∂L/∂e * ∂e/∂a (one intermediate step)\n  - ∂L/∂b = ∂L/∂e * ∂e/∂d * ∂d/∂b (two intermediate steps)"
                }
            },
            "deepseek": {
                "L": {
                    "L_deepseek_prompt2": "**Adapted Explanation:**  \nThink of a neural network’s computation graph as a **syntactic tree** for a sentence. Just as a tree breaks a sentence into nested phrases (e.g., [S [NP][VP]]), the graph breaks a mathematical function into smaller operations. The \"backward pass\" is like tracing dependencies in reverse to diagnose how each word (or node) contributed to a grammatical error (loss).  \n\n**Chain Rule = Nested Dependencies**  \nImagine calculating the acceptability of a sentence like *\"The cat, which was hungry, meowed.\"* The overall structure depends on smaller clauses. Similarly, if a function \\( f(x) = u(v(w(x))) \\), its \"derivative\" (sensitivity to change) is computed by chaining local sensitivities:  \n\\[ \\frac{df}{dx} = \\frac{du}{dv} \\cdot \\frac{dv}{dw} \\cdot \\frac{dw}{dx} \\]  \nThis mirrors how a misplaced relative clause (*which was hungry*) propagates errors through the sentence structure.  \n\n**Backward Differentiation = Error Diagnosis**  \nSuppose your model’s output \\( L \\) (e.g., a sentence’s grammaticality score) depends on nodes \\( c \\) and \\( e \\) (\\( L = c \\cdot e \\)). To adjust weights (like fixing syntax):  \n1. **Direct influence**: \\( \\frac{\\partial L}{\\partial c} = e \\) (how much \\( c \\) alone affects \\( L \\)).  \n2. **Indirect influence**: For nodes like \\( a \\) or \\( b \\), trace backward through dependencies (chain rule):  \n   - \\( \\frac{\\partial L}{\\partial a} = \\frac{\\partial L}{\\partial e} \\cdot \\frac{\\partial e}{\\partial a} \\) (how \\( a \\) affects \\( e \\), which then affects \\( L \\)).  \n\n**Analogy**: Just as you’d revise a verb’s tense (*meowed* → *meows*) by checking its role in the VP and NP, backward pass identifies which \"nodes\" (words/operations) need adjustment to minimize errors."
                },
                "CL": {
                    "CL_deepseek_prompt5": "In neural network training (particularly for NLP tasks like machine translation), the computation graph enables efficient gradient calculation through backpropagation - analogous to how syntactic trees represent hierarchical language structure. The backward pass calculates partial derivatives using the chain rule from calculus: just as a dependency parse traces relationships between words, gradients flow inversely through computational nodes.  \n\nConsider a text classification model where loss \\( L \\) propagates through layers. For output layer gradients (\\( \\partial L/\\partial c \\)), we directly measure prediction error. Hidden layer gradients (e.g., \\( \\partial L/\\partial a \\)) require chaining derivatives across operations, like how POS tag probabilities depend on preceding word embeddings. Each network layer acts like a function composition \\( f(x) = u(v(x)) \\), where the total derivative becomes \\( du/dv \\cdot dv/dx \\) - mirroring how transformer layers process token sequences through attention and feed-forward sublayers. Nodes multiply upstream gradients (from later layers) with local gradients (layer-specific transformations), effectively distributing error signals to optimize parameters, similar to how language models adjust word embeddings based on contextual prediction errors."
                }
            }
        }
    },
    "T9": {
        "instance_code": "T9",
        "original_category": "CS",
        "original_text_title": "Recurrent Neural Networks (RNNs)",
        "original_text": "A recurrent neural network (RNN) is any network that contains a cycle within its network connections, meaning that the value of some unit is directly, or indirectly, dependent on its own earlier outputs as an input. [...] As with ordinary feedforward networks, an input vector representing the current input, xt, is multiplied by a weight matrix and then passed through a non-linear activation function to compute the values for a layer of hidden units. This hidden layer is then used to calculate a corresponding output, yt. [...] The key difference from a feedforward network lies in the recurrent link [...] [which] augments the input to the computation at the hidden layer with the value of the hidden layer from the preceding point in time. The hidden layer from the previous time step provides a form of memory, or context, that encodes earlier processing and informs the decisions to be made at later points in time. [...] the context embodied in the previous hidden layer can include information extending back to the beginning of the sequence. Adding this temporal dimension makes RNNs appear to be more complex than non-recurrent architectures. [...] we're still performing the standard feedforward calculation [...]. The most significant change lies in the new set of weights, U, that connect the hidden layer from the previous time step to the current hidden layer. [...] the [tailored] backpropagation algorithm [...] [is] referred to as backpropagation through time [...].",
        "selected_texts": {
            "mistral": {
                "L": {
                    "L_mistral_prompt2": "As a Linguistics student, you are well-versed in language structures and patterns. Now, imagine a model that can mimic these same structures and patterns in data processing. That's what a Recurrent Neural Network (RNN) does.\n\n     RNNs are special networks with loops in their connections, which means that the output of a unit at one time can influence its output at another time. To break it down, an RNN takes an input vector, multiplies it by a set of weights, and passes it through an activation function to produce a set of hidden units. These hidden units are then used to generate an output.\n\n     What makes RNNs unique compared to standard networks is the recurrent link. This link adds the output of the previous hidden layer to the computation of the current hidden layer, thus serving as a form of memory or context that stores earlier processing and informs future decisions. In essence, this context can include information from the entire sequence, giving RNNs a temporal dimension.\n\n     While RNNs may seem more complex due to this temporal dimension, they essentially still perform the same feedforward calculation as standard networks. The main difference lies in the new set of weights, U, that connect the hidden layer from the previous time step to the current hidden layer.\n\n     To train RNNs, we use a modified version of the backpropagation algorithm, called backpropagation through time, which takes into account the temporal dimension and helps the network learn and make predictions more accurately over time.\n\n     In short, RNNs are powerful tools for processing sequential data, leveraging their unique memory structure to make predictions based on patterns and structures found within the data sequence."
                },
                "CL": {
                    "CL_mistral_prompt1": "Recurrent Neural Networks (RNNs) are a type of network that includes loops in their connections, allowing the present output to be influenced by past outputs. In simple terms, RNNs remember their previous computations, which can be useful for understanding sequences of data like sentences or speech.\n\n    Just like traditional Feed-forward Neural Networks (FNNs), RNNs take an input vector, x, and multiply it by a weight matrix before passing it through an activation function to produce a set of hidden unit values. These hidden unit values are then used to calculate the output, y.\n\n    The main difference between RNNs and FNNs is the recurrent link, which incorporates the hidden unit values from the previous time step into the current hidden layer computation. This previous hidden layer serves as a kind of memory or context that informs the decisions made at later time steps. The context can include information going back to the start of the sequence, giving RNNs a temporal dimension that makes them seem more complex than non-recurrent architectures.\n\n    However, the basic feedforward calculation remains the same. The main change is in the weights, U, connecting the hidden layer from the previous time step to the current hidden layer.\n\n    To train RNNs, we use a specialized version of backpropagation called backpropagation through time (BPTT), which accounts for the temporal dimension and allows us to adjust the weights effectively. In essence, BPTT allows the network to learn from sequences of data, rather than just individual examples."
                }
            },
            "llama": {
                "L": {
                    "L_llama_prompt4": "- Imagine you are listening to a story, and the storyteller is using the information from the beginning to make sense of what they are telling you now. This is similar to how a Recurrent Neural Network (RNN) works. In an RNN, the network uses a \"memory\" of its previous computations to inform its current processing, much like how the storyteller uses the context of the story to make sense of the current sentence. \n    - The RNN has a special connection, called a recurrent link, that allows it to retain information from previous time steps and use it to make decisions at later points in time. This is similar to how we use our own memory to recall previous events and make sense of the present moment. \n    - To process the information, the RNN uses a non-linear activation function, similar to how we use our brain to process and understand language. \n    - The key to the RNN's ability to learn and understand sequential data, such as speech or text, lies in its ability to maintain a \"context\" or \"memory\" of previous events. \n    - This context is built up over time, as the network processes each new piece of information and updates its internal state, much like how we build up our understanding of a story as we listen to it. \n    - The RNN's use of this context allows it to make predictions or take actions based on the entire sequence of information, rather than just the current piece of information. \n    - The process of training an RNN is similar to how we learn a new language, where we use the context of the conversation to understand the meaning of the words and phrases. \n    - The backpropagation algorithm used to train RNNs is an extension of the standard backpropagation algorithm, and is referred to as backpropagation through time, which takes into account the temporal relationships between the different pieces of information in the sequence. \n    - This allows the RNN to learn and understand complex patterns in sequential data, such as speech or text, and to make accurate predictions or take appropriate actions based on that understanding. \n    - The RNN's ability to maintain a context or memory of previous events, and to use that context to inform its current processing, makes it a powerful tool for modeling and understanding complex sequential data. \n    - For example, an RNN can be used to predict the next word in a sentence, based on the context of the previous words, or to generate text that is similar in style and structure to a given piece of text. \n    - The use of RNNs has many applications, such as speech recognition, language translation, and text generation, where the ability to understand and model complex sequential data is essential. \n    - In summary, an RNN is a type of neural network that uses a \"memory\" of its previous computations to inform its current processing, and is particularly well-suited to modeling and understanding complex sequential data, such as speech or text. \n    - The RNN's ability to maintain a context or memory of previous events, and to use that context to inform its current processing, makes it a powerful tool for a wide range of applications, from speech recognition and language translation, to text generation and beyond. \n\n    However, I need only one explanation for NLP/Computational Linguistics Practitioners in the field of Linguistics (L). I will select the most fitting one from the explanations I have provided, which is the first one. \n    Imagine you are listening to a story, and the storyteller is using the information from the beginning to make sense of what they are telling you now. This is similar to how a Recurrent Neural Network (RNN) works. In an RNN, the network uses a \"memory\" of its previous computations to inform its current processing, much like how the storyteller uses the context of the story to make sense of the current sentence."
                },
                "CL": {
                    "CL_llama_prompt5": "Imagine you're training a language model to predict the next word in a sentence. The model needs to understand the context of the previous words to make an accurate prediction. A Recurrent Neural Network (RNN) is designed to handle this type of task. It's a network that uses feedback connections to keep track of information from previous time steps, allowing it to capture temporal relationships in sequential data, like the order of words in a sentence. Think of it like trying to understand a conversation - you need to remember what was said earlier to respond appropriately. In a feedforward network, the input is processed and then discarded, but in an RNN, the hidden layer's output from the previous time step is fed back into the network, providing a form of memory that informs the model's decisions. This memory can stretch back to the beginning of the sequence, allowing the model to capture long-range dependencies. To train an RNN, we use a variation of the backpropagation algorithm, called backpropagation through time, which takes into account the temporal relationships between the inputs and outputs. This allows the model to learn from the entire sequence, not just individual time steps. In essence, RNNs are like conversational AI models that can remember and understand the context of a conversation, making them well-suited for tasks like language translation, text summarization, and chatbots."
                }
            },
            "gpt4o": {
                "L": {
                    "L_gpt4o_prompt2": "A recurrent neural network (RNN) is a type of artificial intelligence model that processes sequences of data by remembering its past outputs. Think of it like a conversation where each sentence you say is influenced by what has been said before. This \"memory\" aspect is what makes RNNs special, unlike other neural networks that treat each input as independent.\n\nHere's how it works: Imagine you have a sentence, and each word is an input for the RNN. The network processes each word in order, and as it does so, it keeps a sort of mental note (like a context) of what it has processed before. This is akin to how understanding the meaning of a sentence often requires knowledge of the preceding words.\n\nThe process involves taking an input word, converting it into a numerical format (an input vector), and multiplying it by a set of weights (like assigning importance to different letter features). This result goes through a function that decides how much of the input should affect the next step, much like how syntax rules influence the meaning of a sentence.\n\nThe crucial difference between RNNs and simpler networks is the feedback loop. This loop uses the \"memory\" from previous words to influence the current word processing, providing context that can span the entire sequence, similar to how earlier parts of a text can influence the interpretation of later parts.\n\nLastly, training an RNN involves a process called backpropagation through time, where the network learns from the entire sequence to improve its understanding, just as one might revise a thesis by considering the text as a whole rather than isolated parts."
                },
                "CL": {
                    "CL_gpt4o_prompt5": "### Computational Linguistics (CL) Audience\n\nA recurrent neural network (RNN) is a type of neural network designed for handling sequential data, such as text or speech. Unlike feedforward networks, which process inputs in a single pass, RNNs have loops in their structure. This means that the output from a previous time step can be used as an input in the current step, allowing the network to maintain a form of memory.\n\nIn practical terms, when an RNN processes an input sequence, each element, represented as a vector \\( x_t \\), is multiplied by a weight matrix and passed through a non-linear function to produce hidden layer values. These values then help produce an output \\( y_t \\). The twist with RNNs is that the hidden layer also incorporates information from the previous time step via weights \\( U \\), making it context-aware. This memory enables the RNN to remember crucial details throughout the sequence, much like how you remember the plot of a story as you read it.\n\nThe process of enhancing this sequence memory is similar to standard backpropagation used in training feedforward networks, but involves a technique called backpropagation through time. This method updates the weights based on how the network performs over a sequence, rather than a single snapshot, allowing RNNs to learn longer dependencies effectively."
                }
            },
            "claude": {
                "L": {
                    "L_claude_prompt2": "Think of a recurrent neural network (RNN) like a language processing system that remembers context - similar to how we understand conversations by keeping track of what was said before. Just as you need previous sentences to understand pronouns like \"it\" or \"they\" in natural language, an RNN maintains memory of previous information to make sense of current input.\n\nIn linguistic terms, imagine you're parsing a sentence. Each word (input) gets processed not just on its own, but in relation to the words that came before it. The network has a \"memory layer\" (hidden layer) that acts like your working memory during language comprehension. When you encounter each new word, you combine its meaning with your memory of previous words to understand the complete message.\n\nThis is similar to how we process anaphora in discourse analysis - the current interpretation depends on previous references. The network uses special connections (weights) that act like grammatical rules, linking previous context to current understanding. Just as language is inherently sequential and context-dependent, RNNs process information in a temporal sequence, with each step informed by what came before.\n\nThe mathematical process behind this (called backpropagation through time) is like analyzing a text backward to understand how earlier elements influence later meaning - similar to how you might trace back references in discourse analysis to understand cohesion and coherence."
                },
                "CL": {
                    "CL_claude_prompt5": "For CL Audience:\n\nRecurrent Neural Networks (RNNs) are like language processing systems with memory, similar to how humans use context from previous words to understand the next ones in a sentence. While traditional neural networks process each input independently (like treating each word in isolation), RNNs maintain a \"memory\" of what came before.\n\nThink of it like parsing a sentence: when you process the word \"it\" in \"The book fell off the shelf and it broke,\" you need to remember \"the book\" from earlier to resolve this reference. RNNs work similarly - they have a special connection (the recurrent link) that passes information from previous processing steps to the current one.\n\nThe network processes input sequences (like words in a sentence) one element at a time, but unlike standard feedforward networks that you might know from basic NLP tasks, RNNs have an additional set of weights (U) that connect the previous state to the current one. This is analogous to how anaphora resolution requires maintaining and accessing previous discourse information.\n\nThe learning process, called backpropagation through time, is similar to regular backpropagation used in other neural networks you've encountered in NLP tasks, but it accounts for these temporal connections. Just as you might analyze a parse tree both forward and backward to check for consistency, this algorithm propagates errors backward through the temporal sequence to adjust the weights.\n\nThis architecture has been particularly useful in sequence-to-sequence tasks like machine translation, where maintaining context across the input sequence is crucial for accurate translation."
                }
            },
            "deepseek": {
                "L": {
                    "L_deepseek_prompt4": "**Adapted Explanation:**  \nA recurrent neural network (RNN) is a computational model designed to process sequences—like sentences or sound waves—by maintaining a \"memory\" of prior elements in the sequence. Imagine parsing a sentence: understanding the meaning of \"bank\" in \"I withdrew cash from the bank\" relies on knowing the preceding words (\"withdrew cash\"), much like how syntax depends on word order. Unlike simpler models that treat each input (e.g., a word) in isolation, an RNN loops its hidden layer’s output back into itself at each step. This loop acts like a mental notepad, retaining contextual clues (e.g., semantic coherence in a paragraph or phonetic patterns in speech). During training, the model adjusts how heavily it relies on past versus current information—akin to refining grammatical intuitions through exposure to more language data. The process mirrors how humans iteratively revise interpretations as a conversation unfolds."
                },
                "CL": {
                    "CL_deepseek_prompt4": "An RNN processes sequences by dynamically updating a hidden state that serves as a compressed memory of previous inputs. Unlike feedforward networks (e.g., standard LLM layers), each step combines the current input (e.g., a word embedding) with the prior hidden state via learned weights (matrix **U**), enabling context-aware predictions—like using preceding words to resolve ambiguous token meanings in a sentence. Training unrolls these steps into a computational graph (**backpropagation through time**), optimizing how past context influences current decisions. For NLP tasks like POS tagging, this mirrors tracking grammatical agreement across a sentence, where verb tense depends on a subject mentioned earlier."
                }
            }
        }
    },
    "T10": {
        "instance_code": "T10",
        "original_category": "CS",
        "original_text_title": "Long Short-Term Memory (LSTMs)",
        "original_text": "[...] it is quite difficult to train RNNs for tasks that require a network to make use of information distant from the current point of processing. [...] the information encoded in hidden states tends to be fairly local, more relevant to the most recent parts of the input sequence and recent decisions. [...] the gradients are eventually driven to zero, a situation called the vanishing gradients problem. To address these issues, more complex network architectures have been designed to explicitly manage the task of maintaining relevant context over time, by enabling the network to learn to forget information that is no longer needed and to remember information required for decisions still to come. The most commonly used such extension to RNNs is the long short-term memory (LSTM) network. LSTMs [...] remove information no longer needed from the context, and adding information likely to be needed for later decision making. [...] LSTMs accomplish this by first adding an explicit context layer to the architecture [...], and control the flow of information into and out of the units [...]. These gates are implemented through the use of additional weights that operate sequentially on the input, and previous hidden layer, and previous context layers. [...] The purpose of [the forget gate] is to delete information from the context that is no longer needed. [...] the add gate to select the information to add to the current context. [...] The final gate is the output gate which is used to decide what information is required for the current hidden state. [...]",
        "selected_texts": {
            "mistral": {
                "L": {
                    "L_mistral_prompt1": "0. LSTM Networks: A Solution to the Vanishing Gradients Problem in RNNs\n\n    Recall that Recurrent Neural Networks (RNNs) are a type of neural network that processes sequences of data, where the hidden state is updated based on the previous hidden state and the current input. However, RNNs struggle with long term dependencies, as the information encoded in the hidden states is typically local and more relevant to recent parts of the input sequence and recent decisions. This issue arises due to the vanishing gradients problem, where gradients are driven to zero, making it difficult for the network to learn effectively from distant parts of the input sequence.\n\n    To tackle these challenges, LSTM Networks have been developed, which explicitly manage the task of maintaining relevant context over time. LSTMs allow the network to learn to forget information that is no longer needed and remember information required for decisions still to come.\n\n    ### 1. LSTM Network Architecture\n\n    LSTMs are an extension of RNNs and consist of three main components: the forget gate, the input gate, and the output gate. These components are designed to manage the flow of information in and out of the units, allowing the network to selectively forget and remember relevant information.\n\n    ### 2. The Forget Gate\n\n    The forget gate decides which information to delete from the context, ensuring that the network focuses on the most important parts of the input sequence. This helps address the vanishing gradients problem by preventing irrelevant information from accumulating and overwhelming the network.\n\n    ### 3. The Input Gate\n\n    The input gate selects the information to add to the current context, ensuring that the network considers only the most relevant information for the task at hand. This helps improve the quality of the hidden state and enables the network to make better decisions.\n\n    ### 4. The Output Gate\n\n    The output gate determines which information is required for the current hidden state, allowing the network to focus on the relevant information and ignore the rest. This helps the network make more accurate predictions and improve its overall performance.\n\n    ### 5. LSTM Network Implementation\n\n    LSTMs introduce an explicit context layer to the architecture, which allows the network to maintain a memory of the input sequence over time. The gates in LSTMs are controlled through the use of additional weights that operate sequentially on the input, previous hidden state, and previous context layer. This enables the network to adapt to different input sequences and tasks.\n\n    ### Conclusion\n\n    LSTM Networks are an essential tool in addressing the vanishing gradients problem in RNNs and improving the long-term dependency issue. By explicitly managing the flow of information in and out of the units, LSTMs enable the network to selectively forget and remember relevant information, resulting in improved performance on tasks that require long-term dependencies."
                },
                "CL": {
                    "CL_mistral_prompt5": "1. Computer Science Audience:\n\n    Training Recurrent Neural Networks (RNNs) can be challenging when they need to utilize information that is far from the current processing point. The information stored in hidden states is typically local, more related to the most recent input and recent decisions. This can lead to the vanishing gradients problem, where gradients are driven to zero. To overcome these difficulties, advanced network architectures have been developed to manage the task of preserving relevant context over time. One such extension to RNNs is the Long Short-Term Memory (LSTM) network. LSTMs control information flow, forgetting unnecessary data and remembering important data for future decisions. LSTMs achieve this by incorporating an explicit context layer into the architecture, managing the flow of information through gates. These gates decide what to forget, add, and output.\n\n    2. AI Audience:\n\n    RNNs can struggle when processing information distant from the current point, as the hidden states usually contain local information, more relevant to recent inputs and decisions. This can lead to the vanishing gradients problem. To address this, complex network architectures like LSTM networks have been developed. LSTMs manage the task of maintaining relevant context over time by learning to forget unneeded data and remember necessary data for future decisions. LSTMs include an explicit context layer in their architecture, control information flow through gates, and use these gates to forget, add, and output data.\n\n    3. NLP Audience:\n\n    It's challenging to train Recurrent Neural Networks (RNNs) for tasks requiring information from distant points in the text. The hidden states in RNNs usually contain local information, relevant to the most recent parts of the input sequence and recent decisions. This can lead to the vanishing gradients problem. To tackle these issues, advanced network architectures like LSTM networks have been created. LSTMs manage the task of maintaining relevant context over time by learning to forget unneeded data and remember necessary data for future decisions. LSTMs include an explicit context layer in their architecture, control information flow through gates, and use these gates to forget, add, and output data.\n\n    4. Corpus Linguistics Audience:\n\n    In text analysis tasks using Recurrent Neural Networks (RNNs), it can be difficult for the network to use information that is far from the current processing point. The hidden states in RNNs usually contain local information, relevant to the most recent parts of the input sequence and recent decisions. This can lead to the vanishing gradients problem. To combat these difficulties, advanced network architectures like LSTM networks have been developed. LSTMs manage the task of maintaining relevant context over time by learning to forget unneeded data and remember necessary data for future decisions. LSTMs include an explicit context layer in their architecture, control information flow through gates, and use these gates to forget, add, and output data.\n\n    5. LLMs Audience:\n\n    When using Recurrent Neural Networks (RNNs) for tasks requiring distant information in the text, the network can struggle. The hidden states in RNNs usually contain local information, relevant to the most recent parts of the input sequence and recent decisions. This can lead to the vanishing gradients problem. To resolve these issues, advanced network architectures like LSTM networks have been designed. LSTMs manage the task of maintaining relevant context over time by learning to forget unneeded data and remember necessary data for future decisions. LSTMs include an explicit context layer in their architecture, control information flow through gates, and use these gates to forget, add, and output data."
                }
            },
            "llama": {
                "L": {
                    "L_llama_prompt2": "Consider a sentence where you want to understand the relationship between words that are far apart, like \"The dog, which ran quickly, caught the ball.\" To comprehend this sentence, you need to keep track of the subject \"dog\" and its actions across the sentence. In linguistic terms, this is similar to analyzing long-distance dependencies in syntax. \n\n    In a similar way, Recurrent Neural Networks (RNNs) process sequential data, like sentences, one step at a time. However, they struggle to retain information from earlier parts of the sequence, much like how it's hard to remember the beginning of a long sentence. This is because the information encoded in the hidden states of RNNs is local, focusing on the most recent parts of the input sequence and recent decisions. As a result, the gradients, which are essential for training the network, tend to disappear, a phenomenon known as the vanishing gradients problem.\n\n    To overcome this issue, more complex network architectures, like Long Short-Term Memory (LSTM) networks, have been developed. LSTMs are designed to manage context over time, allowing the network to learn what information to discard and what to retain for future decisions. This is analogous to a linguistic process where you selectively focus on specific words or phrases in a sentence to understand its meaning.\n\n    LSTMs achieve this by introducing an explicit context layer and control gates that regulate the flow of information. These gates operate sequentially, using additional weights to determine what information to add, remove, or output. The forget gate acts like a filter, removing irrelevant information from the context, while the add gate selects relevant information to include. The output gate then decides what information is necessary for the current hidden state, much like how you choose the relevant words to focus on when parsing a sentence.\n\n    By using LSTMs, the network can effectively handle long-distance dependencies in sequential data, much like how linguists analyze complex sentence structures. This enables the network to make more accurate predictions and decisions, making it a powerful tool for natural language processing tasks. \n\n    Think of LSTMs as a skilled reader who can keep track of the narrative thread, retaining relevant information and discarding unnecessary details, to ultimately comprehend the meaning of a text. This ability to manage context and selectively focus on relevant information makes LSTMs a valuable asset in understanding and generating human language."
                },
                "CL": {
                    "CL_llama_prompt1": "In the realm of Natural Language Processing (NLP), one of the primary challenges in training Recurrent Neural Networks (RNNs) is their inability to effectively utilize information that is far away from the current processing point. This limitation arises because the information encoded in the hidden states of RNNs tends to be localized, primarily relevant to the most recent segments of the input sequence and recent decisions made by the network. As a result, the gradients used to update the network's weights during backpropagation are often driven to zero, a phenomenon known as the vanishing gradients problem. \n\n    To overcome this issue, more sophisticated network architectures have been developed, such as the Long Short-Term Memory (LSTM) network. LSTMs are an extension of RNNs that enable the network to learn what information to forget and what to remember for future decisions, effectively managing context over time. This is achieved through the introduction of an explicit context layer and the use of gates that control the flow of information into and out of the units. \n\n    These gates, which include the forget gate, add gate, and output gate, are implemented using additional weights that operate sequentially on the input, previous hidden layer, and previous context layers. The forget gate's purpose is to remove information from the context that is no longer necessary, while the add gate selects the information to be added to the current context. The output gate determines what information is required for the current hidden state, allowing the network to make informed decisions. \n\n    For instance, consider a language model trying to predict the next word in a sentence. If the sentence is \"The cat, which had been sleeping, is now...\", the model needs to remember the subject \"cat\" to predict the correct verb form. An LSTM network would use its gates to retain the relevant information about the subject, allowing it to make accurate predictions. This ability to manage context and retain relevant information makes LSTMs particularly useful in NLP tasks, such as language modeling, machine translation, and text summarization. \n\n    In essence, LSTMs provide a way to bridge the gap between the sequential nature of language and the need for neural networks to capture long-range dependencies, making them a crucial component in many state-of-the-art NLP systems, including those utilizing Large Language Models (LLMs). By understanding how LSTMs work and how they address the limitations of traditional RNNs, you can better appreciate the complexities of NLP and the importance of context in language understanding."
                }
            },
            "gpt4o": {
                "L": {
                    "L_gpt4o_prompt3": "In the realm of neural networks, particularly Recurrent Neural Networks (RNNs), there's a challenge similar to understanding a sentence where key information is spread out. RNNs often struggle with remembering details from earlier in a sequence, much like trying to recall the beginning of a long sentence while reading the end. This happens because the information stored in the network's memory, or \"hidden states,\" tends to focus on recent input, akin to emphasizing the last few words you've just read. This can lead to the \"vanishing gradients problem,\" where early information fades away, much like how earlier parts of a conversation might become hazy.\n\nTo tackle this, an advanced version of RNNs called Long Short-Term Memory networks (LSTMs) was developed. Imagine LSTMs as having a more sophisticated memory system that can decide what to forget and what to remember, similar to how you might choose to remember a phone number but forget a random number you just heard. LSTMs achieve this through a series of \"gates,\" which are mechanisms that control the flow of information.\n\nFirstly, there's the \"forget gate,\" which clears out unnecessary information, like erasing irrelevant details. Then there's the \"add gate,\" which selects new information to store, akin to jotting down important notes. Finally, the \"output gate\" determines what information is essential for the current task, much like choosing the right words when forming a sentence. By using these gates, LSTMs manage to maintain relevant context over time, allowing them to handle sequences where information is scattered across various points, much like understanding a complex sentence structure."
                },
                "CL": {
                    "CL_gpt4o_prompt2": "In computational linguistics, you're likely familiar with using neural networks to process language data. One such network type, Recurrent Neural Networks (RNNs), processes sequences by maintaining a hidden state that updates with each new piece of data. However, RNNs struggle when they need to retain information from earlier in a sequence, a challenge known as the vanishing gradients problem, where important information fades away over time.\n\nTo overcome this, we use a more advanced type of RNN called Long Short-Term Memory (LSTM) networks. Think of LSTMs as having a sophisticated memory system. They can decide what information to keep, what to throw away, and what new information to add to their \"memory\" as they process data. This is similar to how you might highlight key points in a large text to remember important details while ignoring less relevant parts.\n\nLSTMs achieve this with three main components, called gates:\n\n1. **Forget Gate**: This gate helps the network decide which information is no longer needed and can be discarded, similar to crossing out notes you no longer need.\n\n2. **Add Gate (Input Gate)**: This gate identifies which new information is important and should be added to the memory, much like when you underline new critical information in your notes.\n\n3. **Output Gate**: This gate determines what information from the memory should be used in the current processing step, akin to picking the right points from your notes to answer a question.\n\nThese gates work together to ensure that the model retains relevant context over time, allowing LSTMs to handle complex language tasks effectively. This makes them a powerful tool in natural language processing tasks, where understanding and context over sequences are crucial."
                }
            },
            "claude": {
                "L": {
                    "L_claude_prompt2": "Think of language processing like reading a long story. Just as we humans need to remember important details from earlier chapters while forgetting irrelevant ones, computers need a similar ability when processing language.\n\nTraditional systems (RNNs) are like readers with very short-term memory - they mainly focus on what they've just read, struggling to connect it with information from many pages ago. This is similar to how we might forget crucial plot details from chapter one when we're deep into chapter ten.\n\nThis is where LSTM networks come in - think of them as a skilled reader with a notepad (the context layer). Just as you might jot down important points while reading a complex text, LSTMs have a system of \"gates\" that work like your decision-making process when taking notes:\n\n- The forget gate is like deciding which notes to erase from your notepad because they're no longer relevant\n- The add gate is similar to deciding what new information is worth writing down\n- The output gate is like choosing which of your notes are relevant for understanding the current paragraph\n\nJust as linguists use morphemes and phonemes as building blocks of language, these gates are the building blocks that help computers maintain long-term understanding of text. They work together like the various levels of language analysis (phonological, syntactic, semantic) to create a comprehensive understanding of the input.\n\nThis system allows computers to process language more like humans do, maintaining important context while discarding irrelevant information, much like how we maintain coherence when analyzing discourse structure."
                },
                "CL": {
                    "CL_claude_prompt2": "In computational linguistics, we often work with sequential data, similar to analyzing sentences word by word. Traditional RNNs (Recurrent Neural Networks) struggle with long-distance dependencies, much like how it's difficult to maintain coherence across a long discourse.\n\nThink of an RNN as a linguist trying to analyze a very long sentence: they might focus too much on recent words and lose track of important information from the beginning. This is analogous to the \"vanishing gradients problem\" - where earlier information becomes increasingly diluted, much like how the significance of earlier discourse markers might get lost in a long text.\n\nLSTM (Long Short-Term Memory) networks solve this by implementing a system similar to discourse management in human language processing. Just as we maintain relevant context while discarding irrelevant information during discourse analysis, LSTMs have specialized mechanisms called \"gates\" that manage information flow:\n\n- The forget gate works like cognitive filtering in discourse processing - it determines which previous context should be discarded, similar to how we stop considering resolved anaphoric references.\n- The add gate is comparable to introducing new discourse referents - it decides what new information should be incorporated into the ongoing context.\n- The output gate functions like information packaging in linguistics - it selects which parts of the stored context are relevant for the current analysis, similar to how we select relevant information when resolving ambiguities.\n\nThese mechanisms work together much like the way we handle discourse coherence, maintaining both local and global context while processing language sequentially."
                }
            },
            "deepseek": {
                "L": {
                    "L_deepseek_prompt3": "**Adapted Explanation:**  \nTraining language models to handle long-distance dependencies—like connecting a pronoun to its antecedent several clauses back—is similar to tracking grammatical agreement across complex sentences. Basic recurrent neural networks (RNNs) struggle here, behaving like a listener who only retains the last few words of a sentence. Their \"working memory\" (hidden states) fades quickly, losing track of earlier context—a technical flaw called *vanishing gradients*, akin to a syntactic dependency chain breaking over distance.  \n\nLSTMs fix this by mimicking how discourse structure manages context. Imagine an editor revising a text:  \n1. **Forget Gate:** Erases irrelevant details (e.g., discarding a resolved pronoun after its referent is no longer needed).  \n2. **Add Gate:** Highlights new, crucial information (e.g., noting a verb’s tense for future agreement).  \n3. **Output Gate:** Decides what to prioritize in the current \"working memory\" (like foregrounding the subject in a passive construction).  \n\nThese \"gates\" operate like syntactic rules, learned through exposure to data, to maintain coherence. Just as you parse a sentence by integrating case markers or word order, LSTMs dynamically update their mental notepad (context layer), ensuring distant elements stay linked—critical for tasks like semantic role labeling or narrative tracking."
                },
                "CL": {
                    "CL_deepseek_prompt4": "For NLP/CL practitioners: RNNs struggle to track long-range dependencies in sequences—like resolving coreference across paragraphs or modeling discourse structure—because their hidden states prioritize local context, and gradient signals diminish over time (vanishing gradients). LSTMs address this by introducing gated memory cells that dynamically manage context. Imagine a system with three neural \"filters\": a forget gate (pruning irrelevant information, e.g., outdated entity mentions), an input gate (updating context with salient features, like new discourse referents), and an output gate (focusing on task-relevant signals for the next prediction). These gates—controlled by learned weights—operate on the current input, previous hidden state, and cell state, forming an adaptive memory buffer. This architecture mirrors how transformers later used attention to scale context, but LSTMs achieved stepwise efficiency for sequential data, enabling early breakthroughs in machine translation and text generation."
                }
            }
        }
    },
    "T11": {
        "instance_code": "T11",
        "original_category": "CL",
        "original_text_title": "Naive Bayes Classifiers",
        "original_text": "In this section we introduce the multinomial naive Bayes classifier, so called because it is a Bayesian classifier that makes a simplifying (naive) assumption about how the features interact. [...] We represent a text document as if it were a bag of words, that is, an unordered set of words with their position ignored, keeping only their frequency in the document. [...] Naive Bayes is a probabilistic classifier, meaning that for a document d, out of all classes c ∈ C the classifier returns the class ˆc which has the maximum posterior probability given the document. […] we use the hat notation ˆ to mean “our estimate of the correct class”, and we use argmax to mean an operation that selects the argument (in this case the class c) that maximizes a function (in this case the probability P(c|d). cˆ = argmax c∈C P(c|d). [...] Thus, we can choose the class that maximizes this simpler formula: cˆ = argmax c∈C P(c|d) = argmax c∈C P(d|c)P(c). [...] we compute the most probable class ˆc given some document d by choosing the class which has the highest product of two probabilities: the prior probability of the class P(c) and the likelihood of the document P(d|c): cˆ = argmax c∈C P(d|c)(likelihood)P(c)(prior). [...] The first is the bag-of-words assumption discussed intuitively above: we assume position doesn't matter [...]. The second is commonly called the naive Bayes assumption: this is the conditional independence assumption that the probabilities P(fi|c) are independent given the class c and hence can be 'naively' multiplied as follows: P(f1, f2,...., fn|c) = P(f1|c)·P(f2|c)· ... ·P(fn|c). The final equation for the class chosen by a naive Bayes classifier is thus: cNB = argmax c∈C P(c) Yf∈F P(f|c). [...] . Classifiers that use a linear combination of the inputs to make a classification decision —like naive Bayes and also logistic regression—are called linear classifiers.",
        "selected_texts": {
            "mistral": {
                "L": {
                    "L_mistral_prompt3": "In this section, we discuss the multinomial naive Bayes classifier, a probabilistic method that makes a simplifying assumption about how words contribute to determining a text's category (class).\n\n     To simplify our analysis, we'll represent a text document as a collection of words, ignoring their order and focusing only on their frequency within the document.\n\n     As a probabilistic classifier, the multinomial naive Bayes classifier aims to predict the class (category) c out of all possible classes C for a given document d, by returning the class ˆc with the highest probability: cˆ = argmax c∈C P(c|d).\n\n     We use the hat notation ˆ to mean our estimate of the correct class, and we use argmax to mean an operation that selects the argument (in this case the class c) that maximizes a function (in this case the probability P(c|d).\n\n     To choose the class that best fits the document, we can maximize this simpler formula: cˆ = argmax c∈C P(c|d) = argmax c∈C P(d|c)P(c).\n\n     We determine the most probable class ˆc for a given document d by selecting the class with the highest product of two probabilities: the prior probability of the class P(c) and the likelihood of the document P(d|c). cˆ = argmax c∈C P(d|c)(likelihood)P(c)(prior).\n\n     The first part of this equation follows from the bag-of-words assumption, where we ignore word positions and assume that their order doesn't affect the text's category.\n\n     The second part is known as the naive Bayes assumption: it states that the probabilities P(fi|c) (where fi represents a word frequency in a document) are independent given the class c, allowing us to naively multiply these probabilities as follows: P(f1, f2, ..., fn|c) = P(f1|c)·P(f2|c)·...·P(fn|c).\n\n     The final equation for the class chosen by a naive Bayes classifier is thus: cNB = argmax c∈C P(c) Yf∈F P(f|c).\n\n     Classifiers that use a linear combination of the inputs to make a classification decision, like naive Bayes and logistic regression, are called linear classifiers."
                },
                "CS": {
                    "CS_mistral_prompt1": "1. In this section, we'll discuss the Multinomial Naive Bayes classifier, a Bayesian classifier that inherently simplifies interactions between features assuming they are independent.\n\n    2. To represent a text document, we treat it as a bag of words — an unordered collection of words with their positions ignored, preserving only their frequencies within the document.\n\n    3. As a probabilistic classifier, the Multinomial Naive Bayes classifier aims to find the class ˆc with the highest probability given a document d, among all classes c ∈ C.\n\n    4. Using the hat notation ˆ to signify our estimated correct class, and argmax to denote selecting the argument that maximizes a function, we write this as: cˆ = argmax c∈C P(c|d).\n\n    5. Essentially, we select the class that maximizes this simplified formula: cˆ = argmax c∈C P(c|d) = argmax c∈C P(d|c)P(c).\n\n    6. To determine the most probable class ˆc for a given document d, we choose the class with the highest product of two probabilities: the prior probability of the class P(c) and the likelihood of the document P(d|c). This can be written as: cˆ = argmax c∈C P(d|c)(likelihood)P(c)(prior).\n\n    7. The Multinomial Naive Bayes classifier makes two main assumptions:\n        a. The bag-of-words assumption, which states that word positions don't matter.\n        b. The naive Bayes assumption, which assumes that the probabilities P(fi|c) are independent given the class c and can be multiplied as follows: P(f1, f2,...., fn|c) = P(f1|c)·P(f2|c)· ... ·P(fn|c). The final equation for the class chosen by a naive Bayes classifier is thus: cNB = argmax c∈C P(c) Yf∈F P(f|c).\n\n    8. Classifiers that employ a linear combination of inputs to make a classification decision, like naive Bayes and logistic regression, are known as linear classifiers."
                }
            },
            "llama": {
                "L": {
                    "L_llama_prompt1": "Imagine we are analyzing a collection of texts to determine their authorship, and we want to categorize them into different genres, such as fiction or non-fiction. To achieve this, we use a statistical approach called the multinomial naive Bayes classifier. This method is \"naive\" because it simplifies the interactions between the features in our texts, much like how we simplify the rules of language when describing them in terms of phonemes or morphemes.\n\n    Think of each text as a \"bag of words,\" where the order of the words doesn't matter, only their frequency. This is similar to how we analyze the distribution of phonemes in a language, where the order of phonemes can change the meaning, but their frequency can give us clues about the language's structure. In this case, we're looking at the frequency of words to understand the genre of the text.\n\n    The multinomial naive Bayes classifier is a probabilistic approach, meaning it calculates the probability of a text belonging to a particular genre based on the words it contains. It's like trying to guess the meaning of a sentence based on the semantics of the individual words, without considering their syntactic structure. The classifier returns the genre with the highest probability, which we can think of as the \"most likely\" genre.\n\n    To calculate this probability, we use a formula that takes into account two things: the prior probability of a genre (how likely it is to occur in general) and the likelihood of the text given that genre (how well the words in the text fit with that genre). This is similar to how we use context to disambiguate words with multiple meanings, where the prior probability of a word's meaning is influenced by the surrounding words.\n\n    The \"naive\" part of the classifier comes from the assumption that the probabilities of each word occurring in a text are independent of each other, given the genre. This means we can multiply the probabilities of each word together to get the overall probability of the text. It's like assuming that the meaning of a sentence is the sum of the meanings of its individual words, without considering how they interact with each other.\n\n    For example, if we're trying to classify a text as fiction or non-fiction, we might look at the frequency of words like \"once upon a time\" or \"in conclusion.\" If a text contains many instances of \"once upon a time,\" it's likely to be fiction, while a text with many instances of \"in conclusion\" is likely to be non-fiction. The classifier would calculate the probability of each genre based on these word frequencies and return the genre with the highest probability.\n\n    This approach is called a \"linear classifier\" because it uses a linear combination of the inputs (in this case, the word frequencies) to make a classification decision. It's a simple yet powerful tool for analyzing and categorizing texts, and it has many applications in linguistics and natural language processing."
                },
                "CS": {
                    "CS_llama_prompt5": "The multinomial naive Bayes classifier is similar to a decision tree in machine learning, where each node represents a feature or attribute. However, in naive Bayes, we make a simplifying assumption that each feature is conditionally independent of the others given the class. Think of it like a bag of words, where the order of words doesn't matter, just their frequency. This is analogous to a term frequency-inverse document frequency (TF-IDF) vector in text analysis, where each word is represented by its frequency and importance in the document. \n\n    To classify a document, the naive Bayes algorithm calculates the posterior probability of each class given the document, and returns the class with the highest probability. This is similar to the argmax function in optimization problems, where we select the argument that maximizes a function. In this case, the function is the probability of the class given the document, P(c|d). We can simplify this calculation by using Bayes' theorem, which states that P(c|d) = P(d|c)P(c). \n\n    The naive Bayes assumption is that the features are conditionally independent given the class, which means we can multiply the probabilities of each feature given the class. This is like calculating the probability of a sequence of independent events, where the probability of the sequence is the product of the probabilities of each event. The final equation for the class chosen by a naive Bayes classifier is: cNB = argmax c∈C P(c) ∏f∈F P(f|c). \n\n    This type of classifier is called a linear classifier, because it uses a linear combination of the inputs to make a classification decision. Other examples of linear classifiers include logistic regression. The key idea is that the decision boundary is a hyperplane, which is a linear subspace that separates the classes. In the case of naive Bayes, the decision boundary is defined by the product of the probabilities of each feature given the class. \n\n    For example, consider a spam filter that uses naive Bayes to classify emails as spam or not spam. The features might be the frequency of certain words in the email, such as \"free\" or \"discount\". The naive Bayes algorithm would calculate the probability of each class given the email, and return the class with the highest probability. If the probability of the email being spam given the features is higher than the probability of it not being spam, then the email is classified as spam. \n\n    In summary, the multinomial naive Bayes classifier is a type of linear classifier that uses a simplifying assumption of conditional independence to calculate the posterior probability of each class given a document. It is similar to other linear classifiers, such as logistic regression, and is commonly used in text classification tasks, such as spam filtering."
                }
            },
            "gpt4o": {
                "L": {
                    "L_gpt4o_prompt1": "In this section, we introduce the multinomial naive Bayes classifier. It is a tool used to categorize documents by using probabilities. The classifier is called \"naive\" because it makes a simplifying assumption about the independence of features—in this case, words in a document.\n\nImagine a text document as a \"bag of words.\" This means we consider the words without worrying about their order, just how often each word appears. This method is like focusing on the frequency of words, ignoring their placement in sentences.\n\nNaive Bayes is a probabilistic classifier. It means that for any given document \\(d\\), it predicts the class \\( \\hat{c} \\) from a set of possible classes \\( C \\), where \\( \\hat{c} \\) is the class with the highest probability of being correct based on the document. The symbol \\( \\hat{c} \\) indicates our estimate of the correct class. \n\nTo determine \\( \\hat{c} \\), we use a function called \"argmax,\" which helps us find the class that maximizes the probability. Specifically, we calculate:\n\n\\[ \\hat{c} = \\arg\\max_{c \\in C} P(c|d) \\]\n\nThis formula tells us to choose the class \\( c \\) that makes \\( P(c|d) \\) largest. We simplify this to:\n\n\\[ \\hat{c} = \\arg\\max_{c \\in C} P(d|c)P(c) \\]\n\nHere, \\( P(d|c) \\) is the likelihood of the document given the class, and \\( P(c) \\) is the prior probability of the class. We select the class with the highest product of these probabilities.\n\nThe naive Bayes classifier operates under two key assumptions:\n\n1. **Bag-of-words assumption:** The position of words in a document is irrelevant. We only care about their frequency.\n   \n2. **Conditional independence assumption:** Given a class \\( c \\), the occurrence of each word is independent of the others. This means we can multiply their probabilities:\n\n\\[ P(f_1, f_2, \\ldots, f_n|c) = P(f_1|c) \\cdot P(f_2|c) \\cdot \\ldots \\cdot P(f_n|c) \\]\n\nThe final equation for selecting the class in a naive Bayes classifier is:\n\n\\[ c_{\\text{NB}} = \\arg\\max_{c \\in C} P(c) \\prod_{f \\in F} P(f|c) \\]\n\nClassifiers like naive Bayes, which use a linear combination of input features to decide the class, are known as linear classifiers. Another example of a linear classifier is logistic regression."
                },
                "CS": {
                    "CS_gpt4o_prompt4": "The multinomial naive Bayes classifier is a type of probabilistic model used for classification tasks. It's called \"naive\" because it assumes that the features in a dataset are independent of each other given the class, which simplifies the calculations. This model is particularly useful for text classification, where we treat a document as a \"bag of words.\" This means we consider only the frequency of each word in the document, ignoring their order.\n\nIn practice, for any given document \\(d\\), the classifier seeks to identify the class \\( \\hat{c} \\) that maximizes the probability \\( P(c|d) \\). This is achieved using the argmax operation, which finds the class \\(c\\) that maximizes the function. The key probability formula is \\( \\hat{c} = \\arg\\max_{c \\in C} P(d|c)P(c) \\), where \\(P(c)\\) is the prior probability of class \\(c\\), and \\(P(d|c)\\) is the likelihood of document \\(d\\) given class \\(c\\).\n\nThe naive Bayes assumption allows us to treat each word's contribution to the class probability as independent, meaning we can multiply each word's probability given the class: \\( P(f_1|c) \\cdot P(f_2|c) \\cdot \\ldots \\cdot P(f_n|c) \\). This assumption simplifies computations considerably. Naive Bayes, along with logistic regression, falls under the category of linear classifiers because they both use a linear combination of input features to make decisions."
                }
            },
            "claude": {
                "L": {
                    "L_claude_prompt1": "Let me explain how the Naive Bayes classifier works, using concepts familiar to linguistics students.\n\nJust as we analyze texts without considering word order in discourse analysis, the Naive Bayes classifier treats a text as a \"bag of words\" - imagine dumping all the words from a text into a bag, where only their frequency matters, not their position or syntax.\n\nThink of this classifier as similar to how we categorize language families: we look for specific features to determine which family a language belongs to. In this case, we're trying to determine which category (or class) a text belongs to based on its features (words).\n\nThe classifier works with two key linguistic probabilities:\n1. The \"prior probability\" - how common a category is (like how common Romance languages are among world languages)\n2. The \"likelihood\" - how likely specific features are in that category (like how likely certain phonemes are in Romance languages)\n\nThe \"naive\" part comes from a simplifying assumption: it treats each feature as independent, similar to how we might initially analyze phonemes in isolation, even though we know they influence each other through processes like assimilation.\n\nThe classifier multiplies these probabilities together to make its decision, choosing the category with the highest probability score. It's \"linear\" because it combines these features in a straightforward way, much like how we might add up distinctive features to identify a phoneme.\n\nThis method is called \"Bayesian\" because it uses probability theory to make predictions, similar to how historical linguists use probability to reconstruct proto-languages based on existing evidence."
                },
                "CS": {
                    "CS_claude_prompt2": "Here's a CS-friendly explanation of Multinomial Naive Bayes Classification:\n\nThink of Multinomial Naive Bayes as a probabilistic algorithm similar to how you'd implement a simple recommendation system. It's a linear classifier that makes classification decisions based on feature vectors, much like how you'd classify data points in machine learning.\n\nThe algorithm treats a document as an array of word frequencies, ignoring their sequential order (similar to converting a sequence into a hashmap of counts). This representation is called a \"bag of words\" - imagine dumping all words into an unordered data structure where only frequency matters.\n\nThe core algorithm follows this pattern:\n1. For input document d, we want to find class c that maximizes P(c|d)\n2. Using Bayes' theorem, we optimize: argmax(P(d|c) * P(c))\n   where P(c) is our prior probability (like class distribution in training data)\n   and P(d|c) is our likelihood\n\nThe \"naive\" part comes from a simplifying assumption in our probability calculation: we treat features (words) as independent variables, similar to treating array elements as independent in parallel processing. This allows us to multiply individual feature probabilities instead of calculating complex dependencies:\n\nP(features|class) = P(feature1|class) * P(feature2|class) * ... * P(featureN|class)\n\nThe final implementation reduces to:\nclassifierOutput = argmax(P(class) * product(P(feature|class)))\n\nThis makes it computationally efficient, O(n) complexity, similar to linear search algorithms you're familiar with. Like other linear classifiers (logistic regression), it creates a decision boundary through feature space using a linear combination of inputs."
                }
            },
            "deepseek": {
                "L": {
                    "L_deepseek_prompt1": "**Adapted Explanation for Linguistics Students:**\n\nThe **multinomial naive Bayes classifier** is a tool for categorizing texts by calculating the most probable linguistic category (like \"syntax\" vs. \"phonetics\") based on word patterns. Here’s how it aligns with your linguistic knowledge:\n\n1. **Bag-of-Words Model**:  \n   Texts are analyzed like a *frequency lexicon*—we ignore word order (similar to studying a corpus’s vocabulary distribution) and focus only on how often words appear. Think of it as creating a histogram of word tokens, detached from syntax.\n\n2. **Bayesian Logic**:  \n   The classifier combines two linguistic intuitions:  \n   - **Prior Probability (P(c))**: The baseline likelihood of a category *before* seeing the text (e.g., \"phonetics\" might be rare in a general corpus).  \n   - **Likelihood (P(d|c))**: How probable the document’s word frequencies are *if* it belongs to category *c* (e.g., a text with \"allophone\" and \"formant\" is likelier in a phonetics category).  \n\n3. **Naive Assumption**:  \n   Like analyzing morphemes independently of their morphological context, we assume **words are statistically independent** within a category (even though this oversimplifies semantic/pragmatic relationships). This lets us multiply individual word probabilities:  \n   *P(\"morphology\", \"syntax\" | c) ≈ P(\"morphology\"|c) · P(\"syntax\"|c)*.  \n\n4. **Classification Rule**:  \n   The chosen category **ĉ** maximizes:  \n   *ĉ = argmaxₐ [P(c) × Π P(word|c)]*  \n   (Here, **argmax** selects the category with the highest score, like identifying a language family from diagnostic features.)  \n\n**Example**: To classify a paper as \"syntax\" or \"semantics\":  \n- If \"theta role\" and \"binding theory\" appear frequently, their combined probabilities under \"syntax\" (weighted by the prior likelihood of syntax papers) might outweigh \"semantics\".  \n\n**Why \"Linear Classifier\"?**  \nIt combines word evidence *additively* (like summing phonetic cues in speech perception), making it efficient but blind to word-order interactions.  \n\nThis method prioritizes computational simplicity over nuanced linguistic structure—useful for rapid text categorization, even if it ignores your field’s deeper relational analyses."
                },
                "CS": {
                    "CS_deepseek_prompt1": "Here's the CS-focused explanation:\n\nThe multinomial naive Bayes classifier is a probabilistic algorithm for categorizing data points (like text documents). Think of it as an optimized way to compute the most likely class label by combining prior knowledge with observed feature patterns.\n\nKey mechanics:\n\n1. **Data Representation**: Documents are encoded as *feature vectors* counting word frequencies (like a hashmap storing word:count pairs). Positional info is discarded - similar to building a histogram of tokens.\n\n2. **Bayesian Core**: For a document **d**, we calculate:\n   ĉ = argmax[P(c) * Π P(wordᵢ|c)]  \n   Where:\n   - P(c) = prior probability (class frequency in training data)\n   - P(wordᵢ|c) = likelihood (word frequency in class c)\n\n3. **Naive Assumption = Conditional Independence**: Features (words) are treated as independent given the class - like assuming pixels in an image classifier don't spatially interact. This allows efficient probability multiplication instead of calculating complex joint probabilities.\n\n4. **Optimization Perspective**: The argmax operation makes this an optimization problem - we're finding the class that maximizes the product of prior and likelihood terms. In practice, we use log probabilities to convert products to sums (avoiding underflow, similar to numerical stability in neural networks).\n\n5. **Linear Classification**: Like logistic regression, this is a linear classifier - decision boundaries are hyperplanes in the feature space. The \"linearity\" becomes clear when taking logs:  \n   log(P(c|d)) ∝ log(P(c)) + Σ count(wordᵢ) * log(P(wordᵢ|c))\n\nExample: Spam detection  \n- Features = word counts  \n- P(spam) = % of spam emails in training set  \n- P(\"free\"|spam) = frequency of \"free\" in spam vs non-spam  \n- New email classified by multiplying all word probabilities with prior\n\nThis approach is computationally efficient (O(n) complexity) and works well with high-dimensional data, making it popular for NLP tasks despite its simplifying assumptions."
                }
            }
        }
    },
    "T12": {
        "instance_code": "T12",
        "original_category": "CL",
        "original_text_title": "Vector Semantics",
        "original_text": "Vector semantics is the standard way to represent word meaning in NLP, helping us model many of the aspects of word meaning we saw in the previous section. [...] to define the meaning of a word by its distribution in language use, meaning its neighboring words or grammatical environments. Their idea was that two words that occur in very similar distributions (whose neighboring words are similar) have similar meanings. [...] The idea of vector semantics is to represent a word as a point in a multidimensional semantic space that is derived (in ways we'll see) from the distributions of embeddings word neighbors. Vectors for representing words are called embeddings, [...] two most commonly used models. In the tf-idf model, an important baseline, the meaning of a word is defined by a simple function of the counts of nearby words. We will see that this method results in very long vectors that are sparse, i.e. mostly zeros (since most words simply never occur in the context of others).[…] the word2vec model family for constructing short, dense vectors that have useful semantic properties.",
        "selected_texts": {
            "mistral": {
                "L": {
                    "L_mistral_prompt4": "In the realm of natural language processing, vector semantics is the predominant approach for conveying word meaning. This method allows us to capture various aspects of word meaning discussed earlier, primarily by considering the words surrounding a given term and its grammatical context. The underlying assumption is that words with similar distributions of surrounding words or contexts have similar meanings.\n\n    To represent words as points in a multi-dimensional semantic space, the concept of vector semantics is employed. In this approach, words are represented as vectors, referred to as embeddings.\n\n    Among the popular models for representing words as embeddings, two stand out: the tf-idf model and the word2vec model family. The former, which serves as a significant baseline, defines word meaning through a simple function of the counts of nearby words. However, it results in lengthy vectors that are sparse, meaning they are dominated by zeros due to the infrequency of most words appearing in the context of others.\n\n    Conversely, the word2vec model family generates compact, dense vectors with beneficial semantic properties."
                },
                "CS": {
                    "CS_mistral_prompt5": "1. Vector semantics is a standard technique to represent the meaning of words in Natural Language Processing (NLP), enabling us to model various aspects of word meanings as discussed earlier. This method defines a word's meaning based on its occurrence within the language, including its neighboring words or grammatical environments. The underlying idea is that words with similar distributions (sharing similar neighboring words) have similar meanings.\n\n    2. Vector semantics represents a word as a point in a multi-dimensional semantic space, which is derived from the distributions of its neighboring words, also known as embeddings.\n\n    3. There are two widely used models for this purpose: the tf-idf model and the word2vec model family. The tf-idf model is a basic method that defines a word's meaning by a simple function of the counts of its neighboring words. However, this method results in lengthy and sparse vectors, as most words rarely occur in the context of others.\n\n    4. In contrast, the word2vec model family constructs shorter, dense vectors that possess useful semantic properties. These vectors are more compact and contain fewer zeros compared to the tf-idf model."
                }
            },
            "llama": {
                "L": {
                    "L_llama_prompt3": "Vector semantics in Natural Language Processing (NLP) works similarly to how you analyze the meaning of a word in a sentence by looking at its neighboring words and grammatical context. Just as you use syntax and semantics to understand how words relate to each other, vector semantics represents word meaning as a point in a multidimensional space based on the words that surround it and the grammatical environments it appears in. \n\n    This approach is rooted in the idea that words with similar meanings tend to appear in similar contexts, much like how words with similar phonetic properties may be pronounced similarly. For instance, just as the sounds /p/ and /b/ are distinct but related, words like \"run\" and \"sprint\" have distinct but related meanings because they often appear in similar distributions - for example, both can be followed by words like \"fast\" or \"quickly\". \n\n    To capture these relationships, vector semantics uses \"embeddings\" - a way of representing words as vectors that can be mathematically manipulated. Think of it like mapping the phonetic features of a sound onto a multidimensional space, where each dimension represents a different aspect of the sound. Similarly, in vector semantics, each dimension in the vector space represents a different aspect of word meaning. \n\n    There are different models for creating these embeddings, such as the tf-idf model, which looks at the frequency of nearby words to define a word's meaning. However, this approach can result in very long and sparse vectors, much like how a detailed phonetic transcription might include many silent or unused features. \n\n    A more efficient approach is the word2vec model family, which generates shorter, denser vectors that capture useful semantic properties. This is similar to how a simplified phonetic transcription can still convey the essential features of a sound, allowing for more effective comparison and analysis. By using vector semantics and these models, NLP can better understand and work with the complex meanings of words in language. \n\n### START OF OUTPUT ###"
                },
                "CS": {
                    "CS_llama_prompt2": "In the realm of NLP, vector semantics is a fundamental approach for representing word meaning, allowing us to model various aspects of word meaning we've encountered. Drawing inspiration from algorithms like collaborative filtering, the core idea is to define a word's meaning by its distribution in language use, i.e., its neighboring words or grammatical contexts. This concept is analogous to how we might cluster similar data points in a k-means algorithm, where words with similar distributions (i.e., similar neighboring words) tend to have similar meanings. \n\n    In vector semantics, each word is represented as a point in a multidimensional semantic space, derived from the distributions of its neighboring words. These word representations are referred to as embeddings. Think of embeddings as high-dimensional vectors, similar to how we represent data points in a clustering algorithm. Two popular models for constructing these embeddings are the tf-idf model and the word2vec model family. \n\n    The tf-idf model, a baseline approach, defines a word's meaning using a simple function of the counts of nearby words. However, this method results in long, sparse vectors (mostly zeros) since most words rarely co-occur. On the other hand, the word2vec model family constructs shorter, denser vectors with useful semantic properties, much like how dimensionality reduction techniques (e.g., PCA) can help reveal underlying patterns in high-dimensional data. By leveraging these vector semantics concepts, we can unlock powerful NLP capabilities, enabling applications like text classification, sentiment analysis, and language translation. \n\n    To illustrate this concept further, consider a scenario where you're building a recommendation system, and you want to recommend articles to users based on their reading history. You can represent each article as a vector in a high-dimensional space, where similar articles are closer together. Similarly, in vector semantics, we represent words as vectors in a semantic space, where semantically similar words are closer together. This allows us to capture nuanced relationships between words and perform tasks like text classification and language translation more effectively. \n\n    By understanding vector semantics and how it represents word meaning, you'll be better equipped to harness the power of NLP in your applications, whether it's building a chatbot, sentiment analysis tool, or language translation system. The key takeaway is that vector semantics provides a robust framework for representing word meaning, enabling you to build more sophisticated and accurate NLP models. \n\n    In essence, vector semantics is a powerful tool for NLP, allowing us to represent word meaning in a way that's both intuitive and computationally efficient. By grasping this concept, you'll be well on your way to building innovative NLP applications that can understand and generate human-like language. \n\n    To recap, vector semantics is a fundamental concept in NLP that represents word meaning as points in a multidimensional semantic space. This allows us to model various aspects of word meaning and perform tasks like text classification, sentiment analysis, and language translation. By leveraging vector semantics, you can build more accurate and sophisticated NLP models, unlocking a wide range of applications in areas like customer service, language translation, and text analysis. \n\n    In conclusion, vector semantics is a crucial component of NLP, providing a robust framework for representing word meaning. By understanding this concept, you'll be able to build more effective NLP models, enabling you to tackle complex tasks like language translation, text classification, and sentiment analysis. With vector semantics, you'll be well-equipped to unlock the full potential of NLP in your applications, driving innovation and growth in the field. \n\n    As a final note, the concept of vector semantics has far-reaching implications for NLP, enabling us to build more sophisticated and accurate models. By grasping this concept, you'll be at the forefront of NLP innovation, poised to drive breakthroughs in areas like language translation, text analysis, and customer service. With vector semantics, the possibilities are endless, and the future of NLP looks brighter than ever. \n\n    To drive the point home, consider the analogy of a graph, where each node represents a word, and the edges represent the relationships between them. In vector semantics, we can represent each word as a vector in a high-dimensional space, where the proximity of vectors represents the similarity between words. This allows us to capture nuanced relationships between words and perform tasks like text classification and language translation more effectively. \n\n    In the context of NLP, vector semantics is a game-changer, providing a robust framework for representing word meaning. By understanding this concept, you'll be able to build more accurate and sophisticated NLP models, unlocking a wide range of applications in areas like customer service, language translation, and text analysis. With vector semantics, you'll be well-equipped to tackle complex tasks like language translation, text classification, and sentiment analysis, driving innovation and growth in the field. \n\n    To further illustrate the concept, consider a scenario where you're building a language translation system. You can represent each word in the source language as a vector in a high-dimensional space, and then use these vectors to find the"
                }
            },
            "gpt4o": {
                "L": {
                    "L_gpt4o_prompt5": "Vector semantics is a method used in Natural Language Processing (NLP) to understand word meanings by examining their contexts in language. This approach is somewhat similar to examining how words function within different grammatical structures or how they appear alongside other words. The core idea is that words sharing similar contexts tend to have similar meanings, much like how synonyms often appear in similar syntactic structures.\n\nImagine representing each word as a point in a large, complex space, akin to a phonetic or syntactic space but based on meaning. This space is defined by the various contexts words appear in, much like how we understand a word's function by its syntactic role. These points, or vectors, are called embeddings. \n\nIn the tf-idf model, we quantify a word's meaning by counting how often other words appear alongside it, similar to how we might categorize phonetic features by frequency. However, this results in large, sparse vectors, filled mostly with zeros, as many words rarely co-occur. Think of it like a syntactic analysis where most possible combinations don't occur in actual speech.\n\nOn the other hand, the word2vec model creates shorter, dense vectors that capture more nuanced semantic relationships, much like how understanding the subtleties of phonetic variation can reveal more about pronunciation than broad categorizations."
                },
                "CS": {
                    "CS_gpt4o_prompt3": "Vector semantics is a technique in Natural Language Processing (NLP) used to represent the meaning of words in a way that's computationally manageable. For those familiar with computer science, think of it as a method to encode words into vectors, similar to how you might represent data points in a machine learning feature space.\n\nThe core idea here is that a word's meaning can be inferred from the company it keeps—essentially, the words that frequently appear around it in sentences. If two words often appear in similar contexts, they likely have similar meanings. This approach allows us to create a multidimensional space where each word is a point, known as an embedding.\n\nIn practice, there are different models to generate these embeddings. One foundational model is tf-idf (term frequency-inverse document frequency), which calculates the importance of a word based on how often it appears near other words. However, this method often results in long vectors that are mostly zeros—sparse vectors—because many words rarely occur together.\n\nA more efficient approach is the word2vec model. It generates shorter, dense vectors that capture semantic properties more effectively, making them useful for various NLP tasks. These dense vectors are akin to compressed, feature-rich representations of words, much like how you might optimize data storage in programming."
                }
            },
            "claude": {
                "L": {
                    "L_claude_prompt2": "Words in language are like stars in a constellation - they gain meaning through their relationships with other words around them. Just as linguists study how words connect in sentences and how meaning emerges from context, vector semantics maps words into a special kind of space where similar words cluster together.\n\nThink of it like creating a detailed map of word relationships. When two words frequently appear in similar contexts - like how \"happy\" and \"joyful\" often appear near similar words - we consider their meaning to be related. This is similar to how you might analyze collocations or semantic fields in linguistics.\n\nThere are two main ways to create these word maps. The first (tf-idf) is like creating an extensive dictionary where we record every time words appear near each other. This creates a very detailed but sparse record - like having a huge chart where most spaces are empty because most words don't commonly appear together.\n\nThe second method (word2vec) is more like creating a concise semantic feature matrix, similar to the distinctive feature matrices used in phonology. It captures the essential relationships between words in a more compact form, making it easier to work with while maintaining the important meaning connections.\n\nThese mathematical representations help computers understand language relationships in a way that mirrors how linguists analyze semantic networks and word associations."
                },
                "CS": {
                    "CS_claude_prompt1": "Let me explain vector semantics like we're discussing a programming concept:\n\nIn NLP, vector semantics is similar to how we represent data points in machine learning - but instead of features like height or weight, we're mapping words into a mathematical space. Think of it as creating a coordinate system for meaning.\n\nJust like how you'd represent a point in a graph using (x,y) coordinates, we represent words as vectors in a high-dimensional space. The key insight is that words with similar meanings will cluster together in this space, much like how similar data points cluster in k-means clustering.\n\nThe two main approaches to creating these word vectors are:\n\n1. tf-idf (term frequency-inverse document frequency): Similar to a sparse matrix in data structures, where most elements are zero. Imagine a huge array where each position represents a possible neighboring word, and most positions are 0 because most words don't appear together. While simple to implement, it's computationally expensive due to its size.\n\n2. word2vec: Think of this as the optimized version, similar to how we use dimensionality reduction in ML. Instead of sparse vectors, it creates dense, compact vectors (like converting a large sparse matrix into a smaller, efficient representation) that preserve semantic relationships. These vectors enable operations like: vector(\"king\") - vector(\"man\") + vector(\"woman\") ≈ vector(\"queen\").\n\nThis representation allows us to apply familiar mathematical operations and machine learning algorithms to process and analyze text, just like we do with numerical data in classification or clustering tasks."
                }
            },
            "deepseek": {
                "L": {
                    "L_deepseek_prompt4": "**Adapted Explanation:**  \nVector semantics operationalizes the distributional hypothesis—familiar from structural linguistics—by encoding a word’s meaning numerically based on its habitual co-occurrence patterns. Just as phonemes are defined by their positional contrasts, words are mapped into a geometric space where proximity reflects semantic similarity: words sharing collocational habitats (like \"river\" and \"bank\") cluster together. Two methods dominate. The first, akin to a frequency-tabulated concordance, generates sparse vectors (mostly zeros), mirroring exhaustive but inefficient lexical inventories. The second, like extracting distinctive features in phonology, produces dense vectors that compactly capture core semantic dimensions (e.g., \"king\" – \"man\" + \"woman\" ≈ \"queen\"). Both transform Firth’s axiom—“a word is known by the company it keeps”—into measurable spatial relationships.  \n\n**Rationale:**  \n- **Linguistic hooks:** References to structuralism (Firth), phonology (contrastive features), and semantics (collocations) align with the jury’s expertise.  \n- **Analogy:** Comparing sparse vectors to concordances and dense vectors to phonological features bridges technical and familiar concepts.  \n- **Example:** Arithmetic on embeddings (king → queen) mirrors componential analysis in semantics.  \n- **Avoids jargon:** \"Embeddings\" become \"numerical vectors\"; \"tf-idf\" is reframed via concordances.  \n- **Flow:** Links distributionalism → methods → linguistic parallels, ensuring coherence."
                },
                "CS": {
                    "CS_deepseek_prompt3": "Vector semantics is like feature engineering for words in NLP – we represent each word as a numerical vector that captures its \"meaning\" based on usage patterns. Think of it as a data-driven approach: a word's meaning is defined by its *context* (the words it typically appears near), similar to how a node in a graph is defined by its neighbors.  \n\nTwo key implementations:  \n1. **TF-IDF (baseline model):** Builds word vectors using weighted word counts from documents. This creates high-dimensional sparse vectors (like a one-hot encoded array where most elements are zero) because most words rarely co-occur.  \n2. **Word2Vec (neural approach):** Uses machine learning to compress these sparse vectors into shorter, dense embeddings (like dimensionality reduction). These embeddings act as learned feature sets – words with similar contexts cluster in vector space, enabling semantic math (e.g., \"king\" – \"man\" + \"woman\" ≈ \"queen\").  \n\nThe core principle mirrors ML feature learning: words are defined by their statistical relationships, not predefined rules. Just as CNNs learn image features, these models learn semantic patterns from raw text data."
                }
            }
        }
    },
    "T13": {
        "instance_code": "T13",
        "original_category": "CL",
        "original_text_title": "Cosine Similarity",
        "original_text": "To measure similarity between two target words v and w, we need a metric that takes two vectors [...] and gives a measure of their similarity. By far the most common similarity metric is the cosine of the angle between the vectors. The cosine—like most measures for vector similarity used in NLP—is based on the dot product operator from linear algebra, also called the inner product [...]. The dot product acts as a similarity metric because it will tend to be high just when the two vectors have large values in the same dimensions. Alternatively, vectors that have zeros in different dimensions—orthogonal vectors—will have a dot product of 0, representing their strong dissimilarity. [...]  The raw dot product thus will be higher for frequent words. [...] we'd like a similarity metric that tells us how similar two words are regardless of their frequency. [...] normalized dot product [...] a ·b / |a||b| = cosθ.",
        "selected_texts": {
            "mistral": {
                "L": {
                    "L_mistral_prompt4": "In linguistics, to gauge the resemblance between two specific words, v and w, we require a measurement that accepts two vector representations and delivers an indication of their similarity. Typically, the most widely applied similarity measure is the cosine of the vectors' angle. This cosine, like the majority of vector similarity measures in NLP, is built upon the dot product operator from linear algebra, alternatively referred to as the inner product. The dot product functions as a similarity metric because it tends to yield high values when the two vectors share substantial values in corresponding dimensions. Conversely, vectors with zeros in disparate dimensions—orthogonal vectors—will produce a dot product of 0, signifying their strong dissimilarity. Initially, the raw dot product will be more extensive for common words. However, to compare the similarity of two words irrespective of their frequency, we prefer a similarity metric. In this context, the normalized dot product, a ·b / |a||b|, is equivalent to the cosine and represents the cosine of the angle between the vectors (θ)."
                },
                "CS": {
                    "CS_mistral_prompt5": "1. To assess the similarity between words v and w, we require a metric that computes the angle between their vector representations. This metric, primarily, is the cosine of the angle between the vectors. The cosine function, similar to many NLP vector similarity measures, is grounded in the dot product operation from linear algebra, also known as the inner product. The dot product serves as a similarity measure because it tends to be high when the vectors have substantial values in the same dimensions. In contrast, vectors with zeros in different dimensions, or orthogonal vectors, have a dot product of 0, signifying their strong difference. The raw dot product will be greater for frequent words. However, we aim for a similarity metric that reveals the similarity of words disregarding their frequency. To achieve this, we resort to the normalized dot product, a · b / |a||b| = cosθ."
                }
            },
            "llama": {
                "L": {
                    "L_llama_prompt2": "When considering the similarity between two words, such as \"dog\" and \"cat,\" we need a way to measure how alike they are. In linguistics, you're familiar with comparing the phonetic, syntactic, and semantic properties of words. Think of our task as analogous to analyzing the distribution of phonemes in two different languages to identify patterns of similarity. \n\n    Just as you would look for commonalities in sound or structure, we use a mathematical concept called vectors to represent words in a high-dimensional space. The position of each word in this space is defined by its various features, much like how a word's meaning is defined by its semantic properties.\n\n    To measure the similarity between these word vectors, we use a metric that calculates the cosine of the angle between them. This is similar to comparing the overlap between two sets of phonetic features. The cosine metric is based on an operation called the dot product, which is akin to counting the number of matching phonemes between two words.\n\n    Imagine you're comparing the phonetic transcriptions of \"dog\" and \"cat.\" If they share many similar sounds, the dot product will be high, indicating a strong similarity. Conversely, if the words have few or no sounds in common, the dot product will be low, signifying a weak similarity.\n\n    However, just as word frequency can affect the analysis of linguistic patterns, the raw dot product can be influenced by the frequency of words. To account for this, we use a normalized dot product, which is calculated as the dot product divided by the product of the vector magnitudes. This is similar to normalizing the frequency of phonemes in a corpus to ensure that the analysis is not biased towards more common sounds.\n\n    The resulting metric, equivalent to the cosine of the angle between the vectors, provides a frequency-independent measure of similarity between words. This allows us to compare words like \"dog\" and \"cat\" based on their semantic properties, rather than their frequency of use. By using this metric, we can identify patterns of similarity between words that might not be immediately apparent through traditional linguistic analysis. \n\n    For example, we can use this metric to identify words that are semantically similar, such as \"big\" and \"large,\" even if they have different frequencies of use. This can be useful in a range of applications, from language translation to text summarization, where understanding the relationships between words is crucial. \n\n    In summary, by representing words as vectors and using the cosine metric to compare them, we can gain insights into the semantic properties of words and identify patterns of similarity that can inform a range of linguistic and computational tasks. \n\n    This approach can be seen as an extension of traditional linguistic analysis, where we use mathematical tools to uncover hidden patterns and relationships in language. By combining the strengths of linguistics and computer science, we can develop new methods for analyzing and understanding language, and apply them to real-world problems. \n\n    Ultimately, this can lead to a deeper understanding of language and its structure, and can inform the development of more sophisticated language technologies that can better capture the nuances and complexities of human language. \n\n    The cosine similarity metric is a powerful tool for analyzing the relationships between words, and has many potential applications in fields such as natural language processing, information retrieval, and machine learning. \n\n    By using this metric, we can uncover new insights into the nature of language and develop more effective methods for analyzing and understanding linguistic data. \n\n    This can lead to breakthroughs in areas such as language translation, sentiment analysis, and text classification, and can help to drive innovation in the field of natural language processing. \n\n    In conclusion, the cosine similarity metric is a valuable tool for analyzing the relationships between words, and has the potential to revolutionize the way we understand and work with language. \n\n    By leveraging the strengths of both linguistics and computer science, we can develop new methods and technologies that can better capture the complexities and nuances of human language, and apply them to real-world problems. \n\n    This can lead to significant advances in our understanding of language, and can help to drive innovation in a range of fields, from natural language processing to machine learning and beyond. \n\n    The future of language analysis and processing is exciting and rapidly evolving, and the cosine similarity metric is an important part of this journey. \n\n    As we continue to develop and apply this metric, we can expect to see significant breakthroughs and innovations in the field of natural language processing, and can look forward to a future where language technologies are more sophisticated, more effective, and more closely aligned with the complexities and nuances of human language. \n\n    The cosine similarity metric is a key part of this future, and its potential applications and implications are vast and exciting. \n\n    By embracing this technology and exploring its potential, we can unlock new insights into the nature of language, and can develop more effective methods for analyzing and understanding linguistic data. \n\n    This can lead to a deeper understanding of language and its structure, and can inform the development of more sophisticated language technologies that can better capture"
                },
                "CS": {
                    "CS_llama_prompt1": "In natural language processing (NLP), measuring the similarity between two words, v and w, can be thought of as a clustering problem where we want to group similar words together. To achieve this, we need a metric that can compare two vectors representing these words. The most commonly used metric is the cosine similarity, which calculates the cosine of the angle between the two vectors. This metric is based on the dot product operator, which you're familiar with from linear algebra. The dot product of two vectors, a and b, is given by the sum of the products of their corresponding components. Think of it like the weighted sum of the similarities between each dimension of the two vectors. \n\n    For instance, if we have two vectors a = [1, 2, 3] and b = [4, 5, 6], the dot product a · b would be (1*4) + (2*5) + (3*6) = 4 + 10 + 18 = 32. This operation can be seen as a way to measure how similar two vectors are, as it will result in higher values when the vectors have large values in the same dimensions. \n\n    However, the raw dot product has a limitation - it's biased towards frequent words. To address this, we use the normalized dot product, also known as cosine similarity, which is given by a · b / |a| |b|. This formula is equivalent to cosθ, where θ is the angle between the two vectors. By normalizing the dot product, we can compare the similarity between words regardless of their frequency. \n\n    To illustrate this, consider two vectors a and b, where a = [1, 0, 0] and b = [0, 1, 0]. In this case, the dot product a · b would be 0, indicating that the vectors are orthogonal and thus dissimilar. On the other hand, if we have two vectors c = [1, 1, 1] and d = [1, 1, 1], the dot product c · d would be 3, and the cosine similarity would be 1, indicating that the vectors are identical. \n\n    This concept is analogous to the k-nearest neighbors (k-NN) algorithm in machine learning, where we find the most similar data points to a given input. In NLP, we're essentially doing the same thing - finding the most similar words to a given word based on their vector representations. By using cosine similarity, we can effectively capture the semantic meaning of words and group similar words together, which is crucial in many NLP applications such as text classification, clustering, and information retrieval. \n\n    In summary, the cosine similarity metric is a powerful tool in NLP that allows us to compare the similarity between two words based on their vector representations. By normalizing the dot product, we can overcome the limitation of frequent words and capture the semantic meaning of words, enabling us to build more accurate and effective NLP models."
                }
            },
            "gpt4o": {
                "L": {
                    "L_gpt4o_prompt4": "To determine how similar two words are, we use a method that compares their representations, known as vectors. A common way to do this is by looking at the cosine of the angle between these vectors. This idea comes from linear algebra and involves something called the dot product, which helps us see how similar two vectors are. If two vectors have high values in the same areas, their dot product—and therefore their similarity—will be high. Conversely, if they don’t share these values, their dot product will be zero, showing they are quite different. However, this raw dot product can be skewed by how often words appear. To get a fair comparison, we normalize this measure, dividing the dot product by the length of each vector, which gives us the cosine value. This way, we can understand the similarity between words without being influenced by how frequently they occur."
                },
                "CS": {
                    "CS_gpt4o_prompt5": "To determine how similar two words are, we can represent each word as a vector and then use a mathematical method to compare these vectors. One common way to do this is by calculating the cosine of the angle between the two vectors. This approach is based on the dot product, a concept from linear algebra that you may be familiar with from machine learning when dealing with data representations.\n\nThe dot product is useful for measuring similarity because it is large when two vectors align closely, meaning they have similar features or characteristics. Conversely, if the vectors are orthogonal, or in other words, unrelated (having zeros in matching positions), their dot product will be zero, indicating no similarity.\n\nHowever, the raw dot product can be influenced by the frequency of the words, as frequent words tend to have larger vector values, which could artificially inflate similarity measurements. To address this, we use the normalized dot product, which divides the dot product by the magnitudes of the vectors. This normalization gives us the cosine similarity, a value between -1 and 1, that indicates how similar two vectors are, independent of their length or frequency. It is akin to normalizing data to ensure fair comparison, a technique often used in data preprocessing."
                }
            },
            "claude": {
                "L": {
                    "L_claude_prompt4": "Here's how we measure word similarity in computational linguistics, explained for language experts:\n\nJust as phonemes combine distinctively to create meaningful contrasts in words, words too have distinctive features that we can measure. Imagine each word living in a multi-dimensional space where each dimension represents a linguistic context - similar to how we map vowels in acoustic space using formants. When we want to find how similar two words are in meaning, we look at how close they sit in this space, specifically the angle between them.\n\nThink of it like measuring the acoustic similarity between two sounds: just as we normalize for speaker differences when comparing vowel qualities, we normalize word representations to ignore frequency effects. The mathematical tool we use (cosine similarity) is like calculating how similar two vowels are regardless of how loudly they're pronounced.\n\nWhen two words appear in very similar contexts - like \"small\" and \"tiny\" - the angle between them is tiny, giving a high similarity score. When they appear in completely different contexts - like \"small\" and \"democracy\" - they're perpendicular (orthogonal), just as /p/ and /b/ are distinctive in their voicing feature, giving a similarity of zero."
                },
                "CS": {
                    "CS_claude_prompt3": "Here's how word similarity works in computational linguistics, explained through a CS lens:\n\nThink of words as vectors in a high-dimensional space, similar to feature vectors in machine learning. To compare two words (let's call them v and w), we need a similarity function that operates on their vector representations.\n\nThe most popular similarity metric is cosine similarity - a concept you might recognize from recommender systems or clustering algorithms. It's derived from the dot product operation, which you've likely encountered in linear algebra or neural network computations.\n\nThe dot product works well as a similarity metric because it essentially performs element-wise multiplication and summation. Just like in sparse matrix operations, when two vectors have non-zero values in matching dimensions, they contribute to higher similarity. When vectors have non-zero values in different dimensions (orthogonal vectors, like perpendicular arrays), their dot product becomes 0, indicating no similarity.\n\nHowever, raw dot product has a limitation: frequently occurring words will naturally have larger vector magnitudes, skewing the results (similar to how common features can dominate in unscaled machine learning models). To normalize for frequency, we divide the dot product by the vectors' magnitudes, giving us the cosine formula: cos(θ) = (a·b)/(|a||b|). This normalization is analogous to feature scaling in machine learning preprocessing."
                }
            },
            "deepseek": {
                "L": {
                    "L_deepseek_prompt5": "To assess how closely two words align in meaning, we can model them as vectors in a multidimensional semantic space—akin to phonological distinctive features but extended to lexical semantics. Just as phonological similarity might involve shared features like [±voice] or [±nasal], vector similarity relies on overlapping values across dimensions (e.g., semantic domains, collocational patterns).  \n\nThe *dot product* here acts like a weighted tally of shared features: vectors with high values in matching dimensions yield a higher score. However, this raw score is influenced by word frequency—much like how high-frequency phonemes might dominate a spectrogram. Orthogonal vectors (e.g., words with no overlapping semantic components, like \"syntax\" and \"trombone\") score zero, paralleling how distinct phonological categories (e.g., plosives vs. fricatives) occupy non-overlapping acoustic spaces.  \n\nTo neutralize frequency effects—similar to normalizing pitch contours across speakers—we use the *cosine*: the dot product divided by the product of the vectors’ magnitudes. This measures the angular proximity between vectors, unaffected by their length (frequency). Imagine comparing vowel formants: whether two vowels are acoustically similar depends not on their amplitude (loudness) but on their relative positions in the F1-F2 space. Similarly, cosine similarity isolates structural alignment, revealing semantic kinship independent of usage frequency."
                },
                "CS": {
                    "CS_deepseek_prompt2": "To compare word vectors (like embeddings in ML), think of cosine similarity as a normalized dot product that ignores vector magnitude—focusing only on directional alignment. Here's the CS breakdown:  \n\n1. **Dot Product Baseline**: Like element-wise multiplication summed across dimensions (e.g., `np.dot(v, w)`), it highlights shared high-value features. But it’s biased toward frequent words (larger magnitudes).  \n\n2. **Normalization Fix**: Just as you’d normalize data for SVM/k-means, divide the dot product by the product of vector magnitudes (`||v|| * ||w||`). This scales vectors to unit length, isolating their *direction* in the high-dimensional space.  \n\n3. **Angle as Similarity**: The result (`cosθ`) measures the angle between vectors. θ=0° (cos=1): identical direction (e.g., \"king\" vs \"monarch\"). θ=90° (cos=0): orthogonal, no feature overlap (e.g., \"database\" vs \"poetry\").  \n\n**Why CS folks care**: It’s feature-space alignment without frequency noise—like checking if two codebases have similar architecture patterns, regardless of total lines. Used in clustering, retrieval, or analogy tasks (`word2vec`'s king - man + woman ≈ queen)."
                }
            }
        }
    },
    "T14": {
        "instance_code": "T14",
        "original_category": "CL",
        "original_text_title": "Machine Translation",
        "original_text": "The standard architecture for MT is the encoder-decoder transformer or sequence-to-sequence model [...]. Given a sentence in a source language, the MT task is then to generate a corresponding sentence in a target language. […]. MT uses supervised machine learning: at training time the system is given a large set of parallel sentences (each sentence in a source language matched with a sentence in the target language), and learns to map source sentences into target sentences. [...] we split the sentences into a sequence of subword tokens (tokens can be words, or subwords, or individual characters). The systems are then trained to maximize the probability of the sequence of tokens in the target language y1,..., ym given the sequence of tokens in the source language x1,..., xn: P(y1,..., ym|x1,..., xn). […] The encoder takes the input words x = [x1,..., xn] and produces an intermediate context h. At decoding time, the system takes h and, word by word, generates the output y [...]. [...] Machine translation models are trained on a parallel corpus, sometimes called a bitext, a text that appears in two (or more) languages. [...] Standard training corpora for MT come as aligned pairs of sentences. [...] To score the similarity of sentences across languages, we need to make use of a multilingual embedding space, in which sentences from different languages are in the same embedding space [...]",
        "selected_texts": {
            "mistral": {
                "L": {
                    "L_mistral_prompt3": "1. Understand the basics of machine translation (MT): it's a system that translates sentences from one language (source) to another (target) using a sequence-to-sequence model called encoder-decoder transformer.\n\n    2. The training process involves providing the system with a large set of sentences in both languages (source and target), so it can learn to convert one into the other.\n\n    3. To do this, we break down the sentences into smaller units, called subword tokens, that can be words, parts of words, or even individual characters.\n\n    4. The system then tries to generate the sequence of these tokens in the target language given the sequence in the source language. It maximizes the probability of the target sequence given the source sequence.\n\n    5. Initially, the system takes the input words from the source language and produces an intermediate context. During the decoding process, it uses this context to generate the output word by word in the target language.\n\n    6. Machine translation models are trained on parallel corpora (bitexts), which are texts that appear in more than one language.\n\n    7. Standard training data for MT comes in the form of aligned pairs of sentences, one in each language.\n\n    8. To evaluate the similarity of sentences across languages, we need a multilingual embedding space, where sentences from different languages can be compared."
                },
                "CS": {
                    "CS_mistral_prompt4": "In the context of machine translation (MT), the typical structure involves an encoder-decoder transformer or sequence-to-sequence model. Given a sentence in a source language, the objective is to generate a corresponding sentence in the target language. Machine translation employs supervised machine learning during training; the system is presented with a large collection of paired sentences (each sentence in the source language corresponding to a sentence in the target language), allowing it to learn to convert source sentences into target sentences. The sentences are first broken down into a series of subword tokens (which can be words, subwords, or individual characters). The objective of the system is to maximize the probability of the sequence of tokens in the target language (y1,..., ym) given the sequence of tokens in the source language (x1,..., xn) [P(y1,..., ym|x1,..., xn)]. The encoder processes the input words (x = [x1,..., xn]) and produces an intermediate context (h). During decoding, the system uses h to progressively generate the output (y). Machine translation models are trained on parallel corpora, often referred to as bitexts, which are texts that exist in two (or more) languages. Standard training sets for MT consist of aligned pairs of sentences. To evaluate the similarity between sentences across languages, we must employ a multilingual embedding space, where sentences from different languages exist within the same space."
                }
            },
            "llama": {
                "L": {
                    "L_llama_prompt2": "```\n\nTo better grasp the concept of machine translation (MT), let's break it down using linguistics terms. In MT, we're essentially dealing with a translation task, where a sentence in one language (the source language) needs to be translated into another language (the target language). This process is similar to how a linguist analyzes the syntax and semantics of a sentence to understand its meaning. \n\nImagine you're translating a sentence from English to Spanish. The MT system uses a technique called supervised machine learning, which is like having a language teacher who provides you with a set of example sentences in both languages. The system learns to map the source sentence to the target sentence by analyzing these examples. \n\nIn linguistics, we split sentences into smaller units like phonemes or morphemes. Similarly, in MT, we break down sentences into subword tokens, which can be words, subwords, or even individual characters. The system is trained to predict the probability of a sequence of tokens in the target language given the sequence of tokens in the source language. \n\nThink of the MT system like a language generator. It takes the input sentence (the source language) and produces an intermediate representation, similar to how a linguist might represent the underlying structure of a sentence using a parse tree. Then, it generates the output sentence (the target language) word by word, much like how a language speaker would construct a sentence. \n\nThe training data for MT systems consists of parallel corpora, which are like bilingual dictionaries, where each sentence in one language is paired with its equivalent in another language. This allows the system to learn the relationships between sentences across languages. \n\nTo compare sentences across languages, we use a concept called multilingual embedding space. Imagine a shared semantic space where sentences from different languages are represented in a way that captures their meanings and relationships. This space enables the system to score the similarity between sentences, even if they're in different languages. \n\nIn essence, machine translation is a complex task that leverages linguistics concepts, such as syntax, semantics, and phonetics, to generate translations. By understanding how MT systems work, you can appreciate the intricacies of language and the challenges of translating meaning from one language to another."
                },
                "CS": {
                    "CS_llama_prompt2": "Consider a neural network architecture similar to the sequence-to-sequence models you've studied in machine learning, but applied to natural language processing. The goal of machine translation (MT) is to take a sentence in one language (source) and generate the equivalent sentence in another language (target). \n     This process leverages supervised learning, where the model is trained on a large dataset of paired sentences in both the source and target languages. The model learns to map inputs from the source language to outputs in the target language, maximizing the probability of generating the correct target sentence given the source sentence.\n     To process the input sentences, we break them down into subword tokens, which can be words, subwords, or even individual characters. This is similar to tokenization in programming, where you split code into individual elements for processing.\n     The MT model consists of an encoder and a decoder. The encoder takes the input sentence and generates an intermediate representation, or context, which captures the essential information from the input. This context is then used by the decoder to generate the output sentence, word by word, in the target language.\n     Training data for MT models comes in the form of parallel corpora, or bitexts, which are texts that exist in multiple languages. These corpora are typically composed of aligned pairs of sentences, where each sentence in the source language is paired with its equivalent in the target language.\n     To compare sentences across languages, we use a multilingual embedding space. This space allows sentences from different languages to be represented in a common vector space, enabling the model to capture similarities and relationships between sentences across languages.\n     Think of this embedding space like a high-dimensional vector space in linear algebra, where each sentence is a point in that space. The closer two points are, the more similar the sentences they represent. This allows the model to learn a shared representation of language, facilitating translation between languages.\n     The key to successful MT is training the model on a large, diverse dataset that covers a wide range of languages and topics. This enables the model to learn the patterns and structures of language, as well as the relationships between languages, resulting in more accurate and fluent translations. \n     By leveraging these concepts and techniques, MT models can achieve state-of-the-art results in translating text from one language to another, opening up new possibilities for communication and information exchange across languages and cultures. \n     In terms of the probability calculation, the model aims to maximize P(y1,..., ym|x1,..., xn), where x represents the input sentence in the source language and y represents the output sentence in the target language. This probability calculation is similar to the likelihood calculations you've encountered in machine learning, where the goal is to find the model parameters that maximize the probability of the observed data.\n     The encoder-decoder architecture and the use of subword tokens are crucial components of MT models, allowing them to effectively capture the nuances of language and generate accurate translations. \n     By understanding how MT models work and how they're trained, you can appreciate the complexity and power of these systems, and how they can be applied to a wide range of tasks and applications. \n     The use of multilingual embedding spaces and parallel corpora enables MT models to learn a shared representation of language, facilitating translation between languages and enabling the model to capture similarities and relationships between sentences across languages. \n     Overall, the combination of supervised learning, sequence-to-sequence models, and multilingual embedding spaces makes MT a powerful tool for translating text from one language to another, with applications in fields such as language translation, language understanding, and language generation. \n     The ability to process and generate human-like language is a fundamental aspect of MT, and the techniques used in MT have far-reaching implications for natural language processing and artificial intelligence as a whole. \n     By applying the principles of machine learning and neural networks to the task of language translation, MT models can learn to generate fluent and accurate translations, opening up new possibilities for communication and information exchange across languages and cultures. \n     The MT model's ability to learn a shared representation of language and capture similarities between sentences across languages enables it to generate translations that are not only accurate but also contextually relevant, taking into account the nuances of language and the relationships between languages. \n     In the context of MT, the encoder-decoder architecture and the use of subword tokens allow the model to effectively capture the structure and meaning of the input sentence, and generate an output sentence that is equivalent in meaning and context. \n     The use of parallel corpora and multilingual embedding spaces enables the model to learn from a large dataset of paired sentences, and to capture the patterns and relationships between languages, resulting in more accurate and fluent translations. \n     By leveraging these techniques and concepts, MT models can achieve state-of-the-art results in translating text from one language to another, and have a significant impact on fields such as language translation, language understanding, and language generation. \n     The application of MT models has far-reaching implications for natural language processing and artificial intelligence, enabling the development of more sophisticated"
                }
            },
            "gpt4o": {
                "L": {
                    "L_gpt4o_prompt4": "In machine translation (MT), we use a model called an encoder-decoder, which is a type of sequence-to-sequence model. Imagine you have a sentence in one language and you want to translate it into another. The MT system learns to do this by being trained on a large collection of sentence pairs, one in the original language and one in the translated language.\n\nThe process starts by breaking down sentences into smaller parts, such as words or even smaller units. The encoder reads the entire input sentence and creates a summary or context. Then, during translation, the decoder uses this context to generate the sentence in the target language, word by word.\n\nTo ensure the translation is accurate, we use a concept called a parallel corpus, which is essentially a bilingual text with aligned sentences in two languages. This helps the system learn the relationships between the two languages.\n\nAdditionally, we use something called a multilingual embedding space. Think of it as a common area where sentences from different languages can be compared and measured for similarity, helping the system understand how similar sentences are across languages."
                },
                "CS": {
                    "CS_gpt4o_prompt1": "In machine translation (MT), we use a model called the encoder-decoder transformer, also known as a sequence-to-sequence model. This model's job is to take a sentence from one language (the source language) and generate a corresponding sentence in another language (the target language). Think of it like translating a sentence from English to French.\n\nMT relies on supervised machine learning. During training, the system is fed a large dataset of parallel sentences, where each sentence in the source language is paired with its translation in the target language. The model learns to map these source sentences to their target counterparts.\n\nTo handle the sentences, we break them down into sequences of subword tokens. These tokens can be whole words, parts of words, or even individual characters. The training process involves maximizing the probability of producing the correct sequence of target tokens (y1, y2, ..., ym) given the source tokens (x1, x2, ..., xn). Formally, this is represented as P(y1, y2, ..., ym | x1, x2, ..., xn).\n\nThe encoder component of the model processes the input words (x1, x2, ..., xn) and generates an intermediate representation called context (h). During decoding, this context is used to generate the output sentence in the target language, one word at a time.\n\nThese translation models are trained using a parallel corpus, or bitext, where the same text is available in two or more languages. The training data consists of sentence pairs that are aligned for translation.\n\nTo evaluate how similar sentences are across different languages, we utilize a multilingual embedding space. This space allows sentences from various languages to be represented in a common framework, making it easier to compare and translate between them."
                }
            },
            "claude": {
                "L": {
                    "L_claude_prompt2": "Machine Translation works much like a highly skilled interpreter who understands both the source and target languages. Think of it as a two-part process, similar to how we process language in linguistics: first understanding (encoding), then producing (decoding).\n\nJust as we learn languages through exposure to parallel examples (like bilingual texts or dictionaries), the system learns from millions of matched sentence pairs - imagine having a massive collection of perfectly aligned bilingual texts. These sentences are broken down into smaller units, similar to how we analyze morphemes and phonemes in linguistic analysis.\n\nThe first part (encoder) functions like our language comprehension system: it takes the input sentence and creates a deep understanding of its meaning, much like how we process semantic content. The second part (decoder) is like our language production system: it takes that understanding and expresses it in the target language, following the target language's syntactic and morphological rules.\n\nThe system needs a way to understand equivalence across languages, similar to how we recognize that \"house\" and \"maison\" refer to the same concept despite being in different languages. It does this through a shared semantic space, much like how cognates or universal semantic features work across languages.\n\nThe training process is similar to how children acquire language through exposure to paired meanings and utterances, but on a much larger scale using structured bilingual texts, like having millions of perfectly annotated parallel texts that linguists would use for comparative analysis."
                },
                "CS": {
                    "CS_claude_prompt2": "Machine Translation (MT) is like building a code compiler that translates between programming languages, but for human languages. The core architecture uses an encoder-decoder transformer model, similar to how compilers have front-end and back-end components.\n\nThe system processes input like parsing code: it breaks down sentences into tokens (think of them as lexical units, similar to how a compiler tokenizes source code). These tokens can be words, subwords, or individual characters, comparable to how programming languages have keywords, operators, and literals.\n\nThe training process uses supervised learning with a dataset of parallel sentences - imagine having two arrays where array[i] in language A corresponds to array[i] in language B. The model learns to maximize the probability function P(y1,...,ym|x1,...,xn), where x is the input sequence and y is the output sequence, similar to how you'd implement a probabilistic mapping function.\n\nThe encoder component processes the input sequence [x1,...,xn] and generates an intermediate representation h (think of it as an abstract syntax tree in compilation). The decoder then uses this representation to generate the output sequence word by word, similar to how a compiler generates target code from an AST.\n\nThe training data comes from a parallel corpus (or bitext) - essentially a synchronized database of texts in multiple languages. To compare sentences across languages, the system uses a multilingual embedding space, conceptually similar to how word2vec or other embedding algorithms create vector representations, but in a unified space that works across multiple languages."
                }
            },
            "deepseek": {
                "L": {
                    "L_deepseek_prompt1": "**Adapted Explanation for Linguistics Students:**  \n\nMachine translation (MT) relies on a *syntax-to-syntax transformer model*—a system that converts a source language sentence into a target language sentence, much like re-mapping syntactic structures across languages. Imagine parsing a sentence into its grammatical components (encoder) and then regenerating it in another language while preserving meaning (decoder).  \n\nMT learns from *aligned bilingual texts* (e.g., paired English-French sentences, akin to parallel corpora in dialectology). During training, it breaks sentences into subword units (like morphemes or graphemes) to handle rare words, similar to morphological analysis. The model then calculates the probability of generating each target unit (*y₁, y₂…*) given the source units (*x₁, x₂…*), optimizing for semantic equivalence.  \n\nThe encoder analyzes the source sentence’s structure, creating a *contextual representation* (like a semantic prototype). The decoder uses this prototype to sequentially produce the target sentence, ensuring alignment in meaning—think of it as code-switching guided by syntactic rules.  \n\nTo evaluate cross-linguistic similarity, MT maps sentences into a shared *semantic vector space* (analogous to universal semantic roles), where \"dog\" (English) and \"perro\" (Spanish) occupy overlapping conceptual regions. This ensures translations preserve not just lexical but also pragmatic meaning.  \n\n**Example:** Translating “She runs” (English) to “Ella corre” (Spanish) involves encoding the VP structure, then decoding using Spanish morphosyntax—a controlled transfer of grammatical and semantic features."
                },
                "CS": {
                    "CS_deepseek_prompt3": "Here's your CS-friendly adaptation:\n\nMachine translation (MT) works like a neural sequence converter, using an encoder-decoder architecture similar to how autoencoders process data. Think of it as a transformer-based pipeline that processes text through computational layers:\n\n1. **Data Processing**  \nInput sentences get tokenized into subword units (like byte-pair encoding in NLP pipelines) to handle rare words efficiently. Imagine splitting \"unhappiness\" into [\"un\", \"happiness\"].\n\n2. **Training Phase**  \nThe model learns from parallel text pairs (source-target sentence pairs in a dataset, structured like key-value pairs in a dictionary). It optimizes weights to maximize the probability P(target_tokens | source_tokens) - similar to minimizing cross-entropy loss in classification tasks.\n\n3. **Encoder (Feature Extractor)**  \nThe encoder processes source tokens (x₁,...xₙ) into context vectors **h** - like generating feature embeddings in CNNs, but for sequential data.\n\n4. **Decoder (Autoregressive Generator)**  \nUsing **h**, the decoder generates target tokens (y₁,...yₘ) sequentially, like predictive text generation but constrained by the source context. Each step uses attention mechanisms (like graph networks focusing on relevant nodes) to align target/source elements.\n\n5. **Multilingual Embedding Space**  \nThe system projects different languages into a shared vector space - similar to how word2vec aligns similar meanings, but across languages. This enables semantic similarity comparisons between sentences in different languages through vector operations.\n\nThe entire system is trained end-to-end using gradient descent, with the parallel corpus acting like a labeled training set where each input (source sentence) has a corresponding output label (target sentence)."
                }
            }
        }
    },
    "T15": {
        "instance_code": "T15",
        "original_category": "CL",
        "original_text_title": "Automatic Speech Recognition (ASR)",
        "original_text": "The first step in ASR is to transform the input waveform into a sequence of acoustic feature vectors, […]. The input to a speech recognizer is a complex series of changes in air pressure […]. We represent sound waves by plotting the change in air pressure over time. [...] The first step in digitizing a sound wave […]. This analog-to-digital conversion has two steps: sampling and quantization. To sample a signal, we measure its amplitude at a particular time; the sampling rate is the number of samples taken per second. [...] This maximum frequency for a given sampling rate is called the Nyquist frequency. [...] Amplitude measurements are stored as integers, […]. This process of representing real-valued quantization numbers as integers is called quantization; [...]. From the digitized, quantized representation of the waveform, we need to extract spectral features from a small window of speech that characterizes part of a particular phoneme. The speech extracted from each window is called a frame. The windowing is characterized by three parameters: the window size or frame size of the window (its width in milliseconds), the frame stride, (also called shift or offset) between successive windows, and the shape of the window. [...] The tool for extracting spectral information […] is the discrete Fourier transform or DFT. [...] If we plot the magnitude against the frequency, we can visualize the spectrum. [...]",
        "selected_texts": {
            "mistral": {
                "L": {
                    "L_mistral_prompt3": "1. Convert the audio input into a sequence of acoustic parameters, such as pitch and volume, for the speech recognizer to process.\n\n    2. The audio input is a sequence of pressure changes in the air. We represent these pressure changes visually as a graph over time.\n\n    3. To convert an analog audio wave into digital form, we first sample it, measuring its amplitude at specific intervals (sampling rate).\n\n    4. The maximum frequency that a certain sampling rate can capture is known as the Nyquist frequency.\n\n    5. We store the sampling results as integers (quantization).\n\n    6. From the digitized, quantized representation of the waveform, we extract spectral features that define parts of specific phonemes (speech sounds). We call each of these chunks of speech a frame.\n\n    7. The size, spacing, and shape of the frames are defined by three parameters: frame size (width in milliseconds), frame stride (offset between frames), and window shape.\n\n    8. We use the Discrete Fourier Transform (DFT) to extract spectral information from each frame.\n\n    9. If we plot the magnitude against the frequency, we can visualize the spectrum."
                },
                "CS": {
                    "CS_mistral_prompt2": "1. To begin with our Automatic Speech Recognition (ASR) solution, we transform the input sound wave into a collection of acoustic feature vectors. A sound wave is represented as a graph of air pressure changes over time.\n\n    2. For digitizing the sound wave, we perform two main steps: sampling and quantization. Sampling involves measuring the wave's amplitude at specific instances, and the sampling rate is the number of samples taken per second. The highest frequency that can be accurately represented by a given sampling rate is known as the Nyquist frequency.\n\n    3. Quantization is the process of representing the amplitude measurements as integers.\n\n    4. From the digitized and quantized representation, we extract spectral features from a small segment of speech, called a frame. Each frame characterizes part of a specific phoneme. The windowing process for creating frames comprises three parameters: the frame size (window width in milliseconds), the frame stride (the distance between successive windows), and the window shape.\n\n    5. To extract spectral information from each frame, we utilize the Discrete Fourier Transform (DFT) tool. This analysis provides a visual representation of the spectrum by plotting the magnitude against the frequency."
                }
            },
            "llama": {
                "L": {
                    "L_llama_prompt4": "Consider sound as a sequence of phonetic segments, similar to a spoken sentence composed of distinct words and phonemes. In speech recognition, the initial step involves capturing the acoustic properties of this sound sequence by converting it into a digital format, much like a linguistic analysis of phonetic transcriptions. This process entails digitizing the sound wave, which is analogous to translating a spoken language into a written, symbolic representation. The two primary steps in this translation are sampling and quantization, comparable to the way a phonetician might measure the acoustic characteristics of speech sounds at regular intervals and then categorize them based on their intensity. Sampling can be thought of as taking snapshots of the sound wave at consistent intervals, with the frequency of these snapshots determining the level of detail in the analysis. Quantization then assigns a numerical value to each snapshot, similar to how a linguist might assign phonetic symbols to represent distinct sounds. The resulting digital representation is composed of a series of frames, each representing a brief window of speech, akin to a single phoneme or syllable. To extract meaningful information from these frames, we employ a mathematical tool known as the discrete Fourier transform (DFT), which is comparable to a spectrogram used in phonetic analysis to visualize the acoustic properties of speech sounds. By applying the DFT, we can visualize the sound wave's spectral characteristics, plotting its magnitude against frequency, much like a linguist might analyze the formant structure of vowels."
                },
                "CS": {
                    "CS_llama_prompt4": "The process of converting audio input into a digital signal can be likened to compiling code from a high-level language to machine code. The initial step involves capturing the audio waveform, akin to reading source code, and transforming it into a sequence of feature vectors, similar to how an interpreter breaks down code into executable instructions. The audio signal is represented as a series of changes in air pressure over time, comparable to how a time series dataset is plotted. To digitize this signal, we employ an analog-to-digital conversion, which consists of sampling, where we measure the amplitude at discrete time intervals, and quantization, where we assign integer values to these amplitudes, much like how floating-point numbers are represented as integers in a computer. The sampling rate determines the maximum frequency that can be captured, known as the Nyquist frequency, analogous to the clock speed of a CPU. We then extract spectral features from small segments of the audio, called frames, using a windowing technique with parameters such as size, stride, and shape, similar to how a sliding window algorithm processes data. The discrete Fourier transform (DFT) is used to extract spectral information, analogous to performing a frequency analysis on a signal, allowing us to visualize the spectrum by plotting magnitude against frequency, much like visualizing a dataset's frequency distribution."
                }
            },
            "gpt4o": {
                "L": {
                    "L_gpt4o_prompt4": "To understand how Automatic Speech Recognition (ASR) works, imagine we're trying to convert spoken words into something a computer can process. We start by capturing the sound waves, which are essentially changes in air pressure over time. These sound waves need to be turned into a digital format the computer can understand. This involves two main steps: sampling and quantization.\n\nSampling is like taking snapshots of the sound wave at regular intervals, similar to how a movie is made up of individual frames. The rate at which we take these snapshots is called the sampling rate. The highest pitch we can accurately capture with a given sampling rate is known as the Nyquist frequency. Once we've sampled the sound wave, we need to convert these snapshots into numbers. This conversion process is called quantization, where we represent the sound's intensity levels as whole numbers.\n\nOnce we have this digital representation, we focus on small segments of the sound wave, called frames, to analyze specific sounds or phonemes. Each frame is extracted from a window of time, and three factors define this window: its size (how many milliseconds it covers), how much it shifts forward for each new frame, and its shape.\n\nTo understand the sound's characteristics in each frame, we use a mathematical tool called the Discrete Fourier Transform (DFT). The DFT helps us see the different frequencies present in the sound by plotting their magnitudes. This visualization is known as the sound spectrum. By analyzing these spectra, we can identify and differentiate between various phonemes, helping the ASR system understand what was said."
                },
                "CS": {
                    "CS_gpt4o_prompt4": "In automatic speech recognition (ASR), the first step is converting the input sound wave into a series of acoustic feature vectors. This process begins with the analog-to-digital conversion of the sound wave, which involves two main steps: sampling and quantization. Sampling means measuring the wave's amplitude at specific intervals, with the sampling rate being the number of measurements per second. The highest frequency that can be accurately captured at a given sampling rate is known as the Nyquist frequency. During quantization, these amplitude measurements are represented as integers.\n\nAfter digitizing and quantizing the waveform, we extract spectral features from small segments of speech known as frames. These frames are characterized by parameters such as window size (duration in milliseconds), frame stride (the gap between successive frames), and window shape. To derive spectral information from these frames, we use the discrete Fourier transform (DFT), which helps analyze the frequency components of the signal. By plotting the magnitude of these components against frequency, we obtain a spectrum that represents the sound's characteristics."
                }
            },
            "claude": {
                "L": {
                    "L_claude_prompt5": "Here's the linguistics-oriented explanation:\n\nSpeech sounds, which linguists analyze through phonetics, can be captured and processed similarly to how we break down language into discrete units. Just as we segment continuous speech into phonemes, the digital processing of speech involves breaking down sound waves into measurable pieces.\n\nThink of it like analyzing a stream of speech: just as we take samples of language at different points to study its features, a digital system samples the sound wave at regular intervals - similar to how we might mark specific points in continuous speech to study intonation patterns.\n\nThe process mirrors phonetic transcription in many ways. Just as we convert continuous speech into discrete phonetic symbols (like IPA), the digital system converts continuous sound waves into discrete numerical values. This is comparable to how we distinguish between gradient phonetic features by categorizing them into distinct phonological units.\n\nThe analysis then moves to examining short segments of speech (called frames), much like how we analyze syllables or phonemes in a utterance. These frames overlap slightly, similar to how coarticulation affects adjacent sounds in natural speech. From these frames, we extract frequency information - analogous to how we analyze formants in acoustic phonetics to distinguish between vowels.\n\nThe resulting frequency analysis (spectrum) is comparable to a spectrogram used in phonetic analysis, showing the distribution of acoustic energy across different frequencies - just like how we visualize formant patterns in vowel analysis.\n\nThis whole process is essentially creating a digital equivalent of what phoneticians do when breaking down speech into analyzable components for linguistic study."
                },
                "CS": {
                    "CS_claude_prompt5": "Here's the CS-adapted explanation:\n\nThink of speech recognition's first phase as signal processing, similar to handling any digital input stream. The raw audio input is essentially a time series of amplitude values - like an array of floating-point numbers representing air pressure changes over time.\n\nThe conversion process is analogous to image processing, where we convert analog signals to digital data. We perform sampling (like setting a frame rate) at regular intervals, typically 16,000 times per second, following the Nyquist theorem - which is similar to how we need sufficient FPS in video to avoid temporal aliasing.\n\nThe quantization step maps continuous values to discrete integers, much like converting floating-point numbers to fixed-point representation in digital signal processing. Think of it as reducing a 32-bit float to an 8-bit integer for efficient storage.\n\nFor feature extraction, we implement a sliding window approach (similar to convolution operations in CNNs) where we process fixed-size chunks of samples called frames. Each frame overlaps with its neighbors, controlled by parameters like:\n- Window size (buffer length in ms)\n- Stride (step size between consecutive windows)\n- Window function (weighting matrix)\n\nThe spectral analysis uses the Discrete Fourier Transform (DFT) algorithm - essentially converting time-domain data into frequency-domain features, similar to how image processing converts spatial data to frequency components. The output is comparable to a feature vector in machine learning, where each dimension represents the energy at a specific frequency band.\n\nThis transforms our raw input array into a matrix of feature vectors that can be fed into subsequent machine learning models."
                }
            },
            "deepseek": {
                "L": {
                    "L_deepseek_prompt5": "In speech recognition, we first convert spoken input into a structured form for analysis, much like phonologists decompose continuous speech into discrete phonemes. The raw input—air pressure fluctuations—is analogous to a dynamic prosodic contour. To digitize this:  \n\n1. **Sampling**: Capture \"snapshots\" of the sound wave at regular intervals (e.g., 16,000 snapshots/second). This mirrors using a narrow transcription rate to avoid aliasing, where undersampling misrepresents high-frequency components (like failing to notate a rapid [ɾ] vs. [d] distinction). The **Nyquist limit** ensures our sampling rate is twice the highest frequency present, akin to needing sufficient phonetic detail to distinguish [s] (high-frequency) from [ʃ].  \n\n2. **Quantization**: Assign integer values to amplitude measurements, similar to categorizing gradient vowel formants into discrete phonemic symbols (e.g., reducing a continuum of [æ~ɛ] to /æ/).  \n\nNext, we divide the digitized signal into **frames**—brief, overlapping temporal windows (e.g., 25ms), like isolating a syllable nucleus or measuring formant stability in a vowel. Each frame’s **spectral features** are extracted via a mathematical tool (DFT), which decomposes the sound into its frequency components, much like a spectrogram visualizes formants (F1/F2) for vowel identification. These features act as acoustic \"distinctive features,\" encoding cues relevant to phoneme identity (e.g., burst frequency for [k] vs. [t]).  \n\nThis process transforms raw speech into a phonetically interpretable sequence, analogous to mapping a phonetic signal to a phonological representation."
                },
                "CS": {
                    "CS_deepseek_prompt4": "In ASR, raw audio is processed like a time-series dataset. First, **analog-to-digital conversion** samples the continuous sound wave at fixed intervals (sampling rate), akin to capturing periodic sensor readings in IoT systems. The Nyquist theorem ensures no aliasing, mirroring how data pipelines avoid undersampling. Next, **quantization** maps amplitude values to integers, similar to casting floating-point sensor data to 8-bit representations for efficiency.  \n\nThe digitized signal is then split into overlapping **frames** using a sliding window (e.g., 25ms chunks with 10ms shifts), analogous to batch processing in ML or convolutional kernel operations. Each frame undergoes **spectral analysis via DFT/FFT**, decomposing the temporal signal into frequency components—like projecting a vector into an orthogonal basis. The resulting magnitude-frequency plot (spectrum) acts as a feature vector, comparable to extracting edge detectors in CNN layers before classification.  \n\nThis pipeline transforms raw waveforms into structured, frequency-domain inputs suitable for sequence models (e.g., RNNs/Transformers), aligning with preprocessing steps in vision or time-series tasks."
                }
            }
        }
    }
}