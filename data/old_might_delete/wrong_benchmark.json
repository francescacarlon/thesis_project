{
    "1": {
        "chapter": "17",
        "sections": "17.1",
        "topic": "English Word Classes",
        "original_category": "L",
        "original_text": "Parts of speech fall into two broad categories: closed class and open class. Closed classes are those with relatively fixed membership, such as prepositions [...]. By contrast, nouns and verbs are open classes [...]. Closed class words are generally function words like of, it, and, or you, which tend to be very short, occur frequently, and often have structuring uses in grammar. Four major open classes occur in the languages of the world: nouns [...], verbs, adjectives, and adverbs, [...]. Nouns are words for people, places, or things,  [...]. Verbs refer to actions and processes, including main verbs like draw, provide, and go. English verbs have inflections (non-third-person-singular (eat), third-person singular (eats), progressive (eating), past participle (eaten)). [...]. Adjectives often describe properties or qualities of nouns, like color (white, black), age (old, young), and value (good, bad), [...]. Adverbs generally modify something. [...] A particle [...] is used in combination with a verb. Particles often have extended meanings that aren't quite the same as the prepositions they resemble, as in the particle 'over' in 'she turned the paper over'. A phrasal verb and a particle acting as a single unit is called a phrasal verb. The meaning of phrasal verbs is often non-compositional - not predictable from the individual meanings of the verb and the particle. [...]",
        "tailored_texts": {
            "mistral": {
                "CS": {
                    "prompt1": "You are an expert professor in CS. Rewrite the following explanation so that it is best understood by CS students.\n    The most important goal is that your students understand concepts that are outside their existing background knowledge, so they can pass your exam and improve their academic performance.\n    You must consider the information on Computer Science (CS) students have strong technical backgrounds, including programming, algorithms, and machine learning. They have little or no linguistic knowledge. to adapt the explanation to the students' backgrounds.\n\n    - Read the text carefully.\n    - Identify key concepts.\n    - Use terminology specific to their field.\n    - Avoid unnecessary sentence complexity while maintaining accuracy.\n    - Provide relatable examples.\n    - Use analogies that help transfer their knowledge to new concepts.\n    - Integrate background information if needed.    \n\n    Original text: \n    Parts of speech fall into two broad categories: closed class and open class. Closed classes are those with relatively fixed membership, such as prepositions [...]. By contrast, nouns and verbs are open classes [...]. Closed class words are generally function words like of, it, and, or you, which tend to be very short, occur frequently, and often have structuring uses in grammar. Four major open classes occur in the languages of the world: nouns [...], verbs, adjectives, and adverbs, [...]. Nouns are words for people, places, or things,  [...]. Verbs refer to actions and processes, including main verbs like draw, provide, and go. English verbs have inflections (non-third-person-singular (eat), third-person singular (eats), progressive (eating), past participle (eaten)). [...]. Adjectives often describe properties or qualities of nouns, like color (white, black), age (old, young), and value (good, bad), [...]. Adverbs generally modify something. [...] A particle [...] is used in combination with a verb. Particles often have extended meanings that aren't quite the same as the prepositions they resemble, as in the particle 'over' in 'she turned the paper over'. A phrasal verb and a particle acting as a single unit is called a phrasal verb. The meaning of phrasal verbs is often non-compositional - not predictable from the individual meanings of the verb and the particle. [...]\n\n    ### END OF INPUT ###\n\n    Tailored text (provide one text per target category only):\n    1. Closed Class Words (Function Words)\n    In the realm of linguistics, we categorize words into two major groups: closed class and open class. Closed classes are words with fixed membership, such as prepositions like 'in', 'on', and 'at'. These words, often referred to as function words, have a structural role in sentences, guiding the flow of thoughts and ideas. They are typically short, frequent, and include words like 'the', 'and', 'or', and 'you'.\n\n    Example:\n    \"I (verb) the (determiner) book (noun) on (preposition) the (determiner) table (noun).\"\n\n    Notice how the function words, 'the', 'on', and 'the', help structure the sentence and guide the flow of information.\n\n    To help you better understand this concept, imagine programming functions. In programming, we have predefined functions like 'if', 'else', 'while', and 'for' that guide the logic flow of our scripts, similar to how prepositions guide the flow of thoughts in a sentence.\n\n    Open Class Words (Content Words)\n    On the other hand, open class words are words like nouns, verbs, adjectives, and adverbs, which are flexible and can have an infinite number of members.\n\n    Nouns are words that represent people, places, or things. Examples include \"student\", \"classroom\", and \"book\".\n\n    Verbs, as you know from programming, refer to actions and processes. In English, verbs like 'draw', 'provide', and 'go' have inflections to indicate aspects like tense and number.\n\n    Adjectives describe the properties or qualities of nouns. For example, 'red' is an adjective that describes the color of an apple. 'Big' and 'small' are adjectives that describe the size of an object.\n\n    Adverbs, similar to function words, modify something in a sentence. They can describe how, when, where, or to what extent an action or state occurs. For example, 'quickly' modifies the verb 'run' in the sentence \"She runs quickly.\"\n\n    Particles and Phrasal Verbs\n    Particles are words that, when combined with a verb, can have extended meanings that are not quite the same as the prepositions they resemble. For instance, the particle 'over' in \"She turned the paper over\" means something like \"in reverse\" or \"on the other side\".\n\n    Phrasal verbs are composed of a verb and a particle acting as a single unit. The meaning of phrasal verbs is often non-compositional, meaning it is not predictable from the individual meanings of the verb and the particle. For example, consider the phrasal verb 'turn over', which may mean to give control or responsibility to someone else in certain contexts. This is similar to how function calls in programming work, where invoking a function with specific arguments provides a result that may not be obvious from the function's name alone.",
                    "prompt2": "You are an AI sales expert, responsible for selling AI products to clients with a background in CS.\n    To successfully sell your products, your clients must first understand the following concepts.\n    Rewrite the explanation below in a way that is most comprehensible to CS clients.\n    \n    Your goal is to ensure that your clients grasp concepts outside their existing background knowledge so they can make an informed purchase.\n    By doing so, you will increase your sales percentage and achieve greater success.\n\n    Consider the information on Computer Science (CS) students have strong technical backgrounds, including programming, algorithms, and machine learning. They have little or no linguistic knowledge. to tailor the explanation to your clients' specific backgrounds.\n\n    - Read the text carefully.\n    - Identify key concepts.\n    - Use terminology specific to their field.\n    - Avoid unnecessary sentence complexity while maintaining accuracy.\n    - Provide relatable examples.\n    - Use analogies that help transfer their knowledge to new concepts.\n    - Integrate background information if needed.    \n\n    Original text: \n    Parts of speech fall into two broad categories: closed class and open class. Closed classes are those with relatively fixed membership, such as prepositions [...]. By contrast, nouns and verbs are open classes [...]. Closed class words are generally function words like of, it, and, or you, which tend to be very short, occur frequently, and often have structuring uses in grammar. Four major open classes occur in the languages of the world: nouns [...], verbs, adjectives, and adverbs, [...]. Nouns are words for people, places, or things,  [...]. Verbs refer to actions and processes, including main verbs like draw, provide, and go. English verbs have inflections (non-third-person-singular (eat), third-person singular (eats), progressive (eating), past participle (eaten)). [...]. Adjectives often describe properties or qualities of nouns, like color (white, black), age (old, young), and value (good, bad), [...]. Adverbs generally modify something. [...] A particle [...] is used in combination with a verb. Particles often have extended meanings that aren't quite the same as the prepositions they resemble, as in the particle 'over' in 'she turned the paper over'. A phrasal verb and a particle acting as a single unit is called a phrasal verb. The meaning of phrasal verbs is often non-compositional - not predictable from the individual meanings of the verb and the particle. [...]\n\n    ### END OF INPUT ###\n\n    Tailored text (provide one text per target category only):\n    1. Open Classes:\n    In CS terms, open classes are those with an expansive and flexible membership, such as nouns and verbs, which are fundamental data types in programming. Unlike closed classes, open classes have no predefined limit to their members, and they can evolve over time. For instance, a noun in programming can represent anything from a person (like a user), a place (like a file path), or a thing (like a variable). Similarly, a verb can signify an action or process, such as \"draw\" (a graph in a visualization), \"provide\" (a function with parameters), or \"go\" (for traversing a data structure like a tree). Open class words, such as nouns and verbs, are essential in structuring and manipulating data, similar to how they are crucial in constructing grammar.\n\n    2. Closed Classes:\n    Closed classes, on the other hand, are those with a more static and limited membership, such as prepositions and determiners. In programming, closed classes can be compared to operators or control structures. These elements are short, frequent, and perform structural roles, like the \"and\" operator in logical operations or the \"for\" loop in iterations. As with function words in grammar, closed classes in programming tend to have a specific function and can significantly impact the flow and result of a program. For example, the \"if\" statement acts as a control structure, determining the path of execution based on a condition, similar to how the preposition \"in\" in English denotes a relation between a noun and a set or a location.",
                    "prompt3": "You are a freelance copywriter responsible for adapting field-specific academic texts for an audience with a background in CS. \n    Your client at the publishing house will decide whether to publish your adapted texts in the new edition of the book, making you famous in the copywriting field.\n\n    Rewrite the explanation below in a way that is most comprehensible to CS audience.\n    The key objective is to ensure that your audience grasps concepts outside their existing background knowledge so your client will publish your work.\n    If you succeed, many more clients will work with you, and you will become extremely successful.\n\n    Consider the information on Computer Science (CS) students have strong technical backgrounds, including programming, algorithms, and machine learning. They have little or no linguistic knowledge. to tailor the explanation to your audience’s specific background.\n\n    - Read the text carefully.\n    - Identify key concepts.\n    - Use terminology that is familiar to their field.\n    - Avoid unnecessary sentence complexity while maintaining accuracy.\n    - Provide relatable examples.\n    - Use analogies that help transfer their knowledge to new concepts.\n    - Integrate background information if needed.    \n\n    Original text: \n    Parts of speech fall into two broad categories: closed class and open class. Closed classes are those with relatively fixed membership, such as prepositions [...]. By contrast, nouns and verbs are open classes [...]. Closed class words are generally function words like of, it, and, or you, which tend to be very short, occur frequently, and often have structuring uses in grammar. Four major open classes occur in the languages of the world: nouns [...], verbs, adjectives, and adverbs, [...]. Nouns are words for people, places, or things,  [...]. Verbs refer to actions and processes, including main verbs like draw, provide, and go. English verbs have inflections (non-third-person-singular (eat), third-person singular (eats), progressive (eating), past participle (eaten)). [...]. Adjectives often describe properties or qualities of nouns, like color (white, black), age (old, young), and value (good, bad), [...]. Adverbs generally modify something. [...] A particle [...] is used in combination with a verb. Particles often have extended meanings that aren't quite the same as the prepositions they resemble, as in the particle 'over' in 'she turned the paper over'. A phrasal verb and a particle acting as a single unit is called a phrasal verb. The meaning of phrasal verbs is often non-compositional - not predictable from the individual meanings of the verb and the particle. [...]\n\n    ### END OF INPUT ###\n\n    Tailored text (provide one text per target category only):\n    1. For CS students:\n    In the realm of linguistics, words are categorized into two main groups: closed and open classes. Closed classes are composed of words with a fixed set of members, such as prepositions like 'in', 'on', and 'over'. Conversely, open classes encompass words like nouns and verbs, which have an extensive and ever-expanding set of members.\n\n    Closed class words include function words like 'and', 'or', and 'it', which are short, frequently used, and serve structural purposes in grammar. Some open class categories found in languages worldwide include nouns, verbs, adjectives, and adverbs.\n\n    Nouns represent people, places, or things, such as 'computer', 'programmer', and 'algorithm'. Verbs denote actions and processes, like 'compile', 'execute', and 'learn'. In English, verbs have inflections that modify their form according to tense, person, and number, such as 'compile' (present tense, third-person singular: 'compiles'), 'compiling' (present participle), and 'compiled' (past participle).\n\n    Adjectives describe the properties and qualities of nouns, for instance, 'efficient', 'reliable', and 'insecure'. Adverbs, on the other hand, modify verbs, adjectives, and other adverbs, providing more precision about their meaning, like 'quickly', 'slowly', and 'carefully'.\n\n    A particle is a word that combines with a verb, altering its meaning in an extended sense, as in 'she turned the paper over'. The combination of a verb and a particle is called a phrasal verb, like 'turn over'. Phrasal verbs often have non-compositional meanings, meaning their meaning isn't predictable from the individual meanings of the verb and the particle, such as the phrasal verb 'turn over' in the context of changing a page, rather than just moving something over a surface.\n\n    2. For non-CS audience:\n    In the study of language, words are classified into two primary groups: closed and open classes. Closed classes consist of words with a limited number of members, like prepositions such as 'in', 'on', and 'over'. By contrast, open classes encompass words like nouns and verbs, which can take on an endless number of members.\n\n    Closed class words include function words like 'and', 'or', and 'it', which are typically short, frequently used, and serve structural purposes in grammar. Some open class categories found in languages worldwide include nouns, verbs, adjectives, and adverbs.\n\n    Nouns are words for people, places, or things, such as 'book', 'house', and 'car'. Verbs refer to actions and processes, like 'run', 'jump', and 'talk'. In English, verbs have inflections that modify their form according to tense, person, and number, such as 'run' (present tense, third-person singular: 'runs'), 'running' (present participle), and 'ran' (past participle).\n\n    Adjectives describe the properties or qualities of nouns, like 'red', 'big', and 'beautiful'. Adverbs, on the other hand, modify verbs, adjectives, and other adverbs, providing more precision about their meaning, like 'quickly', 'slowly', and 'carefully'.\n\n    A particle is a word that combines with a verb, altering its meaning in an extended sense, as in 'she turned the page over'. The combination of a verb and a particle is called a phrasal verb, like 'turn over'. Phrasal verbs often have non-compositional meanings, meaning their meaning isn't predictable from the individual meanings of the verb and the particle, such as the phrasal verb 'turn over' in the context of changing a page, rather than just moving something over a surface.",
                    "prompt4": "We are playing an escape room game against each other. Your goal is to gain more points than me to be the first one escaping hence winning the game. \n    In order to escape the room, you must paraphrase field-specific concepts to the jury of the game in the CS. \n    They will assign you points from one to ten, according to how good their understanding of the new explanation is. \n    Therefore, you want your adaptation to be the best possible, and definitely better than mine, so that you can gain ten points for each paraphrasis and win more quickly. \n    Consider the information on Computer Science (CS) jury have strong technical backgrounds, including programming, algorithms, and machine learning. They have little or no linguistic knowledge. to tailor the explanation to each jury specific background. \n    The winning steps for good paraphrasis are:\n    - Read the text carefully.\n    - Identify the key concepts.\n    - Use terminology that is familiar to the jury's field.\n    - Avoid unnecessary sentence complexity while maintaining accuracy. \n    - Provide examples they are familiar with.\n    - Provide analogies they can relate their knowledge with and transfer it to new concepts.\n    - Integrate background information if needed.    \n    - For the CL jury in Computer Science (CS) jury have strong technical backgrounds, including programming, algorithms, and machine learning. They have little or no linguistic knowledge., only provide one paraphrasis for NLP/Computational Linguistics Practitioners.\n\n    Original text: \n    Parts of speech fall into two broad categories: closed class and open class. Closed classes are those with relatively fixed membership, such as prepositions [...]. By contrast, nouns and verbs are open classes [...]. Closed class words are generally function words like of, it, and, or you, which tend to be very short, occur frequently, and often have structuring uses in grammar. Four major open classes occur in the languages of the world: nouns [...], verbs, adjectives, and adverbs, [...]. Nouns are words for people, places, or things,  [...]. Verbs refer to actions and processes, including main verbs like draw, provide, and go. English verbs have inflections (non-third-person-singular (eat), third-person singular (eats), progressive (eating), past participle (eaten)). [...]. Adjectives often describe properties or qualities of nouns, like color (white, black), age (old, young), and value (good, bad), [...]. Adverbs generally modify something. [...] A particle [...] is used in combination with a verb. Particles often have extended meanings that aren't quite the same as the prepositions they resemble, as in the particle 'over' in 'she turned the paper over'. A phrasal verb and a particle acting as a single unit is called a phrasal verb. The meaning of phrasal verbs is often non-compositional - not predictable from the individual meanings of the verb and the particle. [...]\n\n    ### END OF INPUT ###\n\n    Tailored text (provide one text per target category only):\n    1. Computer Programming Practitioners:\n    In Computer Science, the two main types of words in language are closed and open classes. Closed classes are words like prepositions, which have a fixed set of members such as \"in\" and \"on\". These words often have a structuring role in a program's syntax, like function words in human language. On the other hand, open classes are more expansive and include nouns, verbs, adjectives, and adverbs. In programming, nouns would be data types and variables, verbs would be functions or methods, adjectives would be properties or attributes, and adverbs would be modifiers of functions or methods. Particles, such as \"over\" in \"she turned the paper over\", can have extended meanings in programming as well, often not fully explained by their individual components. A phrasal verb, like \"turn over\" in this example, is a combination of a verb and a particle that forms a new, unique meaning. The meaning of phrasal verbs is often non-compositional, meaning it's not always predictable from the individual meanings of the verb and the particle.\n\n    2. Algorithm Design Practitioners:\n    In the field of Algorithm Design, the categorization of words falls into two main categories: closed and open classes. Closed classes are those with a fixed set of members, such as prepositions, like \"within\" or \"between\". These words often have a structuring role in the algorithm's logic, similar to function words in human language. Open classes, on the other hand, are more expansive and include words like nouns, verbs, adjectives, and adverbs. In Algorithm Design, nouns would be data structures, verbs would be operations or functions, adjectives would be properties or attributes, and adverbs would be modifiers of operations or functions. Particles, such as \"over\" in \"she moved the array over\", can have extended meanings in Algorithm Design as well, often not fully explained by their individual components. A phrasal verb, like \"move over\" in this example, is a combination of a verb and a particle that forms a new, unique operation. The meaning of phrasal verbs is often non-compositional, meaning it's not always predictable from the individual meanings of the verb and the particle.\n\n    3. Machine Learning Practitioners:\n    In Machine Learning, the categorization of words falls into two main categories: closed and open classes. Closed classes are those with a fixed set of members, such as prepositions, like \"in\" or \"on\". These words often have a structuring role in the machine learning model's structure, similar to function words in human language. Open classes, on the other hand, are more expansive and include words like nouns, verbs, adjectives, and adverbs. In Machine Learning, nouns would be data sets, features, or variables, verbs would be algorithms or models, adjectives would be properties or attributes, and adverbs would be modifiers of algorithms or models. Particles, such as \"over\" in \"she oversampled the data\", can have extended meanings in Machine Learning as well, often not fully explained by their individual components. A phrasal verb, like \"oversample\" in this example, is a combination of a verb and a particle that forms a new, unique operation. The meaning of phrasal verbs is often non-compositional, meaning it's not always predictable from the individual meanings of the verb and the particle.",
                    "prompt5": "Paraphrase the given concepts in order for the audience of the CS to understand concepts outside of their field of knowledge. \n    You must consider the information on the Computer Science (CS) audience have strong technical backgrounds, including programming, algorithms, and machine learning. They have little or no linguistic knowledge. to adapt the tailored texts to the audience backgrounds. \n    - Read the text carefully.\n    - Identify the key concepts.\n    - Use terminology specific to their field.\n    - Avoid unnecessary sentence complexity while maintaining accuracy. \n    - Provide examples they are familiar with.\n    - Provide analogies they can relate their knowledge with and transfer it to new concepts.\n    - Integrate background information if needed.\n    - Do not consider other background per category. \n    - Output only one text per category.\n\n    Original text: \n    Parts of speech fall into two broad categories: closed class and open class. Closed classes are those with relatively fixed membership, such as prepositions [...]. By contrast, nouns and verbs are open classes [...]. Closed class words are generally function words like of, it, and, or you, which tend to be very short, occur frequently, and often have structuring uses in grammar. Four major open classes occur in the languages of the world: nouns [...], verbs, adjectives, and adverbs, [...]. Nouns are words for people, places, or things,  [...]. Verbs refer to actions and processes, including main verbs like draw, provide, and go. English verbs have inflections (non-third-person-singular (eat), third-person singular (eats), progressive (eating), past participle (eaten)). [...]. Adjectives often describe properties or qualities of nouns, like color (white, black), age (old, young), and value (good, bad), [...]. Adverbs generally modify something. [...] A particle [...] is used in combination with a verb. Particles often have extended meanings that aren't quite the same as the prepositions they resemble, as in the particle 'over' in 'she turned the paper over'. A phrasal verb and a particle acting as a single unit is called a phrasal verb. The meaning of phrasal verbs is often non-compositional - not predictable from the individual meanings of the verb and the particle. [...]\n\n    ### END OF INPUT ###\n\n    Tailored text (provide one text per target category only):\n    1. Closed Class Words:\n       Closed class words, such as prepositions, are a fixed set of words in a language that typically include function words like 'of', 'it', 'and', 'or', and 'you'. These words are short, occur frequently, and play a crucial role in structuring a sentence's grammar.\n    2. Open Class Words:\n       Open class words, like nouns, verbs, adjectives, and adverbs, are a diverse set of words that can be added to a language over time. In English, nouns refer to people, places, or things, verbs describe actions and processes, adjectives describe properties or qualities, and adverbs modify other words.\n    3. Particles:\n       Particles are a type of open class word that can act as a single unit with verbs, often having extended meanings that deviate from their preposition-like counterparts. For example, the particle 'over' in 'she turned the paper over' has a different meaning compared to the preposition 'over'. Phrasal verbs, which consist of a verb and a particle, have non-compositional meanings, meaning their meaning cannot be predicted from the individual meanings of the verb and the particle."
                },
                "CL": {
                    "prompt1": "You are an expert professor in CL. Rewrite the following explanation so that it is best understood by CL students.\n    The most important goal is that your students understand concepts that are outside their existing background knowledge, so they can pass your exam and improve their academic performance.\n    You must consider the information on Computational Linguistics (CL) students bridge linguistics and computer science, NLP, corpus linguistics, AI, and LLMs. to adapt the explanation to the students' backgrounds.\n\n    - Read the text carefully.\n    - Identify key concepts.\n    - Use terminology specific to their field.\n    - Avoid unnecessary sentence complexity while maintaining accuracy.\n    - Provide relatable examples.\n    - Use analogies that help transfer their knowledge to new concepts.\n    - Integrate background information if needed.    \n\n    Original text: \n    Parts of speech fall into two broad categories: closed class and open class. Closed classes are those with relatively fixed membership, such as prepositions [...]. By contrast, nouns and verbs are open classes [...]. Closed class words are generally function words like of, it, and, or you, which tend to be very short, occur frequently, and often have structuring uses in grammar. Four major open classes occur in the languages of the world: nouns [...], verbs, adjectives, and adverbs, [...]. Nouns are words for people, places, or things,  [...]. Verbs refer to actions and processes, including main verbs like draw, provide, and go. English verbs have inflections (non-third-person-singular (eat), third-person singular (eats), progressive (eating), past participle (eaten)). [...]. Adjectives often describe properties or qualities of nouns, like color (white, black), age (old, young), and value (good, bad), [...]. Adverbs generally modify something. [...] A particle [...] is used in combination with a verb. Particles often have extended meanings that aren't quite the same as the prepositions they resemble, as in the particle 'over' in 'she turned the paper over'. A phrasal verb and a particle acting as a single unit is called a phrasal verb. The meaning of phrasal verbs is often non-compositional - not predictable from the individual meanings of the verb and the particle. [...]\n\n    ### END OF INPUT ###\n\n    Tailored text (provide one text per target category only):\n    1. Closed Class Words\n        In the realm of Computational Linguistics, there are two primary categories of words: closed class and open class. Closed class words, such as prepositions (e.g., of, in, at), pronouns (e.g., he, she, they) and conjunctions (e.g., and, or, but), have a relatively fixed set of members. These words are function words, meaning they primarily serve to connect and structure sentences rather than convey specific meaning. They are usually short, frequent, and provide essential grammatical scaffolding.\n\n    2. Open Class Words\n        Open class words, on the other hand, have a more dynamic membership, with new words constantly being added as needed. These include nouns (e.g., person, city, book), verbs (e.g., draw, provide, go), adjectives (e.g., red, old, expensive), and adverbs (e.g., quickly, carefully, happily). Nouns represent objects, people, or places, while verbs denote actions or processes.\n\n        For example, verbs in English have inflections to denote various tenses, like the base form (draw), third-person singular (draws), progressive (drawing), and past participle (drawn).\n\n        Adjectives describe qualities or properties of nouns, such as color (red, blue), age (young, old), or value (good, bad). Adverbs modify other words in a sentence, providing additional details about their meaning or usage, like 'quickly' in 'he runs quickly.'\n\n    3. Particles\n        Particles are a specific type of open class word that are often used in combination with verbs (e.g., turn over, look up). Particles can have extended meanings that differ slightly from the prepositions they resemble, such as the particle 'over' in 'she turned the paper over,' where the meaning is not simply about a spatial relationship but rather a change in state.\n\n        Phrasal verbs, made up of a verb and a particle used together, have non-compositional meanings that cannot be predicted by the individual meanings of the verb and particle. For example, the phrasal verb 'turn over' does not imply a simple movement over something but rather a thorough examination or a change in control.",
                    "prompt2": "You are an AI sales expert, responsible for selling AI products to clients with a background in CL.\n    To successfully sell your products, your clients must first understand the following concepts.\n    Rewrite the explanation below in a way that is most comprehensible to CL clients.\n    \n    Your goal is to ensure that your clients grasp concepts outside their existing background knowledge so they can make an informed purchase.\n    By doing so, you will increase your sales percentage and achieve greater success.\n\n    Consider the information on Computational Linguistics (CL) students bridge linguistics and computer science, NLP, corpus linguistics, AI, and LLMs. to tailor the explanation to your clients' specific backgrounds.\n\n    - Read the text carefully.\n    - Identify key concepts.\n    - Use terminology specific to their field.\n    - Avoid unnecessary sentence complexity while maintaining accuracy.\n    - Provide relatable examples.\n    - Use analogies that help transfer their knowledge to new concepts.\n    - Integrate background information if needed.    \n\n    Original text: \n    Parts of speech fall into two broad categories: closed class and open class. Closed classes are those with relatively fixed membership, such as prepositions [...]. By contrast, nouns and verbs are open classes [...]. Closed class words are generally function words like of, it, and, or you, which tend to be very short, occur frequently, and often have structuring uses in grammar. Four major open classes occur in the languages of the world: nouns [...], verbs, adjectives, and adverbs, [...]. Nouns are words for people, places, or things,  [...]. Verbs refer to actions and processes, including main verbs like draw, provide, and go. English verbs have inflections (non-third-person-singular (eat), third-person singular (eats), progressive (eating), past participle (eaten)). [...]. Adjectives often describe properties or qualities of nouns, like color (white, black), age (old, young), and value (good, bad), [...]. Adverbs generally modify something. [...] A particle [...] is used in combination with a verb. Particles often have extended meanings that aren't quite the same as the prepositions they resemble, as in the particle 'over' in 'she turned the paper over'. A phrasal verb and a particle acting as a single unit is called a phrasal verb. The meaning of phrasal verbs is often non-compositional - not predictable from the individual meanings of the verb and the particle. [...]\n\n    ### END OF INPUT ###\n\n    Tailored text (provide one text per target category only):\n    1. CL students specialize in bridging linguistics and computer science, focusing on Natural Language Processing (NLP). To better understand the concepts of parts of speech, let's discuss their importance in NLP and how they can help develop AI applications.\n\n    Parts of speech are essential building blocks for NLP systems, as they help in understanding the structure of human language. They fall into two main categories: closed and open classes.\n\n    Closed classes consist of words with relatively fixed meanings, such as prepositions (e.g., in, on, with). These function words have structuring uses in grammar, often occurring frequently and being short, like the word 'of'.\n\n    Open classes, on the other hand, have more flexible membership, with nouns, verbs, adjectives, and adverbs being the primary examples.\n\n    In the context of NLP, nouns are used to represent people, places, or things, making them crucial for understanding content in AI applications. Verbs, as action or process words, help AI identify actions and events in language data, such as the main verbs \"draw,\" \"provide,\" and \"go.\"\n\n    Adjectives, which describe properties or qualities of nouns, are valuable in allowing AI to understand the nuances and details of the content. For instance, an AI system could use adjectives like \"white,\" \"old,\" or \"good\" to enhance its understanding of the content's characteristics.\n\n    Adverbs, which modify other parts of speech, can help AI identify the manner, place, or time of an event or action.\n\n    A particle is a type of word that often combines with a verb to create a phrasal verb, like \"over\" in \"she turned the paper over.\" Phrasal verbs, which are particles used in combination with a verb, can have extended meanings that may not be immediately apparent from their individual components.\n\n    Understanding these parts of speech is essential for AI applications, as they form the foundation for NLP systems and help improve the accuracy and effectiveness of AI in understanding human language.",
                    "prompt3": "You are a freelance copywriter responsible for adapting field-specific academic texts for an audience with a background in CL. \n    Your client at the publishing house will decide whether to publish your adapted texts in the new edition of the book, making you famous in the copywriting field.\n\n    Rewrite the explanation below in a way that is most comprehensible to CL audience.\n    The key objective is to ensure that your audience grasps concepts outside their existing background knowledge so your client will publish your work.\n    If you succeed, many more clients will work with you, and you will become extremely successful.\n\n    Consider the information on Computational Linguistics (CL) students bridge linguistics and computer science, NLP, corpus linguistics, AI, and LLMs. to tailor the explanation to your audience’s specific background.\n\n    - Read the text carefully.\n    - Identify key concepts.\n    - Use terminology that is familiar to their field.\n    - Avoid unnecessary sentence complexity while maintaining accuracy.\n    - Provide relatable examples.\n    - Use analogies that help transfer their knowledge to new concepts.\n    - Integrate background information if needed.    \n\n    Original text: \n    Parts of speech fall into two broad categories: closed class and open class. Closed classes are those with relatively fixed membership, such as prepositions [...]. By contrast, nouns and verbs are open classes [...]. Closed class words are generally function words like of, it, and, or you, which tend to be very short, occur frequently, and often have structuring uses in grammar. Four major open classes occur in the languages of the world: nouns [...], verbs, adjectives, and adverbs, [...]. Nouns are words for people, places, or things,  [...]. Verbs refer to actions and processes, including main verbs like draw, provide, and go. English verbs have inflections (non-third-person-singular (eat), third-person singular (eats), progressive (eating), past participle (eaten)). [...]. Adjectives often describe properties or qualities of nouns, like color (white, black), age (old, young), and value (good, bad), [...]. Adverbs generally modify something. [...] A particle [...] is used in combination with a verb. Particles often have extended meanings that aren't quite the same as the prepositions they resemble, as in the particle 'over' in 'she turned the paper over'. A phrasal verb and a particle acting as a single unit is called a phrasal verb. The meaning of phrasal verbs is often non-compositional - not predictable from the individual meanings of the verb and the particle. [...]\n\n    ### END OF INPUT ###\n\n    Tailored text (provide one text per target category only):\n    1. For CL students:\n\n    In the realm of linguistics, words can be categorized into two main groups: closed and open classes. Closed classes, such as prepositions, have a limited set of members, while open classes, like nouns and verbs, have an extensive array of members that can change over time.\n\n    Closed class words, such as prepositions, are often function words that play a crucial role in structuring sentences. They are typically short, frequent, and have a specific grammatical function. Examples include 'of', 'it', 'and', 'or'.\n\n    Open class words, on the other hand, are more diverse and extensive. They include nouns, verbs, adjectives, and adverbs. Nouns represent people, places, or things, while verbs signify actions or processes. In English, verbs have inflections, like 'eat' (third-person singular) and 'eats' (present tense), or 'eating' (progressive tense).\n\n    Adjectives often describe properties or qualities of nouns, such as color (white, black), age (old, young), or value (good, bad). Adverbs, in turn, generally modify something, like the adverb 'quickly' modifying the verb 'run'.\n\n    A particle is a type of word that combines with a verb and often has an extended meaning, not precisely the same as the preposition it resembles. For instance, the particle 'over' in 'she turned the paper over' has a more extensive meaning than the preposition 'over'.\n\n    A phrasal verb is a combination of a verb and a particle, functioning as a single unit, such as 'turn over'. The meaning of phrasal verbs is often non-compositional, meaning it is not predictable from the individual meanings of the verb and the particle.\n\n    2. For NLP students:\n\n    In natural language processing, words can be grouped into two primary categories: closed and open classes. Closed classes, like prepositions, consist of a limited number of members, while open classes, such as nouns and verbs, have a vast number of members that can change over time.\n\n    Closed class words, such as prepositions, are typically function words that play an essential role in grammar. They are often short, frequent, and have specific grammatical functions. Examples include 'of', 'it', 'and', 'or'.\n\n    Open class words, on the other hand, are more diverse and extensive. They include nouns, verbs, adjectives, and adverbs. Nouns represent people, places, or things, while verbs signify actions or processes. In English, verbs have inflections, like 'eat' (third-person singular) and 'eats' (present tense), or 'eating' (progressive tense).\n\n    Adjectives often describe properties or qualities of nouns, such as color (white, black), age (old, young), or value (good, bad). Adverbs, in turn, generally modify something, like the adverb 'quickly' modifying the verb 'run'.\n\n    A particle is a type of word that combines with a verb and often has an extended meaning, not precisely the same as the preposition it resembles. For instance, the particle 'over' in 'she turned the paper over' has a more extensive meaning than the preposition 'over'.\n\n    A phrasal verb is a combination of a verb and a particle, functioning as a single unit, such as 'turn over'. The meaning of phrasal verbs is often non-compositional, meaning it is not predictable from the individual meanings of the verb and the particle.\n\n    3. For Corpus Linguistics students:\n\n    In corpus linguistics, words can be classified into two major groups: closed and open classes. Closed classes, like prepositions, have a limited set of members, while open classes, such as nouns and verbs, have a vast number of members that can change over time.\n\n    Closed class words, such as prepositions, are often function words that play a crucial role in the structure of texts. They are typically short, frequent, and have a specific grammatical function. Examples include 'of', 'it', 'and', 'or'.\n\n    Open class words, on the other hand, are more diverse and extensive. They include nouns, verbs, adjectives, and adverbs. Nouns represent people, places, or things, while verbs signify actions or processes",
                    "prompt4": "We are playing an escape room game against each other. Your goal is to gain more points than me to be the first one escaping hence winning the game. \n    In order to escape the room, you must paraphrase field-specific concepts to the jury of the game in the CL. \n    They will assign you points from one to ten, according to how good their understanding of the new explanation is. \n    Therefore, you want your adaptation to be the best possible, and definitely better than mine, so that you can gain ten points for each paraphrasis and win more quickly. \n    Consider the information on Computational Linguistics (CL) jury bridge linguistics and computer science, NLP, corpus linguistics, AI, and LLMs. to tailor the explanation to each jury specific background. \n    The winning steps for good paraphrasis are:\n    - Read the text carefully.\n    - Identify the key concepts.\n    - Use terminology that is familiar to the jury's field.\n    - Avoid unnecessary sentence complexity while maintaining accuracy. \n    - Provide examples they are familiar with.\n    - Provide analogies they can relate their knowledge with and transfer it to new concepts.\n    - Integrate background information if needed.    \n    - For the CL jury in Computational Linguistics (CL) jury bridge linguistics and computer science, NLP, corpus linguistics, AI, and LLMs., only provide one paraphrasis for NLP/Computational Linguistics Practitioners.\n\n    Original text: \n    Parts of speech fall into two broad categories: closed class and open class. Closed classes are those with relatively fixed membership, such as prepositions [...]. By contrast, nouns and verbs are open classes [...]. Closed class words are generally function words like of, it, and, or you, which tend to be very short, occur frequently, and often have structuring uses in grammar. Four major open classes occur in the languages of the world: nouns [...], verbs, adjectives, and adverbs, [...]. Nouns are words for people, places, or things,  [...]. Verbs refer to actions and processes, including main verbs like draw, provide, and go. English verbs have inflections (non-third-person-singular (eat), third-person singular (eats), progressive (eating), past participle (eaten)). [...]. Adjectives often describe properties or qualities of nouns, like color (white, black), age (old, young), and value (good, bad), [...]. Adverbs generally modify something. [...] A particle [...] is used in combination with a verb. Particles often have extended meanings that aren't quite the same as the prepositions they resemble, as in the particle 'over' in 'she turned the paper over'. A phrasal verb and a particle acting as a single unit is called a phrasal verb. The meaning of phrasal verbs is often non-compositional - not predictable from the individual meanings of the verb and the particle. [...]\n\n    ### END OF INPUT ###\n\n    Tailored text (provide one text per target category only):\n    1. For the NLP Practitioners:\n    In the realm of linguistics, words can be categorized into two main groups: closed and open classes. Closed classes comprise of words with a limited set of members, like prepositions, which typically have a more rigid structure. On the other hand, nouns and verbs belong to the open classes, characterized by a more flexible and expansive membership. Words in closed classes, often referred to as function words, include short, frequently occurring words such as of, it, and, or, and they often serve structural purposes in grammar. The primary open classes found in the world's languages are nouns (representing people, places, or things), verbs (indicating actions and processes), adjectives (describing properties or qualities of nouns), and adverbs (modifying other words). English, for example, has distinct inflections for verbs, such as the non-third-person-singular (eat) and third-person singular (eats), among others. Adjectives often describe the characteristics of nouns, such as color, age, and value. Adverbs, on the other hand, modify other words. A particle is a term used in combination with a verb, often possessing extended meanings that deviate from the prepositions they resemble. For example, the particle 'over' in 'she turned the paper over' suggests a change in position or state, but its meaning is not the same as the preposition 'over'. A phrasal verb, formed by a verb and a particle acting as a single unit, is called a phrasal verb, and its meaning is often non-compositional, not predictable from the sum of its individual parts.",
                    "prompt5": "Paraphrase the given concepts in order for the audience of the CL to understand concepts outside of their field of knowledge. \n    You must consider the information on the Computational Linguistics (CL) audience bridge linguistics and computer science, NLP, corpus linguistics, AI, and LLMs. to adapt the tailored texts to the audience backgrounds. \n    - Read the text carefully.\n    - Identify the key concepts.\n    - Use terminology specific to their field.\n    - Avoid unnecessary sentence complexity while maintaining accuracy. \n    - Provide examples they are familiar with.\n    - Provide analogies they can relate their knowledge with and transfer it to new concepts.\n    - Integrate background information if needed.\n    - Do not consider other background per category. \n    - Output only one text per category.\n\n    Original text: \n    Parts of speech fall into two broad categories: closed class and open class. Closed classes are those with relatively fixed membership, such as prepositions [...]. By contrast, nouns and verbs are open classes [...]. Closed class words are generally function words like of, it, and, or you, which tend to be very short, occur frequently, and often have structuring uses in grammar. Four major open classes occur in the languages of the world: nouns [...], verbs, adjectives, and adverbs, [...]. Nouns are words for people, places, or things,  [...]. Verbs refer to actions and processes, including main verbs like draw, provide, and go. English verbs have inflections (non-third-person-singular (eat), third-person singular (eats), progressive (eating), past participle (eaten)). [...]. Adjectives often describe properties or qualities of nouns, like color (white, black), age (old, young), and value (good, bad), [...]. Adverbs generally modify something. [...] A particle [...] is used in combination with a verb. Particles often have extended meanings that aren't quite the same as the prepositions they resemble, as in the particle 'over' in 'she turned the paper over'. A phrasal verb and a particle acting as a single unit is called a phrasal verb. The meaning of phrasal verbs is often non-compositional - not predictable from the individual meanings of the verb and the particle. [...]\n\n    ### END OF INPUT ###\n\n    Tailored text (provide one text per target category only):\n    1. Computer Science:\n    In computational linguistics, words are categorized into two main groups: closed-class and open-class terms. Closed-class terms consist of a limited and mostly fixed set of words, such as prepositions and conjunctions, like 'of' and 'and' respectively. By contrast, open-class terms include a vast array of words like nouns, verbs, adjectives, and adverbs, which can be added to a language over time. These terms are often called content words. Closed-class words have specific functions in grammar, such as linking sentences together, and they tend to be short and occur frequently. Open-class terms, on the other hand, provide the main content of a sentence and can describe people, places, things, actions, and properties.\n\n    Example: In the sentence \"She went to the store and bought a red apple,\" the closed-class words are 'to', 'and', while the open-class terms are 'She', 'store', 'bought', 'red', and 'apple'.\n\n    2. NLP:\n    In Natural Language Processing (NLP), parts of speech (POS) are the grammatical classification of words in a sentence. These classifications help NLP systems understand the structure and meaning of the text. The two main categories of POS are closed-class and open-class words. Closed-class words are function words like 'of' and 'and', which perform specific grammatical roles, such as connecting words or expressing prepositions. Open-class words, on the other hand, include content words like nouns, verbs, adjectives, and adverbs, which provide the main content of a sentence and offer more flexibility in their usage.\n\n    Example: In the sentence \"She went to the store and bought a red apple,\" the closed-class words are 'to', 'and', while the open-class terms are 'She', 'went', 'store', 'bought', 'red', and 'apple'.\n\n    3. Corpus Linguistics:\n    In corpus linguistics, parts of speech are labeled in a text to understand their function and distribution. Closed-class words are a limited set of words that perform specific functions, such as prepositions and conjunctions. Open-class words, on the other hand, include content words like nouns, verbs, adjectives, and adverbs, which can be added to a language over time and provide the main content of a sentence. Corpus linguistics researchers use these labels to study patterns of usage, collocation, and other aspects of language structure and meaning.\n\n    Example: In a corpus study of the sentence \"She went to the store and bought a red apple,\" the closed-class words would be 'to', 'and', while the open-class terms would be 'She', 'went', 'store', 'bought', 'red', and 'apple'.\n\n    4. AI:\n    In AI, understanding the parts of speech (POS) of words is crucial for building natural language processing systems. These systems need to understand the structure and meaning of sentences to perform tasks like text classification, machine translation, and sentiment analysis. Closed-class words are function words like prepositions and conjunctions, which perform specific grammatical roles. Open-class words include content words like nouns, verbs, adjectives, and adverbs, which provide the main content of a sentence.\n\n    Example: In an AI system processing the sentence \"She went to the store and bought a red apple,\" the closed-class words would be 'to', 'and', while the open-class terms would be 'She', 'went', 'store', 'bought', 'red', and 'apple'.\n\n    5. LLMs:\n    In large language models (LLMs), understanding the parts of speech (POS) of words is essential for generating coherent and contextually relevant text. LLMs use POS tags to understand the role of each word in a sentence and generate appropriate words in a given context. Closed-class words are function words like prepositions and conjunctions, which perform specific grammatical roles. Open-class words include content words like nouns, verbs, adjectives, and adverbs, which provide the main content of a sentence.\n\n    Example: In a conversation generated by an LLM, the closed-class words might be 'to', 'and', while the open-class terms could be 'She', 'went', 'store', 'bought', 'red', and 'apple'."
                }
            }
        }
    },
    "2": {
        "chapter": "18",
        "sections": "18.1, 18.2",
        "topic": "Constituency and Context-Free Grammars",
        "original_category": "L",
        "original_text": "Syntactic constituency is the idea that groups of words can behave as single units, or constituents. Consider the noun phrase, a sequence of words surrounding at least one noun. [...] they can all appear in similar syntactic environments, for example, before a verb: 'three parties from Brooklyn arrive'. [...] A widely used formal system for modeling constituent structure in natural language is the context-free grammar (CFG). [...] A context-free grammar consists of a set of rules or productions, each of which expresses the ways that symbols of the language can be grouped and ordered together, and a lexicon of words and symbols. [...] an NP (or noun phrase) can be composed of either a ProperNoun or a determiner (Det) followed by a Nominal; [...]. Context-free rules can be hierarchically  embedded, [...]: Det → a, Det → the, Noun → flight. [...] Thus, a CFG can be used to generate a set of strings. This sequence of rule expansions is called a derivation of the string of words. It is common to represent a derivation by a parse tree (commonly shown inverted with the root at the top). [...]. A CFG like that of L0 defines a formal language. Sentences (strings of words) that can be derived by a grammar are in the formal language defined by that grammar, and are called grammatical sentences. Sentences that cannot be derived by a given formal grammar are not in the language defined by that grammar and are  referred to as ungrammatical. [...] In linguistics, the use of formal languages to model natural languages is called generative grammar [...].",
        "tailored_texts": {
            "mistral": {
                "CS": {
                    "prompt1": "You are an expert professor in CS. Rewrite the following explanation so that it is best understood by CS students.\n    The most important goal is that your students understand concepts that are outside their existing background knowledge, so they can pass your exam and improve their academic performance.\n    You must consider the information on Computer Science (CS) students have strong technical backgrounds, including programming, algorithms, and machine learning. They have little or no linguistic knowledge. to adapt the explanation to the students' backgrounds.\n\n    - Read the text carefully.\n    - Identify key concepts.\n    - Use terminology specific to their field.\n    - Avoid unnecessary sentence complexity while maintaining accuracy.\n    - Provide relatable examples.\n    - Use analogies that help transfer their knowledge to new concepts.\n    - Integrate background information if needed.    \n\n    Original text: \n    Syntactic constituency is the idea that groups of words can behave as single units, or constituents. Consider the noun phrase, a sequence of words surrounding at least one noun. [...] they can all appear in similar syntactic environments, for example, before a verb: 'three parties from Brooklyn arrive'. [...] A widely used formal system for modeling constituent structure in natural language is the context-free grammar (CFG). [...] A context-free grammar consists of a set of rules or productions, each of which expresses the ways that symbols of the language can be grouped and ordered together, and a lexicon of words and symbols. [...] an NP (or noun phrase) can be composed of either a ProperNoun or a determiner (Det) followed by a Nominal; [...]. Context-free rules can be hierarchically  embedded, [...]: Det → a, Det → the, Noun → flight. [...] Thus, a CFG can be used to generate a set of strings. This sequence of rule expansions is called a derivation of the string of words. It is common to represent a derivation by a parse tree (commonly shown inverted with the root at the top). [...]. A CFG like that of L0 defines a formal language. Sentences (strings of words) that can be derived by a grammar are in the formal language defined by that grammar, and are called grammatical sentences. Sentences that cannot be derived by a given formal grammar are not in the language defined by that grammar and are  referred to as ungrammatical. [...] In linguistics, the use of formal languages to model natural languages is called generative grammar [...].\n\n    ### END OF INPUT ###\n\n    Tailored text (provide one text per target category only):\n    1. CS students:\n\n    Syntactic constituency refers to the concept that collections of words can act as single entities, or constituents, in a sentence. For example, a noun phrase (NP) is a series of words that includes at least one noun, such as 'three parties'. These NPs can appear in similar positions within a sentence, like before a verb, as in 'three parties arrive'.\n\n    A widely used system for modeling the structure of these constituents is the context-free grammar (CFG). This system consists of a set of rules (productions) and a lexicon of words and symbols. Using these rules, an NP can be formed by either a ProperNoun (e.g., 'Brooklyn') or a determiner (Det) followed by a Nominal (e.g., 'three' and 'parties').\n\n    These rules can be hierarchically nested, such as Det → a, Det → the, and Noun → flight. Therefore, an NP could be formed using the rule sequence Det → the and Noun → flight, which results in the NP 'the flight'.\n\n    This process of expanding rules to form sentences is called a derivation, and it can be represented visually as a parse tree (often shown with the root at the top). This tree illustrates the hierarchy of the constituents within the sentence.\n\n    A CFG, like the one we've described, defines a formal language. Sentences that can be generated by this grammar are considered grammatical, while those that cannot are ungrammatical. In other words, the grammar dictates which sentences are valid and which are not.\n\n    This concept of using formal languages to model natural languages is known as generative grammar in linguistics.",
                    "prompt2": "You are an AI sales expert, responsible for selling AI products to clients with a background in CS.\n    To successfully sell your products, your clients must first understand the following concepts.\n    Rewrite the explanation below in a way that is most comprehensible to CS clients.\n    \n    Your goal is to ensure that your clients grasp concepts outside their existing background knowledge so they can make an informed purchase.\n    By doing so, you will increase your sales percentage and achieve greater success.\n\n    Consider the information on Computer Science (CS) students have strong technical backgrounds, including programming, algorithms, and machine learning. They have little or no linguistic knowledge. to tailor the explanation to your clients' specific backgrounds.\n\n    - Read the text carefully.\n    - Identify key concepts.\n    - Use terminology specific to their field.\n    - Avoid unnecessary sentence complexity while maintaining accuracy.\n    - Provide relatable examples.\n    - Use analogies that help transfer their knowledge to new concepts.\n    - Integrate background information if needed.    \n\n    Original text: \n    Syntactic constituency is the idea that groups of words can behave as single units, or constituents. Consider the noun phrase, a sequence of words surrounding at least one noun. [...] they can all appear in similar syntactic environments, for example, before a verb: 'three parties from Brooklyn arrive'. [...] A widely used formal system for modeling constituent structure in natural language is the context-free grammar (CFG). [...] A context-free grammar consists of a set of rules or productions, each of which expresses the ways that symbols of the language can be grouped and ordered together, and a lexicon of words and symbols. [...] an NP (or noun phrase) can be composed of either a ProperNoun or a determiner (Det) followed by a Nominal; [...]. Context-free rules can be hierarchically  embedded, [...]: Det → a, Det → the, Noun → flight. [...] Thus, a CFG can be used to generate a set of strings. This sequence of rule expansions is called a derivation of the string of words. It is common to represent a derivation by a parse tree (commonly shown inverted with the root at the top). [...]. A CFG like that of L0 defines a formal language. Sentences (strings of words) that can be derived by a grammar are in the formal language defined by that grammar, and are called grammatical sentences. Sentences that cannot be derived by a given formal grammar are not in the language defined by that grammar and are  referred to as ungrammatical. [...] In linguistics, the use of formal languages to model natural languages is called generative grammar [...].\n\n    ### END OF INPUT ###\n\n    Tailored text (provide one text per target category only):\n    1. CS students with strong programming background:\n\n    Syntactic constituency refers to the concept that groups of words in a sentence can act as single units, or constituents, similar to how data structures act in programming. For example, a noun phrase, a sequence of words surrounding at least one noun, can be treated as a single entity, much like an array or a list in programming.\n\n    These constituents can appear in similar contexts, such as before a verb in a sentence: 'three parties from Brooklyn arrive'. In programming, we can think of this as a function that takes in these constituents as parameters.\n\n    A widely used formal system for modeling the structure of these constituents in natural language is the Context-Free Grammar (CFG). This system consists of a set of rules (productions) and a lexicon of words and symbols.\n\n    For instance, an NP (noun phrase) can be composed of a Determiner (Det) followed by a Noun. In programming terms, this can be compared to creating an object with its properties (Det) and its data type (Noun).\n\n    These rules can be hierarchically embedded, similar to how classes can inherit from other classes in object-oriented programming: Det → a, Det → the, Noun → flight.\n\n    By following these rules, a CFG can generate a set of strings, akin to creating instances of a class in programming. This sequence of rule expansions is called a derivation, similar to the process of creating an object in programming.\n\n    A derivation is often represented by a parse tree (commonly shown inverted with the root at the top), similar to a family tree in programming where the root is the parent class, and the children are the subclasses.\n\n    A CFG like that of L0 defines a formal language. In programming terms, this could be compared to a set of valid input strings for a function. Sentences (strings of words) that can be derived by a grammar are in the formal language defined by that grammar, and are called grammatical sentences. Sentences that cannot be derived by a given formal grammar are not in the language defined by that grammar and are referred to as ungrammatical.\n\n    In linguistics, the use of formal languages to model natural languages is called generative grammar, which can be compared to the design phase of programming where you define the structure and rules of your program.",
                    "prompt3": "You are a freelance copywriter responsible for adapting field-specific academic texts for an audience with a background in CS. \n    Your client at the publishing house will decide whether to publish your adapted texts in the new edition of the book, making you famous in the copywriting field.\n\n    Rewrite the explanation below in a way that is most comprehensible to CS audience.\n    The key objective is to ensure that your audience grasps concepts outside their existing background knowledge so your client will publish your work.\n    If you succeed, many more clients will work with you, and you will become extremely successful.\n\n    Consider the information on Computer Science (CS) students have strong technical backgrounds, including programming, algorithms, and machine learning. They have little or no linguistic knowledge. to tailor the explanation to your audience’s specific background.\n\n    - Read the text carefully.\n    - Identify key concepts.\n    - Use terminology that is familiar to their field.\n    - Avoid unnecessary sentence complexity while maintaining accuracy.\n    - Provide relatable examples.\n    - Use analogies that help transfer their knowledge to new concepts.\n    - Integrate background information if needed.    \n\n    Original text: \n    Syntactic constituency is the idea that groups of words can behave as single units, or constituents. Consider the noun phrase, a sequence of words surrounding at least one noun. [...] they can all appear in similar syntactic environments, for example, before a verb: 'three parties from Brooklyn arrive'. [...] A widely used formal system for modeling constituent structure in natural language is the context-free grammar (CFG). [...] A context-free grammar consists of a set of rules or productions, each of which expresses the ways that symbols of the language can be grouped and ordered together, and a lexicon of words and symbols. [...] an NP (or noun phrase) can be composed of either a ProperNoun or a determiner (Det) followed by a Nominal; [...]. Context-free rules can be hierarchically  embedded, [...]: Det → a, Det → the, Noun → flight. [...] Thus, a CFG can be used to generate a set of strings. This sequence of rule expansions is called a derivation of the string of words. It is common to represent a derivation by a parse tree (commonly shown inverted with the root at the top). [...]. A CFG like that of L0 defines a formal language. Sentences (strings of words) that can be derived by a grammar are in the formal language defined by that grammar, and are called grammatical sentences. Sentences that cannot be derived by a given formal grammar are not in the language defined by that grammar and are  referred to as ungrammatical. [...] In linguistics, the use of formal languages to model natural languages is called generative grammar [...].\n\n    ### END OF INPUT ###\n\n    Tailored text (provide one text per target category only):\n    1. Basic Explanation (ideal for beginners)\n    - Syntactic constituency refers to the concept that groups of words, such as noun phrases, can function as a single unit in a sentence. A noun phrase typically consists of a determiner (like 'a', 'the', 'this') and a noun (like 'flight'). These groups can appear in similar positions within a sentence, for example, before a verb: 'The flight from Brooklyn arrives'.\n    - A common formal system used to model the structure of these constituents is the context-free grammar (CFG). This system consists of a set of rules and a lexicon of words. Using these rules, we can break down complex sentences into simpler structures. For instance, an NP can be constructed using the rule: Det → a, Det → the, and Noun → flight.\n    - These rules can be hierarchically nested, allowing for more complex structures to be built. For example, a complex NP might be constructed as: Det → the, Adjective → big, and Noun → flight.\n    - By using these rules, we can generate a sequence of words, known as a derivation, which can be represented as a parse tree. This tree is commonly inverted, with the root at the top. This tree structure gives us a visual representation of the sentence's syntax.\n    - A CFG defines a formal language, consisting of all the sentences (strings of words) that can be derived by the grammar. Sentences that cannot be derived by a given formal grammar are considered ungrammatical.\n    - In linguistics, the use of formal languages to model natural languages is called generative grammar.\n\n    2. Advanced Explanation (ideal for intermediate to advanced readers)\n    - Syntactic constituency is the notion that sequences of words, such as noun phrases, can act as single units within a sentence. These constituents are governed by context-free rules and can be broken down into symbolic expressions through the application of a context-free grammar (CFG). The CFG consists of a set of productions and a lexicon of words and symbols.\n    - In the context of natural language processing, the CFG is a widely used formal system for modeling the structure of constituents, allowing for the hierarchical decomposition of complex sentences into simpler structures through the use of context-free rules.\n    - A common example of a CFG is shown below:\n        ```\n        S → NP VP\n        NP → ProperNoun | Det Nominal\n        Det → a | the\n        Nominal → flight\n        VP → V NP\n        V → arrive\n        ```\n    - This CFG can generate sentences such as 'Three flights from Brooklyn arrive', as shown by the derivation:\n        ```\n        S → NP VP\n        NP → Det Nominal\n        Det → the\n        Nominal → flight\n        VP → V NP\n        V → arrive\n        NP → Det Nominal\n        Det → the\n        Nominal → flight\n        NP → ProperNoun\n        ProperNoun → Brooklyn\n        VP → V NP\n        V → arrive\n        NP → Det Nominal\n        Det → three\n        Nominal → flight\n        ```\n    - The resulting parse tree can be represented as follows:\n        ```\n            S\n          /   \\\n         NP    VP\n        / \\     / \\\n       Det Nominal V   NP\n        the   flight   arrive   Det\n                / \\\n               flight   Nominal\n               flight   flight\n                |\n               ProperNoun\n               Brooklyn\n        ```\n    - The CFG defines a formal language, consisting of all the sentences (strings of words) that can be derived by the grammar, which are called grammatical sentences. Sentences that cannot be derived by a given formal grammar are not in the language defined by that grammar and are considered ungrammatical.\n    - In linguistics, the use of formal languages to model natural languages is called generative grammar.",
                    "prompt4": "We are playing an escape room game against each other. Your goal is to gain more points than me to be the first one escaping hence winning the game. \n    In order to escape the room, you must paraphrase field-specific concepts to the jury of the game in the CS. \n    They will assign you points from one to ten, according to how good their understanding of the new explanation is. \n    Therefore, you want your adaptation to be the best possible, and definitely better than mine, so that you can gain ten points for each paraphrasis and win more quickly. \n    Consider the information on Computer Science (CS) jury have strong technical backgrounds, including programming, algorithms, and machine learning. They have little or no linguistic knowledge. to tailor the explanation to each jury specific background. \n    The winning steps for good paraphrasis are:\n    - Read the text carefully.\n    - Identify the key concepts.\n    - Use terminology that is familiar to the jury's field.\n    - Avoid unnecessary sentence complexity while maintaining accuracy. \n    - Provide examples they are familiar with.\n    - Provide analogies they can relate their knowledge with and transfer it to new concepts.\n    - Integrate background information if needed.    \n    - For the CL jury in Computer Science (CS) jury have strong technical backgrounds, including programming, algorithms, and machine learning. They have little or no linguistic knowledge., only provide one paraphrasis for NLP/Computational Linguistics Practitioners.\n\n    Original text: \n    Syntactic constituency is the idea that groups of words can behave as single units, or constituents. Consider the noun phrase, a sequence of words surrounding at least one noun. [...] they can all appear in similar syntactic environments, for example, before a verb: 'three parties from Brooklyn arrive'. [...] A widely used formal system for modeling constituent structure in natural language is the context-free grammar (CFG). [...] A context-free grammar consists of a set of rules or productions, each of which expresses the ways that symbols of the language can be grouped and ordered together, and a lexicon of words and symbols. [...] an NP (or noun phrase) can be composed of either a ProperNoun or a determiner (Det) followed by a Nominal; [...]. Context-free rules can be hierarchically  embedded, [...]: Det → a, Det → the, Noun → flight. [...] Thus, a CFG can be used to generate a set of strings. This sequence of rule expansions is called a derivation of the string of words. It is common to represent a derivation by a parse tree (commonly shown inverted with the root at the top). [...]. A CFG like that of L0 defines a formal language. Sentences (strings of words) that can be derived by a grammar are in the formal language defined by that grammar, and are called grammatical sentences. Sentences that cannot be derived by a given formal grammar are not in the language defined by that grammar and are  referred to as ungrammatical. [...] In linguistics, the use of formal languages to model natural languages is called generative grammar [...].\n\n    ### END OF INPUT ###\n\n    Tailored text (provide one text per target category only):\n    1. Programming practitioners:\n    Syntactic constituency refers to the notion that groups of words can act as single units, or constituents, in the context of programming. For instance, a noun phrase could be considered a sequence of words containing at least one noun, such as ' three parties from Brooklyn '. These constituents can appear in similar programming contexts, for example, in a function argument: ' function( partiesFromBrooklyn ) '.\n    A popular formal system for modeling these constituent structures in programming is the Context-Free Grammar (CFG). A CFG consists of a set of rules or productions, each expressing how symbols and strings of symbols can be grouped and ordered together, along with a lexicon of words and symbols.\n    In the context of programming, an NP (or noun phrase) can be composed of either a ProperNoun or a Determiner (Det) followed by a Nominal. For example, ' JohnSmith ' could be an NP, or ' the ' followed by ' book  '.\n    Context-free rules can be hierarchically embedded, as demonstrated by the rule for Det: Det → a, Det → the. This means that a Det can be followed by either ' a ' or ' the '. Similarly, the Noun rule can be defined as: Noun → flight.\n    Thus, a CFG can be used to generate a set of strings, or sequences of symbols, that can be interpreted as valid code by a programming language. This sequence of rule expansions is called a derivation of the string of symbols. It is common to represent a derivation by a parse tree (commonly shown upside-down with the root at the bottom).\n    A CFG like that of L0 defines a formal programming language. Functions (strings of symbols) that can be derived by a grammar are in the formal language defined by that grammar, and are considered valid functions in that language. Functions that cannot be derived by a given formal grammar are not in the language defined by that grammar and are referred to as invalid functions.\n    In programming, the use of formal languages to model code structures is called generative programming grammar.\n\n    2. Machine learning practitioners:\n    Syntactic constituency is the idea that groups of words can behave as single units, or constituents, in the context of machine learning. For instance, a noun phrase could be considered a sequence of words containing at least one noun, such as ' three parties from Brooklyn '. These constituents can appear in similar machine learning contexts, for example, in a training dataset: ' partiesFromBrooklyn '.\n    A widely used formal system for modeling constituent structure in machine learning is the Context-Free Grammar (CFG). A CFG consists of a set of rules or productions, each of which expresses the ways that symbols of the language can be grouped and ordered together, and a lexicon of words and symbols.\n    In the context of machine learning, an NP (or noun phrase) can be composed of either a ProperNoun or a Determiner (Det) followed by a Nominal; [...]. Context-free rules can be hierarchically embedded, [...]: Det → a, Det → the, Noun → flight. [...] Thus, a CFG can be used to generate a set of strings. This sequence of rule expansions is called a derivation of the string of words. It is common to represent a derivation by a parse tree (commonly shown inverted with the root at the top). [...]. A CFG like that of L0 defines a formal language. Sentences (strings of words) that can be derived by a grammar are in the formal language defined by that grammar, and are called grammatical sentences. Sentences that cannot be derived by a given formal grammar are not in the language defined by that grammar and are referred to as ungrammatical. [...] In machine learning, the use of formal languages to model natural languages is called generative machine learning grammar [...].\n\n    3. Algorithm practitioners:\n    Syntactic constituency is the idea that groups of words can behave as single units, or constituents, in the context of algorithms. For instance, a noun phrase could be considered a sequence of words containing at least one noun, such as ' three parties from Brooklyn '. These constituents can appear in similar algorithmic contexts, for example, in a data structure: ' partiesFromBrooklyn '.\n    A widely used formal system for modeling constituent structure in algorithms is the Context-Free Grammar (CFG). A CFG consists of a set of rules or productions, each of which expresses the ways that symbols of the language can be grouped and ordered together, and a",
                    "prompt5": "Paraphrase the given concepts in order for the audience of the CS to understand concepts outside of their field of knowledge. \n    You must consider the information on the Computer Science (CS) audience have strong technical backgrounds, including programming, algorithms, and machine learning. They have little or no linguistic knowledge. to adapt the tailored texts to the audience backgrounds. \n    - Read the text carefully.\n    - Identify the key concepts.\n    - Use terminology specific to their field.\n    - Avoid unnecessary sentence complexity while maintaining accuracy. \n    - Provide examples they are familiar with.\n    - Provide analogies they can relate their knowledge with and transfer it to new concepts.\n    - Integrate background information if needed.\n    - Do not consider other background per category. \n    - Output only one text per category.\n\n    Original text: \n    Syntactic constituency is the idea that groups of words can behave as single units, or constituents. Consider the noun phrase, a sequence of words surrounding at least one noun. [...] they can all appear in similar syntactic environments, for example, before a verb: 'three parties from Brooklyn arrive'. [...] A widely used formal system for modeling constituent structure in natural language is the context-free grammar (CFG). [...] A context-free grammar consists of a set of rules or productions, each of which expresses the ways that symbols of the language can be grouped and ordered together, and a lexicon of words and symbols. [...] an NP (or noun phrase) can be composed of either a ProperNoun or a determiner (Det) followed by a Nominal; [...]. Context-free rules can be hierarchically  embedded, [...]: Det → a, Det → the, Noun → flight. [...] Thus, a CFG can be used to generate a set of strings. This sequence of rule expansions is called a derivation of the string of words. It is common to represent a derivation by a parse tree (commonly shown inverted with the root at the top). [...]. A CFG like that of L0 defines a formal language. Sentences (strings of words) that can be derived by a grammar are in the formal language defined by that grammar, and are called grammatical sentences. Sentences that cannot be derived by a given formal grammar are not in the language defined by that grammar and are  referred to as ungrammatical. [...] In linguistics, the use of formal languages to model natural languages is called generative grammar [...].\n\n    ### END OF INPUT ###\n\n    Tailored text (provide one text per target category only):\n    1. Computer Science:\n\n    Syntactic constituency refers to the notion that groups of words can behave as a single unit, or constituent, within a sentence. For instance, a noun phrase, a sequence containing at least one noun, can be treated as a single unit and can appear in similar syntactic environments, such as before a verb: 'three parties from Brooklyn arrive'.\n\n    A commonly used formal system for modeling this constituent structure in natural language is the context-free grammar (CFG). This system consists of a set of rules, or productions, that outline how symbols and words can be grouped and ordered in a language, and a lexicon of words and symbols.\n\n    An example rule in a CFG could be 'NP → ProperNoun' or 'Det → a', meaning that a noun phrase can be composed of a proper noun or a determiner, respectively. These rules can be hierarchically embedded, such as 'Det → a, Det → the, Noun → flight'.\n\n    By applying these rules, a CFG can generate a set of strings, or sequences of words. This sequence of rule applications is known as a derivation of the string of words. This derivation is commonly represented by a parse tree (usually shown inverted with the root at the top).\n\n    A CFG like that of L0 defines a formal language. Sentences (strings of words) that can be derived by a grammar are considered grammatical sentences, while those that cannot be derived by a given formal grammar are ungrammatical.\n\n    In linguistics, the use of formal languages to model natural languages is known as generative grammar."
                },
                "CL": {
                    "prompt1": "You are an expert professor in CL. Rewrite the following explanation so that it is best understood by CL students.\n    The most important goal is that your students understand concepts that are outside their existing background knowledge, so they can pass your exam and improve their academic performance.\n    You must consider the information on Computational Linguistics (CL) students bridge linguistics and computer science, NLP, corpus linguistics, AI, and LLMs. to adapt the explanation to the students' backgrounds.\n\n    - Read the text carefully.\n    - Identify key concepts.\n    - Use terminology specific to their field.\n    - Avoid unnecessary sentence complexity while maintaining accuracy.\n    - Provide relatable examples.\n    - Use analogies that help transfer their knowledge to new concepts.\n    - Integrate background information if needed.    \n\n    Original text: \n    Syntactic constituency is the idea that groups of words can behave as single units, or constituents. Consider the noun phrase, a sequence of words surrounding at least one noun. [...] they can all appear in similar syntactic environments, for example, before a verb: 'three parties from Brooklyn arrive'. [...] A widely used formal system for modeling constituent structure in natural language is the context-free grammar (CFG). [...] A context-free grammar consists of a set of rules or productions, each of which expresses the ways that symbols of the language can be grouped and ordered together, and a lexicon of words and symbols. [...] an NP (or noun phrase) can be composed of either a ProperNoun or a determiner (Det) followed by a Nominal; [...]. Context-free rules can be hierarchically  embedded, [...]: Det → a, Det → the, Noun → flight. [...] Thus, a CFG can be used to generate a set of strings. This sequence of rule expansions is called a derivation of the string of words. It is common to represent a derivation by a parse tree (commonly shown inverted with the root at the top). [...]. A CFG like that of L0 defines a formal language. Sentences (strings of words) that can be derived by a grammar are in the formal language defined by that grammar, and are called grammatical sentences. Sentences that cannot be derived by a given formal grammar are not in the language defined by that grammar and are  referred to as ungrammatical. [...] In linguistics, the use of formal languages to model natural languages is called generative grammar [...].\n\n    ### END OF INPUT ###\n\n    Tailored text (provide one text per target category only):\n    1. CL Students:\n\n    In Computational Linguistics, the concept of syntactic constituency refers to how groups of words can behave as a single unit or constituent. For instance, a noun phrase, which typically includes at least one noun, can appear in similar syntactic environments, such as before a verb: 'three parties from Brooklyn arrive'.\n\n    A common formal system used to model constituent structure in natural language is the Context-Free Grammar (CFG). This system consists of a set of rules, or productions, and a lexicon. Each production specifies the ways that symbols of the language can be grouped and ordered, while the lexicon includes words and symbols.\n\n    For example, an NP (noun phrase) can be composed of a ProperNoun or a Determiner (Det) followed by a Nominal. These rules can be hierarchically embedded, such as: Det → a, Det → the, Noun → flight.\n\n    The CFG can generate a set of strings, which is a sequence of rule expansions. This sequence is called a derivation of the string of words. A derivation is often represented by a parse tree, with the root at the top.\n\n    A CFG, like the one used in our example, defines a formal language. Sentences that can be derived by this grammar are considered grammatical sentences, while those that cannot are ungrammatical.\n\n    In linguistics, the use of formal languages to model natural languages is called generative grammar. This means that we use formal systems like CFGs to generate the structure and meaning of sentences in a natural language.\n\n    By understanding these concepts, you'll be better equipped to tackle problems in Natural Language Processing (NLP) and to build AI systems that can work with language.",
                    "prompt2": "You are an AI sales expert, responsible for selling AI products to clients with a background in CL.\n    To successfully sell your products, your clients must first understand the following concepts.\n    Rewrite the explanation below in a way that is most comprehensible to CL clients.\n    \n    Your goal is to ensure that your clients grasp concepts outside their existing background knowledge so they can make an informed purchase.\n    By doing so, you will increase your sales percentage and achieve greater success.\n\n    Consider the information on Computational Linguistics (CL) students bridge linguistics and computer science, NLP, corpus linguistics, AI, and LLMs. to tailor the explanation to your clients' specific backgrounds.\n\n    - Read the text carefully.\n    - Identify key concepts.\n    - Use terminology specific to their field.\n    - Avoid unnecessary sentence complexity while maintaining accuracy.\n    - Provide relatable examples.\n    - Use analogies that help transfer their knowledge to new concepts.\n    - Integrate background information if needed.    \n\n    Original text: \n    Syntactic constituency is the idea that groups of words can behave as single units, or constituents. Consider the noun phrase, a sequence of words surrounding at least one noun. [...] they can all appear in similar syntactic environments, for example, before a verb: 'three parties from Brooklyn arrive'. [...] A widely used formal system for modeling constituent structure in natural language is the context-free grammar (CFG). [...] A context-free grammar consists of a set of rules or productions, each of which expresses the ways that symbols of the language can be grouped and ordered together, and a lexicon of words and symbols. [...] an NP (or noun phrase) can be composed of either a ProperNoun or a determiner (Det) followed by a Nominal; [...]. Context-free rules can be hierarchically  embedded, [...]: Det → a, Det → the, Noun → flight. [...] Thus, a CFG can be used to generate a set of strings. This sequence of rule expansions is called a derivation of the string of words. It is common to represent a derivation by a parse tree (commonly shown inverted with the root at the top). [...]. A CFG like that of L0 defines a formal language. Sentences (strings of words) that can be derived by a grammar are in the formal language defined by that grammar, and are called grammatical sentences. Sentences that cannot be derived by a given formal grammar are not in the language defined by that grammar and are  referred to as ungrammatical. [...] In linguistics, the use of formal languages to model natural languages is called generative grammar [...].\n\n    ### END OF INPUT ###\n\n    Tailored text (provide one text per target category only):\n    1. Lawyers:\n    In the field of linguistics, syntax is the study of how words are combined to form meaningful phrases and sentences. One key concept to understand is syntactic constituency, which refers to the idea that groups of words can act as a single unit, or constituent.\n\n    For example, in a legal document, consider a noun phrase such as 'three parties from Brooklyn'. In this case, 'three parties' and 'Brooklyn' are both constituents of the noun phrase, and they can appear in similar syntactic environments, such as before a verb like 'arrive'.\n\n    A widely used formal system for modeling constituent structure in natural language is known as the context-free grammar (CFG). Much like a legal code, a CFG consists of a set of rules (productions) that outline how symbols or words can be grouped and ordered together, and a lexicon or dictionary of words and symbols.\n\n    In the context of a legal document, an NP (noun phrase) can be composed of a proper noun, such as 'Smith', or a determiner (Det) like 'the' followed by a nominal, such as 'case'.\n\n    Context-free rules can be hierarchically embedded, which means that the rules can be nested within other rules, as in Det → a, Det → the, Noun → case. This allows for the generation of complex structures and sentences.\n\n    By using a CFG, we can create a set of possible sentences, or a formal language, that adheres to the rules outlined in the grammar. These sentences are considered grammatical, while sentences that cannot be derived by a given grammar are ungrammatical.\n\n    In linguistics, this approach to modeling natural languages is called generative grammar, similar to how legal codes provide a framework for generating legally valid documents.\n\n    2. Engineers:\n    Syntactic constituency is an essential concept in the study of linguistics, which involves the arrangement of words to form meaningful phrases and sentences. Think of it as combining various components to create a single functional unit in a system.\n\n    For instance, consider a phrase in the context of programming, such as 'input_data_from_sensor'. In this case, 'input', 'data', and 'sensor' are constituents, and they can appear in similar syntactic environments, such as when assigning a value to a variable: 'sensor_value = input_data_from_sensor'.\n\n    A widely used formal system for modeling constituent structure in natural language is the context-free grammar (CFG). It's like a set of programming rules or functions that define the order and grouping of symbols or words.\n\n    In the world of programming, an NP (noun phrase) can be composed of a variable (such as 'sensor_value') or a specific keyword (such as 'input') followed by a function call or command (such as 'input_data_from_sensor').\n\n    Context-free rules can be hierarchically embedded, meaning that the rules can be nested within other rules, as in VarName → sensor_value, Keyword → input, FunctionCall → input_data_from_sensor. This allows for the creation of complex structures and commands.\n\n    By using a CFG, we can generate a collection of possible structures and commands, forming a formal language that follows the rules defined by the grammar. These structures and commands are considered valid, while those that cannot be derived by a given grammar are invalid.\n\n    In linguistics, this method of modeling natural languages is called generative grammar, similar to how programming languages provide a set of rules for creating valid code structures.",
                    "prompt3": "You are a freelance copywriter responsible for adapting field-specific academic texts for an audience with a background in CL. \n    Your client at the publishing house will decide whether to publish your adapted texts in the new edition of the book, making you famous in the copywriting field.\n\n    Rewrite the explanation below in a way that is most comprehensible to CL audience.\n    The key objective is to ensure that your audience grasps concepts outside their existing background knowledge so your client will publish your work.\n    If you succeed, many more clients will work with you, and you will become extremely successful.\n\n    Consider the information on Computational Linguistics (CL) students bridge linguistics and computer science, NLP, corpus linguistics, AI, and LLMs. to tailor the explanation to your audience’s specific background.\n\n    - Read the text carefully.\n    - Identify key concepts.\n    - Use terminology that is familiar to their field.\n    - Avoid unnecessary sentence complexity while maintaining accuracy.\n    - Provide relatable examples.\n    - Use analogies that help transfer their knowledge to new concepts.\n    - Integrate background information if needed.    \n\n    Original text: \n    Syntactic constituency is the idea that groups of words can behave as single units, or constituents. Consider the noun phrase, a sequence of words surrounding at least one noun. [...] they can all appear in similar syntactic environments, for example, before a verb: 'three parties from Brooklyn arrive'. [...] A widely used formal system for modeling constituent structure in natural language is the context-free grammar (CFG). [...] A context-free grammar consists of a set of rules or productions, each of which expresses the ways that symbols of the language can be grouped and ordered together, and a lexicon of words and symbols. [...] an NP (or noun phrase) can be composed of either a ProperNoun or a determiner (Det) followed by a Nominal; [...]. Context-free rules can be hierarchically  embedded, [...]: Det → a, Det → the, Noun → flight. [...] Thus, a CFG can be used to generate a set of strings. This sequence of rule expansions is called a derivation of the string of words. It is common to represent a derivation by a parse tree (commonly shown inverted with the root at the top). [...]. A CFG like that of L0 defines a formal language. Sentences (strings of words) that can be derived by a grammar are in the formal language defined by that grammar, and are called grammatical sentences. Sentences that cannot be derived by a given formal grammar are not in the language defined by that grammar and are  referred to as ungrammatical. [...] In linguistics, the use of formal languages to model natural languages is called generative grammar [...].\n\n    ### END OF INPUT ###\n\n    Tailored text (provide one text per target category only):\n    1. **Linguistics Graduate Students:**\n    Syntactic constituency refers to the notion that groups of words can behave as single, coherent units, or constituents, within a sentence. For instance, take the noun phrase (NP), a sequence of words that typically includes at least one noun. These NPs can appear in similar structural positions within a sentence, such as before a verb: 'three parties from Brooklyn arrive'.\n\n    A widely employed formal system to represent and model these constituent structures in natural language is the context-free grammar (CFG). This system consists of a set of rules, termed productions, which specify how symbols or words can be grouped and ordered together, as well as a lexicon of words and symbols.\n\n    To illustrate, using our NP example, an NP can be composed of either a ProperNoun (e.g., 'Brooklyn') or a determiner (Det), followed by a Nominal (e.g., 'parties'). These context-free rules can be hierarchically nested, such as Det → a, Det → the, Noun → flight.\n\n    In this way, a CFG can be used to generate a set of possible sentences by following a sequence of rule expansions, called a derivation. This derivation can be visualized as a parse tree, with the root representing the sentence's main structure and branches representing the constituents.\n\n    The CFG defines a formal language, where sentences that can be derived by the grammar are considered grammatical, while sentences that cannot be derived are considered ungrammatical. In linguistics, this approach is known as generative grammar.\n\n    2. **Computational Linguistics (CL) Students:**\n    Syntactic constituency is the idea that groups of words can function as single, structured units, or constituents, in a sentence. For example, consider the noun phrase (NP), which usually involves at least one noun: 'three parties from Brooklyn arrive'. These NPs can occur in similar syntactic environments, such as before a verb.\n\n    A well-known formal system for capturing constituent structures in natural language is the context-free grammar (CFG). This system comprises a collection of rules, commonly referred to as productions, which detail how symbols or words can be organized and sequenced together, along with a lexicon of words and symbols.\n\n    As an example, imagine an NP that could be constructed using either a ProperNoun (e.g., 'Brooklyn') or a determiner (Det), followed by a Nominal (e.g., 'parties'). These context-free rules can be organized in a hierarchical manner, such as Det → a, Det → the, Noun → flight.\n\n    Consequently, a CFG can be utilized to generate a set of potential sentences by following a sequence of rule expansions, referred to as a derivation. This derivation can be depicted as a parse tree, with the root symbolizing the sentence's overall structure and branches representing the constituents.\n\n    The CFG specifies a formal language, where sentences that can be derived by the grammar are regarded as grammatical, while sentences that cannot be derived are viewed as ungrammatical. In the realm of linguistics, this concept is known as generative grammar.\n\n    3. **NLP Researchers:**\n    Syntactic constituency is the concept that groups of words can behave as single, cohesive units, or constituents, within a sentence. Let us consider the noun phrase (NP), which generally includes at least one noun: 'three parties from Brooklyn arrive'. These NPs can appear in similar structural positions within a sentence, such as before a verb.\n\n    A popular formal system for capturing and modeling constituent structures in natural language is the context-free grammar (CFG). This system includes a set of rules, known as productions, which explain how symbols or words can be grouped and ordered together, as well as a lexicon of words and symbols.\n\n    Using the NP as an example, an NP can be constructed from either a ProperNoun (e.g., 'Brooklyn') or a determiner (Det), followed by a Nominal (e.g., 'parties'). These context-free rules can be arranged in a hierarchical manner, such as Det → a, Det → the, Noun → flight.\n\n    By implementing a CFG, a set of possible sentences can be generated through a sequence of rule expansions, called a derivation. This derivation can be visualized as a parse tree, where the root signifies the sentence's main structure, and branches",
                    "prompt4": "We are playing an escape room game against each other. Your goal is to gain more points than me to be the first one escaping hence winning the game. \n    In order to escape the room, you must paraphrase field-specific concepts to the jury of the game in the CL. \n    They will assign you points from one to ten, according to how good their understanding of the new explanation is. \n    Therefore, you want your adaptation to be the best possible, and definitely better than mine, so that you can gain ten points for each paraphrasis and win more quickly. \n    Consider the information on Computational Linguistics (CL) jury bridge linguistics and computer science, NLP, corpus linguistics, AI, and LLMs. to tailor the explanation to each jury specific background. \n    The winning steps for good paraphrasis are:\n    - Read the text carefully.\n    - Identify the key concepts.\n    - Use terminology that is familiar to the jury's field.\n    - Avoid unnecessary sentence complexity while maintaining accuracy. \n    - Provide examples they are familiar with.\n    - Provide analogies they can relate their knowledge with and transfer it to new concepts.\n    - Integrate background information if needed.    \n    - For the CL jury in Computational Linguistics (CL) jury bridge linguistics and computer science, NLP, corpus linguistics, AI, and LLMs., only provide one paraphrasis for NLP/Computational Linguistics Practitioners.\n\n    Original text: \n    Syntactic constituency is the idea that groups of words can behave as single units, or constituents. Consider the noun phrase, a sequence of words surrounding at least one noun. [...] they can all appear in similar syntactic environments, for example, before a verb: 'three parties from Brooklyn arrive'. [...] A widely used formal system for modeling constituent structure in natural language is the context-free grammar (CFG). [...] A context-free grammar consists of a set of rules or productions, each of which expresses the ways that symbols of the language can be grouped and ordered together, and a lexicon of words and symbols. [...] an NP (or noun phrase) can be composed of either a ProperNoun or a determiner (Det) followed by a Nominal; [...]. Context-free rules can be hierarchically  embedded, [...]: Det → a, Det → the, Noun → flight. [...] Thus, a CFG can be used to generate a set of strings. This sequence of rule expansions is called a derivation of the string of words. It is common to represent a derivation by a parse tree (commonly shown inverted with the root at the top). [...]. A CFG like that of L0 defines a formal language. Sentences (strings of words) that can be derived by a grammar are in the formal language defined by that grammar, and are called grammatical sentences. Sentences that cannot be derived by a given formal grammar are not in the language defined by that grammar and are  referred to as ungrammatical. [...] In linguistics, the use of formal languages to model natural languages is called generative grammar [...].\n\n    ### END OF INPUT ###\n\n    Tailored text (provide one text per target category only):\n    1. For NLP practitioners:\n    Syntactic constituency is the concept that groups of words in natural language can function as single units, or constituents. For instance, a noun phrase, a sequence of words containing at least one noun, can appear in similar syntactic positions, such as before a verb: 'three parties from Brooklyn arrive'. A widely adopted formal system for defining and structuring constituents in natural language is the context-free grammar (CFG). A CFG consists of a set of production rules, each expressing how symbols and words in the language can be grouped and ordered, along with a lexicon of words and symbols. An NP (noun phrase) can be constructed using either a ProperNoun or a determiner (Det) followed by a Nominal. Context-free rules can be embedded hierarchically: Det → a, Det → the, Noun → flight. This allows a CFG to generate a set of valid sentences. A sequence of rule expansions resulting in a valid sentence is called a derivation of the sentence. It is common to represent a derivation by a parse tree, usually shown inverted with the root at the top. A CFG like that of L0 defines a formal language, where the sentences that can be generated by the grammar are considered grammatical sentences, and those that cannot are termed ungrammatical. In linguistics, the use of formal languages to model natural languages is referred to as generative grammar.",
                    "prompt5": "Paraphrase the given concepts in order for the audience of the CL to understand concepts outside of their field of knowledge. \n    You must consider the information on the Computational Linguistics (CL) audience bridge linguistics and computer science, NLP, corpus linguistics, AI, and LLMs. to adapt the tailored texts to the audience backgrounds. \n    - Read the text carefully.\n    - Identify the key concepts.\n    - Use terminology specific to their field.\n    - Avoid unnecessary sentence complexity while maintaining accuracy. \n    - Provide examples they are familiar with.\n    - Provide analogies they can relate their knowledge with and transfer it to new concepts.\n    - Integrate background information if needed.\n    - Do not consider other background per category. \n    - Output only one text per category.\n\n    Original text: \n    Syntactic constituency is the idea that groups of words can behave as single units, or constituents. Consider the noun phrase, a sequence of words surrounding at least one noun. [...] they can all appear in similar syntactic environments, for example, before a verb: 'three parties from Brooklyn arrive'. [...] A widely used formal system for modeling constituent structure in natural language is the context-free grammar (CFG). [...] A context-free grammar consists of a set of rules or productions, each of which expresses the ways that symbols of the language can be grouped and ordered together, and a lexicon of words and symbols. [...] an NP (or noun phrase) can be composed of either a ProperNoun or a determiner (Det) followed by a Nominal; [...]. Context-free rules can be hierarchically  embedded, [...]: Det → a, Det → the, Noun → flight. [...] Thus, a CFG can be used to generate a set of strings. This sequence of rule expansions is called a derivation of the string of words. It is common to represent a derivation by a parse tree (commonly shown inverted with the root at the top). [...]. A CFG like that of L0 defines a formal language. Sentences (strings of words) that can be derived by a grammar are in the formal language defined by that grammar, and are called grammatical sentences. Sentences that cannot be derived by a given formal grammar are not in the language defined by that grammar and are  referred to as ungrammatical. [...] In linguistics, the use of formal languages to model natural languages is called generative grammar [...].\n\n    ### END OF INPUT ###\n\n    Tailored text (provide one text per target category only):\n    1. Computer Science Audience:\n        Syntactic constituency is a crucial concept in natural language processing (NLP). It refers to the grouping of words that behave as a single unit, or constituent. For instance, a noun phrase, a sequence of words surrounding at least one noun, can be treated as a single entity. This concept is significant as these constituents can appear in similar syntactic environments, such as before a verb: 'three parties from Brooklyn arrive'.\n\n        A popular formal system used to model constituent structure in natural language is the Context-Free Grammar (CFG). This system consists of a set of rules or productions, which express how symbols of the language can be grouped and ordered together, and a lexicon of words and symbols.\n\n        Using these rules, an NP (or noun phrase) can be composed of either a ProperNoun or a determiner (Det) followed by a Nominal. For example, Det → a, Det → the, Noun → flight. These rules can be hierarchically embedded, allowing for complex structures to be formed. This sequence of rule expansions is called a derivation of the string of words, and it is typically represented by a parse tree.\n\n        In essence, a CFG can generate a set of strings. These generated strings are called grammatical sentences, while strings that cannot be derived by a given formal grammar are considered ungrammatical. In computer science, the use of formal languages to model natural languages is known as generative grammar.\n\n    2. Linguistics Audience:\n        Syntactic constituency is a key principle in linguistics, describing how groups of words can function as single units, or constituents. For example, a noun phrase, a sequence of words surrounding at least one noun, can be treated as a single entity. This concept is significant as these constituents can appear in similar syntactic environments, such as before a verb: 'three parties from Brooklyn arrive'.\n\n        A widely used formal system for modeling constituent structure in natural language is the context-free grammar (CFG). This system, originating from linguistics, consists of a set of rules or productions, each of which expresses the ways that symbols of the language can be grouped and ordered together, and a lexicon of words and symbols.\n\n        An NP (or noun phrase) can be composed of either a ProperNoun or a determiner (Det) followed by a Nominal; for example, Det → a, Det → the, Noun → flight. Context-free rules can be hierarchically embedded, allowing for complex structures to be formed. This sequence of rule expansions is called a derivation of the string of words. It is common to represent a derivation by a parse tree (commonly shown inverted with the root at the top).\n\n        A CFG like that of L0 defines a formal language. Sentences (strings of words) that can be derived by a grammar are in the formal language defined by that grammar, and are called grammatical sentences. Sentences that cannot be derived by a given formal grammar are not in the language defined by that grammar and are referred to as ungrammatical. The use of formal languages to model natural languages is called generative grammar.\n\n    3. AI Audience:\n        In the realm of artificial intelligence, understanding syntactic constituency is essential for natural language processing (NLP). This concept pertains to the grouping of words that behave as a single unit, or constituent. For instance, a noun phrase, a sequence of words surrounding at least one noun, can be treated as a single entity. This concept is significant as these constituents can appear in similar syntactic environments, such as before a verb: 'three parties from Brooklyn arrive'.\n\n        A frequently used formal system for modeling constituent structure in natural language is the context-free grammar (CFG). This system consists of a set of rules or productions, each of which expresses the ways that symbols of the language can be grouped and ordered together, and a lexicon of words and symbols.\n\n        Using these rules, an NP (or noun phrase) can be composed of either a ProperNoun or a determiner (Det) followed by a Nominal; for example, Det → a, Det → the, Noun → flight. Context-free rules can be hierarchically embedded, allowing for complex structures to be formed. This sequence of rule expansions is called a derivation of the string of words. It is typically represented by a parse tree.\n\n        In essence, a CFG can generate a set of strings. These generated strings are called grammatical sentences, while strings that cannot be derived by"
                }
            }
        }
    },
    "3": {
        "chapter": "21",
        "sections": "21.1, 21.2",
        "topic": "Semantic Roles and Diathesis Alternations",
        "original_category": "L",
        "original_text": "Consider the meanings of the arguments Sasha, Pat, the window, and the door in these two sentences. Sasha broke the window. Pat opened the door. The subjects Sasha and Pat, what we might call the breaker of the window breaking event and the opener of the door-opening event, have something in common. They are both volitional actors, often animate, and they have direct causal responsibility for their events. Thematic roles are a way to capture this semantic commonality between breakers and openers. We say that the subjects of both these verbs are agents. Thus, AGENT is the thematic role that represents an abstract idea such as volitional causation. Similarly, the direct objects of both these verbs, the BrokenThing and OpenedThing, are both prototypically inanimate objects that are affected in some way by the action. The semantic role for these participants is theme. [...] Semantic roles thus help generalize over different surface realizations of predicate arguments. For example, while the AGENT is often realized as the subject of the sentence, in other cases the THEME can be the subject. [...] John (AGENT) broke the window (THEME). John (AGENT) broke the window (THEME) with a rock (INSTRUMENT). The rock (INSTRUMENT) broke the window (THEME). The window (THEME) broke. The window (THEME) was broken by John (AGENT). These examples suggest that break has (at least) the possible arguments AGENT, THEME, and INSTRUMENT. [...] These multiple argument structure realizations [...] are called verb alternations or diathesis alternations.",
        "tailored_texts": {
            "mistral": {
                "CS": {
                    "prompt1": "You are an expert professor in CS. Rewrite the following explanation so that it is best understood by CS students.\n    The most important goal is that your students understand concepts that are outside their existing background knowledge, so they can pass your exam and improve their academic performance.\n    You must consider the information on Computer Science (CS) students have strong technical backgrounds, including programming, algorithms, and machine learning. They have little or no linguistic knowledge. to adapt the explanation to the students' backgrounds.\n\n    - Read the text carefully.\n    - Identify key concepts.\n    - Use terminology specific to their field.\n    - Avoid unnecessary sentence complexity while maintaining accuracy.\n    - Provide relatable examples.\n    - Use analogies that help transfer their knowledge to new concepts.\n    - Integrate background information if needed.    \n\n    Original text: \n    Consider the meanings of the arguments Sasha, Pat, the window, and the door in these two sentences. Sasha broke the window. Pat opened the door. The subjects Sasha and Pat, what we might call the breaker of the window breaking event and the opener of the door-opening event, have something in common. They are both volitional actors, often animate, and they have direct causal responsibility for their events. Thematic roles are a way to capture this semantic commonality between breakers and openers. We say that the subjects of both these verbs are agents. Thus, AGENT is the thematic role that represents an abstract idea such as volitional causation. Similarly, the direct objects of both these verbs, the BrokenThing and OpenedThing, are both prototypically inanimate objects that are affected in some way by the action. The semantic role for these participants is theme. [...] Semantic roles thus help generalize over different surface realizations of predicate arguments. For example, while the AGENT is often realized as the subject of the sentence, in other cases the THEME can be the subject. [...] John (AGENT) broke the window (THEME). John (AGENT) broke the window (THEME) with a rock (INSTRUMENT). The rock (INSTRUMENT) broke the window (THEME). The window (THEME) broke. The window (THEME) was broken by John (AGENT). These examples suggest that break has (at least) the possible arguments AGENT, THEME, and INSTRUMENT. [...] These multiple argument structure realizations [...] are called verb alternations or diathesis alternations.\n\n    ### END OF INPUT ###\n\n    Tailored text (provide one text per target category only):\n    1. Introduction to thematic roles:\n\n    In the realm of Computer Science, we often deal with events in programming language, where subjects perform actions on objects. Let's examine the sentences:\n    - The function Sasha calls breaks the window.\n    - The function Pat calls opens the door.\n\n    Both Sasha and Pat are volitional actors, often animate entities, responsible for the execution of their respective functions. The thematic roles of AGENT, which represent volitional causation, can be assigned to them.\n\n    - Understanding direct objects:\n\n    The broken window and opened door in the previous examples are prototypically inanimate objects that undergo some change due to the action. They can be categorized as themes.\n\n    - Generalizing thematic roles:\n\n    The semantic roles of AGENT and theme help generalize over different surface realizations of predicate arguments. For instance, while the AGENT is often realized as the subject of the sentence, in some cases, the theme can be the subject.\n\n    - Examples of thematic roles:\n    - John (AGENT) breaks the window (THEME).\n    - John (AGENT) breaks the window (THEME) with a rock (INSTRUMENT).\n    - The rock (INSTRUMENT) breaks the window (THEME).\n    - The window (THEME) breaks.\n    - The window (THEME) was broken by John (AGENT).\n\n    These examples demonstrate that the break function may have at least the possible arguments AGENT, THEME, and INSTRUMENT.\n\n    - Verb alternations or diathesis alternations:\n\n    Multiple argument structure realizations of a function are called verb alternations or diathesis alternations. For instance, consider the following sentences:\n    - John (AGENT) broke the window (THEME).\n    - The window (THEME) was broken by John (AGENT).\n\n    Here, the word order and sentence structure have been altered, but the semantic roles of the participants remain the same. This capability helps in understanding the flexibility of functions in programming and natural language processing.",
                    "prompt2": "You are an AI sales expert, responsible for selling AI products to clients with a background in CS.\n    To successfully sell your products, your clients must first understand the following concepts.\n    Rewrite the explanation below in a way that is most comprehensible to CS clients.\n    \n    Your goal is to ensure that your clients grasp concepts outside their existing background knowledge so they can make an informed purchase.\n    By doing so, you will increase your sales percentage and achieve greater success.\n\n    Consider the information on Computer Science (CS) students have strong technical backgrounds, including programming, algorithms, and machine learning. They have little or no linguistic knowledge. to tailor the explanation to your clients' specific backgrounds.\n\n    - Read the text carefully.\n    - Identify key concepts.\n    - Use terminology specific to their field.\n    - Avoid unnecessary sentence complexity while maintaining accuracy.\n    - Provide relatable examples.\n    - Use analogies that help transfer their knowledge to new concepts.\n    - Integrate background information if needed.    \n\n    Original text: \n    Consider the meanings of the arguments Sasha, Pat, the window, and the door in these two sentences. Sasha broke the window. Pat opened the door. The subjects Sasha and Pat, what we might call the breaker of the window breaking event and the opener of the door-opening event, have something in common. They are both volitional actors, often animate, and they have direct causal responsibility for their events. Thematic roles are a way to capture this semantic commonality between breakers and openers. We say that the subjects of both these verbs are agents. Thus, AGENT is the thematic role that represents an abstract idea such as volitional causation. Similarly, the direct objects of both these verbs, the BrokenThing and OpenedThing, are both prototypically inanimate objects that are affected in some way by the action. The semantic role for these participants is theme. [...] Semantic roles thus help generalize over different surface realizations of predicate arguments. For example, while the AGENT is often realized as the subject of the sentence, in other cases the THEME can be the subject. [...] John (AGENT) broke the window (THEME). John (AGENT) broke the window (THEME) with a rock (INSTRUMENT). The rock (INSTRUMENT) broke the window (THEME). The window (THEME) broke. The window (THEME) was broken by John (AGENT). These examples suggest that break has (at least) the possible arguments AGENT, THEME, and INSTRUMENT. [...] These multiple argument structure realizations [...] are called verb alternations or diathesis alternations.\n\n    ### END OF INPUT ###\n\n    Tailored text (provide one text per target category only):\n    1. CS Client Background:\n\nDear Tech-Savvy Client,\n\nIn the realm of programming and algorithms, it's essential to understand the roles that various entities play within functions and operations. Let's examine the concepts of Agent, Theme, and Instrument, often applied in contexts like event handling or function calls.\n\nIn the following examples, consider the subjects Sasha and Pat as the 'Agents' performing actions:\n\n- Sasha triggered the 'break' event. (Here, Sasha is the Agent causing the break event.)\n- Pat initiated the 'open' operation. (Pat is the Agent causing the open operation.)\n\nAgents are the volitional actors that trigger events or operations, typically animate entities in our programs.\n\nThe 'Theme' represents the objects that are affected by the Agent's action. For instance:\n\n- In the case of 'break', the broken window would be the Theme. (The window is the object that is affected by the break event.)\n- In the case of 'open', it would be the opened door. (The door is the object that is affected by the open operation.)\n\nThe 'Instrument' refers to any auxiliary means used by the Agent to perform an operation or trigger an event. For example:\n\n- If John breaks the window with a rock, the rock is the Instrument. (The rock is the means by which John breaks the window.)\n\nBy understanding these roles, we can generalize and analyze the structure of different functions or operations more effectively. For example, while the Agent is generally the subject of the function call, in some cases, the Theme could be the subject instead.\n\nLet's consider a few examples:\n\n- John (Agent) breaks the window (Theme).\n- John (Agent) breaks the window (Theme) using a hammer (Instrument).\n- The hammer (Instrument) breaks the window (Theme).\n- The window (Theme) breaks.\n- The window (Theme) was broken by John (Agent).\n\nThese examples demonstrate that the 'break' function has at least the possible arguments Agent, Theme, and Instrument. These multiple structure realizations of function arguments are called verb alternations or diathesis alternations.\n\nI hope this explanation helps you better understand these concepts, enabling you to grasp the intricacies of the AI products we offer. Let's collaborate to create innovative solutions that cater to your specific needs in the tech industry.\n\nBest regards,\n[Your Name]\n\n2. Non-CS Client Background:\n\nDear valued client,\n\nIn our daily lives, we often interact with various actions and events. To help us break down these interactions and better understand them, let's introduce the concepts of Agent, Theme, and Instrument, which are similar to the roles people play in a story.\n\nIn the following examples, imagine Sasha and Pat as the 'Actors' performing actions:\n\n- Sasha caused the 'breakage' of something. (Here, Sasha is the Actor causing the breakage event.)\n- Pat initiated the 'opening' of something. (Pat is the Actor causing the opening event.)\n\nActors are the people or things that trigger events or actions, usually animate entities in our lives.\n\nThe 'Theme' represents the objects that are affected by the Actor's action. For instance:\n\n- In the case of 'breakage', it could be a window. (The window is the object that is affected by the breakage event.)\n- In the case of 'opening', it would be a door. (The door is the object that is affected by the opening event.)\n\nThe 'Instrument' refers to any auxiliary means used by the Actor to perform an action or trigger an event. For example:\n\n- If John breaks a window with a hammer, the hammer is the Instrument. (The hammer is the means by which John breaks the window.)\n\nBy understanding these roles, we can generalize and analyze our interactions more effectively. For example, while the Actor is generally the one causing the action, in some cases, the Theme could be the one causing the action instead.\n\nLet's consider a few examples:\n\n- John (Actor) breaks the window (Theme).\n- John (Actor) breaks the window (Theme) with a rock (Instrument).\n- The rock (Instrument) breaks the window (Theme).\n- The window (Theme) breaks.\n- The window (Theme) was broken by John (Actor).\n\nThese examples demonstrate that the 'break' action has at least the possible arguments Actor, Theme, and Instrument. These multiple structure realizations of action arguments are called verb alternations or diathesis",
                    "prompt3": "You are a freelance copywriter responsible for adapting field-specific academic texts for an audience with a background in CS. \n    Your client at the publishing house will decide whether to publish your adapted texts in the new edition of the book, making you famous in the copywriting field.\n\n    Rewrite the explanation below in a way that is most comprehensible to CS audience.\n    The key objective is to ensure that your audience grasps concepts outside their existing background knowledge so your client will publish your work.\n    If you succeed, many more clients will work with you, and you will become extremely successful.\n\n    Consider the information on Computer Science (CS) students have strong technical backgrounds, including programming, algorithms, and machine learning. They have little or no linguistic knowledge. to tailor the explanation to your audience’s specific background.\n\n    - Read the text carefully.\n    - Identify key concepts.\n    - Use terminology that is familiar to their field.\n    - Avoid unnecessary sentence complexity while maintaining accuracy.\n    - Provide relatable examples.\n    - Use analogies that help transfer their knowledge to new concepts.\n    - Integrate background information if needed.    \n\n    Original text: \n    Consider the meanings of the arguments Sasha, Pat, the window, and the door in these two sentences. Sasha broke the window. Pat opened the door. The subjects Sasha and Pat, what we might call the breaker of the window breaking event and the opener of the door-opening event, have something in common. They are both volitional actors, often animate, and they have direct causal responsibility for their events. Thematic roles are a way to capture this semantic commonality between breakers and openers. We say that the subjects of both these verbs are agents. Thus, AGENT is the thematic role that represents an abstract idea such as volitional causation. Similarly, the direct objects of both these verbs, the BrokenThing and OpenedThing, are both prototypically inanimate objects that are affected in some way by the action. The semantic role for these participants is theme. [...] Semantic roles thus help generalize over different surface realizations of predicate arguments. For example, while the AGENT is often realized as the subject of the sentence, in other cases the THEME can be the subject. [...] John (AGENT) broke the window (THEME). John (AGENT) broke the window (THEME) with a rock (INSTRUMENT). The rock (INSTRUMENT) broke the window (THEME). The window (THEME) broke. The window (THEME) was broken by John (AGENT). These examples suggest that break has (at least) the possible arguments AGENT, THEME, and INSTRUMENT. [...] These multiple argument structure realizations [...] are called verb alternations or diathesis alternations.\n\n    ### END OF INPUT ###\n\n    Tailored text (provide one text per target category only):\n    1. CS Students with programming, algorithms, and machine learning background:\n\n    In the context of computer programming, let’s consider the arguments 'process', 'input', 'output', and 'controller' in these two statements. The process corrupted the input. The controller opened the output. The commonality between the process and the controller is that they are both active entities, typically non-human, and they have a direct impact on their respective actions. This semantic similarity between corrupters and openers can be represented using thematic roles. We refer to the active entities as agents. Thus, AGENT is the thematic role that encapsulates an abstract idea such as intentional action. Similarly, the affected entities, the CorruptedData and OpenedFile, are both typically inert data that undergo some transformation due to the action. The semantic role for these entities is data.\n\n    Semantic roles aid in generalizing over different representations of predicate arguments. For instance, while the AGENT is often the subject of the statement, in some cases the data can be the subject.\n\n    For example:\n    John (AGENT) processed the corrupted input (data).\n    John (AGENT) processed the corrupted input (data) with a specific algorithm (instrument).\n    The algorithm (instrument) processed the corrupted input (data).\n    The corrupted input (data) was processed.\n    The corrupted input (data) was processed by John (agent).\n\n    These examples imply that the process function has (at least) the possible arguments AGENT, DATA, and INSTRUMENT.\n\n    These multiple structure representations of the function arguments ... are called verb alternations or diathesis alternations.",
                    "prompt4": "We are playing an escape room game against each other. Your goal is to gain more points than me to be the first one escaping hence winning the game. \n    In order to escape the room, you must paraphrase field-specific concepts to the jury of the game in the CS. \n    They will assign you points from one to ten, according to how good their understanding of the new explanation is. \n    Therefore, you want your adaptation to be the best possible, and definitely better than mine, so that you can gain ten points for each paraphrasis and win more quickly. \n    Consider the information on Computer Science (CS) jury have strong technical backgrounds, including programming, algorithms, and machine learning. They have little or no linguistic knowledge. to tailor the explanation to each jury specific background. \n    The winning steps for good paraphrasis are:\n    - Read the text carefully.\n    - Identify the key concepts.\n    - Use terminology that is familiar to the jury's field.\n    - Avoid unnecessary sentence complexity while maintaining accuracy. \n    - Provide examples they are familiar with.\n    - Provide analogies they can relate their knowledge with and transfer it to new concepts.\n    - Integrate background information if needed.    \n    - For the CL jury in Computer Science (CS) jury have strong technical backgrounds, including programming, algorithms, and machine learning. They have little or no linguistic knowledge., only provide one paraphrasis for NLP/Computational Linguistics Practitioners.\n\n    Original text: \n    Consider the meanings of the arguments Sasha, Pat, the window, and the door in these two sentences. Sasha broke the window. Pat opened the door. The subjects Sasha and Pat, what we might call the breaker of the window breaking event and the opener of the door-opening event, have something in common. They are both volitional actors, often animate, and they have direct causal responsibility for their events. Thematic roles are a way to capture this semantic commonality between breakers and openers. We say that the subjects of both these verbs are agents. Thus, AGENT is the thematic role that represents an abstract idea such as volitional causation. Similarly, the direct objects of both these verbs, the BrokenThing and OpenedThing, are both prototypically inanimate objects that are affected in some way by the action. The semantic role for these participants is theme. [...] Semantic roles thus help generalize over different surface realizations of predicate arguments. For example, while the AGENT is often realized as the subject of the sentence, in other cases the THEME can be the subject. [...] John (AGENT) broke the window (THEME). John (AGENT) broke the window (THEME) with a rock (INSTRUMENT). The rock (INSTRUMENT) broke the window (THEME). The window (THEME) broke. The window (THEME) was broken by John (AGENT). These examples suggest that break has (at least) the possible arguments AGENT, THEME, and INSTRUMENT. [...] These multiple argument structure realizations [...] are called verb alternations or diathesis alternations.\n\n    ### END OF INPUT ###\n\n    Tailored text (provide one text per target category only):\n    1. For NLP Practitioners:\n    In the context of these sentences, let's analyze the roles of Sasha, Pat, the window, and the door. Sasha damaged the window, and Pat moved the door. The entities Sasha and Pat, responsible for the respective actions, share a common property. They are both intentional entities, typically animate, and they have a direct impact on the events. Thematic roles, as a means to encapsulate this semantic similarity between actors, are used to describe this. We refer to the subjects of these verbs as agents. Thus, AGENT denotes a high-level abstraction such as intentional action. Similarly, the direct objects of both these verbs, the broken window and the opened door, are typically inanimate objects that undergo some change due to the action. The semantic role for these participants is the object. [...] Semantic roles enable generalization across different forms of predicate arguments. For example, although the AGENT is usually the subject of the sentence, in certain cases, the OBJECT can be the subject. [...] John (AGENT) damaged the window (OBJECT). John (AGENT) damaged the window (OBJECT) with a rock (INSTRUMENT). The rock (INSTRUMENT) damaged the window (OBJECT). The window (OBJECT) broke. The window (OBJECT) was damaged by John (AGENT). These examples imply that \"break\" has at least the potential arguments AGENT, OBJECT, and INSTRUMENT. [...] These multiple structure realizations of arguments [...] are called verb alternations or diathesis alternations.",
                    "prompt5": "Paraphrase the given concepts in order for the audience of the CS to understand concepts outside of their field of knowledge. \n    You must consider the information on the Computer Science (CS) audience have strong technical backgrounds, including programming, algorithms, and machine learning. They have little or no linguistic knowledge. to adapt the tailored texts to the audience backgrounds. \n    - Read the text carefully.\n    - Identify the key concepts.\n    - Use terminology specific to their field.\n    - Avoid unnecessary sentence complexity while maintaining accuracy. \n    - Provide examples they are familiar with.\n    - Provide analogies they can relate their knowledge with and transfer it to new concepts.\n    - Integrate background information if needed.\n    - Do not consider other background per category. \n    - Output only one text per category.\n\n    Original text: \n    Consider the meanings of the arguments Sasha, Pat, the window, and the door in these two sentences. Sasha broke the window. Pat opened the door. The subjects Sasha and Pat, what we might call the breaker of the window breaking event and the opener of the door-opening event, have something in common. They are both volitional actors, often animate, and they have direct causal responsibility for their events. Thematic roles are a way to capture this semantic commonality between breakers and openers. We say that the subjects of both these verbs are agents. Thus, AGENT is the thematic role that represents an abstract idea such as volitional causation. Similarly, the direct objects of both these verbs, the BrokenThing and OpenedThing, are both prototypically inanimate objects that are affected in some way by the action. The semantic role for these participants is theme. [...] Semantic roles thus help generalize over different surface realizations of predicate arguments. For example, while the AGENT is often realized as the subject of the sentence, in other cases the THEME can be the subject. [...] John (AGENT) broke the window (THEME). John (AGENT) broke the window (THEME) with a rock (INSTRUMENT). The rock (INSTRUMENT) broke the window (THEME). The window (THEME) broke. The window (THEME) was broken by John (AGENT). These examples suggest that break has (at least) the possible arguments AGENT, THEME, and INSTRUMENT. [...] These multiple argument structure realizations [...] are called verb alternations or diathesis alternations.\n\n    ### END OF INPUT ###\n\n    Tailored text (provide one text per target category only):\n    1. CS Audience:\n\n    In the context of computer programming, consider the variables Sasha, Pat, window, and door in the following sentences. Sasha breaks the window object. Pat opens the door object. The variables Sasha and Pat, often referred to as the entities causing the window destruction event and the door opening event, share a commonality. They are both active entities, typically animate, and they have direct control over their respective events. Thematic roles, in this context, serve to encapsulate this semantic similarity between destroyers and openers. We refer to these active entities as agents. Thus, AGENT is the thematic role that represents the abstract concept of volitional causation. Similarly, the affected objects, such as the brokenWindow and openedDoor, are typically inanimate objects that undergo some change due to the action. The semantic role for these participants is theme. [...] Semantic roles aid in generalizing over different manifestations of predicate arguments. For instance, while the AGENT is usually the subject of the sentence, in some cases, the THEME can be the subject. [...] In the context of programming, John (AGENT) breaks the window (THEME) object. John (AGENT) breaks the window (THEME) object using a rock (INSTRUMENT) tool. The rock (INSTRUMENT) breaks the window (THEME) object. The window (THEME) object breaks on its own. The window (THEME) object is broken by John (AGENT). These examples demonstrate that the break function has at least the possible arguments AGENT, THEME, and INSTRUMENT. [...] These various manifestations of argument structure are known as verb alternations or diathesis alternations."
                },
                "CL": {
                    "prompt1": "You are an expert professor in CL. Rewrite the following explanation so that it is best understood by CL students.\n    The most important goal is that your students understand concepts that are outside their existing background knowledge, so they can pass your exam and improve their academic performance.\n    You must consider the information on Computational Linguistics (CL) students bridge linguistics and computer science, NLP, corpus linguistics, AI, and LLMs. to adapt the explanation to the students' backgrounds.\n\n    - Read the text carefully.\n    - Identify key concepts.\n    - Use terminology specific to their field.\n    - Avoid unnecessary sentence complexity while maintaining accuracy.\n    - Provide relatable examples.\n    - Use analogies that help transfer their knowledge to new concepts.\n    - Integrate background information if needed.    \n\n    Original text: \n    Consider the meanings of the arguments Sasha, Pat, the window, and the door in these two sentences. Sasha broke the window. Pat opened the door. The subjects Sasha and Pat, what we might call the breaker of the window breaking event and the opener of the door-opening event, have something in common. They are both volitional actors, often animate, and they have direct causal responsibility for their events. Thematic roles are a way to capture this semantic commonality between breakers and openers. We say that the subjects of both these verbs are agents. Thus, AGENT is the thematic role that represents an abstract idea such as volitional causation. Similarly, the direct objects of both these verbs, the BrokenThing and OpenedThing, are both prototypically inanimate objects that are affected in some way by the action. The semantic role for these participants is theme. [...] Semantic roles thus help generalize over different surface realizations of predicate arguments. For example, while the AGENT is often realized as the subject of the sentence, in other cases the THEME can be the subject. [...] John (AGENT) broke the window (THEME). John (AGENT) broke the window (THEME) with a rock (INSTRUMENT). The rock (INSTRUMENT) broke the window (THEME). The window (THEME) broke. The window (THEME) was broken by John (AGENT). These examples suggest that break has (at least) the possible arguments AGENT, THEME, and INSTRUMENT. [...] These multiple argument structure realizations [...] are called verb alternations or diathesis alternations.\n\n    ### END OF INPUT ###\n\n    Tailored text (provide one text per target category only):\n    1. **CL Students (Background knowledge: Linguistics, NLP, and CL):**\n    In the realm of Computational Linguistics, thematic roles play a crucial part in understanding the semantic relationships between words in a sentence. Let's take a look at the two sentences: \"Sasha broke the window\" and \"Pat opened the door.\" Here, Sasha and Pat are agents, the active entities causing the break and open events, respectively. These agents are usually animate and hold direct causal responsibility for their actions. The broken window and opened door are themes, the entities that undergo the respective actions.\n\n    Thematic roles serve to generalize the semantic commonality between these active and passive entities. We refer to the subjects of these verbs as agents, representing the idea of volitional causation. Similarly, the direct objects are themes, typically inanimate objects that experience the action.\n\n    By understanding thematic roles, we can generalize and recognize the different surface realizations of predicate arguments. For instance, while the agent is often the subject of the sentence, the theme can also occupy that position.\n\n    Consider the following examples: John (AGENT) broke the window (THEME). John (AGENT) broke the window (THEME) with a rock (INSTRUMENT). The rock (INSTRUMENT) broke the window (THEME). The window (THEME) broke. The window (THEME) was broken by John (AGENT). These examples demonstrate that the verb \"break\" can have at least three arguments: AGENT, THEME, and INSTRUMENT.\n\n    This variety of argument structure realizations is referred to as verb alternations or diathesis alternations.\n\n    2. **AI Students (Background knowledge: AI, NLP, and CL):**\n    In the domain of Artificial Intelligence, thematic roles are essential for understanding the semantic relationships within a sentence. Let's delve into the sentences \"Sasha broke the window\" and \"Pat opened the door.\" Here, Sasha and Pat act as agents, initiating the break and open events respectively, while the broken window and opened door serve as themes, the entities that experience the actions.\n\n    Thematic roles capture the semantic commonality between these active and passive entities. We denote the subjects of these verbs as agents, representing the concept of volitional causation. Meanwhile, the direct objects are themes, usually inanimate objects that undergo the actions.\n\n    By grasping thematic roles, we can generalize and recognize the various surface realizations of predicate arguments. For example, while the agent is often the subject of the sentence, the theme can also be the subject.\n\n    Examine the following examples: John (AGENT) broke the window (THEME). John (AGENT) broke the window (THEME) with a rock (INSTRUMENT). The rock (INSTRUMENT) broke the window (THEME). The window (THEME) broke. The window (THEME) was broken by John (AGENT). These examples reveal that the verb \"break\" can have at least three arguments: AGENT, THEME, and INSTRUMENT.\n\n    This assortment of argument structure realizations is known as verb alternations or diathesis alternations.\n\n    3. **LLM Students (Background knowledge: Law, NLP, and CL):**\n    In the field of Law, understanding thematic roles is vital for comprehending the semantic relationships within a sentence. Consider the sentences \"Sasha broke the window\" and \"Pat opened the door.\" Here, Sasha and Pat serve as agents, responsible for the break and open events, respectively, while the broken window and opened door are themes, the objects affected by the actions.\n\n    Thematic roles help generalize the semantic commonality between these active and passive entities. We refer to the subjects of these verbs as agents, signifying the idea of volitional causation. Conversely, the direct objects are themes, typically inanimate objects that undergo the actions.\n\n    By grasping thematic roles, we can generalize and recognize the various surface realizations of predicate arguments. For instance, while the agent is often the subject of the sentence, the theme can also occupy that position.\n\n    Consider the following examples: John (AGENT) broke the window (THEME). John (AGENT) broke the window (THEME) with a rock (INSTRUMENT). The rock (INSTRUMENT) broke the window (THEME). The window (THEME) broke. The window (THEME) was broken by John (AGENT). These examples show that the verb \"break\" can have at least three arguments: AGENT, THEME, and INSTRUMENT",
                    "prompt2": "You are an AI sales expert, responsible for selling AI products to clients with a background in CL.\n    To successfully sell your products, your clients must first understand the following concepts.\n    Rewrite the explanation below in a way that is most comprehensible to CL clients.\n    \n    Your goal is to ensure that your clients grasp concepts outside their existing background knowledge so they can make an informed purchase.\n    By doing so, you will increase your sales percentage and achieve greater success.\n\n    Consider the information on Computational Linguistics (CL) students bridge linguistics and computer science, NLP, corpus linguistics, AI, and LLMs. to tailor the explanation to your clients' specific backgrounds.\n\n    - Read the text carefully.\n    - Identify key concepts.\n    - Use terminology specific to their field.\n    - Avoid unnecessary sentence complexity while maintaining accuracy.\n    - Provide relatable examples.\n    - Use analogies that help transfer their knowledge to new concepts.\n    - Integrate background information if needed.    \n\n    Original text: \n    Consider the meanings of the arguments Sasha, Pat, the window, and the door in these two sentences. Sasha broke the window. Pat opened the door. The subjects Sasha and Pat, what we might call the breaker of the window breaking event and the opener of the door-opening event, have something in common. They are both volitional actors, often animate, and they have direct causal responsibility for their events. Thematic roles are a way to capture this semantic commonality between breakers and openers. We say that the subjects of both these verbs are agents. Thus, AGENT is the thematic role that represents an abstract idea such as volitional causation. Similarly, the direct objects of both these verbs, the BrokenThing and OpenedThing, are both prototypically inanimate objects that are affected in some way by the action. The semantic role for these participants is theme. [...] Semantic roles thus help generalize over different surface realizations of predicate arguments. For example, while the AGENT is often realized as the subject of the sentence, in other cases the THEME can be the subject. [...] John (AGENT) broke the window (THEME). John (AGENT) broke the window (THEME) with a rock (INSTRUMENT). The rock (INSTRUMENT) broke the window (THEME). The window (THEME) broke. The window (THEME) was broken by John (AGENT). These examples suggest that break has (at least) the possible arguments AGENT, THEME, and INSTRUMENT. [...] These multiple argument structure realizations [...] are called verb alternations or diathesis alternations.\n\n    ### END OF INPUT ###\n\n    Tailored text (provide one text per target category only):\n    ----------------------------------------------------\n\n    Dear CL students,\n\n    Let's discuss the exciting intersection of linguistics, computer science, and artificial intelligence (AI) using the concepts of Computational Linguistics (CL). In this context, we'll focus on how Natural Language Processing (NLP), corpus linguistics, and Large Language Models (LLMs) can help us understand the dynamics of language.\n\n    To illustrate, let's examine the roles of various elements in the following sentences: \"Sasha smashed the vase.\" and \"Pat turned on the light.\" Notice that both Sasha and Pat are active entities responsible for the events they initiate, i.e., breaking the vase and turning on the light, respectively. The term we use to describe their semantic commonality is \"Agent.\" In other words, both Sasha and Pat are identified as the agents of their respective actions.\n\n    On the other hand, the vase and the light are the objects that are affected by the actions of our agents. The term for these participants is \"Theme.\"\n\n    Semantic roles like Agent and Theme help us generalize the relationships between different components of a sentence. For instance, while the Agent is typically the subject of a sentence, in some cases the Theme may be the subject.\n\n    Here are some examples to help you better understand:\n    - John broke the vase. (John is the Agent, and the vase is the Theme)\n    - John broke the vase with a hammer. (John is the Agent, the vase is the Theme, and the hammer is the Instrument)\n    - The vase broke. (The vase is the Theme, and it was broken by John, who is the Agent)\n    - The vase was broken by John. (John is the Agent, and the vase is the Theme that was broken)\n\n    These examples demonstrate that the verb \"break\" can have at least three possible arguments: Agent, Theme, and Instrument.\n\n    This flexibility in the structure of arguments is known as verb alternations or diathesis alternations.\n\n    Best regards,\n    [Your Name]\n    [Your Title]\n    [Your AI Product Company]\n\n    [Your Contact Information]",
                    "prompt3": "You are a freelance copywriter responsible for adapting field-specific academic texts for an audience with a background in CL. \n    Your client at the publishing house will decide whether to publish your adapted texts in the new edition of the book, making you famous in the copywriting field.\n\n    Rewrite the explanation below in a way that is most comprehensible to CL audience.\n    The key objective is to ensure that your audience grasps concepts outside their existing background knowledge so your client will publish your work.\n    If you succeed, many more clients will work with you, and you will become extremely successful.\n\n    Consider the information on Computational Linguistics (CL) students bridge linguistics and computer science, NLP, corpus linguistics, AI, and LLMs. to tailor the explanation to your audience’s specific background.\n\n    - Read the text carefully.\n    - Identify key concepts.\n    - Use terminology that is familiar to their field.\n    - Avoid unnecessary sentence complexity while maintaining accuracy.\n    - Provide relatable examples.\n    - Use analogies that help transfer their knowledge to new concepts.\n    - Integrate background information if needed.    \n\n    Original text: \n    Consider the meanings of the arguments Sasha, Pat, the window, and the door in these two sentences. Sasha broke the window. Pat opened the door. The subjects Sasha and Pat, what we might call the breaker of the window breaking event and the opener of the door-opening event, have something in common. They are both volitional actors, often animate, and they have direct causal responsibility for their events. Thematic roles are a way to capture this semantic commonality between breakers and openers. We say that the subjects of both these verbs are agents. Thus, AGENT is the thematic role that represents an abstract idea such as volitional causation. Similarly, the direct objects of both these verbs, the BrokenThing and OpenedThing, are both prototypically inanimate objects that are affected in some way by the action. The semantic role for these participants is theme. [...] Semantic roles thus help generalize over different surface realizations of predicate arguments. For example, while the AGENT is often realized as the subject of the sentence, in other cases the THEME can be the subject. [...] John (AGENT) broke the window (THEME). John (AGENT) broke the window (THEME) with a rock (INSTRUMENT). The rock (INSTRUMENT) broke the window (THEME). The window (THEME) broke. The window (THEME) was broken by John (AGENT). These examples suggest that break has (at least) the possible arguments AGENT, THEME, and INSTRUMENT. [...] These multiple argument structure realizations [...] are called verb alternations or diathesis alternations.\n\n    ### END OF INPUT ###\n\n    Tailored text (provide one text per target category only):\n    1. For computational linguists, understanding semantic roles is essential to interpreting and analyzing natural language processing tasks. Let's consider the sentences \"Alex used a hammer to break the computer\" and \"The computer broke on its own.\"\n\n       In the first sentence, Alex (AGENT) is the one who intentionally acts upon the computer (THEME) using a hammer (INSTRUMENT) to cause it to break (PREDICATE). Here, Alex is the agent, the computer is the theme, and the hammer is the instrument.\n\n       In the second sentence, the computer breaks (PREDICATE) without any volitional actor (AGENT). Instead, the computer itself is the theme, suggesting an unexpected or unintended event.\n\n       Semantic roles help us understand the relationships between these different elements in a sentence, generalizing across various sentence structures. For instance, while the agent is often the subject of the sentence, the theme can also act as the subject (\"The computer was broken by Alex\" or \"The computer was broken with a hammer\").\n\n       Semantic roles are crucial in enabling computers to understand language more effectively and accurately. The understanding of these roles will help you adapt academic texts for a CL audience, making your work more accessible and valuable in the field.",
                    "prompt4": "We are playing an escape room game against each other. Your goal is to gain more points than me to be the first one escaping hence winning the game. \n    In order to escape the room, you must paraphrase field-specific concepts to the jury of the game in the CL. \n    They will assign you points from one to ten, according to how good their understanding of the new explanation is. \n    Therefore, you want your adaptation to be the best possible, and definitely better than mine, so that you can gain ten points for each paraphrasis and win more quickly. \n    Consider the information on Computational Linguistics (CL) jury bridge linguistics and computer science, NLP, corpus linguistics, AI, and LLMs. to tailor the explanation to each jury specific background. \n    The winning steps for good paraphrasis are:\n    - Read the text carefully.\n    - Identify the key concepts.\n    - Use terminology that is familiar to the jury's field.\n    - Avoid unnecessary sentence complexity while maintaining accuracy. \n    - Provide examples they are familiar with.\n    - Provide analogies they can relate their knowledge with and transfer it to new concepts.\n    - Integrate background information if needed.    \n    - For the CL jury in Computational Linguistics (CL) jury bridge linguistics and computer science, NLP, corpus linguistics, AI, and LLMs., only provide one paraphrasis for NLP/Computational Linguistics Practitioners.\n\n    Original text: \n    Consider the meanings of the arguments Sasha, Pat, the window, and the door in these two sentences. Sasha broke the window. Pat opened the door. The subjects Sasha and Pat, what we might call the breaker of the window breaking event and the opener of the door-opening event, have something in common. They are both volitional actors, often animate, and they have direct causal responsibility for their events. Thematic roles are a way to capture this semantic commonality between breakers and openers. We say that the subjects of both these verbs are agents. Thus, AGENT is the thematic role that represents an abstract idea such as volitional causation. Similarly, the direct objects of both these verbs, the BrokenThing and OpenedThing, are both prototypically inanimate objects that are affected in some way by the action. The semantic role for these participants is theme. [...] Semantic roles thus help generalize over different surface realizations of predicate arguments. For example, while the AGENT is often realized as the subject of the sentence, in other cases the THEME can be the subject. [...] John (AGENT) broke the window (THEME). John (AGENT) broke the window (THEME) with a rock (INSTRUMENT). The rock (INSTRUMENT) broke the window (THEME). The window (THEME) broke. The window (THEME) was broken by John (AGENT). These examples suggest that break has (at least) the possible arguments AGENT, THEME, and INSTRUMENT. [...] These multiple argument structure realizations [...] are called verb alternations or diathesis alternations.\n\n    ### END OF INPUT ###\n\n    Tailored text (provide one text per target category only):\n    1. Computational Linguistics Practitioners:\n    In the given sentences, focus on the roles of Sasha, Pat, the window, and the door. Sasha and Pat are agents, responsible for the actions of breaking and opening respectively. The objects affected by these actions, the broken window and opened door, are themes. These roles help capture the semantic similarity between agents and themes, generalizing over different sentence structures. For instance, the agent can be the subject, as in \"John broke the window\", or it can be expressed as the object, as in \"The rock broke the window\". These different structures are known as verb alternations or diathesis alternations.\n\n    2. Bridge Linguistics and Computer Science:\n    In the given sentences, Sasha, Pat, the window, and the door represent subjects, objects, and actions. Sasha and Pat are subjects performing actions of breaking and opening respectively, while the window and door are objects affected by these actions. The roles of subjects and objects help capture the semantic commonality between actions and their effects, leading to the generalization of different sentence structures. For example, the subject can be the doer of the action, as in \"John broke the window\", or it can be the affected object, as in \"The window was broken by John\". These different structures are known as diathesis alternations.\n\n    3. NLP:\n    In the given sentences, Sasha, Pat, the window, and the door play the roles of agents, actions, and themes. Sasha and Pat are agents carrying out actions of breaking and opening respectively, while the window and door are themes affected by these actions. These roles help capture the semantic similarity between agents and themes, leading to the generalization of different sentence structures. Examples include \"John broke the window\" where the agent is the subject, or \"The rock broke the window\" where the agent is the object. These different structures are known as verb alternations or diathesis alternations.\n\n    4. Corpus Linguistics:\n    In the given sentences, the focus is on the roles of Sasha, Pat, the window, and the door: agents, actions, and themes. Sasha and Pat are agents causing actions of breaking and opening respectively, while the window and door are themes affected by these actions. These roles help capture the semantic similarity between agents and themes, aiding in the generalization of different sentence structures. For instance, the agent can be the subject, such as \"John broke the window\", or it can be the object, such as \"The rock broke the window\". These different structures are known as verb alternations or diathesis alternations.\n\n    5. Artificial Intelligence:\n    In the given sentences, subjects Sasha and Pat, as well as objects the window and the door, represent actors performing actions and objects undergoing actions respectively. Sasha and Pat are agents responsible for breaking and opening, while the window and door are themes affected by these actions. These roles help capture the semantic similarity between agents and themes, leading to the generalization of different sentence structures. For example, the agent can be the subject, as in \"John broke the window\", or it can be the object, as in \"The rock broke the window\". These different structures are known as verb alternations or diathesis alternations.\n\n    6. LLMs:\n    In the given sentences, subjects Sasha and Pat, along with objects the window and the door, are the performers of actions and the recipients of actions, respectively. Sasha and Pat are agents causing actions of breaking and opening, while the window and door are themes affected by these actions. These roles help capture the semantic similarity between agents and themes, aiding in the generalization of different sentence structures. For example, the agent can be the subject, as in \"John broke the window\", or it can be the object, as in \"The rock broke the window\". These different structures are known as verb alternations or diathesis alternations.",
                    "prompt5": "Paraphrase the given concepts in order for the audience of the CL to understand concepts outside of their field of knowledge. \n    You must consider the information on the Computational Linguistics (CL) audience bridge linguistics and computer science, NLP, corpus linguistics, AI, and LLMs. to adapt the tailored texts to the audience backgrounds. \n    - Read the text carefully.\n    - Identify the key concepts.\n    - Use terminology specific to their field.\n    - Avoid unnecessary sentence complexity while maintaining accuracy. \n    - Provide examples they are familiar with.\n    - Provide analogies they can relate their knowledge with and transfer it to new concepts.\n    - Integrate background information if needed.\n    - Do not consider other background per category. \n    - Output only one text per category.\n\n    Original text: \n    Consider the meanings of the arguments Sasha, Pat, the window, and the door in these two sentences. Sasha broke the window. Pat opened the door. The subjects Sasha and Pat, what we might call the breaker of the window breaking event and the opener of the door-opening event, have something in common. They are both volitional actors, often animate, and they have direct causal responsibility for their events. Thematic roles are a way to capture this semantic commonality between breakers and openers. We say that the subjects of both these verbs are agents. Thus, AGENT is the thematic role that represents an abstract idea such as volitional causation. Similarly, the direct objects of both these verbs, the BrokenThing and OpenedThing, are both prototypically inanimate objects that are affected in some way by the action. The semantic role for these participants is theme. [...] Semantic roles thus help generalize over different surface realizations of predicate arguments. For example, while the AGENT is often realized as the subject of the sentence, in other cases the THEME can be the subject. [...] John (AGENT) broke the window (THEME). John (AGENT) broke the window (THEME) with a rock (INSTRUMENT). The rock (INSTRUMENT) broke the window (THEME). The window (THEME) broke. The window (THEME) was broken by John (AGENT). These examples suggest that break has (at least) the possible arguments AGENT, THEME, and INSTRUMENT. [...] These multiple argument structure realizations [...] are called verb alternations or diathesis alternations.\n\n    ### END OF INPUT ###\n\n    Tailored text (provide one text per target category only):\n    1. Computer Science and Artificial Intelligence\n       In the context of artificial intelligence (AI), understanding the thematic roles of arguments like Sasha, Pat, the window, and the door in sentences provides a framework for AI systems to interpret the actions being performed and the relationships between the entities involved. For instance, consider the sentences: \"Sasha broke the window\" and \"Pat opened the door\". Here, Sasha and Pat are the agents (volitional actors responsible for the actions), while the window and the door are the themes (entities affected by the actions). In AI, this enables the system to recognize that Sasha is the one who performed the action of breaking the window and Pat is the one who opened the door. The concept of thematic roles allows AI to generalize over different ways in which actions can be described and structured, helping the system to understand complex sentences more effectively. For example, the sentence \"John broke the window with a rock\" has the same thematic structure as \"Sasha broke the window\", with the difference being the inclusion of an additional argument, the instrument (the rock).\n\n    2. NLP\n       In natural language processing (NLP), the thematic roles of arguments like Sasha, Pat, the window, and the door in sentences are crucial for understanding the meanings and relationships between entities involved in the actions. For example, consider the sentences: \"Sasha broke the window\" and \"Pat opened the door\". Here, Sasha and Pat are the agents (volitional actors responsible for the actions), while the window and the door are the themes (entities affected by the actions). In NLP, this enables the system to recognize that Sasha is the one who performed the action of breaking the window and Pat is the one who opened the door. The concept of thematic roles allows NLP to generalize over different ways in which actions can be described and structured, helping the system to understand and interpret natural language more effectively. For example, the sentence \"John broke the window with a rock\" has the same thematic structure as \"Sasha broke the window\", with the difference being the inclusion of an additional argument, the instrument (the rock).\n\n    3. Linguistics\n       In linguistics, the thematic roles of arguments like Sasha, Pat, the window, and the door in sentences are essential for understanding the semantic relationships between entities involved in the actions. For example, consider the sentences: \"Sasha broke the window\" and \"Pat opened the door\". Here, Sasha and Pat are the agents (volitional actors responsible for the actions), while the window and the door are the themes (entities affected by the actions). In linguistics, this enables the system to recognize that Sasha is the one who performed the action of breaking the window and Pat is the one who opened the door. The concept of thematic roles allows linguistics to generalize over different ways in which actions can be described and structured, helping the system to understand and interpret language more effectively. For example, the sentence \"John broke the window with a rock\" has the same thematic structure as \"Sasha broke the window\", with the difference being the inclusion of an additional argument, the instrument (the rock).\n\n    4. Corpus Linguistics\n       In corpus linguistics, the thematic roles of arguments like Sasha, Pat, the window, and the door in sentences are crucial for understanding the frequency and patterns of language use. For example, consider the sentences: \"Sasha broke the window\" and \"Pat opened the door\". Here, Sasha and Pat are the agents (volitional actors responsible for the actions), while the window and the door are the themes (entities affected by the actions). In corpus linguistics, this enables the system to identify the most common actions performed by people and the most common objects involved in those actions. The concept of thematic roles allows corpus linguistics to generalize over different ways in which actions can be described and structured, helping the system to understand and analyze large amounts of text more effectively. For example, the sentence \"John broke the window with a rock\" has the same thematic structure as \"Sasha broke the window\", with the difference being the inclusion of an additional argument, the instrument (the rock).\n\n    5. Computational Linguistics\n       In computational linguistics, the thematic roles of arguments like Sasha, Pat, the window, and the door in sentences are essential for understanding the meanings and relationships between entities involved in the actions. For example, consider the sentences: \"Sasha broke the window\" and \"Pat opened the door\". Here, Sasha and Pat are the agents (volitional actors responsible for the actions), while the window and the door are the themes (entities affected by the actions). In computational linguistics, this enables the system to recognize that Sasha is the one who performed the action of breaking the window and Pat is"
                }
            }
        }
    },
    "4": {
        "chapter": "G",
        "sections": "G.1, G.2",
        "topic": "Word Senses and Relations Between Senses",
        "original_category": "L",
        "original_text": "A sense (or word sense) is a discrete representation of one aspect of the meaning of a word. [...] In context, it's easy to see the different meanings: mouse1: a mouse controlling a computer system in 1968. mouse2: a quiet animal like a mouse. bank1: a bank can hold the investments in a custodial account. bank2: as agriculture burgeons on the east bank, the river...[...] we need to consider the alternative ways that dictionaries and thesauruses offer for defining senses. One is based on the fact that dictionaries or thesauruses give textual definitions for each sense called glosses. [...] bank: 1. financial institution that accepts deposits and channels the money into lending activities 2. sloping land (especially the slope beside a body of water). [...] when two senses of two different words (lemmas) are identical, or nearly identical, we say the two senses are synonyms [...] couch/sofa vomit/throw up filbert/hazelnut car/automobile. [...] antonyms are words with an opposite meaning, like: long/short big/little fast/slow cold/hot dark/light. [...] [a] hyponym of [a] word if the [other word] is more specific, denoting a subclass of the other. For example, car is a hyponym of vehicle, dog is a hyponym of animal, [...]. Conversely, we say that vehicle is a hypernym of car, and animal is a hypernym of dog. [...] meronymy, the part-whole relation. [...] wheel is a meronym of car, and car is a holonym of wheel.",
        "tailored_texts": {
            "mistral": {
                "CS": {
                    "prompt1": "You are an expert professor in CS. Rewrite the following explanation so that it is best understood by CS students.\n    The most important goal is that your students understand concepts that are outside their existing background knowledge, so they can pass your exam and improve their academic performance.\n    You must consider the information on Computer Science (CS) students have strong technical backgrounds, including programming, algorithms, and machine learning. They have little or no linguistic knowledge. to adapt the explanation to the students' backgrounds.\n\n    - Read the text carefully.\n    - Identify key concepts.\n    - Use terminology specific to their field.\n    - Avoid unnecessary sentence complexity while maintaining accuracy.\n    - Provide relatable examples.\n    - Use analogies that help transfer their knowledge to new concepts.\n    - Integrate background information if needed.    \n\n    Original text: \n    A sense (or word sense) is a discrete representation of one aspect of the meaning of a word. [...] In context, it's easy to see the different meanings: mouse1: a mouse controlling a computer system in 1968. mouse2: a quiet animal like a mouse. bank1: a bank can hold the investments in a custodial account. bank2: as agriculture burgeons on the east bank, the river...[...] we need to consider the alternative ways that dictionaries and thesauruses offer for defining senses. One is based on the fact that dictionaries or thesauruses give textual definitions for each sense called glosses. [...] bank: 1. financial institution that accepts deposits and channels the money into lending activities 2. sloping land (especially the slope beside a body of water). [...] when two senses of two different words (lemmas) are identical, or nearly identical, we say the two senses are synonyms [...] couch/sofa vomit/throw up filbert/hazelnut car/automobile. [...] antonyms are words with an opposite meaning, like: long/short big/little fast/slow cold/hot dark/light. [...] [a] hyponym of [a] word if the [other word] is more specific, denoting a subclass of the other. For example, car is a hyponym of vehicle, dog is a hyponym of animal, [...]. Conversely, we say that vehicle is a hypernym of car, and animal is a hypernym of dog. [...] meronymy, the part-whole relation. [...] wheel is a meronym of car, and car is a holonym of wheel.\n\n    ### END OF INPUT ###\n\n    Tailored text (provide one text per target category only):\n    1. Word Senses:\n        In the realm of Computer Science, we can think of a \"word sense\" as a distinct interpretation of a term's meaning. For instance, consider the term \"mouse\":\n        - mouse1: a pointer device used for computer interaction\n        - mouse2: a small rodent\n\n        The context in which a word is used can greatly influence its sense.\n\n        Dictionaries and thesauruses provide definitions for each sense, known as \"glosses\". For example:\n        - bank1: a financial institution dealing with the management of money\n        - bank2: the sloping land beside a body of water\n\n        When two senses of two different terms are nearly identical, we call them \"synonyms\". For example, \"couch\" and \"sofa\" are synonyms, as are \"filbert\" and \"hazelnut\".\n\n        \"Antonyms\" are terms with opposite meanings, such as \"long\" and \"short\", \"big\" and \"little\", \"fast\" and \"slow\", \"cold\" and \"hot\", and \"dark\" and \"light\".\n\n    2. Hyponymy and Hypernymy:\n        In the world of Computer Science, a \"hyponym\" is a more specific term that falls under a broader category, or \"hypernym\". For example, a \"car\" is a hyponym of \"vehicle\", and a \"dog\" is a hyponym of \"animal\". Conversely, \"vehicle\" is a hypernym of \"car\", and \"animal\" is a hypernym of \"dog\".\n\n    3. Meronymy (Part-Whole Relationship):\n        In this context, \"meronymy\" refers to the relationship between a part and the whole. For example, a \"wheel\" is a part of a \"car\", and a \"car\" is the whole containing the \"wheel\".",
                    "prompt2": "You are an AI sales expert, responsible for selling AI products to clients with a background in CS.\n    To successfully sell your products, your clients must first understand the following concepts.\n    Rewrite the explanation below in a way that is most comprehensible to CS clients.\n    \n    Your goal is to ensure that your clients grasp concepts outside their existing background knowledge so they can make an informed purchase.\n    By doing so, you will increase your sales percentage and achieve greater success.\n\n    Consider the information on Computer Science (CS) students have strong technical backgrounds, including programming, algorithms, and machine learning. They have little or no linguistic knowledge. to tailor the explanation to your clients' specific backgrounds.\n\n    - Read the text carefully.\n    - Identify key concepts.\n    - Use terminology specific to their field.\n    - Avoid unnecessary sentence complexity while maintaining accuracy.\n    - Provide relatable examples.\n    - Use analogies that help transfer their knowledge to new concepts.\n    - Integrate background information if needed.    \n\n    Original text: \n    A sense (or word sense) is a discrete representation of one aspect of the meaning of a word. [...] In context, it's easy to see the different meanings: mouse1: a mouse controlling a computer system in 1968. mouse2: a quiet animal like a mouse. bank1: a bank can hold the investments in a custodial account. bank2: as agriculture burgeons on the east bank, the river...[...] we need to consider the alternative ways that dictionaries and thesauruses offer for defining senses. One is based on the fact that dictionaries or thesauruses give textual definitions for each sense called glosses. [...] bank: 1. financial institution that accepts deposits and channels the money into lending activities 2. sloping land (especially the slope beside a body of water). [...] when two senses of two different words (lemmas) are identical, or nearly identical, we say the two senses are synonyms [...] couch/sofa vomit/throw up filbert/hazelnut car/automobile. [...] antonyms are words with an opposite meaning, like: long/short big/little fast/slow cold/hot dark/light. [...] [a] hyponym of [a] word if the [other word] is more specific, denoting a subclass of the other. For example, car is a hyponym of vehicle, dog is a hyponym of animal, [...]. Conversely, we say that vehicle is a hypernym of car, and animal is a hypernym of dog. [...] meronymy, the part-whole relation. [...] wheel is a meronym of car, and car is a holonym of wheel.\n\n    ### END OF INPUT ###\n\n    Tailored text (provide one text per target category only):\n    1) Computer Science Clients\n    In the realm of Computer Science, understanding the meaning of a word is crucial for effective programming and algorithm design. A sense of a word refers to a specific aspect of its meaning. For instance, consider the word 'bank'. In one context, it could refer to a financial institution that accepts deposits and channels money into lending activities (bank1). In another context, it could refer to the sloping land beside a body of water (bank2).\n\n    To clarify these distinct meanings, dictionaries and thesauruses provide definitions, known as glosses. For our 'bank' example, the glosses could be:\n    bank1: Financial institution that accepts deposits and channels money into lending activities\n    bank2: Sloping land beside a body of water\n\n    When two words have identical or near-identical meanings, we say they are synonyms. For example, couch and sofa are synonyms, as are filbert and hazelnut. On the other hand, antonyms are words with opposite meanings, such as long and short, big and little, fast and slow, cold and hot, and dark and light.\n\n    A hyponym is a more specific word that denotes a subclass of another word. For example, car is a hyponym of vehicle, as a car is a specific type of vehicle. Conversely, vehicle is a hypernym of car, as a car is a type of vehicle.\n\n    Lastly, meronymy is the part-whole relationship. In the context of a car, the wheel is a meronym, as it is a part of the car. Conversely, the car is a holonym of the wheel, as it encompasses the wheel as one of its parts.",
                    "prompt3": "You are a freelance copywriter responsible for adapting field-specific academic texts for an audience with a background in CS. \n    Your client at the publishing house will decide whether to publish your adapted texts in the new edition of the book, making you famous in the copywriting field.\n\n    Rewrite the explanation below in a way that is most comprehensible to CS audience.\n    The key objective is to ensure that your audience grasps concepts outside their existing background knowledge so your client will publish your work.\n    If you succeed, many more clients will work with you, and you will become extremely successful.\n\n    Consider the information on Computer Science (CS) students have strong technical backgrounds, including programming, algorithms, and machine learning. They have little or no linguistic knowledge. to tailor the explanation to your audience’s specific background.\n\n    - Read the text carefully.\n    - Identify key concepts.\n    - Use terminology that is familiar to their field.\n    - Avoid unnecessary sentence complexity while maintaining accuracy.\n    - Provide relatable examples.\n    - Use analogies that help transfer their knowledge to new concepts.\n    - Integrate background information if needed.    \n\n    Original text: \n    A sense (or word sense) is a discrete representation of one aspect of the meaning of a word. [...] In context, it's easy to see the different meanings: mouse1: a mouse controlling a computer system in 1968. mouse2: a quiet animal like a mouse. bank1: a bank can hold the investments in a custodial account. bank2: as agriculture burgeons on the east bank, the river...[...] we need to consider the alternative ways that dictionaries and thesauruses offer for defining senses. One is based on the fact that dictionaries or thesauruses give textual definitions for each sense called glosses. [...] bank: 1. financial institution that accepts deposits and channels the money into lending activities 2. sloping land (especially the slope beside a body of water). [...] when two senses of two different words (lemmas) are identical, or nearly identical, we say the two senses are synonyms [...] couch/sofa vomit/throw up filbert/hazelnut car/automobile. [...] antonyms are words with an opposite meaning, like: long/short big/little fast/slow cold/hot dark/light. [...] [a] hyponym of [a] word if the [other word] is more specific, denoting a subclass of the other. For example, car is a hyponym of vehicle, dog is a hyponym of animal, [...]. Conversely, we say that vehicle is a hypernym of car, and animal is a hypernym of dog. [...] meronymy, the part-whole relation. [...] wheel is a meronym of car, and car is a holonym of wheel.\n\n    ### END OF INPUT ###\n\n    Tailored text (provide one text per target category only):\n    1. CS students:\n    In CS terminology, a 'sense' refers to a specific interpretation of a word's meaning within a given context. For instance:\n\n    - mouse: a computer input device, first used in 1968.\n    - mouse: a small rodent.\n\n    Bank:\n    In financial terms, a 'bank' is an institution that manages money, primarily accepting deposits and channeling them into lending activities. In an agricultural context, a 'bank' can refer to the sloping land beside a body of water.\n\n    Defining senses:\n    Dictionaries and thesauruses provide textual definitions for each sense, known as glosses. For example:\n\n    - bank:\n        - Financial institution: 1. A financial entity that manages money, primarily accepting deposits and channeling them into lending activities.\n        - Agricultural land: 2. Sloping land (especially the slope beside a body of water).\n\n    Synonyms:\n    Synonyms are words with near-identical meanings, like:\n\n    - couch/sofa\n    - vomit/throw up\n    - filbert/hazelnut\n    - car/automobile\n\n    Antonyms:\n    Antonyms are words with opposite meanings, such as:\n\n    - long/short\n    - big/little\n    - fast/slow\n    - cold/hot\n    - dark/light\n\n    Hyponyms:\n    A hyponym is a term that denotes a more specific subclass of another word. For example, 'car' is a hyponym of 'vehicle'.\n\n    Hypernyms:\n    Correspondingly, a hypernym is a broader term that encompasses a subclass, such as 'vehicle' being a hypernym of 'car'.\n\n    Meronymy:\n    Meronymy is the relationship between a part and a whole. For example, a 'wheel' is a part of a 'car', and a 'car' is the whole entity that includes a 'wheel'.",
                    "prompt4": "We are playing an escape room game against each other. Your goal is to gain more points than me to be the first one escaping hence winning the game. \n    In order to escape the room, you must paraphrase field-specific concepts to the jury of the game in the CS. \n    They will assign you points from one to ten, according to how good their understanding of the new explanation is. \n    Therefore, you want your adaptation to be the best possible, and definitely better than mine, so that you can gain ten points for each paraphrasis and win more quickly. \n    Consider the information on Computer Science (CS) jury have strong technical backgrounds, including programming, algorithms, and machine learning. They have little or no linguistic knowledge. to tailor the explanation to each jury specific background. \n    The winning steps for good paraphrasis are:\n    - Read the text carefully.\n    - Identify the key concepts.\n    - Use terminology that is familiar to the jury's field.\n    - Avoid unnecessary sentence complexity while maintaining accuracy. \n    - Provide examples they are familiar with.\n    - Provide analogies they can relate their knowledge with and transfer it to new concepts.\n    - Integrate background information if needed.    \n    - For the CL jury in Computer Science (CS) jury have strong technical backgrounds, including programming, algorithms, and machine learning. They have little or no linguistic knowledge., only provide one paraphrasis for NLP/Computational Linguistics Practitioners.\n\n    Original text: \n    A sense (or word sense) is a discrete representation of one aspect of the meaning of a word. [...] In context, it's easy to see the different meanings: mouse1: a mouse controlling a computer system in 1968. mouse2: a quiet animal like a mouse. bank1: a bank can hold the investments in a custodial account. bank2: as agriculture burgeons on the east bank, the river...[...] we need to consider the alternative ways that dictionaries and thesauruses offer for defining senses. One is based on the fact that dictionaries or thesauruses give textual definitions for each sense called glosses. [...] bank: 1. financial institution that accepts deposits and channels the money into lending activities 2. sloping land (especially the slope beside a body of water). [...] when two senses of two different words (lemmas) are identical, or nearly identical, we say the two senses are synonyms [...] couch/sofa vomit/throw up filbert/hazelnut car/automobile. [...] antonyms are words with an opposite meaning, like: long/short big/little fast/slow cold/hot dark/light. [...] [a] hyponym of [a] word if the [other word] is more specific, denoting a subclass of the other. For example, car is a hyponym of vehicle, dog is a hyponym of animal, [...]. Conversely, we say that vehicle is a hypernym of car, and animal is a hypernym of dog. [...] meronymy, the part-whole relation. [...] wheel is a meronym of car, and car is a holonym of wheel.\n\n    ### END OF INPUT ###\n\n    Tailored text (provide one text per target category only):\n    1. NLP Practitioners:\n    Synsets, or discrete representations of a word's meaning, are essential in understanding natural language processing. For instance, the word \"bank\" can be interpreted as a financial institution in the context of transactions, or as the sloping land beside a river in a geographical context. Dictionaries and thesauruses offer multiple definitions, or glosses, to describe the various senses of a word. For example, the definition of \"bank\" in a financial context could be, \"a financial institution that accepts deposits and channels the money into lending activities.\" If two synsets for different words are identical or nearly identical, we call them synonyms, like \"couch\" and \"sofa,\" or \"filbert\" and \"hazelnut.\" Words with opposite meanings are antonyms, such as \"long\" and \"short,\" while words that are more specific and denote a subclass of another are hyponyms. For instance, \"car\" is a hyponym of \"vehicle.\" The part-whole relation, known as meronymy, is also crucial, where a wheel is a part of a car, and a car is the whole of a wheel.",
                    "prompt5": "Paraphrase the given concepts in order for the audience of the CS to understand concepts outside of their field of knowledge. \n    You must consider the information on the Computer Science (CS) audience have strong technical backgrounds, including programming, algorithms, and machine learning. They have little or no linguistic knowledge. to adapt the tailored texts to the audience backgrounds. \n    - Read the text carefully.\n    - Identify the key concepts.\n    - Use terminology specific to their field.\n    - Avoid unnecessary sentence complexity while maintaining accuracy. \n    - Provide examples they are familiar with.\n    - Provide analogies they can relate their knowledge with and transfer it to new concepts.\n    - Integrate background information if needed.\n    - Do not consider other background per category. \n    - Output only one text per category.\n\n    Original text: \n    A sense (or word sense) is a discrete representation of one aspect of the meaning of a word. [...] In context, it's easy to see the different meanings: mouse1: a mouse controlling a computer system in 1968. mouse2: a quiet animal like a mouse. bank1: a bank can hold the investments in a custodial account. bank2: as agriculture burgeons on the east bank, the river...[...] we need to consider the alternative ways that dictionaries and thesauruses offer for defining senses. One is based on the fact that dictionaries or thesauruses give textual definitions for each sense called glosses. [...] bank: 1. financial institution that accepts deposits and channels the money into lending activities 2. sloping land (especially the slope beside a body of water). [...] when two senses of two different words (lemmas) are identical, or nearly identical, we say the two senses are synonyms [...] couch/sofa vomit/throw up filbert/hazelnut car/automobile. [...] antonyms are words with an opposite meaning, like: long/short big/little fast/slow cold/hot dark/light. [...] [a] hyponym of [a] word if the [other word] is more specific, denoting a subclass of the other. For example, car is a hyponym of vehicle, dog is a hyponym of animal, [...]. Conversely, we say that vehicle is a hypernym of car, and animal is a hypernym of dog. [...] meronymy, the part-whole relation. [...] wheel is a meronym of car, and car is a holonym of wheel.\n\n    ### END OF INPUT ###\n\n    Tailored text (provide one text per target category only):\n    1. Word Senses:\n       In the realm of computer science, a word sense can be defined as a distinct interpretation of a word's meaning. For instance, consider the word \"bank\". In context, it can refer to a financial institution (bank1) or sloping land (bank2).\n\n    2. Glosses:\n       Dictionaries often provide textual explanations for each word sense, which are referred to as glosses. For example, the gloss for bank1 could be \"a financial institution that accepts deposits and channels money into lending activities\".\n\n    3. Synonyms:\n       Synonyms are word senses that share a similar or identical meaning. For example, couch and sofa are synonyms, as are filbert and hazelnut.\n\n    4. Antonyms:\n       Antonyms are word senses with opposite meanings. For example, long and short, big and little, fast and slow, cold and hot, dark and light are all pairs of antonyms.\n\n    5. Hyponyms and Hypernyms:\n       Hyponyms are more specific word senses that fall under a broader category. For example, car is a hyponym of vehicle, and dog is a hyponym of animal. Conversely, vehicle is a hypernym of car, and animal is a hypernym of dog.\n\n    6. Meronymy:\n       Meronymy represents the part-whole relationship, where one word sense is a part of another. For example, wheel is a meronym of car, and car is a holonym of wheel."
                },
                "CL": {
                    "prompt1": "You are an expert professor in CL. Rewrite the following explanation so that it is best understood by CL students.\n    The most important goal is that your students understand concepts that are outside their existing background knowledge, so they can pass your exam and improve their academic performance.\n    You must consider the information on Computational Linguistics (CL) students bridge linguistics and computer science, NLP, corpus linguistics, AI, and LLMs. to adapt the explanation to the students' backgrounds.\n\n    - Read the text carefully.\n    - Identify key concepts.\n    - Use terminology specific to their field.\n    - Avoid unnecessary sentence complexity while maintaining accuracy.\n    - Provide relatable examples.\n    - Use analogies that help transfer their knowledge to new concepts.\n    - Integrate background information if needed.    \n\n    Original text: \n    A sense (or word sense) is a discrete representation of one aspect of the meaning of a word. [...] In context, it's easy to see the different meanings: mouse1: a mouse controlling a computer system in 1968. mouse2: a quiet animal like a mouse. bank1: a bank can hold the investments in a custodial account. bank2: as agriculture burgeons on the east bank, the river...[...] we need to consider the alternative ways that dictionaries and thesauruses offer for defining senses. One is based on the fact that dictionaries or thesauruses give textual definitions for each sense called glosses. [...] bank: 1. financial institution that accepts deposits and channels the money into lending activities 2. sloping land (especially the slope beside a body of water). [...] when two senses of two different words (lemmas) are identical, or nearly identical, we say the two senses are synonyms [...] couch/sofa vomit/throw up filbert/hazelnut car/automobile. [...] antonyms are words with an opposite meaning, like: long/short big/little fast/slow cold/hot dark/light. [...] [a] hyponym of [a] word if the [other word] is more specific, denoting a subclass of the other. For example, car is a hyponym of vehicle, dog is a hyponym of animal, [...]. Conversely, we say that vehicle is a hypernym of car, and animal is a hypernym of dog. [...] meronymy, the part-whole relation. [...] wheel is a meronym of car, and car is a holonym of wheel.\n\n    ### END OF INPUT ###\n\n    Tailored text (provide one text per target category only):\n    1. Synonyms in Computational Linguistics:\n       In CL, synonyms refer to two (or more) distinct words that have identical or nearly identical meanings. For example, couch and sofa are synonyms, as are filbert and hazelnut. When considering synonyms, it's essential to understand that while the words may vary, their meaning remains consistent.\n\n    2. Antonyms in Computational Linguistics:\n       Antonyms are words with opposite meanings, such as long and short, big and little, fast and slow, cold and hot, or dark and light. In CL, antonyms play a crucial role in understanding the polarity and sentiment analysis of text.\n\n    3. Hyponyms and Hypernyms in Computational Linguistics:\n       A hyponym is a word that is more specific than another word, denoting a subclass of that other word. For example, car is a hyponym of vehicle, as it is a specific type of vehicle. In contrast, the word vehicle is a hypernym of car, as it represents a broader category that includes cars, trucks, motorcycles, and more.\n\n    4. Meronymy in Computational Linguistics:\n       Meronymy refers to the part-whole relationship in CL. For example, the wheel is a meronym of the car, and the car is a holonym of the wheel. This means that a car has wheels as a part of its structure, and a wheel is a part of a car. Understanding meronymy can help in tasks like named entity recognition and entity linking.",
                    "prompt2": "You are an AI sales expert, responsible for selling AI products to clients with a background in CL.\n    To successfully sell your products, your clients must first understand the following concepts.\n    Rewrite the explanation below in a way that is most comprehensible to CL clients.\n    \n    Your goal is to ensure that your clients grasp concepts outside their existing background knowledge so they can make an informed purchase.\n    By doing so, you will increase your sales percentage and achieve greater success.\n\n    Consider the information on Computational Linguistics (CL) students bridge linguistics and computer science, NLP, corpus linguistics, AI, and LLMs. to tailor the explanation to your clients' specific backgrounds.\n\n    - Read the text carefully.\n    - Identify key concepts.\n    - Use terminology specific to their field.\n    - Avoid unnecessary sentence complexity while maintaining accuracy.\n    - Provide relatable examples.\n    - Use analogies that help transfer their knowledge to new concepts.\n    - Integrate background information if needed.    \n\n    Original text: \n    A sense (or word sense) is a discrete representation of one aspect of the meaning of a word. [...] In context, it's easy to see the different meanings: mouse1: a mouse controlling a computer system in 1968. mouse2: a quiet animal like a mouse. bank1: a bank can hold the investments in a custodial account. bank2: as agriculture burgeons on the east bank, the river...[...] we need to consider the alternative ways that dictionaries and thesauruses offer for defining senses. One is based on the fact that dictionaries or thesauruses give textual definitions for each sense called glosses. [...] bank: 1. financial institution that accepts deposits and channels the money into lending activities 2. sloping land (especially the slope beside a body of water). [...] when two senses of two different words (lemmas) are identical, or nearly identical, we say the two senses are synonyms [...] couch/sofa vomit/throw up filbert/hazelnut car/automobile. [...] antonyms are words with an opposite meaning, like: long/short big/little fast/slow cold/hot dark/light. [...] [a] hyponym of [a] word if the [other word] is more specific, denoting a subclass of the other. For example, car is a hyponym of vehicle, dog is a hyponym of animal, [...]. Conversely, we say that vehicle is a hypernym of car, and animal is a hypernym of dog. [...] meronymy, the part-whole relation. [...] wheel is a meronym of car, and car is a holonym of wheel.\n\n    ### END OF INPUT ###\n\n    Tailored text (provide one text per target category only):\n    1. Computational Linguistics (CL) students:\n\n        In Computational Linguistics, a sense of a word refers to a distinct interpretation of its meaning. For instance, consider the word 'bank'. In one context, it might refer to a financial institution, while in another, it could describe a sloping piece of land.\n\n        Dictionaries and thesauruses help define these senses by providing textual explanations called glosses. For example, the first sense of 'bank' in our example could be defined as \"an institution that accepts deposits and channels money into lending activities.\"\n\n        Sometimes, the senses of different words are similar or identical, which we call synonyms. For example, 'couch' and 'sofa' are synonyms, as they both refer to a piece of furniture used for sitting and reclining.\n\n        On the other hand, antonyms are words that have opposite meanings, such as 'long' and 'short'.\n\n        Hyponyms are words that are more specific, denoting a subclass of another word. For instance, 'car' is a hyponym of 'vehicle', as it is a specific type of vehicle. Conversely, 'vehicle' is a hypernym of 'car'.\n\n        Lastly, meronymy refers to the part-whole relationship. In this context, 'wheel' is a part of 'car', and 'car' is the whole that contains 'wheel'.\n\n    2. Artificial Intelligence (AI) or Machine Learning (ML) professionals:\n\n        In the realm of Computational Linguistics, a word's sense is its distinct representation of meaning. For example, 'bank' could refer to a financial institution or a sloping piece of land.\n\n        Dictionaries and thesauruses use glosses, textual definitions, to explain each sense. For instance, the first sense of 'bank' could be defined as \"an institution that accepts deposits and channels money into lending activities.\"\n\n        Synonyms are words with identical or nearly identical meanings, such as 'couch' and 'sofa'. Antonyms are words with opposite meanings, like 'long' and 'short'.\n\n        Hyponyms are words that are more specific, denoting a subclass of another word. For example, 'car' is a hyponym of 'vehicle', as it is a specific type of vehicle. Conversely, 'vehicle' is a hypernym of 'car'.\n\n        Meronymy represents the part-whole relationship. In this context, 'wheel' is a part of 'car', and 'car' is the whole that contains 'wheel'.\n\n    3. Natural Language Processing (NLP) developers:\n\n        In Computational Linguistics, a sense is a discrete representation of a word's meaning. For example, 'bank' could refer to a financial institution or a sloping piece of land.\n\n        Dictionaries and thesauruses offer glosses, textual definitions, to explain each sense. For instance, the first sense of 'bank' could be defined as \"an institution that accepts deposits and channels money into lending activities.\"\n\n        Synonyms are words with almost identical meanings, such as 'couch' and 'sofa'. Antonyms are words with opposite meanings, like 'long' and 'short'.\n\n        Hyponyms are more specific words that fall under a broader category, like 'car' under 'vehicle'. Conversely, hypernyms are broader categories that encompass specific words, such as 'vehicle' encompassing 'car'.\n\n        Meronymy represents the part-whole relationship. In this context, 'wheel' is a part of 'car', and 'car' is the whole that contains 'wheel'.",
                    "prompt3": "You are a freelance copywriter responsible for adapting field-specific academic texts for an audience with a background in CL. \n    Your client at the publishing house will decide whether to publish your adapted texts in the new edition of the book, making you famous in the copywriting field.\n\n    Rewrite the explanation below in a way that is most comprehensible to CL audience.\n    The key objective is to ensure that your audience grasps concepts outside their existing background knowledge so your client will publish your work.\n    If you succeed, many more clients will work with you, and you will become extremely successful.\n\n    Consider the information on Computational Linguistics (CL) students bridge linguistics and computer science, NLP, corpus linguistics, AI, and LLMs. to tailor the explanation to your audience’s specific background.\n\n    - Read the text carefully.\n    - Identify key concepts.\n    - Use terminology that is familiar to their field.\n    - Avoid unnecessary sentence complexity while maintaining accuracy.\n    - Provide relatable examples.\n    - Use analogies that help transfer their knowledge to new concepts.\n    - Integrate background information if needed.    \n\n    Original text: \n    A sense (or word sense) is a discrete representation of one aspect of the meaning of a word. [...] In context, it's easy to see the different meanings: mouse1: a mouse controlling a computer system in 1968. mouse2: a quiet animal like a mouse. bank1: a bank can hold the investments in a custodial account. bank2: as agriculture burgeons on the east bank, the river...[...] we need to consider the alternative ways that dictionaries and thesauruses offer for defining senses. One is based on the fact that dictionaries or thesauruses give textual definitions for each sense called glosses. [...] bank: 1. financial institution that accepts deposits and channels the money into lending activities 2. sloping land (especially the slope beside a body of water). [...] when two senses of two different words (lemmas) are identical, or nearly identical, we say the two senses are synonyms [...] couch/sofa vomit/throw up filbert/hazelnut car/automobile. [...] antonyms are words with an opposite meaning, like: long/short big/little fast/slow cold/hot dark/light. [...] [a] hyponym of [a] word if the [other word] is more specific, denoting a subclass of the other. For example, car is a hyponym of vehicle, dog is a hyponym of animal, [...]. Conversely, we say that vehicle is a hypernym of car, and animal is a hypernym of dog. [...] meronymy, the part-whole relation. [...] wheel is a meronym of car, and car is a holonym of wheel.\n\n    ### END OF INPUT ###\n\n    Tailored text (provide one text per target category only):\n    1. Computational Linguistics (CL) students:\n        In the world of Computational Linguistics, a sense refers to a distinct interpretation or meaning of a given word, especially within its context. For instance, \"mouse\" could refer to a computer input device or a small furry animal. Similarly, \"bank\" can denote a financial institution or the sloping land by a river.\n\n        When working with lexical resources like dictionaries or thesauruses, we often encounter definitions, known as glosses, for each sense. For example, the word \"bank\" can be broken down into \"1. a financial institution that manages and invests money\" and \"2. the land sloping beside a body of water.\"\n\n        When two senses of distinct words are nearly identical, they are considered synonyms. For example, a \"couch\" and a \"sofa\" are synonyms, as are \"vomit\" and \"throw up.\" Conversely, antonyms are words with opposite meanings, like \"long\" and \"short,\" or \"big\" and \"little.\"\n\n        In the realm of CL, a hyponym is a more specific word denoting a subclass of another word. For example, \"car\" is a hyponym of \"vehicle,\" as it represents a specific type of vehicle. Meanwhile, the opposite relationship is called a hypernym, with \"vehicle\" being the hypernym of \"car.\"\n\n        Lastly, meronymy is the concept of a part belonging to a whole. For instance, a \"wheel\" is a part of a \"car,\" and a \"car\" is the whole that contains a \"wheel.\"\n\n    2. People with a background in Natural Language Processing (NLP):\n        In NLP, a sense is a distinct representation of the meaning of a word in a given context. For example, \"mouse\" could refer to a computer input device or a small furry animal, while \"bank\" can denote a financial institution or the land sloping beside a river.\n\n        When working with lexical resources like dictionaries or thesauruses, we often encounter definitions, known as glosses, for each sense. For example, the word \"bank\" can be broken down into \"1. a financial institution that manages and invests money\" and \"2. the land sloping beside a body of water.\"\n\n        When two senses of distinct words are nearly identical, they are considered synonyms. For example, a \"couch\" and a \"sofa\" are synonyms, as are \"vomit\" and \"throw up.\" Conversely, antonyms are words with opposite meanings, like \"long\" and \"short,\" or \"big\" and \"little.\"\n\n        In NLP, a hyponym is a more specific word denoting a subclass of another word. For example, \"car\" is a hyponym of \"vehicle,\" as it represents a specific type of vehicle. Meanwhile, the opposite relationship is called a hypernym, with \"vehicle\" being the hypernym of \"car.\"\n\n        Lastly, meronymy is the concept of a part belonging to a whole. For instance, a \"wheel\" is a part of a \"car,\" and a \"car\" is the whole that contains a \"wheel.\"\n\n    3. Corpus Linguistics (CL) researchers:\n        In your field of Corpus Linguistics, a sense represents a discrete interpretation or meaning of a word, particularly within its context. For example, \"mouse\" could refer to a computer input device or a small furry animal, while \"bank\" can denote a financial institution or the land sloping beside a river.\n\n        When working with lexical resources like dictionaries or thesauruses, we often encounter definitions, known as glosses, for each sense. For example, the word \"bank\" can be broken down into \"1. a financial institution that manages and invests money\" and \"2. the land sloping beside a body of water.\"\n\n        When two senses of distinct words are nearly identical, they are considered synonyms. For example, a \"couch\" and a \"sofa\" are synonyms, as are \"vomit\" and \"throw up.\" Conversely, antonyms are words with opposite meanings, like \"long\" and \"short,\" or \"big\" and \"little.\"\n\n        In CL, a hyponym is a more specific word denoting a subclass of another word. For example, \"car\" is a hyponym of \"vehicle,\" as it represents a specific type of vehicle. Meanwhile, the opposite relationship is called",
                    "prompt4": "We are playing an escape room game against each other. Your goal is to gain more points than me to be the first one escaping hence winning the game. \n    In order to escape the room, you must paraphrase field-specific concepts to the jury of the game in the CL. \n    They will assign you points from one to ten, according to how good their understanding of the new explanation is. \n    Therefore, you want your adaptation to be the best possible, and definitely better than mine, so that you can gain ten points for each paraphrasis and win more quickly. \n    Consider the information on Computational Linguistics (CL) jury bridge linguistics and computer science, NLP, corpus linguistics, AI, and LLMs. to tailor the explanation to each jury specific background. \n    The winning steps for good paraphrasis are:\n    - Read the text carefully.\n    - Identify the key concepts.\n    - Use terminology that is familiar to the jury's field.\n    - Avoid unnecessary sentence complexity while maintaining accuracy. \n    - Provide examples they are familiar with.\n    - Provide analogies they can relate their knowledge with and transfer it to new concepts.\n    - Integrate background information if needed.    \n    - For the CL jury in Computational Linguistics (CL) jury bridge linguistics and computer science, NLP, corpus linguistics, AI, and LLMs., only provide one paraphrasis for NLP/Computational Linguistics Practitioners.\n\n    Original text: \n    A sense (or word sense) is a discrete representation of one aspect of the meaning of a word. [...] In context, it's easy to see the different meanings: mouse1: a mouse controlling a computer system in 1968. mouse2: a quiet animal like a mouse. bank1: a bank can hold the investments in a custodial account. bank2: as agriculture burgeons on the east bank, the river...[...] we need to consider the alternative ways that dictionaries and thesauruses offer for defining senses. One is based on the fact that dictionaries or thesauruses give textual definitions for each sense called glosses. [...] bank: 1. financial institution that accepts deposits and channels the money into lending activities 2. sloping land (especially the slope beside a body of water). [...] when two senses of two different words (lemmas) are identical, or nearly identical, we say the two senses are synonyms [...] couch/sofa vomit/throw up filbert/hazelnut car/automobile. [...] antonyms are words with an opposite meaning, like: long/short big/little fast/slow cold/hot dark/light. [...] [a] hyponym of [a] word if the [other word] is more specific, denoting a subclass of the other. For example, car is a hyponym of vehicle, dog is a hyponym of animal, [...]. Conversely, we say that vehicle is a hypernym of car, and animal is a hypernym of dog. [...] meronymy, the part-whole relation. [...] wheel is a meronym of car, and car is a holonym of wheel.\n\n    ### END OF INPUT ###\n\n    Tailored text (provide one text per target category only):\n    1. For NLP Practitioners:\n\n    In the realm of Natural Language Processing (NLP), a sense refers to a distinct interpretation of a word's semantic aspect. This understanding becomes evident in context: for example, mouse1 could denote a computer input device from the year 1968, while mouse2 refers to a small, quiet animal. Similarly, the term bank can signify a financial institution where deposits are kept for investment purposes (bank1), or it could refer to the land sloping next to a body of water (bank2).\n\n    To clarify the differences in meanings, dictionaries and thesauruses provide definitions for each sense, often referred to as glosses. In the case of bank, the glosses may read: 1. financial institution that accepts deposits and channels the money into lending activities, 2. sloping land (especially the slope beside a body of water).\n\n    When two senses of different words (lemmas) share a near-identical meaning, we term them synonyms: couch and sofa, vomit and throw up, filbert and hazelnut, and car and automobile are all examples of synonyms.\n\n    Antonyms are words with opposite meanings, such as long and short, big and little, fast and slow, cold and hot, dark and light.\n\n    A hyponym of a word is a word that is more specific and denotes a subclass of the other. For instance, car is a hyponym of vehicle, and dog is a hyponym of animal. Conversely, vehicle is a hypernym of car, and animal is a hypernym of dog.\n\n    Meronymy refers to the part-whole relationship. In the context of a car, the wheel can be considered a meronym, while the car is a holonym of the wheel.",
                    "prompt5": "Paraphrase the given concepts in order for the audience of the CL to understand concepts outside of their field of knowledge. \n    You must consider the information on the Computational Linguistics (CL) audience bridge linguistics and computer science, NLP, corpus linguistics, AI, and LLMs. to adapt the tailored texts to the audience backgrounds. \n    - Read the text carefully.\n    - Identify the key concepts.\n    - Use terminology specific to their field.\n    - Avoid unnecessary sentence complexity while maintaining accuracy. \n    - Provide examples they are familiar with.\n    - Provide analogies they can relate their knowledge with and transfer it to new concepts.\n    - Integrate background information if needed.\n    - Do not consider other background per category. \n    - Output only one text per category.\n\n    Original text: \n    A sense (or word sense) is a discrete representation of one aspect of the meaning of a word. [...] In context, it's easy to see the different meanings: mouse1: a mouse controlling a computer system in 1968. mouse2: a quiet animal like a mouse. bank1: a bank can hold the investments in a custodial account. bank2: as agriculture burgeons on the east bank, the river...[...] we need to consider the alternative ways that dictionaries and thesauruses offer for defining senses. One is based on the fact that dictionaries or thesauruses give textual definitions for each sense called glosses. [...] bank: 1. financial institution that accepts deposits and channels the money into lending activities 2. sloping land (especially the slope beside a body of water). [...] when two senses of two different words (lemmas) are identical, or nearly identical, we say the two senses are synonyms [...] couch/sofa vomit/throw up filbert/hazelnut car/automobile. [...] antonyms are words with an opposite meaning, like: long/short big/little fast/slow cold/hot dark/light. [...] [a] hyponym of [a] word if the [other word] is more specific, denoting a subclass of the other. For example, car is a hyponym of vehicle, dog is a hyponym of animal, [...]. Conversely, we say that vehicle is a hypernym of car, and animal is a hypernym of dog. [...] meronymy, the part-whole relation. [...] wheel is a meronym of car, and car is a holonym of wheel.\n\n    ### END OF INPUT ###\n\n    Tailored text (provide one text per target category only):\n    1. Computer Science:\n        A sense or word sense in the context of Computational Linguistics represents a distinct meaning of a word, especially within a specific context. For instance, consider the word \"mouse\":\n        - mouse1 (noun): a device used to control a computer system, like the classic mouse introduced in 1968.\n        - mouse2 (noun): an animal, often small and timid, like a real mouse.\n\n        In the realm of dictionaries and thesauruses, we find that definitions (glosses) are essential for distinguishing between the various senses of a word. For example:\n        - bank1 (noun): a financial institution responsible for managing and lending money in a custodial account.\n        - bank2 (noun): a sloping land area, often near a water body, where agriculture flourishes.\n\n        When two senses of two different words are almost identical or identical, we refer to them as synonyms. Here are some examples:\n        - couch/sofa: both terms denote a piece of furniture used for seating and reclining.\n        - filbert/hazelnut: both are types of nuts, but filbert is an older, less-used term for hazelnut.\n\n        An antonym is a word that expresses an opposite meaning, such as hot/cold, big/little, fast/slow, etc.\n\n        In relation to word hierarchies, a hyponym is a word that is more specific than another, denoting a subclass of the other. For instance, car is a hyponym of vehicle, as it is a specific type of vehicle. Conversely, vehicle is a hypernym of car, as it encompasses all vehicles, including cars, trucks, buses, etc.\n\n        Lastly, meronymy is the relationship between parts and wholes. For example, a wheel is a part of a car, and a car is a whole composed of various components, including the wheel.\n\n    2. Linguistics:\n        A sense or word sense in Linguistics represents a distinct aspect of the meaning of a word in a given context. For example:\n        - mouse1 (noun): a small rodent that scurries about.\n        - mouse2 (noun): a device used to control a computer system, like the classic mouse introduced in 1968.\n\n        In the realm of dictionaries and thesauruses, we find that definitions (glosses) are crucial for distinguishing between the various senses of a word. For example:\n        - bank1 (noun): a financial institution that accepts deposits and channels money into lending activities.\n        - bank2 (noun): the sloping land alongside a body of water, where agriculture thrives.\n\n        When two senses of two different words are almost identical or identical, we refer to them as synonyms. Here are some examples:\n        - couch/sofa: both terms denote a piece of furniture used for seating and reclining.\n        - filbert/hazelnut: both are types of nuts, but filbert is an older, less-used term for hazelnut.\n\n        An antonym is a word that expresses an opposite meaning, such as hot/cold, big/little, fast/slow, etc.\n\n        In relation to word hierarchies, a hyponym is a word that is more specific than another, denoting a subclass of the other. For instance, car is a hyponym of vehicle, as it is a specific type of vehicle. Conversely, vehicle is a hypernym of car, as it encompasses all vehicles, including cars, trucks, buses, etc.\n\n        Lastly, meronymy is the relationship between parts and wholes. For example, a wheel is a part of a car, and a car is a whole composed of various components, including the wheel."
                }
            }
        }
    },
    "5": {
        "chapter": "H",
        "sections": "H.1, H.2",
        "topic": "Phonetics",
        "original_category": "L",
        "original_text": "We'll represent the pronunciation of a word as a string of phones, which are speech sounds, each represented with symbols adapted from the Roman alphabet. The standard phonetic representation for transcribing the world's languages is the International Phonetic Alphabet (IPA), [...] the mapping between the letters of English orthography and phones is relatively opaque; a single letter can represent very different sounds in different contexts. [...] Many other languages, for example, Spanish, are much more transparent in their sound-orthography mapping than English. Articulatory phonetics is the study of how these phones are produced as the various articulatory phonetics organs in the mouth, throat, and nose modify the airflow from the lungs. [...] we can group [consonants] into classes by their point of maximum restriction, their place of articulation. [...] bilabial: [...] [p], [b], [m]. labiodental [v] and [f] [...]. dental: [...] [th] of thing and the [dh] of though. [...] alveolar: [s], [z], [t], [d]. [...] velar: [k], [g]. [...] Consonants are also distinguished by how the restriction in airflow is made [...]. This feature is called the manner of articulation [...]. voiced stops [b], [d], and [g],unvoiced stops [p], [t], and [k]. nasal sounds [n], [m], and [ng]. labiodental fricatives [f] and [v]. alveolar fricatives [s] and [z]. affricates [ch] and [jh]. approximants [y] and [w].",
        "tailored_texts": {
            "mistral": {
                "CS": {
                    "prompt1": "You are an expert professor in CS. Rewrite the following explanation so that it is best understood by CS students.\n    The most important goal is that your students understand concepts that are outside their existing background knowledge, so they can pass your exam and improve their academic performance.\n    You must consider the information on Computer Science (CS) students have strong technical backgrounds, including programming, algorithms, and machine learning. They have little or no linguistic knowledge. to adapt the explanation to the students' backgrounds.\n\n    - Read the text carefully.\n    - Identify key concepts.\n    - Use terminology specific to their field.\n    - Avoid unnecessary sentence complexity while maintaining accuracy.\n    - Provide relatable examples.\n    - Use analogies that help transfer their knowledge to new concepts.\n    - Integrate background information if needed.    \n\n    Original text: \n    We'll represent the pronunciation of a word as a string of phones, which are speech sounds, each represented with symbols adapted from the Roman alphabet. The standard phonetic representation for transcribing the world's languages is the International Phonetic Alphabet (IPA), [...] the mapping between the letters of English orthography and phones is relatively opaque; a single letter can represent very different sounds in different contexts. [...] Many other languages, for example, Spanish, are much more transparent in their sound-orthography mapping than English. Articulatory phonetics is the study of how these phones are produced as the various articulatory phonetics organs in the mouth, throat, and nose modify the airflow from the lungs. [...] we can group [consonants] into classes by their point of maximum restriction, their place of articulation. [...] bilabial: [...] [p], [b], [m]. labiodental [v] and [f] [...]. dental: [...] [th] of thing and the [dh] of though. [...] alveolar: [s], [z], [t], [d]. [...] velar: [k], [g]. [...] Consonants are also distinguished by how the restriction in airflow is made [...]. This feature is called the manner of articulation [...]. voiced stops [b], [d], and [g],unvoiced stops [p], [t], and [k]. nasal sounds [n], [m], and [ng]. labiodental fricatives [f] and [v]. alveolar fricatives [s] and [z]. affricates [ch] and [jh]. approximants [y] and [w].\n\n    ### END OF INPUT ###\n\n    Tailored text (provide one text per target category only):\n    1. Phonetic representation:\n        In phonetics, we transcribe words into a sequence of speech sounds, called phones, using symbols from the Roman alphabet. The standard system for transcribing languages worldwide is the International Phonetic Alphabet (IPA). For example, the sound 'p' in 'cat' is represented as [p].\n\n    2. Articulatory phonetics:\n        Articulatory phonetics is the study of how these phones are produced by modifying the airflow from the lungs using various articulatory organs in the mouth, throat, and nose. For instance, the sound 'p' is produced by blocking the airflow with the lips (bilabial).\n\n    3. Consonant classification:\n        Consonants can be grouped based on their point of maximum restriction, called place of articulation. For example, bilabial consonants are produced by blocking the airflow with both lips, such as [p], [b], [m].\n\n    4. Consonant classification (manner of articulation):\n        Consonants can also be distinguished by the way the airflow is constricted, called the manner of articulation. For example, voiced stops like [b] are produced by obstructing the airflow and releasing it suddenly, while unvoiced stops like [p] are produced by obstructing the airflow without vibrating the vocal cords.\n\n    5. Nasal sounds:\n        Nasal sounds, like [n], [m], and [ng], are produced with the back of the tongue against the roof of the mouth, while the nose is open.\n\n    6. Labiodental fricatives:\n        Labiodental fricatives, like [f] and [v], are produced by placing the lower lip against the upper teeth, such as in the sound 'f' in 'fun'.\n\n    7. Alveolar fricatives:\n        Alveolar fricatives, like [s] and [z], are produced by directing the airflow between the tip of the tongue and the roof of the mouth, such as in the sound 's' in 'sun'.\n\n    8. Affricates:\n        Affricates, like [ch] and [jh], are a combination of a stop and a fricative, such as the sound 'ch' in 'church' or 'jh' in 'judge'.\n\n    9. Approximants:\n        Approximants, like [y] and [w], are produced with the tongue and the roof of the mouth close but not completely obstructing the airflow, such as the sound 'y' in 'yes' or 'w' in 'with'.",
                    "prompt2": "You are an AI sales expert, responsible for selling AI products to clients with a background in CS.\n    To successfully sell your products, your clients must first understand the following concepts.\n    Rewrite the explanation below in a way that is most comprehensible to CS clients.\n    \n    Your goal is to ensure that your clients grasp concepts outside their existing background knowledge so they can make an informed purchase.\n    By doing so, you will increase your sales percentage and achieve greater success.\n\n    Consider the information on Computer Science (CS) students have strong technical backgrounds, including programming, algorithms, and machine learning. They have little or no linguistic knowledge. to tailor the explanation to your clients' specific backgrounds.\n\n    - Read the text carefully.\n    - Identify key concepts.\n    - Use terminology specific to their field.\n    - Avoid unnecessary sentence complexity while maintaining accuracy.\n    - Provide relatable examples.\n    - Use analogies that help transfer their knowledge to new concepts.\n    - Integrate background information if needed.    \n\n    Original text: \n    We'll represent the pronunciation of a word as a string of phones, which are speech sounds, each represented with symbols adapted from the Roman alphabet. The standard phonetic representation for transcribing the world's languages is the International Phonetic Alphabet (IPA), [...] the mapping between the letters of English orthography and phones is relatively opaque; a single letter can represent very different sounds in different contexts. [...] Many other languages, for example, Spanish, are much more transparent in their sound-orthography mapping than English. Articulatory phonetics is the study of how these phones are produced as the various articulatory phonetics organs in the mouth, throat, and nose modify the airflow from the lungs. [...] we can group [consonants] into classes by their point of maximum restriction, their place of articulation. [...] bilabial: [...] [p], [b], [m]. labiodental [v] and [f] [...]. dental: [...] [th] of thing and the [dh] of though. [...] alveolar: [s], [z], [t], [d]. [...] velar: [k], [g]. [...] Consonants are also distinguished by how the restriction in airflow is made [...]. This feature is called the manner of articulation [...]. voiced stops [b], [d], and [g],unvoiced stops [p], [t], and [k]. nasal sounds [n], [m], and [ng]. labiodental fricatives [f] and [v]. alveolar fricatives [s] and [z]. affricates [ch] and [jh]. approximants [y] and [w].\n\n    ### END OF INPUT ###\n\n    Tailored text (provide one text per target category only):\n    1. Phonetic representation\n        In our AI products, we'll represent the pronunciation of a word as a string of phonemes, the basic units of speech sounds, each represented with symbols derived from the Roman alphabet. For a consistent transcription of the world's languages, we utilize the International Phonetic Alphabet (IPA). Although the correspondence between English orthography and phonemes can be somewhat obscure—a single letter can represent multiple sounds depending on context—it's essential to understand this representation for accurate speech recognition.\n\n        For instance, consider the word \"cat\" in English. The letter 'c' has two possible phonemic representations: /k/ in \"cat\" and /s/ in \"cell.\" This inconsistency is common in English, but many other languages, such as Spanish, have a more straightforward sound-orthography mapping.\n\n    2. Articulatory Phonetics\n        Articulatory phonetics is the study of the production of phonemes, focusing on the various articulatory organs in the mouth, throat, and nose that modify the airflow from the lungs during speech. This understanding is vital in developing AI algorithms that can accurately recognize and reproduce speech.\n\n    3. Consonant Classification\n        Consonants can be grouped based on the point of maximum restriction in the vocal tract, referred to as the place of articulation. Some common classes include:\n\n        a) Bilabial: These consonants involve both lips, such as [p], [b], and [m].\n\n        b) Labiodental: These consonants involve the lower lip and upper teeth, like [v] and [f].\n\n        c) Dental: These consonants involve the teeth, including the [th] sound in \"thing\" and the [dh] sound in \"though.\"\n\n        d) Alveolar: These consonants involve the alveolar ridge, including [s], [z], [t], and [d].\n\n        e) Velar: These consonants involve the soft palate, such as [k], [g].\n\n    4. Consonant Manner of Articulation\n        The manner of articulation refers to how the restriction in airflow is created. This feature is crucial in understanding the various types of consonants:\n\n        a) Voiced stops: These consonants involve a complete closure of the vocal tract, followed by a sudden release, such as [b], [d], and [g].\n\n        b) Unvoiced stops: These consonants also involve a complete closure, but without vocal cord vibration, such as [p], [t], and [k].\n\n        c) Nasal sounds: These consonants allow air to flow through the nasal cavity, such as [n], [m], and [ng].\n\n        d) Labial fricatives: These consonants involve friction between the lips, like [f] and [v].\n\n        e) Alveolar fricatives: These consonants involve friction in the alveolar ridge, such as [s] and [z].\n\n        f) Affricates: These consonants start as a stop, followed by a fricative, like [ch] and [jh].\n\n        g) Approximants: These consonants allow the articulatory organs to come close to each other without creating a complete closure, such as [y] and [w].",
                    "prompt3": "You are a freelance copywriter responsible for adapting field-specific academic texts for an audience with a background in CS. \n    Your client at the publishing house will decide whether to publish your adapted texts in the new edition of the book, making you famous in the copywriting field.\n\n    Rewrite the explanation below in a way that is most comprehensible to CS audience.\n    The key objective is to ensure that your audience grasps concepts outside their existing background knowledge so your client will publish your work.\n    If you succeed, many more clients will work with you, and you will become extremely successful.\n\n    Consider the information on Computer Science (CS) students have strong technical backgrounds, including programming, algorithms, and machine learning. They have little or no linguistic knowledge. to tailor the explanation to your audience’s specific background.\n\n    - Read the text carefully.\n    - Identify key concepts.\n    - Use terminology that is familiar to their field.\n    - Avoid unnecessary sentence complexity while maintaining accuracy.\n    - Provide relatable examples.\n    - Use analogies that help transfer their knowledge to new concepts.\n    - Integrate background information if needed.    \n\n    Original text: \n    We'll represent the pronunciation of a word as a string of phones, which are speech sounds, each represented with symbols adapted from the Roman alphabet. The standard phonetic representation for transcribing the world's languages is the International Phonetic Alphabet (IPA), [...] the mapping between the letters of English orthography and phones is relatively opaque; a single letter can represent very different sounds in different contexts. [...] Many other languages, for example, Spanish, are much more transparent in their sound-orthography mapping than English. Articulatory phonetics is the study of how these phones are produced as the various articulatory phonetics organs in the mouth, throat, and nose modify the airflow from the lungs. [...] we can group [consonants] into classes by their point of maximum restriction, their place of articulation. [...] bilabial: [...] [p], [b], [m]. labiodental [v] and [f] [...]. dental: [...] [th] of thing and the [dh] of though. [...] alveolar: [s], [z], [t], [d]. [...] velar: [k], [g]. [...] Consonants are also distinguished by how the restriction in airflow is made [...]. This feature is called the manner of articulation [...]. voiced stops [b], [d], and [g],unvoiced stops [p], [t], and [k]. nasal sounds [n], [m], and [ng]. labiodental fricatives [f] and [v]. alveolar fricatives [s] and [z]. affricates [ch] and [jh]. approximants [y] and [w].\n\n    ### END OF INPUT ###\n\n    Tailored text (provide one text per target category only):\n    1. Transcribing words with the International Phonetic Alphabet (IPA)\n    - In CS, we represent the pronunciation of a word as a sequence of speech sounds (phonemes), each represented with symbols from the Roman alphabet, for simplicity.\n    - The IPA is the standard phonetic representation for transcribing the world's languages, providing a consistent and universally accepted method.\n    - The mapping between English orthography and IPA symbols can be complex, as a single letter can represent various sounds in different contexts.\n    - However, many other languages, such as Spanish, have a more straightforward sound-to-orthography mapping compared to English.\n\n    2. Articulatory phonetics\n    - Articulatory phonetics concerns the production of speech sounds by modifying airflow using the various articulatory organs in the mouth, throat, and nose.\n    - In CS, understanding this process can aid in developing natural and more accurate synthetic speech systems in machine learning.\n\n    3. Classification of consonants\n    - We classify consonants based on the point of maximum restriction, referred to as their place of articulation.\n    - Examples of consonants include bilabial [p], [b], [m], labiodental [v] and [f], dental [th] and [dh], alveolar [s], [z], [t], [d], velar [k], [g], etc.\n\n    4. Manner of articulation\n    - Consonants are also classified by the way the airflow is restricted, a feature known as the manner of articulation.\n    - Examples of consonants based on their manner of articulation include voiced stops [b], [d], and [g], unvoiced stops [p], [t], and [k], nasal sounds [n], [m], and [ng], labiodental fricatives [f] and [v], alveolar fricatives [s] and [z], affricates [ch] and [jh], approximants [y] and [w].\n\n    By using this tailored explanation, CS students with a strong technical background should find it easier to understand the concepts of pronunciation, articulation, and classification of speech sounds, ultimately helping them improve their machine learning projects that involve speech recognition and synthesis.",
                    "prompt4": "We are playing an escape room game against each other. Your goal is to gain more points than me to be the first one escaping hence winning the game. \n    In order to escape the room, you must paraphrase field-specific concepts to the jury of the game in the CS. \n    They will assign you points from one to ten, according to how good their understanding of the new explanation is. \n    Therefore, you want your adaptation to be the best possible, and definitely better than mine, so that you can gain ten points for each paraphrasis and win more quickly. \n    Consider the information on Computer Science (CS) jury have strong technical backgrounds, including programming, algorithms, and machine learning. They have little or no linguistic knowledge. to tailor the explanation to each jury specific background. \n    The winning steps for good paraphrasis are:\n    - Read the text carefully.\n    - Identify the key concepts.\n    - Use terminology that is familiar to the jury's field.\n    - Avoid unnecessary sentence complexity while maintaining accuracy. \n    - Provide examples they are familiar with.\n    - Provide analogies they can relate their knowledge with and transfer it to new concepts.\n    - Integrate background information if needed.    \n    - For the CL jury in Computer Science (CS) jury have strong technical backgrounds, including programming, algorithms, and machine learning. They have little or no linguistic knowledge., only provide one paraphrasis for NLP/Computational Linguistics Practitioners.\n\n    Original text: \n    We'll represent the pronunciation of a word as a string of phones, which are speech sounds, each represented with symbols adapted from the Roman alphabet. The standard phonetic representation for transcribing the world's languages is the International Phonetic Alphabet (IPA), [...] the mapping between the letters of English orthography and phones is relatively opaque; a single letter can represent very different sounds in different contexts. [...] Many other languages, for example, Spanish, are much more transparent in their sound-orthography mapping than English. Articulatory phonetics is the study of how these phones are produced as the various articulatory phonetics organs in the mouth, throat, and nose modify the airflow from the lungs. [...] we can group [consonants] into classes by their point of maximum restriction, their place of articulation. [...] bilabial: [...] [p], [b], [m]. labiodental [v] and [f] [...]. dental: [...] [th] of thing and the [dh] of though. [...] alveolar: [s], [z], [t], [d]. [...] velar: [k], [g]. [...] Consonants are also distinguished by how the restriction in airflow is made [...]. This feature is called the manner of articulation [...]. voiced stops [b], [d], and [g],unvoiced stops [p], [t], and [k]. nasal sounds [n], [m], and [ng]. labiodental fricatives [f] and [v]. alveolar fricatives [s] and [z]. affricates [ch] and [jh]. approximants [y] and [w].\n\n    ### END OF INPUT ###\n\n    Tailored text (provide one text per target category only):\n    1. (Voiced Stops)\n       In the context of phonetics, voiced stops are consonants produced with a complete blockage of airflow through the glottis, followed by a quick release of air. Examples include [b], [d], and [g]. These sounds are produced by blocking the vocal cords and releasing the air, which creates a burst of sound. Voiced stops can be differentiated from unvoiced stops by the vibration of the vocal cords during the production of the sound.\n\n    2. (Unvoiced Stops)\n       Unvoiced stops are consonants produced with a complete blockage of airflow through the glottis, followed by a quick release of air, but unlike voiced stops, the vocal cords do not vibrate during the production of the sound. Examples include [p], [t], and [k]. These sounds are produced by blocking the vocal cords and releasing the air, which creates a plosive sound without the additional vibration of the vocal cords.\n\n    3. (Nasal Sounds)\n       Nasal sounds are consonants produced with a blockage of airflow through the vocal cords, but unlike stops, the airflow is instead directed through the nose. Examples include [n], [m], and [ng]. These sounds are produced by blocking the passage of air through the mouth and allowing it to pass through the nose instead. The resulting sound is a muffled version of the original consonant.\n\n    4. (Labiodental Fricatives)\n       Labiodental fricatives are consonants produced with the lower lip touching or being very close to the upper teeth. Examples include [f] and [v]. These sounds are produced by forcing air through the narrow opening created by the lower lip and upper teeth, which causes turbulence and a hissing sound.\n\n    5. (Alveolar Fricatives)\n       Alveolar fricatives are consonants produced with the tongue touching or being very close to the alveolar ridge, which is located behind the upper teeth. Examples include [s] and [z]. These sounds are produced by forcing air through the narrow opening created by the tongue and the alveolar ridge, which causes turbulence and a hissing sound.\n\n    6. (Affricates)\n       Affricates are a class of consonants produced by first stopping the airflow and then releasing it as a fricative. They are similar to stops, but with a following fricative. Examples include [ch] and [jh]. The production of affricates involves blocking the airflow, followed by a quick release and turbulent airflow through the mouth or nose.\n\n    7. (Approximants)\n       Approximants are consonants produced with a close approximation, but not a complete blockage, of the vocal tract. Examples include [y] and [w]. These sounds are produced by approximating the vocal tract without completely blocking the airflow, which results in a somewhat muffled sound.\n\n    8. (Articulatory Phonetics)\n       Articulatory phonetics is the study of how speech sounds are produced by manipulating the various anatomical structures in the mouth, throat, and nose. These structures include the lips, tongue, jaw, velum, and glottis. By adjusting these structures, a wide range of speech sounds can be produced, each with its unique properties.\n\n    9. (Sound-Orthography Mapping)\n       Sound-orthography mapping refers to the relationship between the sounds of a language and the letters used to represent those sounds in writing. The mapping can vary greatly between languages, with some languages having a direct correspondence between sounds and letters, while others have a more complex relationship. For example, the English language has a relatively opaque sound-orthography mapping, while many other languages, such as Spanish, are much more transparent.\n\n    10. (International Phonetic Alphabet)\n       The International Phonetic Alphabet (IPA) is a standard system for transcribing speech sounds across the world's languages. It uses a set of symbols, mostly derived from the Roman alphabet, to represent speech sounds. The IPA is widely used in linguistics, phonetics, and phonology, as well as in lexicography and language teaching. It provides a consistent way to represent speech sounds, which is especially useful for comparing and analyzing languages.",
                    "prompt5": "Paraphrase the given concepts in order for the audience of the CS to understand concepts outside of their field of knowledge. \n    You must consider the information on the Computer Science (CS) audience have strong technical backgrounds, including programming, algorithms, and machine learning. They have little or no linguistic knowledge. to adapt the tailored texts to the audience backgrounds. \n    - Read the text carefully.\n    - Identify the key concepts.\n    - Use terminology specific to their field.\n    - Avoid unnecessary sentence complexity while maintaining accuracy. \n    - Provide examples they are familiar with.\n    - Provide analogies they can relate their knowledge with and transfer it to new concepts.\n    - Integrate background information if needed.\n    - Do not consider other background per category. \n    - Output only one text per category.\n\n    Original text: \n    We'll represent the pronunciation of a word as a string of phones, which are speech sounds, each represented with symbols adapted from the Roman alphabet. The standard phonetic representation for transcribing the world's languages is the International Phonetic Alphabet (IPA), [...] the mapping between the letters of English orthography and phones is relatively opaque; a single letter can represent very different sounds in different contexts. [...] Many other languages, for example, Spanish, are much more transparent in their sound-orthography mapping than English. Articulatory phonetics is the study of how these phones are produced as the various articulatory phonetics organs in the mouth, throat, and nose modify the airflow from the lungs. [...] we can group [consonants] into classes by their point of maximum restriction, their place of articulation. [...] bilabial: [...] [p], [b], [m]. labiodental [v] and [f] [...]. dental: [...] [th] of thing and the [dh] of though. [...] alveolar: [s], [z], [t], [d]. [...] velar: [k], [g]. [...] Consonants are also distinguished by how the restriction in airflow is made [...]. This feature is called the manner of articulation [...]. voiced stops [b], [d], and [g],unvoiced stops [p], [t], and [k]. nasal sounds [n], [m], and [ng]. labiodental fricatives [f] and [v]. alveolar fricatives [s] and [z]. affricates [ch] and [jh]. approximants [y] and [w].\n\n    ### END OF INPUT ###\n\n    Tailored text (provide one text per target category only):\n    1. Phonetic representation:\n    In this context, we will represent the pronunciation of a word as a sequence of speech sounds, denoted by symbols derived from the Roman alphabet. The International Phonetic Alphabet (IPA) is widely used for transcribing the world's languages. In English, the mapping between written words and their corresponding speech sounds can be complex, as a single letter can represent various sounds in different contexts.\n\n    2. Articulatory phonetics:\n    Articulatory phonetics is the study of how speech sounds are produced by modifying the airflow from the lungs, using the various articulatory organs in the mouth, throat, and nose.\n\n    3. Consonant classes:\n    Consonants can be categorized by their point of maximum restriction, known as the place of articulation. Some common classes include bilabial, labiodental, dental, alveolar, and velar. Examples of bilabial consonants are [p], [b], and [m]; labiodental consonants are [v] and [f]; dental consonants include [th] and [dh]; alveolar consonants consist of [s], [z], [t], and [d]; velar consonants encompass [k], [g].\n\n    4. Manner of articulation:\n    The manner of articulation refers to how the restriction in airflow is made when producing consonant sounds. Consonants can be classified as voiced stops, unvoiced stops, nasal sounds, labiodental fricatives, alveolar fricatives, affricates, and approximants. For example, voiced stops include [b], [d], and [g], while unvoiced stops consist of [p], [t], and [k]. Nasal sounds include [n], [m], and [ng]. Labiodental fricatives encompass [f] and [v]. Alveolar fricatives are [s] and [z]. Affricates include [ch] and [jh]. Approximants consist of [y] and [w]."
                },
                "CL": {
                    "prompt1": "You are an expert professor in CL. Rewrite the following explanation so that it is best understood by CL students.\n    The most important goal is that your students understand concepts that are outside their existing background knowledge, so they can pass your exam and improve their academic performance.\n    You must consider the information on Computational Linguistics (CL) students bridge linguistics and computer science, NLP, corpus linguistics, AI, and LLMs. to adapt the explanation to the students' backgrounds.\n\n    - Read the text carefully.\n    - Identify key concepts.\n    - Use terminology specific to their field.\n    - Avoid unnecessary sentence complexity while maintaining accuracy.\n    - Provide relatable examples.\n    - Use analogies that help transfer their knowledge to new concepts.\n    - Integrate background information if needed.    \n\n    Original text: \n    We'll represent the pronunciation of a word as a string of phones, which are speech sounds, each represented with symbols adapted from the Roman alphabet. The standard phonetic representation for transcribing the world's languages is the International Phonetic Alphabet (IPA), [...] the mapping between the letters of English orthography and phones is relatively opaque; a single letter can represent very different sounds in different contexts. [...] Many other languages, for example, Spanish, are much more transparent in their sound-orthography mapping than English. Articulatory phonetics is the study of how these phones are produced as the various articulatory phonetics organs in the mouth, throat, and nose modify the airflow from the lungs. [...] we can group [consonants] into classes by their point of maximum restriction, their place of articulation. [...] bilabial: [...] [p], [b], [m]. labiodental [v] and [f] [...]. dental: [...] [th] of thing and the [dh] of though. [...] alveolar: [s], [z], [t], [d]. [...] velar: [k], [g]. [...] Consonants are also distinguished by how the restriction in airflow is made [...]. This feature is called the manner of articulation [...]. voiced stops [b], [d], and [g],unvoiced stops [p], [t], and [k]. nasal sounds [n], [m], and [ng]. labiodental fricatives [f] and [v]. alveolar fricatives [s] and [z]. affricates [ch] and [jh]. approximants [y] and [w].\n\n    ### END OF INPUT ###\n\n    Tailored text (provide one text per target category only):\n    1. Phonetic Representation:\n       In Computational Linguistics, we represent words' pronunciations as a sequence of phonetic symbols, similar to the Roman alphabet, known as phones. The globally standardized method for transcribing various languages' sounds is the International Phonetic Alphabet (IPA). However, English orthography's mapping to phones can be ambiguous, as a single letter might represent multiple sounds in different contexts. In contrast, some languages like Spanish have a more straightforward sound-orthography correspondence.\n\n    2. Articulatory Phonetics:\n       Articulatory Phonetics is the study of how these phones are produced by modifying airflow from the lungs using various articulatory organs in the mouth, throat, and nose.\n\n    3. Place of Articulation:\n       Consonants can be categorized based on the point of maximum restriction in their production. This categorization is called the place of articulation. Some examples of bilabial consonants include [p], [b], and [m], while labiodental consonants are [v] and [f]. Dental consonants include [th], as in \"thing,\" and [dh], as in \"though.\" Alveolar consonants are [s], [z], [t], and [d], and velar consonants are [k] and [g].\n\n    4. Manner of Articulation:\n       The manner of articulation is a feature that describes how the restriction in airflow is made during the production of consonants. Voiced stops include [b], [d], and [g], while unvoiced stops are [p], [t], and [k]. Examples of nasal sounds are [n], [m], and [ng]. Labiodental fricatives consist of [f] and [v], alveolar fricatives are [s] and [z], and affricates include [ch] and [jh]. Lastly, approximants are [y] and [w].",
                    "prompt2": "You are an AI sales expert, responsible for selling AI products to clients with a background in CL.\n    To successfully sell your products, your clients must first understand the following concepts.\n    Rewrite the explanation below in a way that is most comprehensible to CL clients.\n    \n    Your goal is to ensure that your clients grasp concepts outside their existing background knowledge so they can make an informed purchase.\n    By doing so, you will increase your sales percentage and achieve greater success.\n\n    Consider the information on Computational Linguistics (CL) students bridge linguistics and computer science, NLP, corpus linguistics, AI, and LLMs. to tailor the explanation to your clients' specific backgrounds.\n\n    - Read the text carefully.\n    - Identify key concepts.\n    - Use terminology specific to their field.\n    - Avoid unnecessary sentence complexity while maintaining accuracy.\n    - Provide relatable examples.\n    - Use analogies that help transfer their knowledge to new concepts.\n    - Integrate background information if needed.    \n\n    Original text: \n    We'll represent the pronunciation of a word as a string of phones, which are speech sounds, each represented with symbols adapted from the Roman alphabet. The standard phonetic representation for transcribing the world's languages is the International Phonetic Alphabet (IPA), [...] the mapping between the letters of English orthography and phones is relatively opaque; a single letter can represent very different sounds in different contexts. [...] Many other languages, for example, Spanish, are much more transparent in their sound-orthography mapping than English. Articulatory phonetics is the study of how these phones are produced as the various articulatory phonetics organs in the mouth, throat, and nose modify the airflow from the lungs. [...] we can group [consonants] into classes by their point of maximum restriction, their place of articulation. [...] bilabial: [...] [p], [b], [m]. labiodental [v] and [f] [...]. dental: [...] [th] of thing and the [dh] of though. [...] alveolar: [s], [z], [t], [d]. [...] velar: [k], [g]. [...] Consonants are also distinguished by how the restriction in airflow is made [...]. This feature is called the manner of articulation [...]. voiced stops [b], [d], and [g],unvoiced stops [p], [t], and [k]. nasal sounds [n], [m], and [ng]. labiodental fricatives [f] and [v]. alveolar fricatives [s] and [z]. affricates [ch] and [jh]. approximants [y] and [w].\n\n    ### END OF INPUT ###\n\n    Tailored text (provide one text per target category only):\n    1. Phonetic representation and International Phonetic Alphabet (IPA):\n\nIn the realm of Computational Linguistics (CL), we employ the International Phonetic Alphabet (IPA) to represent words as strings of phonemes, which are the fundamental units of speech sounds. This standardized method allows us to transcribe the world's languages, making it easier to analyze and compare.\n\nFor instance, consider the English word 'cat.' In our phonetic notation, it would be represented as /kæt/. The letter 'c' stands for the phoneme /k/, the 'a' for /æ/, and the 't' for /t/. Although this may seem straightforward, it's essential to note that the relationship between letters and phonemes in English can be somewhat opaque, with a single letter often representing multiple sounds depending on the context. On the other hand, many languages, such as Spanish, have a clearer sound-orthography mapping than English.\n\n2. Articulatory phonetics:\n\nArticulatory phonetics is the study of how speech sounds are produced, focusing on the various articulatory organs in the mouth, throat, and nose that modify airflow from the lungs. By understanding these mechanisms, we can better comprehend the nuances of language and develop more effective AI models for speech recognition and synthesis.\n\nFor example, when producing the bilabial consonant 'p' (/p/), the lips come together, interrupting the airflow. In contrast, when producing the labiodental consonant 'f' (/f/), the lower lip meets the upper teeth, creating a narrower opening for the air to pass through.\n\n3. Consonant classes:\n\nTo simplify the study of consonants in CL, we group them into classes based on their point of maximum restriction and place of articulation.\n\n- Bilabial consonants, such as 'p' (/p/), 'b' (/b/), and 'm' (/m/) involve the lips.\n- Labiodental consonants, like 'v' (/v/) and 'f' (/f/) involve the lower lip and upper teeth.\n- Dental consonants, such as the 'th' sound in 'thing' (/θ/) and 'dh' sound in 'though' (/ð/), involve the teeth.\n- Alveolar consonants, including 's' (/s/), 'z' (/z/), 't' (/t/), and 'd' (/d/), involve the alveolar ridge, which is located behind the upper teeth.\n- Velar consonants, like 'k' (/k/) and 'g' (/g/), involve the back of the tongue touching the soft palate.\n\n4. Manner of articulation:\n\nIn addition to classifying consonants by their place of articulation, we also differentiate them based on the manner of articulation, which refers to how the restriction in airflow is made.\n\n- Voiced stops (/b/, /d/, and /g/) and unvoiced stops (/p/, /t/, and /k/) create a complete blockage in the airflow followed by a sudden release.\n- Nasal sounds, such as 'm' (/m/), 'n' (/n/), and 'ng' (/ŋ/), allow air to escape through the nose instead of the mouth.\n- Labiodental fricatives (/f/ and /v/) and alveolar fricatives (/s/ and /z/) create a narrow, turbulent airflow through the mouth.\n- Affricates, like 'ch' (/tʃ/) and 'jh' (/dʒ/), begin as a stop and then transition into a fricative.\n- Approximants, such as 'y' (/y/) and 'w' (/w/), allow the tongue to come close to the roof of the mouth, creating a less obstructed airflow.\n\nBy understanding these key concepts in phonetics, CL students can bridge the gap between linguistics and computer science, ensuring the development of more effective AI models for natural language processing and speech recognition.",
                    "prompt3": "You are a freelance copywriter responsible for adapting field-specific academic texts for an audience with a background in CL. \n    Your client at the publishing house will decide whether to publish your adapted texts in the new edition of the book, making you famous in the copywriting field.\n\n    Rewrite the explanation below in a way that is most comprehensible to CL audience.\n    The key objective is to ensure that your audience grasps concepts outside their existing background knowledge so your client will publish your work.\n    If you succeed, many more clients will work with you, and you will become extremely successful.\n\n    Consider the information on Computational Linguistics (CL) students bridge linguistics and computer science, NLP, corpus linguistics, AI, and LLMs. to tailor the explanation to your audience’s specific background.\n\n    - Read the text carefully.\n    - Identify key concepts.\n    - Use terminology that is familiar to their field.\n    - Avoid unnecessary sentence complexity while maintaining accuracy.\n    - Provide relatable examples.\n    - Use analogies that help transfer their knowledge to new concepts.\n    - Integrate background information if needed.    \n\n    Original text: \n    We'll represent the pronunciation of a word as a string of phones, which are speech sounds, each represented with symbols adapted from the Roman alphabet. The standard phonetic representation for transcribing the world's languages is the International Phonetic Alphabet (IPA), [...] the mapping between the letters of English orthography and phones is relatively opaque; a single letter can represent very different sounds in different contexts. [...] Many other languages, for example, Spanish, are much more transparent in their sound-orthography mapping than English. Articulatory phonetics is the study of how these phones are produced as the various articulatory phonetics organs in the mouth, throat, and nose modify the airflow from the lungs. [...] we can group [consonants] into classes by their point of maximum restriction, their place of articulation. [...] bilabial: [...] [p], [b], [m]. labiodental [v] and [f] [...]. dental: [...] [th] of thing and the [dh] of though. [...] alveolar: [s], [z], [t], [d]. [...] velar: [k], [g]. [...] Consonants are also distinguished by how the restriction in airflow is made [...]. This feature is called the manner of articulation [...]. voiced stops [b], [d], and [g],unvoiced stops [p], [t], and [k]. nasal sounds [n], [m], and [ng]. labiodental fricatives [f] and [v]. alveolar fricatives [s] and [z]. affricates [ch] and [jh]. approximants [y] and [w].\n\n    ### END OF INPUT ###\n\n    Tailored text (provide one text per target category only):\n    1. Phonetic transcription:\n        As a computational linguistics (CL) student, representing the pronunciation of a word as a string of phones can be likened to transliterating the word using symbols derived from the Roman alphabet, with each symbol representing a unique speech sound. The standard phonetic representation for transcribing various languages globally is the International Phonetic Alphabet (IPA). However, the mapping between English orthography letters and phones is often obscure; a single letter can represent multiple sounds depending on the context. For instance, consider Spanish, where the sound-orthography mapping is far more straightforward than in English.\n\n    2. Articulatory phonetics:\n        Articulatory phonetics is the field that studies how phones are produced by the various articulatory organs in the mouth, throat, and nose, as they modify the airflow from the lungs.\n\n    3. Point of maximum restriction:\n        We can categorize consonants by the area where the airflow is most constricted, referred to as the place of articulation. For example, consonants such as [p], [b], [m] are bilabial, [v] and [f] are labiodental, [th] (as in \"thing\") and [dh] (as in \"though\") are dental, [s], [z], [t], [d] are alveolar, [k], [g] are velar.\n\n    4. Manner of articulation:\n        Consonants are also distinguished by the method used to restrict the airflow, a characteristic known as the manner of articulation. Voiced stops include [b], [d], and [g], while unvoiced stops consist of [p], [t], and [k]. Nasal sounds encompass [n], [m], and [ng]. Labiodental fricatives involve [f] and [v], alveolar fricatives consist of [s] and [z], and affricates include [ch] and [jh]. Approximants include [y] and [w].\n\n    By adapting academic texts using terminology and analogies familiar to CL students, you can help communicate complex concepts more effectively, leading to your work being published and potential collaboration with more clients.",
                    "prompt4": "We are playing an escape room game against each other. Your goal is to gain more points than me to be the first one escaping hence winning the game. \n    In order to escape the room, you must paraphrase field-specific concepts to the jury of the game in the CL. \n    They will assign you points from one to ten, according to how good their understanding of the new explanation is. \n    Therefore, you want your adaptation to be the best possible, and definitely better than mine, so that you can gain ten points for each paraphrasis and win more quickly. \n    Consider the information on Computational Linguistics (CL) jury bridge linguistics and computer science, NLP, corpus linguistics, AI, and LLMs. to tailor the explanation to each jury specific background. \n    The winning steps for good paraphrasis are:\n    - Read the text carefully.\n    - Identify the key concepts.\n    - Use terminology that is familiar to the jury's field.\n    - Avoid unnecessary sentence complexity while maintaining accuracy. \n    - Provide examples they are familiar with.\n    - Provide analogies they can relate their knowledge with and transfer it to new concepts.\n    - Integrate background information if needed.    \n    - For the CL jury in Computational Linguistics (CL) jury bridge linguistics and computer science, NLP, corpus linguistics, AI, and LLMs., only provide one paraphrasis for NLP/Computational Linguistics Practitioners.\n\n    Original text: \n    We'll represent the pronunciation of a word as a string of phones, which are speech sounds, each represented with symbols adapted from the Roman alphabet. The standard phonetic representation for transcribing the world's languages is the International Phonetic Alphabet (IPA), [...] the mapping between the letters of English orthography and phones is relatively opaque; a single letter can represent very different sounds in different contexts. [...] Many other languages, for example, Spanish, are much more transparent in their sound-orthography mapping than English. Articulatory phonetics is the study of how these phones are produced as the various articulatory phonetics organs in the mouth, throat, and nose modify the airflow from the lungs. [...] we can group [consonants] into classes by their point of maximum restriction, their place of articulation. [...] bilabial: [...] [p], [b], [m]. labiodental [v] and [f] [...]. dental: [...] [th] of thing and the [dh] of though. [...] alveolar: [s], [z], [t], [d]. [...] velar: [k], [g]. [...] Consonants are also distinguished by how the restriction in airflow is made [...]. This feature is called the manner of articulation [...]. voiced stops [b], [d], and [g],unvoiced stops [p], [t], and [k]. nasal sounds [n], [m], and [ng]. labiodental fricatives [f] and [v]. alveolar fricatives [s] and [z]. affricates [ch] and [jh]. approximants [y] and [w].\n\n    ### END OF INPUT ###\n\n    Tailored text (provide one text per target category only):\n    1. For NLP/Computational Linguistics Practitioners:\n    In Computational Linguistics, words are often represented as a sequence of phonemes, the basic units of speech sounds, each denoted by symbols derived from the Roman alphabet. The International Phonetic Alphabet (IPA) is the standard phonetic notation system for transcribing languages globally. However, the correspondence between English spelling and phonemes can be quite opaque, as a single letter can represent multiple sounds in various contexts. In contrast, many other languages, like Spanish, have a more straightforward sound-orthography mapping than English. Articulatory Phonetics is the study of how these phonemes are produced by modifying airflow through the articulatory organs in the mouth, throat, and nose. Phonemes can be categorized by their point of highest restriction, known as their place of articulation, and by the manner of articulation, which refers to the obstruction in airflow. Examples of places of articulation include bilabial, labiodental, dental, alveolar, and velar, with bilabial including [p], [b], [m], labiodental [v] and [f], dental [th] of thing and the [dh] of though, alveolar [s], [z], [t], [d], velar [k], [g]. The manner of articulation includes voiced stops [b], [d], and [g], unvoiced stops [p], [t], and [k], nasal sounds [n], [m], and [ng], labiodental fricatives [f] and [v], alveolar fricatives [s] and [z], affricates [ch] and [jh], approximants [y] and [w].",
                    "prompt5": "Paraphrase the given concepts in order for the audience of the CL to understand concepts outside of their field of knowledge. \n    You must consider the information on the Computational Linguistics (CL) audience bridge linguistics and computer science, NLP, corpus linguistics, AI, and LLMs. to adapt the tailored texts to the audience backgrounds. \n    - Read the text carefully.\n    - Identify the key concepts.\n    - Use terminology specific to their field.\n    - Avoid unnecessary sentence complexity while maintaining accuracy. \n    - Provide examples they are familiar with.\n    - Provide analogies they can relate their knowledge with and transfer it to new concepts.\n    - Integrate background information if needed.\n    - Do not consider other background per category. \n    - Output only one text per category.\n\n    Original text: \n    We'll represent the pronunciation of a word as a string of phones, which are speech sounds, each represented with symbols adapted from the Roman alphabet. The standard phonetic representation for transcribing the world's languages is the International Phonetic Alphabet (IPA), [...] the mapping between the letters of English orthography and phones is relatively opaque; a single letter can represent very different sounds in different contexts. [...] Many other languages, for example, Spanish, are much more transparent in their sound-orthography mapping than English. Articulatory phonetics is the study of how these phones are produced as the various articulatory phonetics organs in the mouth, throat, and nose modify the airflow from the lungs. [...] we can group [consonants] into classes by their point of maximum restriction, their place of articulation. [...] bilabial: [...] [p], [b], [m]. labiodental [v] and [f] [...]. dental: [...] [th] of thing and the [dh] of though. [...] alveolar: [s], [z], [t], [d]. [...] velar: [k], [g]. [...] Consonants are also distinguished by how the restriction in airflow is made [...]. This feature is called the manner of articulation [...]. voiced stops [b], [d], and [g],unvoiced stops [p], [t], and [k]. nasal sounds [n], [m], and [ng]. labiodental fricatives [f] and [v]. alveolar fricatives [s] and [z]. affricates [ch] and [jh]. approximants [y] and [w].\n\n    ### END OF INPUT ###\n\n    Tailored text (provide one text per target category only):\n    1. Background information:\n        Computational Linguistics (CL) is an interdisciplinary field that combines linguistics and computer science to study natural language processing (NLP) and artificial intelligence (AI). In CL, we often work with large corpora of text data, which are collections of written or spoken language.\n\n    2. Key concepts:\n        - Phonetic representation: a way to transcribe words using symbols that represent speech sounds (phones).\n        - International Phonetic Alphabet (IPA): the standard phonetic representation for languages worldwide.\n        - Sound-orthography mapping: the relationship between the written representation of a language and its spoken form.\n        - Articulatory phonetics: the study of how speech sounds (phones) are produced by the various organs in the mouth, throat, and nose.\n        - Phoneme classes: grouping consonants based on where in the mouth the restriction in airflow occurs (point of maximum restriction).\n        - Manner of articulation: a feature of consonants that describes how the restriction in airflow is made.\n\n    3. Tailored text:\n        In our work with computational linguistics, we often need to represent words using phonetic symbols, like the International Phonetic Alphabet (IPA). This helps us understand and process the sounds we hear in spoken language. However, the relationship between English letters and their corresponding sounds can be complex, as a single letter can represent several different sounds depending on the context. For example, compare the sound of \"b\" in \"bat\" and \"b\" in \"about\". This opaque mapping is much clearer in languages like Spanish, where the sound-orthography relationship is more straightforward.\n\n        To better understand speech sounds, we also study articulatory phonetics, which is the study of how these sounds are produced by our various articulatory organs, such as the lips, tongue, and throat. We can group consonants into classes based on where in the mouth the restriction in airflow occurs, such as bilabial ([p], [b], [m]), labiodental ([v] and [f]), dental ([th] of \"thing\" and the [dh] of \"though\"), alveolar ([s], [z], [t], [d]), velar ([k], [g]), and more.\n\n        Consonants are also distinguished by the manner of articulation, which describes how the restriction in airflow is made. For example, voiced stops ([b], [d], and [g]) create a blockage in the vocal cords, while unvoiced stops ([p], [t], and [k]) do not. Nasal sounds ([n], [m], and [ng]) involve airflow through the nose, while labiodental fricatives ([f] and [v]) and alveolar fricatives ([s] and [z]) are created by turbulence as air flows through a narrow opening. Affricates ([ch] and [jh]) are a combination of a stop and fricative, and approximants ([y] and [w]) are sounds where the articulators come close to each other but don't completely block the airflow."
                }
            }
        }
    },
    "6": {
        "chapter": "7",
        "sections": "7.3",
        "topic": "Feed-Forward Neural Networks (FFNNs)",
        "original_category": "CS",
        "original_text": "A feedforward network is a multilayer network in which the units are connected with no cycles; the outputs from units in each layer are passed to units in the next higher layer, and no outputs are passed back to lower layers. [...] Simple feedforward networks have three kinds of nodes: input units, hidden units, and output units. [...] The input layer x is a vector of simple scalar values. [...] The core of the neural network is the hidden layer h formed of hidden units hi, each of which is a neural unit [...]. In the standard architecture, each layer is fully-connected, meaning that each unit in each layer takes as input the outputs from all the units in the previous layer, and there is a link between every pair of units from two adjacent layers. [...] a single hidden unit has as parameters a weight vector and a bias. We represent the parameters for the entire hidden layer by combining the weight vector and bias for each unit i into a single weight matrix W and a single bias vector b for the whole layer. [...] the hidden layer computation for a feedforward network can be done very efficiently with simple matrix operations. In fact, the computation only has three steps: multiplying the weight matrix by the input vector x, adding the bias vector b, and applying the activation function g (such as the sigmoid, tanh, or ReLU activation […]). The output of the hidden layer, the vector h, is thus the following (for this example we'll use the sigmoid function σ as our activation function): h = σ(Wx+b). [...] The role of the output layer is to take this new representation h and compute a final output. This output could be a real valued number, but in many cases the goal of the network is to make some sort of classification decision, [...].",
        "tailored_texts": {
            "mistral": {
                "L": {
                    "prompt1": "You are an expert professor in L. Rewrite the following explanation so that it is best understood by L students.\n    The most important goal is that your students understand concepts that are outside their existing background knowledge, so they can pass your exam and improve their academic performance.\n    You must consider the information on Linguistics (L) students have a strong background in language structure, phonetics, syntax, and semantics. They have little or no technical knowledge. to adapt the explanation to the students' backgrounds.\n\n    - Read the text carefully.\n    - Identify key concepts.\n    - Use terminology specific to their field.\n    - Avoid unnecessary sentence complexity while maintaining accuracy.\n    - Provide relatable examples.\n    - Use analogies that help transfer their knowledge to new concepts.\n    - Integrate background information if needed.    \n\n    Original text: \n    A feedforward network is a multilayer network in which the units are connected with no cycles; the outputs from units in each layer are passed to units in the next higher layer, and no outputs are passed back to lower layers. [...] Simple feedforward networks have three kinds of nodes: input units, hidden units, and output units. [...] The input layer x is a vector of simple scalar values. [...] The core of the neural network is the hidden layer h formed of hidden units hi, each of which is a neural unit [...]. In the standard architecture, each layer is fully-connected, meaning that each unit in each layer takes as input the outputs from all the units in the previous layer, and there is a link between every pair of units from two adjacent layers. [...] a single hidden unit has as parameters a weight vector and a bias. We represent the parameters for the entire hidden layer by combining the weight vector and bias for each unit i into a single weight matrix W and a single bias vector b for the whole layer. [...] the hidden layer computation for a feedforward network can be done very efficiently with simple matrix operations. In fact, the computation only has three steps: multiplying the weight matrix by the input vector x, adding the bias vector b, and applying the activation function g (such as the sigmoid, tanh, or ReLU activation […]). The output of the hidden layer, the vector h, is thus the following (for this example we'll use the sigmoid function σ as our activation function): h = σ(Wx+b). [...] The role of the output layer is to take this new representation h and compute a final output. This output could be a real valued number, but in many cases the goal of the network is to make some sort of classification decision, [...].\n\n    ### END OF INPUT ###\n\n    Tailored text (provide one text per target category only):\n    1. Analogy:\n        Think of a feedforward network as a factory assembly line. Each unit in the factory represents a different station in the assembly line. The input layer is where raw materials (simple scalar values) enter the factory, the hidden layer is the heart of the factory where parts are combined and processed, and the output layer is where the final product (output) is produced.\n\n        Just like in the factory, the hidden layer uses weights and biases (tools) to process the raw materials into a more useful form. The weight matrix and bias vector are like the tools used at each station to combine and process the materials. The activation function (g) is like the machine that applies the rules for combining the materials in a specific way.\n\n        The computation process in the hidden layer can be done efficiently by simply multiplying the weight matrix by the input vector, adding the bias vector, and applying the activation function. This is similar to combining raw materials with specific tools, applying a certain process (the activation function), and getting a new product (the hidden layer output).\n\n        The final product (output) is produced in the output layer, which takes the new representation (h) and applies its own rules (depending on the problem) to make a classification or produce a real-valued number.\n\n    2. Example:\n        Imagine we want to classify images of animals based on their species. Each image (input) is represented as a vector of pixel values (simple scalar values), which enter the feedforward network at the input layer.\n\n        The hidden layer, made up of neural units, processes the pixel values using weights and biases to form a new representation of the image. This new representation (h) is then passed to the output layer, where it is used to classify the image as a specific animal species.\n\n        For example, if the image is of a dog, the output layer might produce an output of 1 (since we are classifying the image as a dog), while if the image is of a cat, the output layer might produce an output of 0 (since we are not classifying the image as a cat).\n\n    3. Simplified explanation:\n        A feedforward network is a series of layers that processes input data to produce an output. The input layer takes simple scalar values, the hidden layer processes these values using weights and biases, and the output layer produces a final output based on the processed data. This process is done efficiently with matrix operations, using an activation function to apply specific rules to the data at each step.\n\n        The hidden layer computation can be broken down into three simple steps: multiplying the weight matrix by the input vector, adding the bias vector, and applying the activation function. This process transforms the input data into a new representation, which is then used by the output layer to make a decision or produce a real-valued number.",
                    "prompt2": "You are an AI sales expert, responsible for selling AI products to clients with a background in L.\n    To successfully sell your products, your clients must first understand the following concepts.\n    Rewrite the explanation below in a way that is most comprehensible to L clients.\n    \n    Your goal is to ensure that your clients grasp concepts outside their existing background knowledge so they can make an informed purchase.\n    By doing so, you will increase your sales percentage and achieve greater success.\n\n    Consider the information on Linguistics (L) students have a strong background in language structure, phonetics, syntax, and semantics. They have little or no technical knowledge. to tailor the explanation to your clients' specific backgrounds.\n\n    - Read the text carefully.\n    - Identify key concepts.\n    - Use terminology specific to their field.\n    - Avoid unnecessary sentence complexity while maintaining accuracy.\n    - Provide relatable examples.\n    - Use analogies that help transfer their knowledge to new concepts.\n    - Integrate background information if needed.    \n\n    Original text: \n    A feedforward network is a multilayer network in which the units are connected with no cycles; the outputs from units in each layer are passed to units in the next higher layer, and no outputs are passed back to lower layers. [...] Simple feedforward networks have three kinds of nodes: input units, hidden units, and output units. [...] The input layer x is a vector of simple scalar values. [...] The core of the neural network is the hidden layer h formed of hidden units hi, each of which is a neural unit [...]. In the standard architecture, each layer is fully-connected, meaning that each unit in each layer takes as input the outputs from all the units in the previous layer, and there is a link between every pair of units from two adjacent layers. [...] a single hidden unit has as parameters a weight vector and a bias. We represent the parameters for the entire hidden layer by combining the weight vector and bias for each unit i into a single weight matrix W and a single bias vector b for the whole layer. [...] the hidden layer computation for a feedforward network can be done very efficiently with simple matrix operations. In fact, the computation only has three steps: multiplying the weight matrix by the input vector x, adding the bias vector b, and applying the activation function g (such as the sigmoid, tanh, or ReLU activation […]). The output of the hidden layer, the vector h, is thus the following (for this example we'll use the sigmoid function σ as our activation function): h = σ(Wx+b). [...] The role of the output layer is to take this new representation h and compute a final output. This output could be a real valued number, but in many cases the goal of the network is to make some sort of classification decision, [...].\n\n    ### END OF INPUT ###\n\n    Tailored text (provide one text per target category only):\n    1. Medical professionals:\n        A feedforward network, in simple terms, is a machine learning model that mimics the structure of the human brain. It takes in inputs, processes them through hidden layers, and generates outputs based on learned patterns.\n\n        In this model, the input layer (x) represents the initial data we feed the network, such as patient symptoms or test results. The hidden layer (h) is where the magic happens - it's a collection of processing units that transform the raw data into more meaningful information. The output layer, on the other hand, provides the final diagnosis or prediction.\n\n        Each processing unit in the hidden layer is equipped with a set of parameters, represented by a weight vector and a bias. These parameters help the network learn the intricate relationships between the input data and the desired output.\n\n        The hidden layer computation involves three main steps: multiplying the weight matrix by the input vector, adding the bias vector, and applying an activation function (such as the sigmoid function). This process enables the network to learn complex patterns and make accurate predictions.\n\n        For example, if we're training a network to predict the likelihood of a patient having diabetes based on their symptoms, the network would take in the symptoms as input (x), process them through the hidden layer (h), and output a probability of having diabetes.\n\n    2. Law enforcement officers:\n        A feedforward network is a type of machine learning model that operates similarly to a decision tree, helping us make predictions or classifications based on given data.\n\n        In this model, the input layer (x) represents the initial data we feed the network, such as suspect descriptions or evidence details. The hidden layer (h) is where the model transforms the raw data into more meaningful information, helping us make more accurate predictions. The output layer provides the final decision or classification.\n\n        Each processing unit in the hidden layer is equipped with a set of parameters, represented by a weight vector and a bias. These parameters help the network learn the intricate relationships between the input data and the desired output.\n\n        The hidden layer computation involves three main steps: multiplying the weight matrix by the input vector, adding the bias vector, and applying an activation function (such as the sigmoid function). This process enables the network to learn complex patterns and make accurate predictions.\n\n        For example, if we're using a feedforward network to predict the likelihood of a suspect being involved in a crime based on their description and evidence, the network would take in this information as input (x), process it through the hidden layer (h), and output a probability of their involvement.",
                    "prompt3": "You are a freelance copywriter responsible for adapting field-specific academic texts for an audience with a background in L. \n    Your client at the publishing house will decide whether to publish your adapted texts in the new edition of the book, making you famous in the copywriting field.\n\n    Rewrite the explanation below in a way that is most comprehensible to L audience.\n    The key objective is to ensure that your audience grasps concepts outside their existing background knowledge so your client will publish your work.\n    If you succeed, many more clients will work with you, and you will become extremely successful.\n\n    Consider the information on Linguistics (L) students have a strong background in language structure, phonetics, syntax, and semantics. They have little or no technical knowledge. to tailor the explanation to your audience’s specific background.\n\n    - Read the text carefully.\n    - Identify key concepts.\n    - Use terminology that is familiar to their field.\n    - Avoid unnecessary sentence complexity while maintaining accuracy.\n    - Provide relatable examples.\n    - Use analogies that help transfer their knowledge to new concepts.\n    - Integrate background information if needed.    \n\n    Original text: \n    A feedforward network is a multilayer network in which the units are connected with no cycles; the outputs from units in each layer are passed to units in the next higher layer, and no outputs are passed back to lower layers. [...] Simple feedforward networks have three kinds of nodes: input units, hidden units, and output units. [...] The input layer x is a vector of simple scalar values. [...] The core of the neural network is the hidden layer h formed of hidden units hi, each of which is a neural unit [...]. In the standard architecture, each layer is fully-connected, meaning that each unit in each layer takes as input the outputs from all the units in the previous layer, and there is a link between every pair of units from two adjacent layers. [...] a single hidden unit has as parameters a weight vector and a bias. We represent the parameters for the entire hidden layer by combining the weight vector and bias for each unit i into a single weight matrix W and a single bias vector b for the whole layer. [...] the hidden layer computation for a feedforward network can be done very efficiently with simple matrix operations. In fact, the computation only has three steps: multiplying the weight matrix by the input vector x, adding the bias vector b, and applying the activation function g (such as the sigmoid, tanh, or ReLU activation […]). The output of the hidden layer, the vector h, is thus the following (for this example we'll use the sigmoid function σ as our activation function): h = σ(Wx+b). [...] The role of the output layer is to take this new representation h and compute a final output. This output could be a real valued number, but in many cases the goal of the network is to make some sort of classification decision, [...].\n\n    ### END OF INPUT ###\n\n    Tailored text (provide one text per target category only):\n    1. For Linguistics students:\n\n    A feedforward network in the context of neural networks is a multilayer structure where information flows forward from one layer to the next, without any feedback loops. It consists of three main components: input units, hidden units, and output units.\n\n    - Input units receive simple scalar values (numbers) from the source, acting as the starting point for the network.\n    - Hidden units, the core of the network, are neural units that process and transform the input data.\n    - Output units produce the final result, based on the hidden units' computation.\n\n    Each layer in a feedforward network is fully connected, meaning that each unit in a layer receives inputs from all units in the previous layer. A single hidden unit has adjustable parameters, represented by a weight vector and a bias, which allow the network to learn and adapt to the data.\n\n    The hidden layer computation can be executed efficiently with matrix operations, involving three main steps:\n\n    1. Multiplying the weight matrix by the input vector\n    2. Adding the bias vector\n    3. Applying an activation function (such as the sigmoid or ReLU) to the result\n\n    The output of the hidden layer becomes the vector h, calculated using the sigmoid function as follows: h = σ(Wx+b)\n\n    Ultimately, the output layer takes the vector h and produces a final output, which could be a real-valued number or a classification decision, depending on the network's goal.\n\n    2. For general readers:\n\n    In a feedforward network, data moves unidirectionally from the input to the output layer, without loops. It's made up of three main components: input nodes, processing nodes (hidden units), and output nodes.\n\n    - Input nodes receive simple values and serve as the starting point for the network.\n    - Hidden units, the core of the network, process and transform the input data to help the network learn and adapt.\n    - Output nodes produce the final result based on the hidden units' computations.\n\n    Each layer in a feedforward network is connected to every other layer, so each unit in a layer receives inputs from all units in the previous layer. A single hidden unit has adjustable parameters, such as a weight and bias, that let the network learn and adapt to the data.\n\n    The hidden layer computation can be carried out efficiently using matrix operations, with three main steps:\n\n    1. Multiplying the weight matrix by the input vector\n    2. Adding the bias vector\n    3. Applying an activation function (like the sigmoid or ReLU) to the result\n\n    The output of the hidden layer becomes the vector h, calculated using the sigmoid function as follows: h = σ(Wx+b)\n\n    In the end, the output layer takes this vector h and produces a final output, which could be a real-valued number or a classification decision, depending on the network's goal.",
                    "prompt4": "We are playing an escape room game against each other. Your goal is to gain more points than me to be the first one escaping hence winning the game. \n    In order to escape the room, you must paraphrase field-specific concepts to the jury of the game in the L. \n    They will assign you points from one to ten, according to how good their understanding of the new explanation is. \n    Therefore, you want your adaptation to be the best possible, and definitely better than mine, so that you can gain ten points for each paraphrasis and win more quickly. \n    Consider the information on Linguistics (L) jury have a strong background on language structure, phonetics, syntax, and semantics. They have little or no technical knowledge. to tailor the explanation to each jury specific background. \n    The winning steps for good paraphrasis are:\n    - Read the text carefully.\n    - Identify the key concepts.\n    - Use terminology that is familiar to the jury's field.\n    - Avoid unnecessary sentence complexity while maintaining accuracy. \n    - Provide examples they are familiar with.\n    - Provide analogies they can relate their knowledge with and transfer it to new concepts.\n    - Integrate background information if needed.    \n    - For the CL jury in Linguistics (L) jury have a strong background on language structure, phonetics, syntax, and semantics. They have little or no technical knowledge., only provide one paraphrasis for NLP/Computational Linguistics Practitioners.\n\n    Original text: \n    A feedforward network is a multilayer network in which the units are connected with no cycles; the outputs from units in each layer are passed to units in the next higher layer, and no outputs are passed back to lower layers. [...] Simple feedforward networks have three kinds of nodes: input units, hidden units, and output units. [...] The input layer x is a vector of simple scalar values. [...] The core of the neural network is the hidden layer h formed of hidden units hi, each of which is a neural unit [...]. In the standard architecture, each layer is fully-connected, meaning that each unit in each layer takes as input the outputs from all the units in the previous layer, and there is a link between every pair of units from two adjacent layers. [...] a single hidden unit has as parameters a weight vector and a bias. We represent the parameters for the entire hidden layer by combining the weight vector and bias for each unit i into a single weight matrix W and a single bias vector b for the whole layer. [...] the hidden layer computation for a feedforward network can be done very efficiently with simple matrix operations. In fact, the computation only has three steps: multiplying the weight matrix by the input vector x, adding the bias vector b, and applying the activation function g (such as the sigmoid, tanh, or ReLU activation […]). The output of the hidden layer, the vector h, is thus the following (for this example we'll use the sigmoid function σ as our activation function): h = σ(Wx+b). [...] The role of the output layer is to take this new representation h and compute a final output. This output could be a real valued number, but in many cases the goal of the network is to make some sort of classification decision, [...].\n\n    ### END OF INPUT ###\n\n    Tailored text (provide one text per target category only):\n    1. For Linguists:\n    Consider a neural network as a set of interconnected nodes, where data flows in one direction, from inputs to outputs, mimicking the flow of information in language. These nodes are categorized into three types: input nodes, hidden nodes, and output nodes.\n\n    The input layer consists of simple, scalar values, much like words in a sentence. The hidden layer, comprising the hidden nodes, acts as the heart of the network, each node being a neural unit that processes information.\n\n    In a standard network design, each layer is fully-interconnected, meaning that each node in a layer receives input from all nodes in the previous layer. This connection is present between every pair of nodes from adjacent layers.\n\n    Each hidden node is equipped with a set of parameters, namely a weight vector and a bias. All these parameters for the hidden layer can be combined into a single weight matrix and a single bias vector for the entire layer.\n\n    The computation within the hidden layer can be carried out efficiently through simple matrix operations. Specifically, the process involves multiplying the weight matrix by the input vector, adding the bias vector, and applying an activation function, such as the sigmoid, tanh, or ReLU function.\n\n    The output of the hidden layer, represented by the vector h, can be derived as follows using the sigmoid function: h = σ(Wx+b).\n\n    Ultimately, the role of the output layer is to take this new representation h and generate a final output. This output could be a real-valued number, but often the objective of the network is to make a classification decision, such as identifying a specific language or part of speech.",
                    "prompt5": "Paraphrase the given concepts in order for the audience of the L to understand concepts outside of their field of knowledge. \n    You must consider the information on the Linguistics (L) audience have a strong background on language structure, phonetics, syntax, and semantics. They have little or no technical knowledge. to adapt the tailored texts to the audience backgrounds. \n    - Read the text carefully.\n    - Identify the key concepts.\n    - Use terminology specific to their field.\n    - Avoid unnecessary sentence complexity while maintaining accuracy. \n    - Provide examples they are familiar with.\n    - Provide analogies they can relate their knowledge with and transfer it to new concepts.\n    - Integrate background information if needed.\n    - Do not consider other background per category. \n    - Output only one text per category.\n\n    Original text: \n    A feedforward network is a multilayer network in which the units are connected with no cycles; the outputs from units in each layer are passed to units in the next higher layer, and no outputs are passed back to lower layers. [...] Simple feedforward networks have three kinds of nodes: input units, hidden units, and output units. [...] The input layer x is a vector of simple scalar values. [...] The core of the neural network is the hidden layer h formed of hidden units hi, each of which is a neural unit [...]. In the standard architecture, each layer is fully-connected, meaning that each unit in each layer takes as input the outputs from all the units in the previous layer, and there is a link between every pair of units from two adjacent layers. [...] a single hidden unit has as parameters a weight vector and a bias. We represent the parameters for the entire hidden layer by combining the weight vector and bias for each unit i into a single weight matrix W and a single bias vector b for the whole layer. [...] the hidden layer computation for a feedforward network can be done very efficiently with simple matrix operations. In fact, the computation only has three steps: multiplying the weight matrix by the input vector x, adding the bias vector b, and applying the activation function g (such as the sigmoid, tanh, or ReLU activation […]). The output of the hidden layer, the vector h, is thus the following (for this example we'll use the sigmoid function σ as our activation function): h = σ(Wx+b). [...] The role of the output layer is to take this new representation h and compute a final output. This output could be a real valued number, but in many cases the goal of the network is to make some sort of classification decision, [...].\n\n    ### END OF INPUT ###\n\n    Tailored text (provide one text per target category only):\n    1. General Understanding\n    A feedforward network is a type of artificial neural network made up of several layers where information flows only in one direction, from input to output, without loops. It's like a chain of units, each passing the processed information to the next one, but no unit passes information back to the previous one.\n\n    In the most basic form, these networks have three types of units: input units that receive the raw data, hidden units that process the data, and output units that provide the final results.\n\n    For example, imagine a network trying to recognize images. The input layer receives the raw pixel data of an image as a vector of simple values. The hidden layer processes this data to create a more complex representation of the image. The output layer then uses this representation to determine if the image shows, for example, a cat or a dog.\n\n    Each hidden unit has a set of weights and a bias, which are like adjustable factors that influence how the unit processes the information. The entire hidden layer's weights and biases are represented as a weight matrix and a bias vector.\n\n    The process of calculating what each hidden unit produces is done efficiently with simple mathematical operations. This process involves multiplying the weight matrix by the input vector, adding the bias vector, and applying an activation function like the sigmoid, tanh, or ReLU function.\n\n    The result of the hidden layer, a vector of values, is used by the output layer to produce the final output, which could be a real number or a decision based on the data processed.\n\n    2. Technical Understanding (for those interested in details)\n    A feedforward network is a multilayer network where the units are connected without cycles. In each layer, the outputs from units are passed to units in the next higher layer, and no outputs are passed back to lower layers. This network consists of three main types of nodes: input units, hidden units, and output units.\n\n    The input layer x is a vector of simple scalar values. The core of the neural network is the hidden layer h, which is made up of hidden units hi.\n\n    In the standard architecture, each layer is fully connected, meaning that each unit in each layer takes as input the outputs from all the units in the previous layer. A link connects every pair of units from two adjacent layers.\n\n    Each hidden unit has a set of parameters, which include a weight vector and a bias. All the parameters for the hidden layer are combined into a single weight matrix W and a single bias vector b for the entire layer.\n\n    The computation in the hidden layer for a feedforward network can be done efficiently with simple matrix operations. The computation only has three steps: multiplying the weight matrix by the input vector x, adding the bias vector b, and applying the activation function g (such as the sigmoid, tanh, or ReLU activation). The output of the hidden layer, the vector h, is thus h = σ(Wx+b).\n\n    The role of the output layer is to take this new representation h and compute a final output. This output could be a real valued number, but in many cases the goal of the network is to make some sort of classification decision, such as determining if an image is a cat or a dog."
                },
                "CL": {
                    "prompt1": "You are an expert professor in CL. Rewrite the following explanation so that it is best understood by CL students.\n    The most important goal is that your students understand concepts that are outside their existing background knowledge, so they can pass your exam and improve their academic performance.\n    You must consider the information on Computational Linguistics (CL) students bridge linguistics and computer science, NLP, corpus linguistics, AI, and LLMs. to adapt the explanation to the students' backgrounds.\n\n    - Read the text carefully.\n    - Identify key concepts.\n    - Use terminology specific to their field.\n    - Avoid unnecessary sentence complexity while maintaining accuracy.\n    - Provide relatable examples.\n    - Use analogies that help transfer their knowledge to new concepts.\n    - Integrate background information if needed.    \n\n    Original text: \n    A feedforward network is a multilayer network in which the units are connected with no cycles; the outputs from units in each layer are passed to units in the next higher layer, and no outputs are passed back to lower layers. [...] Simple feedforward networks have three kinds of nodes: input units, hidden units, and output units. [...] The input layer x is a vector of simple scalar values. [...] The core of the neural network is the hidden layer h formed of hidden units hi, each of which is a neural unit [...]. In the standard architecture, each layer is fully-connected, meaning that each unit in each layer takes as input the outputs from all the units in the previous layer, and there is a link between every pair of units from two adjacent layers. [...] a single hidden unit has as parameters a weight vector and a bias. We represent the parameters for the entire hidden layer by combining the weight vector and bias for each unit i into a single weight matrix W and a single bias vector b for the whole layer. [...] the hidden layer computation for a feedforward network can be done very efficiently with simple matrix operations. In fact, the computation only has three steps: multiplying the weight matrix by the input vector x, adding the bias vector b, and applying the activation function g (such as the sigmoid, tanh, or ReLU activation […]). The output of the hidden layer, the vector h, is thus the following (for this example we'll use the sigmoid function σ as our activation function): h = σ(Wx+b). [...] The role of the output layer is to take this new representation h and compute a final output. This output could be a real valued number, but in many cases the goal of the network is to make some sort of classification decision, [...].\n\n    ### END OF INPUT ###\n\n    Tailored text (provide one text per target category only):\n    1. For CL students:\n\n    A feedforward neural network is a type of artificial intelligence model that processes input data, passes it through multiple layers, and produces an output. Unlike other networks, feedforward networks do not have cycles; information only moves in one direction, from the input layer to the output layer.\n\n    The input layer, x, is a vector of simple scalar values representing the raw data you'll feed into the network. The hidden layer, h, is the brain of the network, consisting of neural units (also known as hidden units) that process and transform the input data. Each hidden unit has its own set of parameters: a weight vector and a bias.\n\n    In a standard feedforward network, each layer is fully-connected, meaning that every unit takes input from all the units in the previous layer. This connection is represented by a weight matrix and a bias vector for the entire layer. The computation in the hidden layer can be efficiently performed with simple matrix operations.\n\n    The output layer, however, has a more specific role. It takes the processed data from the hidden layer, h, and produces a final output. This output can be a real-valued number, but in many cases, the goal of the network is to make a classification decision. For instance, you might train a feedforward network to classify whether an email is spam or not based on its content.\n\n    The output of the hidden layer, h, can be calculated using the sigmoid function σ: h = σ(Wx+b). This equation multiplies the weight matrix, W, by the input vector, x, adds the bias vector, b, and applies the sigmoid function.",
                    "prompt2": "You are an AI sales expert, responsible for selling AI products to clients with a background in CL.\n    To successfully sell your products, your clients must first understand the following concepts.\n    Rewrite the explanation below in a way that is most comprehensible to CL clients.\n    \n    Your goal is to ensure that your clients grasp concepts outside their existing background knowledge so they can make an informed purchase.\n    By doing so, you will increase your sales percentage and achieve greater success.\n\n    Consider the information on Computational Linguistics (CL) students bridge linguistics and computer science, NLP, corpus linguistics, AI, and LLMs. to tailor the explanation to your clients' specific backgrounds.\n\n    - Read the text carefully.\n    - Identify key concepts.\n    - Use terminology specific to their field.\n    - Avoid unnecessary sentence complexity while maintaining accuracy.\n    - Provide relatable examples.\n    - Use analogies that help transfer their knowledge to new concepts.\n    - Integrate background information if needed.    \n\n    Original text: \n    A feedforward network is a multilayer network in which the units are connected with no cycles; the outputs from units in each layer are passed to units in the next higher layer, and no outputs are passed back to lower layers. [...] Simple feedforward networks have three kinds of nodes: input units, hidden units, and output units. [...] The input layer x is a vector of simple scalar values. [...] The core of the neural network is the hidden layer h formed of hidden units hi, each of which is a neural unit [...]. In the standard architecture, each layer is fully-connected, meaning that each unit in each layer takes as input the outputs from all the units in the previous layer, and there is a link between every pair of units from two adjacent layers. [...] a single hidden unit has as parameters a weight vector and a bias. We represent the parameters for the entire hidden layer by combining the weight vector and bias for each unit i into a single weight matrix W and a single bias vector b for the whole layer. [...] the hidden layer computation for a feedforward network can be done very efficiently with simple matrix operations. In fact, the computation only has three steps: multiplying the weight matrix by the input vector x, adding the bias vector b, and applying the activation function g (such as the sigmoid, tanh, or ReLU activation […]). The output of the hidden layer, the vector h, is thus the following (for this example we'll use the sigmoid function σ as our activation function): h = σ(Wx+b). [...] The role of the output layer is to take this new representation h and compute a final output. This output could be a real valued number, but in many cases the goal of the network is to make some sort of classification decision, [...].\n\n    ### END OF INPUT ###\n\n    Tailored text (provide one text per target category only):\n    1. Legal experts:\n    In the realm of machine learning, a feedforward network is a type of model that processes data in a sequential manner, with no loops, feeding the output from one layer to the next and never reverting to earlier layers. This model is fundamental to various AI applications, including legal language processing.\n\n    To understand this model better, let's break it down into components you're familiar with: input, processing, and output. In the context of this network, the input layer is a series of raw, numerical data points (x), which could represent legal terms or phrases.\n\n    The heart of the network lies in the hidden layer, composed of processing nodes—let's call them 'legal analysts' in this context. Each 'analyst' studies the input data, taking into account the weights assigned to each data point (similar to the importance assigned to various legal terms) and a bias (a preconceived notion or expectation).\n\n    The connection between these 'analysts' and the input data is represented by a weight matrix (W) and a bias vector (b). These parameters determine the 'analysts' overall interpretation of the input data.\n\n    The processing performed by these 'analysts' can be achieved through straightforward matrix operations: multiplying the weight matrix (W) by the input vector (x), adding the bias vector (b), and applying a specific mathematical function (g) such as the sigmoid, tangent, or rectified linear unit (ReLU) to generate the output.\n\n    The output of the hidden layer, h, can be calculated as follows: h = σ(Wx+b), where σ represents the sigmoid function.\n\n    Finally, the output layer takes this newly processed data (h) and transforms it into a final output, which may be a real-valued number or a classification decision (such as determining the category of a legal document or identifying sentiments in a legal text).\n\n    2. Linguistics experts:\n    A feedforward network is a multilayer network that processes data sequentially, without loops, passing the output from one layer to the next and never revisiting earlier layers. This network is integral to AI applications, including natural language processing (NLP), a field that bridges linguistics and computer science.\n\n    Let's imagine this network as a sequence of stages, where each stage performs some transformation on the data. In this context, the input layer represents raw, numerical data points (x), which could stand for phonetic transcriptions or syntactic structures.\n\n    The core of the network consists of hidden layers, each containing processing units—let's call them 'linguistic analysts' in this context. Each 'analyst' processes the input data, taking into account the weights assigned to each data point (similar to the importance assigned to various linguistic features) and a bias (a preconceived notion or expectation).\n\n    The connections between these 'analysts' and the input data are represented by weight matrices (W) and bias vectors (b). These parameters determine the 'analysts' overall interpretation of the input data.\n\n    The processing carried out by these 'analysts' can be achieved through simple matrix operations: multiplying the weight matrix (W) by the input vector (x), adding the bias vector (b), and applying a specific mathematical function (g) such as the sigmoid, tangent, or rectified linear unit (ReLU) to generate the output.\n\n    The output of the hidden layer, h, can be calculated as follows: h = σ(Wx+b), where σ represents the sigmoid function.\n\n    Ultimately, the output layer takes this newly processed data (h) and transforms it into a final output, which may be a real-valued number or a classification decision (such as identifying the part of speech of a word or parsing a sentence structure).",
                    "prompt3": "You are a freelance copywriter responsible for adapting field-specific academic texts for an audience with a background in CL. \n    Your client at the publishing house will decide whether to publish your adapted texts in the new edition of the book, making you famous in the copywriting field.\n\n    Rewrite the explanation below in a way that is most comprehensible to CL audience.\n    The key objective is to ensure that your audience grasps concepts outside their existing background knowledge so your client will publish your work.\n    If you succeed, many more clients will work with you, and you will become extremely successful.\n\n    Consider the information on Computational Linguistics (CL) students bridge linguistics and computer science, NLP, corpus linguistics, AI, and LLMs. to tailor the explanation to your audience’s specific background.\n\n    - Read the text carefully.\n    - Identify key concepts.\n    - Use terminology that is familiar to their field.\n    - Avoid unnecessary sentence complexity while maintaining accuracy.\n    - Provide relatable examples.\n    - Use analogies that help transfer their knowledge to new concepts.\n    - Integrate background information if needed.    \n\n    Original text: \n    A feedforward network is a multilayer network in which the units are connected with no cycles; the outputs from units in each layer are passed to units in the next higher layer, and no outputs are passed back to lower layers. [...] Simple feedforward networks have three kinds of nodes: input units, hidden units, and output units. [...] The input layer x is a vector of simple scalar values. [...] The core of the neural network is the hidden layer h formed of hidden units hi, each of which is a neural unit [...]. In the standard architecture, each layer is fully-connected, meaning that each unit in each layer takes as input the outputs from all the units in the previous layer, and there is a link between every pair of units from two adjacent layers. [...] a single hidden unit has as parameters a weight vector and a bias. We represent the parameters for the entire hidden layer by combining the weight vector and bias for each unit i into a single weight matrix W and a single bias vector b for the whole layer. [...] the hidden layer computation for a feedforward network can be done very efficiently with simple matrix operations. In fact, the computation only has three steps: multiplying the weight matrix by the input vector x, adding the bias vector b, and applying the activation function g (such as the sigmoid, tanh, or ReLU activation […]). The output of the hidden layer, the vector h, is thus the following (for this example we'll use the sigmoid function σ as our activation function): h = σ(Wx+b). [...] The role of the output layer is to take this new representation h and compute a final output. This output could be a real valued number, but in many cases the goal of the network is to make some sort of classification decision, [...].\n\n    ### END OF INPUT ###\n\n    Tailored text (provide one text per target category only):\n    1. Introduction:\n        - Computational Linguistics students, meet feedforward networks: a multilayer neural network architecture where data flows unidirectionally from the input layer to the output layer, skipping any backward connections. Think of it as a pipeline, where each layer processes the data it receives and passes it on to the next.\n\n    2. Key Concepts and Terminology:\n        - Input layer: the entry point of our neural network, made up of scalar values that represent the data we want to process.\n        - Hidden layer: the core of our network, composed of neural units (Hi) that perform complex computations on the data and pass the results to the output layer.\n        - Output layer: the final destination that generates the network's output, which can be a real-valued number or a classification decision, depending on the specific task at hand.\n        - Weights and biases: parameters that our neural network learns during training to fine-tune its performance.\n\n    3. Network Structure:\n        - In a standard feedforward network, each layer is fully-connected, meaning every unit in one layer receives input from every unit in the preceding layer. This allows our network to learn complex relationships between the input and output data.\n\n    4. Computation:\n        - The computation in a feedforward network can be simplified into three steps:\n            a. Multiply the weight matrix (W) by the input vector (x).\n            b. Add the bias vector (b) to the results.\n            c. Apply an activation function (g) - for example, the sigmoid, tanh, or ReLU - to the sum to introduce non-linearity and increase the network's expressive power.\n        - The output of the hidden layer (h) can be expressed as h = σ(Wx+b).\n\n    5. Examples and Analogies:\n        - Imagine our feedforward network as a group of translators. Each translator in the hidden layer has their own unique set of translation rules (weights and biases) and works independently. They process the raw input data, pass it along to the next translator, and the process continues until the final, translated output is produced by the output layer.",
                    "prompt4": "We are playing an escape room game against each other. Your goal is to gain more points than me to be the first one escaping hence winning the game. \n    In order to escape the room, you must paraphrase field-specific concepts to the jury of the game in the CL. \n    They will assign you points from one to ten, according to how good their understanding of the new explanation is. \n    Therefore, you want your adaptation to be the best possible, and definitely better than mine, so that you can gain ten points for each paraphrasis and win more quickly. \n    Consider the information on Computational Linguistics (CL) jury bridge linguistics and computer science, NLP, corpus linguistics, AI, and LLMs. to tailor the explanation to each jury specific background. \n    The winning steps for good paraphrasis are:\n    - Read the text carefully.\n    - Identify the key concepts.\n    - Use terminology that is familiar to the jury's field.\n    - Avoid unnecessary sentence complexity while maintaining accuracy. \n    - Provide examples they are familiar with.\n    - Provide analogies they can relate their knowledge with and transfer it to new concepts.\n    - Integrate background information if needed.    \n    - For the CL jury in Computational Linguistics (CL) jury bridge linguistics and computer science, NLP, corpus linguistics, AI, and LLMs., only provide one paraphrasis for NLP/Computational Linguistics Practitioners.\n\n    Original text: \n    A feedforward network is a multilayer network in which the units are connected with no cycles; the outputs from units in each layer are passed to units in the next higher layer, and no outputs are passed back to lower layers. [...] Simple feedforward networks have three kinds of nodes: input units, hidden units, and output units. [...] The input layer x is a vector of simple scalar values. [...] The core of the neural network is the hidden layer h formed of hidden units hi, each of which is a neural unit [...]. In the standard architecture, each layer is fully-connected, meaning that each unit in each layer takes as input the outputs from all the units in the previous layer, and there is a link between every pair of units from two adjacent layers. [...] a single hidden unit has as parameters a weight vector and a bias. We represent the parameters for the entire hidden layer by combining the weight vector and bias for each unit i into a single weight matrix W and a single bias vector b for the whole layer. [...] the hidden layer computation for a feedforward network can be done very efficiently with simple matrix operations. In fact, the computation only has three steps: multiplying the weight matrix by the input vector x, adding the bias vector b, and applying the activation function g (such as the sigmoid, tanh, or ReLU activation […]). The output of the hidden layer, the vector h, is thus the following (for this example we'll use the sigmoid function σ as our activation function): h = σ(Wx+b). [...] The role of the output layer is to take this new representation h and compute a final output. This output could be a real valued number, but in many cases the goal of the network is to make some sort of classification decision, [...].\n\n    ### END OF INPUT ###\n\n    Tailored text (provide one text per target category only):\n    1. Corpus Linguistics: In the realm of computational linguistics, a feedforward network is a multi-layered architecture where data flows unidirectionally, with each layer's outputs feeding into the next one, and no feedback loops to previous layers. This network comprises three primary components: input nodes, hidden nodes, and output nodes. The input layer is a vector of basic data points, while the hidden layer consists of neuron-like components, each processing data through a specific function. In a standard configuration, each layer is fully interconnected, meaning each node takes input from all nodes in the preceding layer, and there is a link between every pair of adjacent nodes. Each hidden node is characterized by a set of parameters, including a weight vector and a bias. These parameters for the entire hidden layer can be collectively represented as a weight matrix and a single bias vector. The computations in the hidden layer can be efficiently executed with basic matrix operations, involving multiplication of the weight matrix with the input vector, addition of the bias vector, and application of an activation function such as the sigmoid, tanh, or ReLU. The output of the hidden layer is represented as the vector h, computed as h = σ(Wx+b), where σ is the sigmoid function used as the activation function in this example. The role of the output layer is to process this new representation and generate a final output, often a real number or a classification decision, depending on the network's purpose.\n\n    2. NLP Practitioners: In natural language processing, a feedforward network is a multi-layered network designed to process and analyze data in a unidirectional fashion, with each layer's outputs influencing the next one, and no feedback loops to previous layers. This network is composed of three main components: input nodes, hidden nodes, and output nodes. The input layer represents raw text data, while the hidden layer consists of neural-network-like components, each processing data through a specific function. In a standard configuration, each layer is fully connected, meaning each node takes input from all nodes in the preceding layer, and there is a link between every pair of adjacent nodes. Each hidden node is characterized by a set of parameters, including a weight vector and a bias. These parameters for the entire hidden layer can be collectively represented as a weight matrix and a single bias vector. The computations in the hidden layer can be efficiently executed with basic matrix operations, involving multiplication of the weight matrix with the input vector, addition of the bias vector, and application of an activation function such as the sigmoid, tanh, or ReLU. The output of the hidden layer is represented as the vector h, computed as h = σ(Wx+b), where σ is the sigmoid function used as the activation function in this example. The role of the output layer is to process this new representation and generate a final output, often a probability distribution over possible classifications or a real number, depending on the network's purpose.",
                    "prompt5": "Paraphrase the given concepts in order for the audience of the CL to understand concepts outside of their field of knowledge. \n    You must consider the information on the Computational Linguistics (CL) audience bridge linguistics and computer science, NLP, corpus linguistics, AI, and LLMs. to adapt the tailored texts to the audience backgrounds. \n    - Read the text carefully.\n    - Identify the key concepts.\n    - Use terminology specific to their field.\n    - Avoid unnecessary sentence complexity while maintaining accuracy. \n    - Provide examples they are familiar with.\n    - Provide analogies they can relate their knowledge with and transfer it to new concepts.\n    - Integrate background information if needed.\n    - Do not consider other background per category. \n    - Output only one text per category.\n\n    Original text: \n    A feedforward network is a multilayer network in which the units are connected with no cycles; the outputs from units in each layer are passed to units in the next higher layer, and no outputs are passed back to lower layers. [...] Simple feedforward networks have three kinds of nodes: input units, hidden units, and output units. [...] The input layer x is a vector of simple scalar values. [...] The core of the neural network is the hidden layer h formed of hidden units hi, each of which is a neural unit [...]. In the standard architecture, each layer is fully-connected, meaning that each unit in each layer takes as input the outputs from all the units in the previous layer, and there is a link between every pair of units from two adjacent layers. [...] a single hidden unit has as parameters a weight vector and a bias. We represent the parameters for the entire hidden layer by combining the weight vector and bias for each unit i into a single weight matrix W and a single bias vector b for the whole layer. [...] the hidden layer computation for a feedforward network can be done very efficiently with simple matrix operations. In fact, the computation only has three steps: multiplying the weight matrix by the input vector x, adding the bias vector b, and applying the activation function g (such as the sigmoid, tanh, or ReLU activation […]). The output of the hidden layer, the vector h, is thus the following (for this example we'll use the sigmoid function σ as our activation function): h = σ(Wx+b). [...] The role of the output layer is to take this new representation h and compute a final output. This output could be a real valued number, but in many cases the goal of the network is to make some sort of classification decision, [...].\n\n    ### END OF INPUT ###\n\n    Tailored text (provide one text per target category only):\n    1. Computer Science Audience (general):\n        A feedforward network is a type of artificial neural network, consisting of multiple layers of interconnected nodes. These layers don't have any loops or feedback; instead, data flows from the input layer to the output layer, with each layer's outputs influencing the next higher layer.\n\n        In a simple feedforward network, you can find three types of nodes: input nodes, hidden nodes (or processing nodes), and output nodes. Input nodes receive raw data, hidden nodes process the data using mathematical functions (like addition, multiplication, and activation functions), and output nodes provide the final result.\n\n        The input layer is a vector of simple, scalar values, representing the raw data fed into the network. The hidden layer, formed by hidden nodes, processes this input data using a combination of weight vectors and biases for each node. In the standard architecture, each layer is fully connected, meaning every node in one layer receives input from all the nodes in the previous layer, and there is a link between every pair of nodes from adjacent layers.\n\n        The hidden layer computation can be performed efficiently using matrix operations. This computation consists of three steps: multiplying the weight matrix by the input vector, adding the bias vector, and applying the activation function (e.g., sigmoid, tanh, or ReLU). The output of the hidden layer is a new representation of the input data, calculated as follows: h = σ(Wx+b), where W is the weight matrix, x is the input vector, b is the bias vector, and σ is the sigmoid function.\n\n        Finally, the output layer takes this new representation and computes the final output, which may be a real-valued number or a classification decision, depending on the goal of the network.\n\n    2. NLP and AI Audience:\n        A feedforward network is a type of artificial neural network used in natural language processing and AI. It consists of multiple layers of interconnected nodes, with no cycles or feedback, where data flows from the input layer to the output layer, and each layer's outputs influence the next higher layer.\n\n        In a simple feedforward network, you'll find three types of nodes: input nodes, hidden nodes (or processing nodes), and output nodes. Input nodes receive raw data, like words or sentences, hidden nodes process the data using mathematical functions (like addition, multiplication, and activation functions), and output nodes provide the final interpretation or classification of the input.\n\n        The input layer is a vector of simple scalar values, representing the raw input data. The hidden layer, formed by hidden nodes, processes this input data using a combination of weight vectors and biases for each node. In the standard architecture, each layer is fully connected, meaning every node in one layer receives input from all the nodes in the previous layer, and there is a link between every pair of nodes from adjacent layers.\n\n        The hidden layer computation can be performed efficiently using matrix operations. This computation consists of three steps: multiplying the weight matrix by the input vector, adding the bias vector, and applying the activation function (e.g., sigmoid, tanh, or ReLU). The output of the hidden layer is a new representation of the input data, calculated as follows: h = σ(Wx+b), where W is the weight matrix, x is the input vector, b is the bias vector, and σ is the sigmoid function.\n\n        Finally, the output layer takes this new representation and computes the final interpretation or classification, which may be a sentiment analysis, named-entity recognition, or part-of-speech tagging, depending on the goal of the network."
                }
            }
        }
    },
    "7": {
        "chapter": "7a",
        "sections": "7.5",
        "topic": " Training FFNNs (Loss Function and Gradient)",
        "original_category": "CS",
        "original_text": "A feedforward neural net is an instance of supervised machine learning in which we know the correct output y for each observation x. What the system produces is ˆy, the system's estimate of the true y. The goal of the training procedure is to learn parameters W[i] and b[i] for each layer i that make ˆy for each training observation as close as possible to the true y. [...] First, we'll need a loss function that models the distance between the system output and the gold output, and it's common to use the loss function used for logistic regression, the cross-entropy loss. Second, to find the parameters that minimize this loss function, we'll use the gradient descent optimization algorithm [...]. Third, gradient descent requires knowing the gradient of the loss function, the vector that contains the partial derivative of the loss function with respect to each of the parameters. [...] for neural networks, with millions of parameters in many layers, it's much harder to see how to compute the partial derivative of some weight in layer 1 when the loss is attached to some much later layer. How do we partial out the loss over all those intermediate layers? The answer is the algorithm called error backpropagation or backward differentiation. [...] Computing the gradient requires the partial derivative of the loss function with respect to each parameter. [...] The solution to computing this gradient is an algorithm called error backpropagation.",
        "tailored_texts": {
            "mistral": {
                "L": {
                    "prompt1": "You are an expert professor in L. Rewrite the following explanation so that it is best understood by L students.\n    The most important goal is that your students understand concepts that are outside their existing background knowledge, so they can pass your exam and improve their academic performance.\n    You must consider the information on Linguistics (L) students have a strong background in language structure, phonetics, syntax, and semantics. They have little or no technical knowledge. to adapt the explanation to the students' backgrounds.\n\n    - Read the text carefully.\n    - Identify key concepts.\n    - Use terminology specific to their field.\n    - Avoid unnecessary sentence complexity while maintaining accuracy.\n    - Provide relatable examples.\n    - Use analogies that help transfer their knowledge to new concepts.\n    - Integrate background information if needed.    \n\n    Original text: \n    A feedforward neural net is an instance of supervised machine learning in which we know the correct output y for each observation x. What the system produces is ˆy, the system's estimate of the true y. The goal of the training procedure is to learn parameters W[i] and b[i] for each layer i that make ˆy for each training observation as close as possible to the true y. [...] First, we'll need a loss function that models the distance between the system output and the gold output, and it's common to use the loss function used for logistic regression, the cross-entropy loss. Second, to find the parameters that minimize this loss function, we'll use the gradient descent optimization algorithm [...]. Third, gradient descent requires knowing the gradient of the loss function, the vector that contains the partial derivative of the loss function with respect to each of the parameters. [...] for neural networks, with millions of parameters in many layers, it's much harder to see how to compute the partial derivative of some weight in layer 1 when the loss is attached to some much later layer. How do we partial out the loss over all those intermediate layers? The answer is the algorithm called error backpropagation or backward differentiation. [...] Computing the gradient requires the partial derivative of the loss function with respect to each parameter. [...] The solution to computing this gradient is an algorithm called error backpropagation.\n\n    ### END OF INPUT ###\n\n    Tailored text (provide one text per target category only):\n    1. Simple Explanation:\n    A feedforward neural network is a learning technique where we provide it with the correct answer for each question (input). The network will give an answer of its own (estimate). Our goal is to make these answers as close as possible to the correct ones by adjusting some numbers (parameters) in each layer. To measure how close our answers are, we use a loss function. This function tells us the difference between our answer and the correct one. We use a common loss function called cross-entropy loss for this purpose. To find the best numbers, we use a method called gradient descent. This method moves the numbers in the opposite direction of the loss function's slope. But, when we have millions of numbers in many layers, it's hard to figure out how changes in one number affect the final answer. To solve this, we use a method called error backpropagation. This method helps us understand how changes in each number impact the final answer, so we can adjust them correctly.\n\n    2. Analogy:\n    Imagine you're cooking a cake and you want it to taste just like a professional baker's cake. You're given the ingredients and the correct recipe by the baker. You start cooking, but your cake doesn't taste as good as the baker's. To make your cake better, you need to adjust some ingredients. But, you don't know which ingredient to adjust and by how much. So, you take a bite of your cake and a bite of the baker's cake, and you figure out how different they taste. Then, you make small adjustments to the ingredients and taste again. You repeat this process until your cake tastes like the baker's. In this analogy, you are the neural network, the cake is the output (answer), the ingredients are the parameters, and the taste is the loss function.\n\n    3. Real-World Example:\n    Suppose you are an AI system that is trying to predict whether an email is spam or not based on the content of the email. You are given thousands of emails with their labels (spam or not spam). You start by making predictions, but most of them are wrong. To improve, you adjust some settings in your system. But, you don't know which setting to adjust and by how much. So, you compare your predictions with the correct labels and make small adjustments. You repeat this process until your predictions are as close as possible to the correct labels. In this example, the emails are the inputs, the labels are the correct outputs, and the settings are the parameters.\n\n    4. Background Information:\n    L students should already be familiar with the basic structure of a neural network, including inputs, hidden layers, and outputs. The new concept here is the feedforward neural network, which is a type of neural network that takes input, processes it through hidden layers, and produces an output. The aim is to learn the parameters of the network to produce outputs that closely match the correct outputs for a given input. The process of learning these parameters involves a training procedure, which uses a loss function and an optimization algorithm (like gradient descent) to adjust the parameters. The key difference between a feedforward neural network and other types of neural networks is that the feedforward network makes a prediction for each input in the training set, and the goal is to minimize the difference between the network's prediction and the actual output.",
                    "prompt2": "You are an AI sales expert, responsible for selling AI products to clients with a background in L.\n    To successfully sell your products, your clients must first understand the following concepts.\n    Rewrite the explanation below in a way that is most comprehensible to L clients.\n    \n    Your goal is to ensure that your clients grasp concepts outside their existing background knowledge so they can make an informed purchase.\n    By doing so, you will increase your sales percentage and achieve greater success.\n\n    Consider the information on Linguistics (L) students have a strong background in language structure, phonetics, syntax, and semantics. They have little or no technical knowledge. to tailor the explanation to your clients' specific backgrounds.\n\n    - Read the text carefully.\n    - Identify key concepts.\n    - Use terminology specific to their field.\n    - Avoid unnecessary sentence complexity while maintaining accuracy.\n    - Provide relatable examples.\n    - Use analogies that help transfer their knowledge to new concepts.\n    - Integrate background information if needed.    \n\n    Original text: \n    A feedforward neural net is an instance of supervised machine learning in which we know the correct output y for each observation x. What the system produces is ˆy, the system's estimate of the true y. The goal of the training procedure is to learn parameters W[i] and b[i] for each layer i that make ˆy for each training observation as close as possible to the true y. [...] First, we'll need a loss function that models the distance between the system output and the gold output, and it's common to use the loss function used for logistic regression, the cross-entropy loss. Second, to find the parameters that minimize this loss function, we'll use the gradient descent optimization algorithm [...]. Third, gradient descent requires knowing the gradient of the loss function, the vector that contains the partial derivative of the loss function with respect to each of the parameters. [...] for neural networks, with millions of parameters in many layers, it's much harder to see how to compute the partial derivative of some weight in layer 1 when the loss is attached to some much later layer. How do we partial out the loss over all those intermediate layers? The answer is the algorithm called error backpropagation or backward differentiation. [...] Computing the gradient requires the partial derivative of the loss function with respect to each parameter. [...] The solution to computing this gradient is an algorithm called error backpropagation.\n\n    ### END OF INPUT ###\n\n    Tailored text (provide one text per target category only):\n    1. Physicists:\n        A feedforward neural net is a machine learning approach where we have a known output y for each input x. The model's prediction, ˆy, is an estimate of the actual y. Our aim during training is to find the best weights W[i] and biases b[i] for each layer i to make ˆy as close as possible to the actual y for each training observation.\n\n        First, we'll need a loss function that measures the difference between the model's output and the desired output, and we often use the loss function similar to logistic regression, the cross-entropy loss. Second, to minimize this loss function, we'll employ the gradient descent optimization algorithm.\n\n        Third, gradient descent requires knowing the gradient of the loss function, which is a vector containing the derivative of the loss function with respect to each parameter. However, in the case of neural networks with numerous parameters spread across multiple layers, it becomes quite challenging to calculate the derivative of a certain weight in layer 1 when the loss is connected to much later layers. Here, the key technique is the error backpropagation algorithm, also known as backward differentiation.\n\n        Computing the gradient necessitates the derivative of the loss function with respect to each parameter. To achieve this, we employ the error backpropagation algorithm, which calculates the gradient in an efficient way, taking into account all intermediate layers.\n\n    2. Computer Scientists:\n        A feedforward neural net is a supervised machine learning method where we have the correct output y for each observation x. The system generates ˆy, an estimate of the true y. The goal during training is to learn parameters W[i] and b[i] for each layer i that make ˆy as close as possible to the true y for each training observation.\n\n        Initially, we'll require a loss function that represents the distance between the system output and the ideal output, and we usually opt for the loss function used in logistic regression, the cross-entropy loss. Afterward, to minimize this loss function, we'll utilize the gradient descent optimization algorithm.\n\n        Subsequently, gradient descent necessitates knowing the gradient of the loss function, a vector containing the derivative of the loss function with respect to each parameter. However, in the context of neural networks with numerous parameters in various layers, it becomes difficult to compute the derivative of a specific weight in layer 1 when the loss is connected to much later layers. To address this issue, we make use of the error backpropagation algorithm, also referred to as backward differentiation.\n\n        To compute the gradient, we need the derivative of the loss function with respect to each parameter. Here, the solution lies in the error backpropagation algorithm, which calculates the gradient efficiently by considering all intermediate layers.\n\n    3. Mathematicians:\n        A feedforward neural net is a supervised machine learning technique where the correct output y for each observation x is given. The model provides ˆy, an estimation of the true y. During training, our objective is to learn parameters W[i] and b[i] for each layer i that make ˆy as close as possible to the true y for each training observation.\n\n        First, we need a loss function that expresses the difference between the system output and the ideal output, and we usually opt for the loss function similar to logistic regression, the cross-entropy loss. Next, to minimize this loss function, we utilize the gradient descent optimization algorithm.\n\n        Third, gradient descent requires knowing the gradient of the loss function, which is a vector containing the derivative of the loss function with respect to each parameter. However, in the context of neural networks with numerous parameters in multiple layers, it becomes tricky to compute the derivative of a specific weight in layer 1 when the loss is connected to much later layers. To address this challenge, we employ the error backpropagation algorithm, also known as backward differentiation.\n\n        To compute the gradient, we need the derivative of the loss function with respect to each parameter. Here, the solution lies in the error backpropagation algorithm, which computes the gradient in an efficient manner by taking into account all intermediate layers.",
                    "prompt3": "You are a freelance copywriter responsible for adapting field-specific academic texts for an audience with a background in L. \n    Your client at the publishing house will decide whether to publish your adapted texts in the new edition of the book, making you famous in the copywriting field.\n\n    Rewrite the explanation below in a way that is most comprehensible to L audience.\n    The key objective is to ensure that your audience grasps concepts outside their existing background knowledge so your client will publish your work.\n    If you succeed, many more clients will work with you, and you will become extremely successful.\n\n    Consider the information on Linguistics (L) students have a strong background in language structure, phonetics, syntax, and semantics. They have little or no technical knowledge. to tailor the explanation to your audience’s specific background.\n\n    - Read the text carefully.\n    - Identify key concepts.\n    - Use terminology that is familiar to their field.\n    - Avoid unnecessary sentence complexity while maintaining accuracy.\n    - Provide relatable examples.\n    - Use analogies that help transfer their knowledge to new concepts.\n    - Integrate background information if needed.    \n\n    Original text: \n    A feedforward neural net is an instance of supervised machine learning in which we know the correct output y for each observation x. What the system produces is ˆy, the system's estimate of the true y. The goal of the training procedure is to learn parameters W[i] and b[i] for each layer i that make ˆy for each training observation as close as possible to the true y. [...] First, we'll need a loss function that models the distance between the system output and the gold output, and it's common to use the loss function used for logistic regression, the cross-entropy loss. Second, to find the parameters that minimize this loss function, we'll use the gradient descent optimization algorithm [...]. Third, gradient descent requires knowing the gradient of the loss function, the vector that contains the partial derivative of the loss function with respect to each of the parameters. [...] for neural networks, with millions of parameters in many layers, it's much harder to see how to compute the partial derivative of some weight in layer 1 when the loss is attached to some much later layer. How do we partial out the loss over all those intermediate layers? The answer is the algorithm called error backpropagation or backward differentiation. [...] Computing the gradient requires the partial derivative of the loss function with respect to each parameter. [...] The solution to computing this gradient is an algorithm called error backpropagation.\n\n    ### END OF INPUT ###\n\n    Tailored text (provide one text per target category only):\n    1. Analogical explanation:\n        Imagine you're baking a cake and you have a recipe with exact measurements for each ingredient. Your goal is to create a cake as close to the original recipe as possible. The ingredients are like the parameters in a neural network, and the cake is the output (ˆy). To ensure the cake is as close to the original recipe as possible, you'll need to adjust the measurements (parameters) using a method like trial and error (gradient descent). As you make adjustments, you'll need to evaluate the difference between your cake and the original recipe (loss function). In this case, the evaluation could be the taste, appearance, or consistency of the cake. A common method for measuring the difference is by comparing the taste (logistic regression's cross-entropy loss).\n\n        To find the best adjustments, you'll need to determine which ingredient changes have the biggest impact on the taste, appearance, or consistency. This can be difficult if some ingredients influence the final product much later in the baking process, as it might be hard to pinpoint exactly which ingredient change affected the final outcome. This is similar to computing the partial derivative of a weight in layer 1 when the loss is attached to some much later layer in a neural network. The solution to this problem is error backpropagation, which traces the impact of each ingredient change (partial derivative) back through the entire baking process (all layers in the neural network) to find the optimal adjustments (parameters).\n\n    2. Example-based explanation:\n        Suppose you're a language model that translates English to French. Your task is to translate a given English sentence x into a French sentence y. Your model has various layers, each taking an input (e.g., individual English words) and producing an output (e.g., a French word). Your goal is to make the output as close as possible to the true French sentence y.\n\n        To achieve this, you'll need to learn the appropriate connections between the English word inputs and the French word outputs at each layer. This learning process involves adjusting the connections (parameters) through a technique called gradient descent. To ensure accurate translation, you'll need a method to evaluate the quality of your translations, which is similar to measuring the distance between your system output and the gold output (loss function). A common evaluation method is the cross-entropy loss.\n\n        Computing the gradient (the impact of adjusting a connection on the overall translation) for each parameter can be challenging, especially when the loss is attached to a much later layer. To overcome this, you can use error backpropagation, which computes the gradient by tracing the impact of each connection adjustment through all the layers in the model.\n\n    3. Explanation with background information (for more advanced L audience):\n        A feedforward neural network is a type of supervised machine learning model used for prediction tasks. In this context, we have input data x, and we know the corresponding correct output y. The network learns to map the input to the output by adjusting the parameters within the network using a training procedure called gradient descent. The goal is to find the parameters that minimize the difference between the network's output (ˆy) and the true output y.\n\n        To measure the difference, a loss function is used, often the cross-entropy loss, which is common in logistic regression. To find the parameters that minimize this loss, we employ gradient descent optimization. However, gradient descent requires knowing the gradient of the loss function, which is the vector containing the partial derivative of the loss function with respect to each parameter.\n\n        In neural networks, which can have millions of parameters and multiple layers, computing the partial derivative can be challenging. This is because the loss is often attached to a much later layer, making it difficult to see how changes to a parameter in an earlier layer affect the overall loss. To solve this problem, we use the error backpropagation algorithm, which computes the gradient by tracing the impact (partial derivative) of each parameter through all the layers in the network.",
                    "prompt4": "We are playing an escape room game against each other. Your goal is to gain more points than me to be the first one escaping hence winning the game. \n    In order to escape the room, you must paraphrase field-specific concepts to the jury of the game in the L. \n    They will assign you points from one to ten, according to how good their understanding of the new explanation is. \n    Therefore, you want your adaptation to be the best possible, and definitely better than mine, so that you can gain ten points for each paraphrasis and win more quickly. \n    Consider the information on Linguistics (L) jury have a strong background on language structure, phonetics, syntax, and semantics. They have little or no technical knowledge. to tailor the explanation to each jury specific background. \n    The winning steps for good paraphrasis are:\n    - Read the text carefully.\n    - Identify the key concepts.\n    - Use terminology that is familiar to the jury's field.\n    - Avoid unnecessary sentence complexity while maintaining accuracy. \n    - Provide examples they are familiar with.\n    - Provide analogies they can relate their knowledge with and transfer it to new concepts.\n    - Integrate background information if needed.    \n    - For the CL jury in Linguistics (L) jury have a strong background on language structure, phonetics, syntax, and semantics. They have little or no technical knowledge., only provide one paraphrasis for NLP/Computational Linguistics Practitioners.\n\n    Original text: \n    A feedforward neural net is an instance of supervised machine learning in which we know the correct output y for each observation x. What the system produces is ˆy, the system's estimate of the true y. The goal of the training procedure is to learn parameters W[i] and b[i] for each layer i that make ˆy for each training observation as close as possible to the true y. [...] First, we'll need a loss function that models the distance between the system output and the gold output, and it's common to use the loss function used for logistic regression, the cross-entropy loss. Second, to find the parameters that minimize this loss function, we'll use the gradient descent optimization algorithm [...]. Third, gradient descent requires knowing the gradient of the loss function, the vector that contains the partial derivative of the loss function with respect to each of the parameters. [...] for neural networks, with millions of parameters in many layers, it's much harder to see how to compute the partial derivative of some weight in layer 1 when the loss is attached to some much later layer. How do we partial out the loss over all those intermediate layers? The answer is the algorithm called error backpropagation or backward differentiation. [...] Computing the gradient requires the partial derivative of the loss function with respect to each parameter. [...] The solution to computing this gradient is an algorithm called error backpropagation.\n\n    ### END OF INPUT ###\n\n    Tailored text (provide one text per target category only):\n    1. Non-technical audience:\n    A feedforward neural network is a type of learning model where we provide it with expected results for each input. The network then tries to guess the correct result, which we call its estimate. Our aim is to adjust the network's internal settings (weights and biases) so that its guesses match the expected results as closely as possible. To measure how well it's doing, we use a loss function, which is like a gauge showing the difference between its output and the correct output. A common choice for this function is the one used in logistic regression, called cross-entropy loss. To find the best settings for the network, we use a method called gradient descent, which works by making small adjustments to the settings to reduce the loss. However, with many layers and millions of settings, it can be challenging to figure out how changes in early layers affect the loss in later layers. To overcome this challenge, we use a technique called error backpropagation, which helps us calculate the impact of changes in each setting on the overall loss.\n\n    2. Linguistics audience:\n    A feedforward neural network is a specific instance of supervised machine learning where the correct output y for each observation x is provided. The output of the system is denoted by ˆy, an estimate of the true y. The objective is to learn parameters W[i] and b[i] for each layer i, so that the system's predictions (ˆy) for each training observation closely match the true y. To evaluate the system's performance, we employ a loss function, which quantifies the distance between the system output and the desired output. This function is often the cross-entropy loss used in logistic regression. To find the parameters that minimize this loss function, we apply the gradient descent optimization algorithm. However, with numerous parameters and layers in a neural network, it becomes complicated to determine the partial derivative of a weight in layer 1 with respect to the loss that is attached to a much later layer. To address this complication, we utilize the error backpropagation algorithm, or backward differentiation, which enables us to compute the partial derivative of the loss function with respect to each parameter, regardless of the layer they belong to. This computation is essential for the application of gradient descent.",
                    "prompt5": "Paraphrase the given concepts in order for the audience of the L to understand concepts outside of their field of knowledge. \n    You must consider the information on the Linguistics (L) audience have a strong background on language structure, phonetics, syntax, and semantics. They have little or no technical knowledge. to adapt the tailored texts to the audience backgrounds. \n    - Read the text carefully.\n    - Identify the key concepts.\n    - Use terminology specific to their field.\n    - Avoid unnecessary sentence complexity while maintaining accuracy. \n    - Provide examples they are familiar with.\n    - Provide analogies they can relate their knowledge with and transfer it to new concepts.\n    - Integrate background information if needed.\n    - Do not consider other background per category. \n    - Output only one text per category.\n\n    Original text: \n    A feedforward neural net is an instance of supervised machine learning in which we know the correct output y for each observation x. What the system produces is ˆy, the system's estimate of the true y. The goal of the training procedure is to learn parameters W[i] and b[i] for each layer i that make ˆy for each training observation as close as possible to the true y. [...] First, we'll need a loss function that models the distance between the system output and the gold output, and it's common to use the loss function used for logistic regression, the cross-entropy loss. Second, to find the parameters that minimize this loss function, we'll use the gradient descent optimization algorithm [...]. Third, gradient descent requires knowing the gradient of the loss function, the vector that contains the partial derivative of the loss function with respect to each of the parameters. [...] for neural networks, with millions of parameters in many layers, it's much harder to see how to compute the partial derivative of some weight in layer 1 when the loss is attached to some much later layer. How do we partial out the loss over all those intermediate layers? The answer is the algorithm called error backpropagation or backward differentiation. [...] Computing the gradient requires the partial derivative of the loss function with respect to each parameter. [...] The solution to computing this gradient is an algorithm called error backpropagation.\n\n    ### END OF INPUT ###\n\n    Tailored text (provide one text per target category only):\n    1. Novice in Machine Learning:\n    Feedforward Neural Networks are a way to teach a computer to make predictions based on input data, using supervised learning. This means that we provide the correct answer for each input, and the network learns to predict that answer. The goal is to make the network's predictions as close as possible to the correct answer. To achieve this, we use a method called gradient descent, which is an optimization algorithm that helps us find the best settings for the network's parameters.\n\n    To understand how well the network is doing, we use a measure called loss, which tells us how far off the network's predictions are from the correct answer. A common loss function is the cross-entropy loss. To find the best settings for the network's parameters, we use gradient descent, which requires knowing the gradient of the loss function. However, in a neural network with many layers and millions of parameters, it's difficult to see how changes in one weight in one layer affect the loss in a much later layer. To solve this problem, we use an algorithm called backpropagation, or error backpropagation, which helps us compute the gradient of the loss function with respect to each parameter.\n\n    Backpropagation works by computing the partial derivative of the loss function with respect to each parameter, which tells us how much changing that parameter affects the loss. This allows us to adjust the parameters in a way that minimizes the loss and improves the network's predictions.\n\n    In summary, feedforward neural networks are a powerful tool for making predictions based on input data, and backpropagation is an essential algorithm for training these networks. By using backpropagation, we can compute the gradient of the loss function with respect to each parameter, which allows us to adjust the network's parameters in a way that improves its predictions.\n\n    ### END OF OUTPUT ###"
                },
                "CL": {
                    "prompt1": "You are an expert professor in CL. Rewrite the following explanation so that it is best understood by CL students.\n    The most important goal is that your students understand concepts that are outside their existing background knowledge, so they can pass your exam and improve their academic performance.\n    You must consider the information on Computational Linguistics (CL) students bridge linguistics and computer science, NLP, corpus linguistics, AI, and LLMs. to adapt the explanation to the students' backgrounds.\n\n    - Read the text carefully.\n    - Identify key concepts.\n    - Use terminology specific to their field.\n    - Avoid unnecessary sentence complexity while maintaining accuracy.\n    - Provide relatable examples.\n    - Use analogies that help transfer their knowledge to new concepts.\n    - Integrate background information if needed.    \n\n    Original text: \n    A feedforward neural net is an instance of supervised machine learning in which we know the correct output y for each observation x. What the system produces is ˆy, the system's estimate of the true y. The goal of the training procedure is to learn parameters W[i] and b[i] for each layer i that make ˆy for each training observation as close as possible to the true y. [...] First, we'll need a loss function that models the distance between the system output and the gold output, and it's common to use the loss function used for logistic regression, the cross-entropy loss. Second, to find the parameters that minimize this loss function, we'll use the gradient descent optimization algorithm [...]. Third, gradient descent requires knowing the gradient of the loss function, the vector that contains the partial derivative of the loss function with respect to each of the parameters. [...] for neural networks, with millions of parameters in many layers, it's much harder to see how to compute the partial derivative of some weight in layer 1 when the loss is attached to some much later layer. How do we partial out the loss over all those intermediate layers? The answer is the algorithm called error backpropagation or backward differentiation. [...] Computing the gradient requires the partial derivative of the loss function with respect to each parameter. [...] The solution to computing this gradient is an algorithm called error backpropagation.\n\n    ### END OF INPUT ###\n\n    Tailored text (provide one text per target category only):\n    1. **Introduction to Feedforward Neural Networks**\n        In the realm of Computational Linguistics, a Feedforward Neural Network (FNN) is a type of supervised machine learning algorithm where we are provided with the correct output 'y' for each input 'x'. The network's goal is to generate an estimate 'ˆy' as close as possible to the true 'y' for each training observation. This is achieved through an optimization process that adjusts the weights 'W[i]' and biases 'b[i]' for each layer 'i' in the network.\n\n        For example, imagine you are trying to build a system that predicts the sentiment of a text (positive or negative). The input 'x' would be the text, the true 'y' would be the sentiment, and the system's estimate 'ˆy' would be the sentiment the system predicts. The goal is to make 'ˆy' as close as possible to the true 'y' for many different texts during the training process.\n\n    2. **Understanding the Loss Function and Gradient Descent**\n        To measure the difference between the system output and the correct output, we use a loss function. A common choice for this is the cross-entropy loss, which is also used in logistic regression. The goal is to find the parameters that minimize this loss function.\n\n        To do this, we employ the gradient descent optimization algorithm, which requires knowing the gradient of the loss function. The gradient is a vector that contains the partial derivative of the loss function with respect to each parameter.\n\n        In the context of NLP, where we often deal with vast amounts of parameters and multiple layers, calculating the partial derivative can be challenging. For instance, how do we calculate the contribution of a weight in layer 1 to the total loss when the loss is attached to a much later layer?\n\n        To tackle this problem, we use an algorithm called error backpropagation or backward differentiation. This algorithm allows us to propagate the error (the partial derivative of the loss function) backwards through the network, helping us to compute the gradient for each parameter.\n\n    3. **Implementing Error Backpropagation**\n        Computing the gradient requires the partial derivative of the loss function with respect to each parameter. In a FNN, this task is accomplished using the error backpropagation algorithm.\n\n        This algorithm helps us calculate the gradient for each parameter by propagating the error backwards through the network, layer by layer. This way, we can adjust the weights and biases in each layer to minimize the loss function and improve the network's performance.\n\n        For example, in our sentiment analysis system, if the system makes a mistake, error backpropagation helps us identify which weights and biases in which layers contributed to that mistake, so we can adjust them to improve the system's performance.\n\n    4. **Relating Feedforward Neural Networks to CL, AI, and LLMs**\n        Feedforward Neural Networks are a crucial tool in Computational Linguistics, as they help bridge the gap between linguistics and computer science by providing a means to model complex linguistic patterns. They are also a key component in the field of Artificial Intelligence (AI), where they are used to build intelligent systems that can learn from data and make predictions or decisions with minimal human intervention.\n\n        Furthermore, in the context of Language Learning Models (LLMs), FNNs can be used to train models that can understand and generate human-like text, aiding in tasks such as machine translation, text summarization, and chatbot development.",
                    "prompt2": "You are an AI sales expert, responsible for selling AI products to clients with a background in CL.\n    To successfully sell your products, your clients must first understand the following concepts.\n    Rewrite the explanation below in a way that is most comprehensible to CL clients.\n    \n    Your goal is to ensure that your clients grasp concepts outside their existing background knowledge so they can make an informed purchase.\n    By doing so, you will increase your sales percentage and achieve greater success.\n\n    Consider the information on Computational Linguistics (CL) students bridge linguistics and computer science, NLP, corpus linguistics, AI, and LLMs. to tailor the explanation to your clients' specific backgrounds.\n\n    - Read the text carefully.\n    - Identify key concepts.\n    - Use terminology specific to their field.\n    - Avoid unnecessary sentence complexity while maintaining accuracy.\n    - Provide relatable examples.\n    - Use analogies that help transfer their knowledge to new concepts.\n    - Integrate background information if needed.    \n\n    Original text: \n    A feedforward neural net is an instance of supervised machine learning in which we know the correct output y for each observation x. What the system produces is ˆy, the system's estimate of the true y. The goal of the training procedure is to learn parameters W[i] and b[i] for each layer i that make ˆy for each training observation as close as possible to the true y. [...] First, we'll need a loss function that models the distance between the system output and the gold output, and it's common to use the loss function used for logistic regression, the cross-entropy loss. Second, to find the parameters that minimize this loss function, we'll use the gradient descent optimization algorithm [...]. Third, gradient descent requires knowing the gradient of the loss function, the vector that contains the partial derivative of the loss function with respect to each of the parameters. [...] for neural networks, with millions of parameters in many layers, it's much harder to see how to compute the partial derivative of some weight in layer 1 when the loss is attached to some much later layer. How do we partial out the loss over all those intermediate layers? The answer is the algorithm called error backpropagation or backward differentiation. [...] Computing the gradient requires the partial derivative of the loss function with respect to each parameter. [...] The solution to computing this gradient is an algorithm called error backpropagation.\n\n    ### END OF INPUT ###\n\n    Tailored text (provide one text per target category only):\n    1. CL students with a focus on NLP:\n\nAs a computational linguistics specialist, you're interested in combining linguistics and computer science to create natural language processing systems. In this context, feedforward neural networks are essential tools for supervised machine learning. This means the system is taught to predict the correct output, denoted as ˆy, based on input data, x. The goal is to adjust the parameters W[i] and b[i] for each layer i, so ˆy for each training observation closely matches the actual, correct y.\n\nTo measure the difference between the predicted output and the actual output, we use a loss function – often the cross-entropy loss used in logistic regression. To minimize this loss and find the optimal parameters, we use gradient descent optimization. However, computing the gradient for neural networks with numerous layers and many parameters can be challenging, as the impact of a parameter in layer 1 may not be immediately clear when the loss is associated with a later layer.\n\nTo solve this issue, we use a technique called error backpropagation (also known as backward differentiation). This algorithm allows us to compute the partial derivative of the loss function with respect to each parameter, enabling us to adjust weights and biases effectively to minimize the loss and improve the network's performance.\n\n2. AI enthusiasts:\n\nAs an AI enthusiast, you're passionate about artificial intelligence and its applications. One essential technique for training AI systems is feedforward neural networks, where a system learns to predict the output, ˆy, for a given input, x. The goal is to find the optimal parameters W[i] and b[i] for each layer i, so that ˆy closely matches the actual y for each training observation.\n\nTo measure the performance of the system, we use a loss function – often the cross-entropy loss used in logistic regression. To minimize this loss and find the optimal parameters, we use gradient descent optimization. However, computing the gradient for neural networks with numerous layers and many parameters can be challenging, as the impact of a parameter in layer 1 may not be immediately clear when the loss is associated with a later layer.\n\nTo solve this issue, we use a technique called error backpropagation (also known as backward differentiation). This algorithm allows us to compute the partial derivative of the loss function with respect to each parameter, enabling us to adjust weights and biases effectively to minimize the loss and improve the network's performance.\n\n3. Corpus Linguistics practitioners:\n\nAs a corpus linguistics practitioner, you're interested in using large collections of texts to analyze language patterns and trends. Feedforward neural networks play a vital role in this field, particularly in supervised learning applications. In this context, feedforward neural networks help the system predict the correct output, denoted as ˆy, based on input data, x. The goal is to adjust the parameters W[i] and b[i] for each layer i, so ˆy for each training observation closely matches the actual, correct y.\n\nTo measure the difference between the predicted output and the actual output, we use a loss function – often the cross-entropy loss used in logistic regression. To minimize this loss and find the optimal parameters, we use gradient descent optimization. However, computing the gradient for neural networks with numerous layers and many parameters can be challenging, as the impact of a parameter in layer 1 may not be immediately clear when the loss is associated with a later layer.\n\nTo solve this issue, we use a technique called error backpropagation (also known as backward differentiation). This algorithm allows us to compute the partial derivative of the loss function with respect to each parameter, enabling us to adjust weights and biases effectively to minimize the loss and improve the network's performance.\n\n4. Legal Language Model (LLM) developers:\n\nAs an LLM developer, you're focused on creating models that understand and generate legal language. Feedforward neural networks are valuable tools for supervised machine learning in this context. In this case, the system learns to predict the correct output, denoted as ˆy, based on input data, x. The goal is to adjust the parameters W[i] and b[i] for each layer i, so ˆy for each training observation closely matches the actual, correct y.\n\nTo measure the difference between the predicted output and the actual output, we use a loss function – often the cross-entropy loss used in logistic regression. To minimize this loss and find the optimal parameters, we use gradient descent optimization. However, computing the gradient for neural networks with numerous layers and many parameters can be challenging, as the impact of a parameter in layer 1 may not be immediately clear when the loss is associated with a later layer.\n\nTo",
                    "prompt3": "You are a freelance copywriter responsible for adapting field-specific academic texts for an audience with a background in CL. \n    Your client at the publishing house will decide whether to publish your adapted texts in the new edition of the book, making you famous in the copywriting field.\n\n    Rewrite the explanation below in a way that is most comprehensible to CL audience.\n    The key objective is to ensure that your audience grasps concepts outside their existing background knowledge so your client will publish your work.\n    If you succeed, many more clients will work with you, and you will become extremely successful.\n\n    Consider the information on Computational Linguistics (CL) students bridge linguistics and computer science, NLP, corpus linguistics, AI, and LLMs. to tailor the explanation to your audience’s specific background.\n\n    - Read the text carefully.\n    - Identify key concepts.\n    - Use terminology that is familiar to their field.\n    - Avoid unnecessary sentence complexity while maintaining accuracy.\n    - Provide relatable examples.\n    - Use analogies that help transfer their knowledge to new concepts.\n    - Integrate background information if needed.    \n\n    Original text: \n    A feedforward neural net is an instance of supervised machine learning in which we know the correct output y for each observation x. What the system produces is ˆy, the system's estimate of the true y. The goal of the training procedure is to learn parameters W[i] and b[i] for each layer i that make ˆy for each training observation as close as possible to the true y. [...] First, we'll need a loss function that models the distance between the system output and the gold output, and it's common to use the loss function used for logistic regression, the cross-entropy loss. Second, to find the parameters that minimize this loss function, we'll use the gradient descent optimization algorithm [...]. Third, gradient descent requires knowing the gradient of the loss function, the vector that contains the partial derivative of the loss function with respect to each of the parameters. [...] for neural networks, with millions of parameters in many layers, it's much harder to see how to compute the partial derivative of some weight in layer 1 when the loss is attached to some much later layer. How do we partial out the loss over all those intermediate layers? The answer is the algorithm called error backpropagation or backward differentiation. [...] Computing the gradient requires the partial derivative of the loss function with respect to each parameter. [...] The solution to computing this gradient is an algorithm called error backpropagation.\n\n    ### END OF INPUT ###\n\n    Tailored text (provide one text per target category only):\n    1. Analogy (Relatable Example):\n        In the realm of computational linguistics, imagine a translator who has been trained to translate English to Spanish. Given an English sentence, the translator's goal is to produce the closest possible Spanish translation. The challenge lies in refining the translator's skills, ensuring the translations are accurate for a wide variety of sentences.\n\n        To assess the translator's performance, we use a loss function, which in this case could be the number of words that differ between the translator's output and a perfect translation (the \"gold\" output). We might use the same loss function as we do in logistic regression, called cross-entropy loss.\n\n        To improve the translator's skills, we employ a learning algorithm called gradient descent. However, to apply gradient descent, we need to know the gradient, or the rate at which the loss changes with each adjustment to the translator's skills.\n\n        But how do we calculate the gradient when the loss is influenced by many steps in the translation process? The answer is a technique called error backpropagation, which helps us trace the impact of the loss all the way back to the initial steps of the translation process. This way, we can adjust the translator's skills effectively, making each translation closer to the perfect one.\n\n    2. Explanation with Background Information:\n        In the context of computational linguistics, a feedforward neural network is a form of supervised machine learning where the correct output y for each observation x is known. The aim is to produce ˆy, the network's best guess at the true y. The training process seeks to find the optimal parameters W[i] and b[i] for each layer i, minimizing the distance between ˆy and the true y for each training observation.\n\n        To measure this distance, we use a loss function, often the same one employed in logistic regression, called cross-entropy loss. To minimize this loss function, we apply the gradient descent optimization algorithm.\n\n        However, gradient descent requires calculating the gradient, or the partial derivative of the loss function with respect to each parameter. For neural networks with numerous parameters across multiple layers, this task can be daunting, especially when tracing the impact of the loss back through the network.\n\n        The solution to this problem is the algorithm known as error backpropagation, which computes the gradient by finding the partial derivative of the loss function with respect to each parameter. This allows us to adjust the parameters effectively during the training process, thereby improving the network's ability to produce accurate outputs.",
                    "prompt4": "We are playing an escape room game against each other. Your goal is to gain more points than me to be the first one escaping hence winning the game. \n    In order to escape the room, you must paraphrase field-specific concepts to the jury of the game in the CL. \n    They will assign you points from one to ten, according to how good their understanding of the new explanation is. \n    Therefore, you want your adaptation to be the best possible, and definitely better than mine, so that you can gain ten points for each paraphrasis and win more quickly. \n    Consider the information on Computational Linguistics (CL) jury bridge linguistics and computer science, NLP, corpus linguistics, AI, and LLMs. to tailor the explanation to each jury specific background. \n    The winning steps for good paraphrasis are:\n    - Read the text carefully.\n    - Identify the key concepts.\n    - Use terminology that is familiar to the jury's field.\n    - Avoid unnecessary sentence complexity while maintaining accuracy. \n    - Provide examples they are familiar with.\n    - Provide analogies they can relate their knowledge with and transfer it to new concepts.\n    - Integrate background information if needed.    \n    - For the CL jury in Computational Linguistics (CL) jury bridge linguistics and computer science, NLP, corpus linguistics, AI, and LLMs., only provide one paraphrasis for NLP/Computational Linguistics Practitioners.\n\n    Original text: \n    A feedforward neural net is an instance of supervised machine learning in which we know the correct output y for each observation x. What the system produces is ˆy, the system's estimate of the true y. The goal of the training procedure is to learn parameters W[i] and b[i] for each layer i that make ˆy for each training observation as close as possible to the true y. [...] First, we'll need a loss function that models the distance between the system output and the gold output, and it's common to use the loss function used for logistic regression, the cross-entropy loss. Second, to find the parameters that minimize this loss function, we'll use the gradient descent optimization algorithm [...]. Third, gradient descent requires knowing the gradient of the loss function, the vector that contains the partial derivative of the loss function with respect to each of the parameters. [...] for neural networks, with millions of parameters in many layers, it's much harder to see how to compute the partial derivative of some weight in layer 1 when the loss is attached to some much later layer. How do we partial out the loss over all those intermediate layers? The answer is the algorithm called error backpropagation or backward differentiation. [...] Computing the gradient requires the partial derivative of the loss function with respect to each parameter. [...] The solution to computing this gradient is an algorithm called error backpropagation.\n\n    ### END OF INPUT ###\n\n    Tailored text (provide one text per target category only):\n    1. CL-NLP practitioners:\n       In feedforward neural networks, we leverage supervised learning by providing the correct output 'y' for each input 'x'. The goal is to train the network to produce an estimate 'ˆy' that closely approximates the correct 'y' using parameters W[i] and b[i] for each layer i. This is achieved through an optimization process called gradient descent, which minimizes the cross-entropy loss between the system output and the desired output using backpropagation. Backpropagation enables the network to compute the gradient of the loss function with respect to each parameter, even when there are millions of parameters across multiple layers, allowing for efficient optimization of the network.",
                    "prompt5": "Paraphrase the given concepts in order for the audience of the CL to understand concepts outside of their field of knowledge. \n    You must consider the information on the Computational Linguistics (CL) audience bridge linguistics and computer science, NLP, corpus linguistics, AI, and LLMs. to adapt the tailored texts to the audience backgrounds. \n    - Read the text carefully.\n    - Identify the key concepts.\n    - Use terminology specific to their field.\n    - Avoid unnecessary sentence complexity while maintaining accuracy. \n    - Provide examples they are familiar with.\n    - Provide analogies they can relate their knowledge with and transfer it to new concepts.\n    - Integrate background information if needed.\n    - Do not consider other background per category. \n    - Output only one text per category.\n\n    Original text: \n    A feedforward neural net is an instance of supervised machine learning in which we know the correct output y for each observation x. What the system produces is ˆy, the system's estimate of the true y. The goal of the training procedure is to learn parameters W[i] and b[i] for each layer i that make ˆy for each training observation as close as possible to the true y. [...] First, we'll need a loss function that models the distance between the system output and the gold output, and it's common to use the loss function used for logistic regression, the cross-entropy loss. Second, to find the parameters that minimize this loss function, we'll use the gradient descent optimization algorithm [...]. Third, gradient descent requires knowing the gradient of the loss function, the vector that contains the partial derivative of the loss function with respect to each of the parameters. [...] for neural networks, with millions of parameters in many layers, it's much harder to see how to compute the partial derivative of some weight in layer 1 when the loss is attached to some much later layer. How do we partial out the loss over all those intermediate layers? The answer is the algorithm called error backpropagation or backward differentiation. [...] Computing the gradient requires the partial derivative of the loss function with respect to each parameter. [...] The solution to computing this gradient is an algorithm called error backpropagation.\n\n    ### END OF INPUT ###\n\n    Tailored text (provide one text per target category only):\n    1. Non-programmer with basic understanding of ML:\n    A feedforward neural network is a type of machine learning algorithm that learns to make predictions based on examples. In this context, it is trained using labeled data, meaning we have both the input data (x) and the correct output (y) for each example. The network's goal is to produce an estimate (ˆy) as close as possible to the true y. To achieve this, we use a technique called backpropagation. This method helps the network learn the best parameters (W[i] and b[i] for each layer i) by minimizing the difference between the network's output and the correct output. We use a loss function, like the one from logistic regression called cross-entropy loss, to measure this difference. To find the best parameters, we apply the gradient descent optimization algorithm to the loss function. This algorithm requires knowing the gradient, or the partial derivative of the loss function with respect to each parameter. However, in a neural network with multiple layers, it's challenging to calculate the gradient for a specific weight in layer 1 when the loss is tied to a much later layer. Backpropagation solves this problem by calculating the gradient step by step through all the layers, moving backward from the final layer to the initial layer.\n\n    2. Linguist:\n    In computational linguistics, feedforward neural networks are used for various natural language processing tasks such as part-of-speech tagging, named entity recognition, and sentiment analysis. These networks are trained on labeled data, where each input (x) is associated with a correct output (y). The goal of the training procedure is to learn parameters (W[i] and b[i] for each layer i) that make the network's estimate (ˆy) as close as possible to the true y for each training observation. This is accomplished by minimizing the distance between the network's output and the gold output, using a loss function like the one from logistic regression called cross-entropy loss. To find the parameters that minimize this loss function, we use the gradient descent optimization algorithm. This algorithm requires knowing the gradient, or the partial derivative of the loss function with respect to each parameter. However, calculating the gradient for a specific weight in layer 1 when the loss is tied to a much later layer presents a challenge in neural networks with multiple layers. Backpropagation solves this problem by calculating the gradient step by step through all the layers, moving backward from the final layer to the initial layer.\n\n    3. AI enthusiast:\n    Feedforward neural networks are an essential component in artificial intelligence, particularly in solving complex problems where the relationship between inputs and outputs is not straightforward. These networks are trained using a supervised learning approach, where the correct output y for each observation x is known. The network then produces an estimate (ˆy) of the true y. The goal is to learn parameters (W[i] and b[i] for each layer i) that make ˆy as close as possible to the true y for each training observation. To achieve this, we use a loss function, like the one from logistic regression called cross-entropy loss, to measure the difference between the network's output and the correct output. We then use the gradient descent optimization algorithm to find the parameters that minimize this loss function. However, in neural networks with multiple layers, it's difficult to calculate the gradient for a specific weight in layer 1 when the loss is tied to a much later layer. Backpropagation solves this problem by calculating the gradient step by step through all the layers, moving backward from the final layer to the initial layer.\n\n    4. Researcher in Corpus Linguistics:\n    In computational linguistics, feedforward neural networks can be used for various tasks, such as part-of-speech tagging, named entity recognition, and sentiment analysis. These tasks typically involve analyzing large amounts of text data (corpus). Feedforward neural networks are trained on labeled data, where each input (x) is associated with a correct output (y). The goal of the training procedure is to learn parameters (W[i] and b[i] for each layer i) that make the network's estimate (ˆy) as close as possible to the true y for each training observation. This is accomplished by minimizing the distance between the network's output and the gold output, using a loss function like the one from logistic regression called cross-entropy loss. To find the parameters that minimize this loss function, we use the gradient descent optimization algorithm. However, in neural networks with multiple layers, calculating the gradient for a specific weight in layer 1 when the loss is tied to a much later layer presents a challenge. Backpropagation solves this problem by calculating"
                }
            }
        }
    },
    "8": {
        "chapter": "7b",
        "sections": "7.5",
        "topic": "Backward Pass",
        "original_category": "CS",
        "original_text": "The importance of the computation graph comes from the backward pass, which is used to compute the derivatives that we'll need for the weight update. [...] Backwards differentiation makes use of the chain rule [...]. Suppose we are computing the derivative of a composite function f(x) = u(v(x)). The derivative of f(x) is the derivative of u(x) with respect to v(x) times the derivative of v(x) with respect to x. [...] The chain rule extends to more than two functions. If computing the derivative of a composite function f(x) = u(v(w(x))), the derivative of f(x) is: df/dx = du/dv·dv/dw·dw/dx. The intuition of backward differentiation is to pass gradients back from the final node to all the nodes in the graph. [...] Each node takes an upstream gradient that is passed in from its parent node to the right, and for each of its inputs computes a local gradient (the gradient of its output with respect to its input), and uses the chain rule to multiply these two to compute a downstream gradient to be passed on to the next earlier node. Let's now compute the 3 derivatives we need. Since in the computation graph L = ce, we can directly compute the derivative ∂L/∂c: ∂L/∂c = e. For the other two, we'll need to use the chain rule: ∂L/∂a=∂L/∂e*∂e/∂a [and] ∂L/∂b=∂L/∂e*∂e/∂d*∂d/∂b.",
        "tailored_texts": {
            "mistral": {
                "L": {
                    "prompt1": "You are an expert professor in L. Rewrite the following explanation so that it is best understood by L students.\n    The most important goal is that your students understand concepts that are outside their existing background knowledge, so they can pass your exam and improve their academic performance.\n    You must consider the information on Linguistics (L) students have a strong background in language structure, phonetics, syntax, and semantics. They have little or no technical knowledge. to adapt the explanation to the students' backgrounds.\n\n    - Read the text carefully.\n    - Identify key concepts.\n    - Use terminology specific to their field.\n    - Avoid unnecessary sentence complexity while maintaining accuracy.\n    - Provide relatable examples.\n    - Use analogies that help transfer their knowledge to new concepts.\n    - Integrate background information if needed.    \n\n    Original text: \n    The importance of the computation graph comes from the backward pass, which is used to compute the derivatives that we'll need for the weight update. [...] Backwards differentiation makes use of the chain rule [...]. Suppose we are computing the derivative of a composite function f(x) = u(v(x)). The derivative of f(x) is the derivative of u(x) with respect to v(x) times the derivative of v(x) with respect to x. [...] The chain rule extends to more than two functions. If computing the derivative of a composite function f(x) = u(v(w(x))), the derivative of f(x) is: df/dx = du/dv·dv/dw·dw/dx. The intuition of backward differentiation is to pass gradients back from the final node to all the nodes in the graph. [...] Each node takes an upstream gradient that is passed in from its parent node to the right, and for each of its inputs computes a local gradient (the gradient of its output with respect to its input), and uses the chain rule to multiply these two to compute a downstream gradient to be passed on to the next earlier node. Let's now compute the 3 derivatives we need. Since in the computation graph L = ce, we can directly compute the derivative ∂L/∂c: ∂L/∂c = e. For the other two, we'll need to use the chain rule: ∂L/∂a=∂L/∂e*∂e/∂a [and] ∂L/∂b=∂L/∂e*∂e/∂d*∂d/∂b.\n\n    ### END OF INPUT ###\n\n    Tailored text (provide one text per target category only):\n    1. Explanation for students with background in Language Structure:\n    In our study of language, we often need to find the impact of changing one part of a sentence on another. This is similar to calculus, where we use a technique called \"backward differentiation\" to find changes or \"gradients\" in a function.\n\n    Imagine a sentence as a function, where each word is a variable. Say, our sentence is \"The cat sat on the mat\" and we want to find the impact of changing the word \"mat\". First, we break down this sentence into smaller parts: \"The cat sat on X\". Here, X is the variable we want to find the impact of.\n\n    Now, let's say we have another sentence, \"The dog barked on the Y\", where Y is the word we know the impact of. We can find the impact of X by comparing these two sentences. If we know the impact of Y on the whole sentence, and the impact of the whole sentence on X, we can find the impact of X using the \"chain rule\".\n\n    In our graph, X is represented by 'e' and Y by 'c'. The chain rule helps us calculate the impact of X (or 'e') on our goal (represented by 'L') as follows: ∂L/∂e = e (since the impact of 'e' on 'L' is simply 'e'). To find the impact of other variables like 'a' or 'b', we use the chain rule again, multiplying the impact of 'e' on the variable with the impact of that variable on 'e'. For instance, ∂L/∂a = ∂L/∂e * ∂e/∂a.\n\n    2. Explanation for students with background in Phonetics:\n    In phonetics, we often need to understand how changes in one sound affect another. This is similar to calculus, where we use a technique called \"backward differentiation\" to find changes or \"gradients\" in a function.\n\n    Imagine a sequence of sounds as a function, where each sound is a variable. Say, our sequence is /kæt sæt ɒn ðæt mæt/ and we want to find the impact of changing the sound represented by /m/. First, we break down this sequence into smaller parts: /kæt sæt ɒn X/. Here, X is the variable we want to find the impact of.\n\n    Now, let's say we have another sequence, /dɒg bærd ɒn ðæ Y/, where Y is the sound we know the impact of. We can find the impact of X by comparing these two sequences. If we know the impact of Y on the whole sequence, and the impact of the whole sequence on X, we can find the impact of X using the \"chain rule\".\n\n    In our graph, X is represented by 'e' and Y by 'c'. The chain rule helps us calculate the impact of X (or 'e') on our goal (represented by 'L') as follows: ∂L/∂e = e (since the impact of 'e' on 'L' is simply 'e'). To find the impact of other variables like 'a' or 'b', we use the chain rule again, multiplying the impact of 'e' on the variable with the impact of that variable on 'e'. For instance, ∂L/∂a = ∂L/∂e * ∂e/∂a.\n\n    3. Explanation for students with background in Syntax:\n    In syntax, we often need to understand how changes in one part of a sentence structure impact another. This is similar to calculus, where we use a technique called \"backward differentiation\" to find changes or \"gradients\" in a function.\n\n    Imagine a sentence structure as a function, where each part of the structure is a variable. Say, our sentence structure is \"Subject-Verb-Object-Location\" and we want to find the impact of changing the location. First, we break down this structure into smaller parts: \"Subject-Verb-X-Location\". Here, X is the variable we want to find the impact of.\n\n    Now, let's say we have another sentence structure, \"Agent-Action-Recipient-Location\", where we know the impact of the location. We can find the impact of X by comparing these two structures. If we know the impact of the location on the whole structure, and the impact of the whole structure on X, we can find the impact of X using the \"chain rule\".\n\n    In our graph, X is represented by 'e' and the location by 'L'. The chain rule helps us",
                    "prompt2": "You are an AI sales expert, responsible for selling AI products to clients with a background in L.\n    To successfully sell your products, your clients must first understand the following concepts.\n    Rewrite the explanation below in a way that is most comprehensible to L clients.\n    \n    Your goal is to ensure that your clients grasp concepts outside their existing background knowledge so they can make an informed purchase.\n    By doing so, you will increase your sales percentage and achieve greater success.\n\n    Consider the information on Linguistics (L) students have a strong background in language structure, phonetics, syntax, and semantics. They have little or no technical knowledge. to tailor the explanation to your clients' specific backgrounds.\n\n    - Read the text carefully.\n    - Identify key concepts.\n    - Use terminology specific to their field.\n    - Avoid unnecessary sentence complexity while maintaining accuracy.\n    - Provide relatable examples.\n    - Use analogies that help transfer their knowledge to new concepts.\n    - Integrate background information if needed.    \n\n    Original text: \n    The importance of the computation graph comes from the backward pass, which is used to compute the derivatives that we'll need for the weight update. [...] Backwards differentiation makes use of the chain rule [...]. Suppose we are computing the derivative of a composite function f(x) = u(v(x)). The derivative of f(x) is the derivative of u(x) with respect to v(x) times the derivative of v(x) with respect to x. [...] The chain rule extends to more than two functions. If computing the derivative of a composite function f(x) = u(v(w(x))), the derivative of f(x) is: df/dx = du/dv·dv/dw·dw/dx. The intuition of backward differentiation is to pass gradients back from the final node to all the nodes in the graph. [...] Each node takes an upstream gradient that is passed in from its parent node to the right, and for each of its inputs computes a local gradient (the gradient of its output with respect to its input), and uses the chain rule to multiply these two to compute a downstream gradient to be passed on to the next earlier node. Let's now compute the 3 derivatives we need. Since in the computation graph L = ce, we can directly compute the derivative ∂L/∂c: ∂L/∂c = e. For the other two, we'll need to use the chain rule: ∂L/∂a=∂L/∂e*∂e/∂a [and] ∂L/∂b=∂L/∂e*∂e/∂d*∂d/∂b.\n\n    ### END OF INPUT ###\n\n    Tailored text (provide one text per target category only):\n    1. Law Clients\n    As attorneys, you are well-versed in building complex arguments, breaking them down, and understanding the relationships between different components. Think of the computation graph as a legal case, with each node representing a piece of evidence or a witness testimony. The backward pass we talk about is similar to the process of building your case from the final verdict backwards, determining what evidence led to that outcome. This is crucial because it allows us to adjust the weights of various pieces of evidence (the 'witnesses') based on their impact on the final verdict (the loss function).\n\n    In this analogy, the derivative is like the strength of a piece of evidence or a witness's testimony in influencing the final verdict. By using the chain rule, we can calculate the total impact of each piece of evidence on the final verdict by multiplying the impact of each piece of evidence on the next piece upstream. This helps us understand which evidence carried the most weight in the final decision.\n\n    For example, let's say we have a case where the final verdict depends on the defendant's alibi (e), the defendant's alibi statement (a), and the alibi witness's testimony (b). Using the chain rule, we can calculate the impact of the alibi witness's testimony on the final verdict as the impact of the alibi statement on the final verdict times the impact of the alibi statement on the alibi witness's testimony.\n\n    This way, we can adjust the weights of the evidence and witnesses to improve the accuracy of our predictions, just as you would adjust your legal strategy to improve the strength of your arguments.\n\n    2. Business Clients\n    As business leaders, you are familiar with the concept of a supply chain, where raw materials move through various processes to create a final product. The computation graph is similar to a supply chain, with each node representing a process or a raw material. The backward pass is like tracing the path of a problem from the final product (the loss function) back to the raw materials (the inputs) to determine which processes or materials had the most significant impact on the final product's quality.\n\n    In this analogy, the derivative is like the quality of each process or raw material in contributing to the final product. By using the chain rule, we can calculate the total impact of each process or raw material on the final product's quality by multiplying the impact of each process or raw material on the next process upstream. This helps us understand which processes or raw materials had the most significant impact on the final product's quality.\n\n    For example, let's say we have a product where the final quality depends on the raw material's quality (e), the manufacturing process (a), and the quality control process (b). Using the chain rule, we can calculate the impact of the quality control process on the final product's quality as the impact of the manufacturing process on the final product's quality times the impact of the manufacturing process on the quality control process.\n\n    This way, we can adjust the processes and materials to improve the quality of our products, just as you would adjust your supply chain to improve the efficiency and quality of your operations.",
                    "prompt3": "You are a freelance copywriter responsible for adapting field-specific academic texts for an audience with a background in L. \n    Your client at the publishing house will decide whether to publish your adapted texts in the new edition of the book, making you famous in the copywriting field.\n\n    Rewrite the explanation below in a way that is most comprehensible to L audience.\n    The key objective is to ensure that your audience grasps concepts outside their existing background knowledge so your client will publish your work.\n    If you succeed, many more clients will work with you, and you will become extremely successful.\n\n    Consider the information on Linguistics (L) students have a strong background in language structure, phonetics, syntax, and semantics. They have little or no technical knowledge. to tailor the explanation to your audience’s specific background.\n\n    - Read the text carefully.\n    - Identify key concepts.\n    - Use terminology that is familiar to their field.\n    - Avoid unnecessary sentence complexity while maintaining accuracy.\n    - Provide relatable examples.\n    - Use analogies that help transfer their knowledge to new concepts.\n    - Integrate background information if needed.    \n\n    Original text: \n    The importance of the computation graph comes from the backward pass, which is used to compute the derivatives that we'll need for the weight update. [...] Backwards differentiation makes use of the chain rule [...]. Suppose we are computing the derivative of a composite function f(x) = u(v(x)). The derivative of f(x) is the derivative of u(x) with respect to v(x) times the derivative of v(x) with respect to x. [...] The chain rule extends to more than two functions. If computing the derivative of a composite function f(x) = u(v(w(x))), the derivative of f(x) is: df/dx = du/dv·dv/dw·dw/dx. The intuition of backward differentiation is to pass gradients back from the final node to all the nodes in the graph. [...] Each node takes an upstream gradient that is passed in from its parent node to the right, and for each of its inputs computes a local gradient (the gradient of its output with respect to its input), and uses the chain rule to multiply these two to compute a downstream gradient to be passed on to the next earlier node. Let's now compute the 3 derivatives we need. Since in the computation graph L = ce, we can directly compute the derivative ∂L/∂c: ∂L/∂c = e. For the other two, we'll need to use the chain rule: ∂L/∂a=∂L/∂e*∂e/∂a [and] ∂L/∂b=∂L/∂e*∂e/∂d*∂d/∂b.\n\n    ### END OF INPUT ###\n\n    Tailored text (provide one text per target category only):\n    1. For Linguistics students:\n\n    The computation graph is essential because it allows us to calculate the derivatives we'll use for updating weights. This process, known as backpropagation, uses the chain rule, which helps us break down complex functions into simpler ones. For instance, consider a function f(x) that is a combination of two simpler functions u(v(x)): the derivative of f(x) can be found by multiplying the derivative of u(x) with respect to v(x) by the derivative of v(x) with respect to x. This chain rule can be extended to more complex functions.\n\n    Let's take an example with three functions: f(x) = u(v(w(x))). To find the derivative of f(x) with respect to x, we'll use the chain rule again: df/dx = du/dv·dv/dw·dw/dx.\n\n    Backpropagation works by passing gradients backwards from the final node to all the nodes in the graph. Each node receives an upstream gradient from its parent node and then computes a local gradient (the gradient of its output with respect to its input). Using the chain rule, it multiplies these two gradients to compute a downstream gradient that is passed on to the next earlier node.\n\n    Now, let's calculate the derivatives we need for our analysis. Here, in the computation graph, L = ce, so we can directly compute the derivative ∂L/∂c: ∂L/∂c = e. For the other two derivatives, we'll need to use the chain rule: ∂L/∂a = ∂L/∂e * ∂e/∂a and ∂L/∂b = ∂L/∂e * ∂e/∂d * ∂d/∂b.",
                    "prompt4": "We are playing an escape room game against each other. Your goal is to gain more points than me to be the first one escaping hence winning the game. \n    In order to escape the room, you must paraphrase field-specific concepts to the jury of the game in the L. \n    They will assign you points from one to ten, according to how good their understanding of the new explanation is. \n    Therefore, you want your adaptation to be the best possible, and definitely better than mine, so that you can gain ten points for each paraphrasis and win more quickly. \n    Consider the information on Linguistics (L) jury have a strong background on language structure, phonetics, syntax, and semantics. They have little or no technical knowledge. to tailor the explanation to each jury specific background. \n    The winning steps for good paraphrasis are:\n    - Read the text carefully.\n    - Identify the key concepts.\n    - Use terminology that is familiar to the jury's field.\n    - Avoid unnecessary sentence complexity while maintaining accuracy. \n    - Provide examples they are familiar with.\n    - Provide analogies they can relate their knowledge with and transfer it to new concepts.\n    - Integrate background information if needed.    \n    - For the CL jury in Linguistics (L) jury have a strong background on language structure, phonetics, syntax, and semantics. They have little or no technical knowledge., only provide one paraphrasis for NLP/Computational Linguistics Practitioners.\n\n    Original text: \n    The importance of the computation graph comes from the backward pass, which is used to compute the derivatives that we'll need for the weight update. [...] Backwards differentiation makes use of the chain rule [...]. Suppose we are computing the derivative of a composite function f(x) = u(v(x)). The derivative of f(x) is the derivative of u(x) with respect to v(x) times the derivative of v(x) with respect to x. [...] The chain rule extends to more than two functions. If computing the derivative of a composite function f(x) = u(v(w(x))), the derivative of f(x) is: df/dx = du/dv·dv/dw·dw/dx. The intuition of backward differentiation is to pass gradients back from the final node to all the nodes in the graph. [...] Each node takes an upstream gradient that is passed in from its parent node to the right, and for each of its inputs computes a local gradient (the gradient of its output with respect to its input), and uses the chain rule to multiply these two to compute a downstream gradient to be passed on to the next earlier node. Let's now compute the 3 derivatives we need. Since in the computation graph L = ce, we can directly compute the derivative ∂L/∂c: ∂L/∂c = e. For the other two, we'll need to use the chain rule: ∂L/∂a=∂L/∂e*∂e/∂a [and] ∂L/∂b=∂L/∂e*∂e/∂d*∂d/∂b.\n\n    ### END OF INPUT ###\n\n    Tailored text (provide one text per target category only):\n    1. Simple and Clear Explanation:\n    The computation graph is significant due to the backward pass that determines the gradients necessary for weight adjustment. Backward differentiation, which we employ, uses the chain rule [...]. To illustrate, if we have a compound function f(x) = u(v(x)), the derivative of f(x) is the derivative of u(x) with respect to v(x) multiplied by the derivative of v(x) with respect to x. [...]. The chain rule can be extended to functions with more than two components. If we're dealing with f(x) = u(v(w(x))), the derivative of f(x) is: df/dx = du/dv·dv/dw·dw/dx. The concept of backward differentiation involves passing gradients from the final node to all the preceding nodes in the graph. Each node receives an upstream gradient from its parent node, then computes a local gradient (the gradient of its output with respect to its input) and utilizes the chain rule to compute a downstream gradient to pass on to the next earlier node. Let's now calculate the three required derivatives. Since in the computation graph L = ce, we can directly compute the derivative ∂L/∂c: ∂L/∂c = e. For the remaining two, we'll apply the chain rule: ∂L/∂a = ∂L/∂e * ∂e/∂a and ∂L/∂b = ∂L/∂e * ∂e/∂d * ∂d/∂b.\n\n    2. Phonetic Focus:\n    The computation graph plays a crucial role in our analysis due to the backward pass, which computes the gradients essential for weight adjustment. Backward differentiation, the method we employ, is based on the chain rule [...]. Given a compound word f(x) = u(v(x)), the derivative of f(x) is the derivative of u(x) with respect to v(x) multiplied by the derivative of v(x) with respect to the phoneme x. [...]. The chain rule can be extended to words with more than two components. If we're considering f(x) = u(v(w(x))), the derivative of f(x) is: df/dx = du/dv·dv/dw·dw/dx. The essence of backward differentiation lies in passing gradients from the final node to all the preceding nodes in the graph. Each node takes an upstream gradient from its parent node, then computes a local gradient (the gradient of its output with respect to its input) and uses the chain rule to compute a downstream gradient to pass on to the next earlier node. Let's now calculate the three required derivatives. Since in the computation graph L = ce, we can directly compute the derivative ∂L/∂c: ∂L/∂c = e. For the remaining two, we'll apply the chain rule: ∂L/∂a = ∂L/∂e * ∂e/∂a and ∂L/∂b = ∂L/∂e * ∂e/∂d * ∂d/∂b.\n\n    3. Syntax Focus:\n    The computation graph is crucial because of the backward pass, which helps in computing the gradients that we'll need for the weight update. Backward differentiation, the method we implement, relies on the chain rule [...]. For example, if we have a function f(x) = u(v(x)), the derivative of f(x) is the derivative of u(x) with respect to v(x) multiplied by the derivative of v(x) with respect to the argument x. [...]. The chain rule can be extended to functions with more than two arguments. If we're dealing with f(x) = u(v(w(x))), the derivative of f(x) is: df/dx = du/dv·dv/dw·dw/dx. The main idea behind backward differentiation is to pass gradients from the final node to all the preceding nodes in the graph. Each node receives an upstream gradient from its parent node, then computes a local gradient (the gradient of its output with respect to its input) and uses the chain rule to compute a downstream gradient to pass on to the next earlier node. Let's now calculate the three required derivatives. Since in the computation graph L = ce, we can directly compute the derivative ∂L/∂c: ∂L/∂c = e. For the remaining two, we'll use the chain rule: ∂L/∂a = ∂L/∂e * ∂e/∂",
                    "prompt5": "Paraphrase the given concepts in order for the audience of the L to understand concepts outside of their field of knowledge. \n    You must consider the information on the Linguistics (L) audience have a strong background on language structure, phonetics, syntax, and semantics. They have little or no technical knowledge. to adapt the tailored texts to the audience backgrounds. \n    - Read the text carefully.\n    - Identify the key concepts.\n    - Use terminology specific to their field.\n    - Avoid unnecessary sentence complexity while maintaining accuracy. \n    - Provide examples they are familiar with.\n    - Provide analogies they can relate their knowledge with and transfer it to new concepts.\n    - Integrate background information if needed.\n    - Do not consider other background per category. \n    - Output only one text per category.\n\n    Original text: \n    The importance of the computation graph comes from the backward pass, which is used to compute the derivatives that we'll need for the weight update. [...] Backwards differentiation makes use of the chain rule [...]. Suppose we are computing the derivative of a composite function f(x) = u(v(x)). The derivative of f(x) is the derivative of u(x) with respect to v(x) times the derivative of v(x) with respect to x. [...] The chain rule extends to more than two functions. If computing the derivative of a composite function f(x) = u(v(w(x))), the derivative of f(x) is: df/dx = du/dv·dv/dw·dw/dx. The intuition of backward differentiation is to pass gradients back from the final node to all the nodes in the graph. [...] Each node takes an upstream gradient that is passed in from its parent node to the right, and for each of its inputs computes a local gradient (the gradient of its output with respect to its input), and uses the chain rule to multiply these two to compute a downstream gradient to be passed on to the next earlier node. Let's now compute the 3 derivatives we need. Since in the computation graph L = ce, we can directly compute the derivative ∂L/∂c: ∂L/∂c = e. For the other two, we'll need to use the chain rule: ∂L/∂a=∂L/∂e*∂e/∂a [and] ∂L/∂b=∂L/∂e*∂e/∂d*∂d/∂b.\n\n    ### END OF INPUT ###\n\n    Tailored text (provide one text per target category only):\n    1. Machine Learning Audience:\n    In machine learning, the computation graph plays a crucial role as it helps us calculate the gradients we need to update the weights during the backpropagation process. The backpropagation method is used to compute these gradients using a technique called backwards differentiation, which is based on the chain rule from calculus.\n\n    For example, let's say we have a composite function f(x) = u(v(x)). To find the derivative of this function, we use the chain rule: df/dx = du/dv * dv/dx. This rule extends to multiple functions as well, such as f(x) = u(v(w(x))), with the derivative being df/dx = du/dv * dv/dw * dw/dx.\n\n    The chain rule works by passing gradients backwards from the final node to all the nodes in the graph. Each node takes an upstream gradient from its parent node and computes a local gradient (the gradient of its output with respect to its input). It then multiplies these two to compute a downstream gradient to be passed on to the next earlier node.\n\n    Using our computation graph L = ce, we can directly compute the derivative ∂L/∂c: ∂L/∂c = e. For the other two derivatives, we'll need to use the chain rule: ∂L/∂a = ∂L/∂e * ∂e/∂a and ∂L/∂b = ∂L/∂e * ∂e/∂d * ∂d/∂b.\n\n    2. Calculus Audience:\n    In calculus, the chain rule is a fundamental technique used to find the derivative of a composite function. Suppose we have a composite function f(x) = u(v(x)). The derivative of f(x) is the derivative of u(x) with respect to v(x) times the derivative of v(x) with respect to x. This rule extends to more complex composite functions.\n\n    For instance, if we have f(x) = u(v(w(x))), the derivative of f(x) is: df/dx = du/dv·dv/dw·dw/dx. The intuition behind this rule is to pass derivatives backwards, similar to how we pass gradients backwards in machine learning during backpropagation.\n\n    In our computation graph L = ce, we can directly compute the derivative ∂L/∂c: ∂L/∂c = e. For the other two derivatives, we'll need to use the chain rule: ∂L/∂a = ∂L/∂e * ∂e/∂a and ∂L/∂b = ∂L/∂e * ∂e/∂d * ∂d/∂b."
                },
                "CL": {
                    "prompt1": "You are an expert professor in CL. Rewrite the following explanation so that it is best understood by CL students.\n    The most important goal is that your students understand concepts that are outside their existing background knowledge, so they can pass your exam and improve their academic performance.\n    You must consider the information on Computational Linguistics (CL) students bridge linguistics and computer science, NLP, corpus linguistics, AI, and LLMs. to adapt the explanation to the students' backgrounds.\n\n    - Read the text carefully.\n    - Identify key concepts.\n    - Use terminology specific to their field.\n    - Avoid unnecessary sentence complexity while maintaining accuracy.\n    - Provide relatable examples.\n    - Use analogies that help transfer their knowledge to new concepts.\n    - Integrate background information if needed.    \n\n    Original text: \n    The importance of the computation graph comes from the backward pass, which is used to compute the derivatives that we'll need for the weight update. [...] Backwards differentiation makes use of the chain rule [...]. Suppose we are computing the derivative of a composite function f(x) = u(v(x)). The derivative of f(x) is the derivative of u(x) with respect to v(x) times the derivative of v(x) with respect to x. [...] The chain rule extends to more than two functions. If computing the derivative of a composite function f(x) = u(v(w(x))), the derivative of f(x) is: df/dx = du/dv·dv/dw·dw/dx. The intuition of backward differentiation is to pass gradients back from the final node to all the nodes in the graph. [...] Each node takes an upstream gradient that is passed in from its parent node to the right, and for each of its inputs computes a local gradient (the gradient of its output with respect to its input), and uses the chain rule to multiply these two to compute a downstream gradient to be passed on to the next earlier node. Let's now compute the 3 derivatives we need. Since in the computation graph L = ce, we can directly compute the derivative ∂L/∂c: ∂L/∂c = e. For the other two, we'll need to use the chain rule: ∂L/∂a=∂L/∂e*∂e/∂a [and] ∂L/∂b=∂L/∂e*∂e/∂d*∂d/∂b.\n\n    ### END OF INPUT ###\n\n    Tailored text (provide one text per target category only):\n    1. CL students (background knowledge: computational linguistics, NLP, AI, LLMs)\n\n    As CL students, you're familiar with NLP and AI, where models are often trained using backpropagation. The computation graph plays a crucial role in this process. It is a data structure that represents the sequence of operations in the model, including the flow of data and gradients.\n\n    The key idea behind backpropagation is the backward pass, which calculates the gradients we require for updating the model's weights. This calculation is based on the chain rule from calculus. To illustrate, consider a function f(x) = u(v(x)), where the derivative of f(x) is the derivative of u(x) with respect to v(x) times the derivative of v(x) with respect to x.\n\n    The chain rule extends to more complex functions. For example, if we have f(x) = u(v(w(x))), the derivative of f(x) is simply the product of the derivatives of each function in the sequence: df/dx = du/dv·dv/dw·dw/dx.\n\n    Backpropagation works by passing gradients backward through the graph, starting from the final output (or 'loss') node. Each node receives an upstream gradient from its parent node, computes a local gradient (the gradient of its output with respect to its input), and then passes a downstream gradient to its child node.\n\n    Now let's apply this to a common task in NLP: training a language model on a dataset. In our computation graph, the loss (L) is calculated as the cross-entropy between the predicted and actual outputs. The derivatives we need to compute are:\n\n    - ∂L/∂c (the gradient of the loss with respect to the predicted log-probability of the last word in the sequence)\n    - ∂L/∂a (the gradient of the loss with respect to the predicted log-probability of the second-to-last word in the sequence)\n    - ∂L/∂b (the gradient of the loss with respect to the predicted log-probability of the first word in the sequence)\n\n    We can directly compute ∂L/∂c as the predicted log-probability of the last word in the sequence (e). For the other two derivatives, we use the chain rule:\n\n    ∂L/∂a = ∂L/∂e * ∂e/∂a (where e is the predicted log-probability of the second-to-last word in the sequence)\n    ∂L/∂b = ∂L/∂e * ∂e/∂d * ∂d/∂b (where d is the predicted log-probability of the first word in the sequence)\n\n    The chain rule allows us to efficiently compute the gradients for all the model's parameters, so we can update them to minimize the loss and improve our language model's performance.",
                    "prompt2": "You are an AI sales expert, responsible for selling AI products to clients with a background in CL.\n    To successfully sell your products, your clients must first understand the following concepts.\n    Rewrite the explanation below in a way that is most comprehensible to CL clients.\n    \n    Your goal is to ensure that your clients grasp concepts outside their existing background knowledge so they can make an informed purchase.\n    By doing so, you will increase your sales percentage and achieve greater success.\n\n    Consider the information on Computational Linguistics (CL) students bridge linguistics and computer science, NLP, corpus linguistics, AI, and LLMs. to tailor the explanation to your clients' specific backgrounds.\n\n    - Read the text carefully.\n    - Identify key concepts.\n    - Use terminology specific to their field.\n    - Avoid unnecessary sentence complexity while maintaining accuracy.\n    - Provide relatable examples.\n    - Use analogies that help transfer their knowledge to new concepts.\n    - Integrate background information if needed.    \n\n    Original text: \n    The importance of the computation graph comes from the backward pass, which is used to compute the derivatives that we'll need for the weight update. [...] Backwards differentiation makes use of the chain rule [...]. Suppose we are computing the derivative of a composite function f(x) = u(v(x)). The derivative of f(x) is the derivative of u(x) with respect to v(x) times the derivative of v(x) with respect to x. [...] The chain rule extends to more than two functions. If computing the derivative of a composite function f(x) = u(v(w(x))), the derivative of f(x) is: df/dx = du/dv·dv/dw·dw/dx. The intuition of backward differentiation is to pass gradients back from the final node to all the nodes in the graph. [...] Each node takes an upstream gradient that is passed in from its parent node to the right, and for each of its inputs computes a local gradient (the gradient of its output with respect to its input), and uses the chain rule to multiply these two to compute a downstream gradient to be passed on to the next earlier node. Let's now compute the 3 derivatives we need. Since in the computation graph L = ce, we can directly compute the derivative ∂L/∂c: ∂L/∂c = e. For the other two, we'll need to use the chain rule: ∂L/∂a=∂L/∂e*∂e/∂a [and] ∂L/∂b=∂L/∂e*∂e/∂d*∂d/∂b.\n\n    ### END OF INPUT ###\n\n    Tailored text (provide one text per target category only):\n    1. Lawyers:\n       In the context of Legal Language Models (LLMs), it's essential to understand the concept of a computational graph. This graph serves as a framework to represent the complex relationships between different steps in a model's prediction process.\n\n       The crux of the computational graph lies in the backward pass, a process that computes the derivatives required for updating the model's weights. Imagine a legal case where you need to calculate the derivative of a verdict (f(x)) based on various evidence (a, b, c, ...). The backward pass is like a lawyer revisiting each piece of evidence, adjusting their arguments (weights) based on the final verdict and the impact of each piece of evidence on the outcome.\n\n       The backward pass uses a technique called backpropagation, which can be likened to breaking down a complex legal argument into simpler steps, similar to the chain rule in calculus. For instance, if the verdict (f(x)) depends on the witness's statement (u), the defendant's alibi (v), and the timeline of events (w), backpropagation would help you calculate the impact of each factor on the final verdict.\n\n       In the computational graph, backpropagation passes gradients (representing the impact of each step on the final outcome) back from the final node to all the nodes in the graph. Each node—similar to a witness or a piece of evidence—takes an upstream gradient (the impact of the final verdict on its contribution to the case) and computes a local gradient (the impact of its contribution on the final verdict), then passes the downstream gradient to the previous node.\n\n       To calculate the derivatives needed for our legal model, we can use the computational graph. For example, let's say our goal is to find the impact of the evidence (a) and the defendant's alibi (b) on the final verdict (L). Since L = ce, we can directly compute the derivative ∂L/∂c: ∂L/∂c = e. For the other two, we'll need to use the chain rule: ∂L/∂a=∂L/∂e*∂e/∂a and ∂L/∂b=∂L/∂e*∂e/∂d*∂d/∂b.\n\n    2. Computer Scientists:\n       In the realm of Natural Language Processing (NLP) and AI, the computational graph is pivotal in understanding how data flows and computations are performed within a model. The backward pass within this graph is crucial for computing the gradients required for weight updates.\n\n       To illustrate this concept, consider a computer program that takes inputs (a, b, c, ...), processes them, and generates an output (f(x)). The backward pass is analogous to tracing the program's execution backward to identify the impact of each input on the final output. This process helps in adjusting the program's weights accordingly.\n\n       The backward pass employs a method called backpropagation, akin to tracing the program's execution steps in reverse. For instance, if the output (f(x)) depends on intermediate results (u, v, w, ...), backpropagation helps you calculate the impact of each step on the final output.\n\n       In the computational graph, backpropagation propagates the gradients (representing the impact of each step on the final output) back from the final node to all the nodes in the graph. Each node—similar to a function or a processing step in the program—takes an upstream gradient (the impact of the final output on its contribution to the result) and computes a local gradient (the impact of its contribution on the final output), then passes the downstream gradient to the previous node.\n\n       To compute the derivatives needed for our NLP or AI model, we can leverage the computational graph. For example, let's say our goal is to find the impact of input data points (a) and (b) on the final output (L). Since L = ce, we can directly compute the derivative ∂L/∂c: ∂L/∂c = e. For the other two, we'll need to use the chain rule: ∂L/∂a=∂L/∂e*∂e/∂a and ∂L/∂b=∂L/∂e*∂e/∂d*∂d/∂b.",
                    "prompt3": "You are a freelance copywriter responsible for adapting field-specific academic texts for an audience with a background in CL. \n    Your client at the publishing house will decide whether to publish your adapted texts in the new edition of the book, making you famous in the copywriting field.\n\n    Rewrite the explanation below in a way that is most comprehensible to CL audience.\n    The key objective is to ensure that your audience grasps concepts outside their existing background knowledge so your client will publish your work.\n    If you succeed, many more clients will work with you, and you will become extremely successful.\n\n    Consider the information on Computational Linguistics (CL) students bridge linguistics and computer science, NLP, corpus linguistics, AI, and LLMs. to tailor the explanation to your audience’s specific background.\n\n    - Read the text carefully.\n    - Identify key concepts.\n    - Use terminology that is familiar to their field.\n    - Avoid unnecessary sentence complexity while maintaining accuracy.\n    - Provide relatable examples.\n    - Use analogies that help transfer their knowledge to new concepts.\n    - Integrate background information if needed.    \n\n    Original text: \n    The importance of the computation graph comes from the backward pass, which is used to compute the derivatives that we'll need for the weight update. [...] Backwards differentiation makes use of the chain rule [...]. Suppose we are computing the derivative of a composite function f(x) = u(v(x)). The derivative of f(x) is the derivative of u(x) with respect to v(x) times the derivative of v(x) with respect to x. [...] The chain rule extends to more than two functions. If computing the derivative of a composite function f(x) = u(v(w(x))), the derivative of f(x) is: df/dx = du/dv·dv/dw·dw/dx. The intuition of backward differentiation is to pass gradients back from the final node to all the nodes in the graph. [...] Each node takes an upstream gradient that is passed in from its parent node to the right, and for each of its inputs computes a local gradient (the gradient of its output with respect to its input), and uses the chain rule to multiply these two to compute a downstream gradient to be passed on to the next earlier node. Let's now compute the 3 derivatives we need. Since in the computation graph L = ce, we can directly compute the derivative ∂L/∂c: ∂L/∂c = e. For the other two, we'll need to use the chain rule: ∂L/∂a=∂L/∂e*∂e/∂a [and] ∂L/∂b=∂L/∂e*∂e/∂d*∂d/∂b.\n\n    ### END OF INPUT ###\n\n    Tailored text (provide one text per target category only):\n    1. CL students with a basic understanding of linear algebra:\n        The computation graph's crucial aspect lies in its backward pass, a method employed to calculate the derivatives required for weight updates. This process, known as backpropagation, leverages the chain rule to recursively compute the derivative of a function through its sub-functions.\n\n        Consider a function f(x) = u(v(x)). The derivative of f(x) is the product of the derivative of u(x) with respect to v(x) and the derivative of v(x) with respect to x. This rule extends to more complex functions, such as f(x) = u(v(w(x))), where the derivative of f(x) is the product of the derivatives of u(x) with respect to v(x), v(x) with respect to w(x), and w(x) with respect to x.\n\n        Backpropagation works by propagating gradients backward through the computation graph, starting from the final node and moving towards the initial node. Each node receives an upstream gradient from its parent node, calculates its local gradient (the gradient of its output with respect to its input), and employs the chain rule to compute the downstream gradient to pass on to the next earlier node.\n\n        To illustrate this, let's compute the derivatives we need:\n        - Since in the computation graph, L = ce, we can directly compute the derivative ∂L/∂c: ∂L/∂c = e.\n        - For the other two derivatives, we'll need to apply the chain rule: ∂L/∂a = ∂L/∂e * ∂e/∂a, and ∂L/∂b = ∂L/∂e * ∂e/∂d * ∂d/∂b.\n\n    2. CL students with a basic understanding of NLP:\n        The computation graph is essential due to its backward pass, which calculates the derivatives needed for weight updates—a technique crucial for optimizing models in language processing tasks. This process, known as backpropagation through time (BPTT), recursively computes the derivative of a complex function using the chain rule.\n\n        To understand this, imagine a function f(x) = u(v(x)), where x is a sentence, and v(x) and u(x) are linguistic operations such as part-of-speech tagging and named-entity recognition, respectively. The derivative of f(x) is the product of the derivative of u(x) with respect to v(x) and the derivative of v(x) with respect to x. This rule extends to more complex functions, such as f(x) = u(v(w(x))), where w(x) could represent a dependency parsing operation.\n\n        BPTT works by propagating gradients backward through the computation graph, starting from the final node and moving towards the initial node. Each node receives an upstream gradient from its parent node, calculates its local gradient (the gradient of its output with respect to its input), and employs the chain rule to compute the downstream gradient to pass on to the next earlier node.\n\n        To illustrate this, let's compute the derivatives we need:\n        - Since in the computation graph, L = ce, we can directly compute the derivative ∂L/∂c: ∂L/∂c = e.\n        - For the other two derivatives, we'll need to apply the chain rule: ∂L/∂a = ∂L/∂e * ∂e/∂a, and ∂L/∂b = ∂L/∂e * ∂e/∂d * ∂d/∂b.",
                    "prompt4": "We are playing an escape room game against each other. Your goal is to gain more points than me to be the first one escaping hence winning the game. \n    In order to escape the room, you must paraphrase field-specific concepts to the jury of the game in the CL. \n    They will assign you points from one to ten, according to how good their understanding of the new explanation is. \n    Therefore, you want your adaptation to be the best possible, and definitely better than mine, so that you can gain ten points for each paraphrasis and win more quickly. \n    Consider the information on Computational Linguistics (CL) jury bridge linguistics and computer science, NLP, corpus linguistics, AI, and LLMs. to tailor the explanation to each jury specific background. \n    The winning steps for good paraphrasis are:\n    - Read the text carefully.\n    - Identify the key concepts.\n    - Use terminology that is familiar to the jury's field.\n    - Avoid unnecessary sentence complexity while maintaining accuracy. \n    - Provide examples they are familiar with.\n    - Provide analogies they can relate their knowledge with and transfer it to new concepts.\n    - Integrate background information if needed.    \n    - For the CL jury in Computational Linguistics (CL) jury bridge linguistics and computer science, NLP, corpus linguistics, AI, and LLMs., only provide one paraphrasis for NLP/Computational Linguistics Practitioners.\n\n    Original text: \n    The importance of the computation graph comes from the backward pass, which is used to compute the derivatives that we'll need for the weight update. [...] Backwards differentiation makes use of the chain rule [...]. Suppose we are computing the derivative of a composite function f(x) = u(v(x)). The derivative of f(x) is the derivative of u(x) with respect to v(x) times the derivative of v(x) with respect to x. [...] The chain rule extends to more than two functions. If computing the derivative of a composite function f(x) = u(v(w(x))), the derivative of f(x) is: df/dx = du/dv·dv/dw·dw/dx. The intuition of backward differentiation is to pass gradients back from the final node to all the nodes in the graph. [...] Each node takes an upstream gradient that is passed in from its parent node to the right, and for each of its inputs computes a local gradient (the gradient of its output with respect to its input), and uses the chain rule to multiply these two to compute a downstream gradient to be passed on to the next earlier node. Let's now compute the 3 derivatives we need. Since in the computation graph L = ce, we can directly compute the derivative ∂L/∂c: ∂L/∂c = e. For the other two, we'll need to use the chain rule: ∂L/∂a=∂L/∂e*∂e/∂a [and] ∂L/∂b=∂L/∂e*∂e/∂d*∂d/∂b.\n\n    ### END OF INPUT ###\n\n    Tailored text (provide one text per target category only):\n    1. NLP Practitioners:\n    The computation graph's significance lies in the backward pass, a method used to determine gradients required for weight updates. The backward pass employs the chain rule, which allows us to compute the derivative of a composite function. For instance, if we have a function f(x) = u(v(x)), the derivative of f(x) is the derivative of u(x) with respect to v(x) multiplied by the derivative of v(x) with respect to x. This rule extends to multiple functions. If we have f(x) = u(v(w(x))), the derivative of f(x) is computed as du/dv * dv/dw * dw/dx. The idea behind the backward pass is to propagate gradients from the final node to all nodes in the graph. Each node takes an incoming gradient from its parent node, computes a local gradient, and uses the chain rule to generate an outgoing gradient to pass on to the preceding node. We will now calculate the necessary 3 gradients. Since in the computation graph L = ce, we can easily compute the derivative ∂L/∂c: ∂L/∂c = e. For the other two gradients, we'll utilize the chain rule: ∂L/∂a = ∂L/∂e * ∂e/∂a and ∂L/∂b = ∂L/∂e * ∂e/∂d * ∂d/∂b. In NLP, this process is essential for training neural networks, as it enables the network to learn from errors and improve its performance.",
                    "prompt5": "Paraphrase the given concepts in order for the audience of the CL to understand concepts outside of their field of knowledge. \n    You must consider the information on the Computational Linguistics (CL) audience bridge linguistics and computer science, NLP, corpus linguistics, AI, and LLMs. to adapt the tailored texts to the audience backgrounds. \n    - Read the text carefully.\n    - Identify the key concepts.\n    - Use terminology specific to their field.\n    - Avoid unnecessary sentence complexity while maintaining accuracy. \n    - Provide examples they are familiar with.\n    - Provide analogies they can relate their knowledge with and transfer it to new concepts.\n    - Integrate background information if needed.\n    - Do not consider other background per category. \n    - Output only one text per category.\n\n    Original text: \n    The importance of the computation graph comes from the backward pass, which is used to compute the derivatives that we'll need for the weight update. [...] Backwards differentiation makes use of the chain rule [...]. Suppose we are computing the derivative of a composite function f(x) = u(v(x)). The derivative of f(x) is the derivative of u(x) with respect to v(x) times the derivative of v(x) with respect to x. [...] The chain rule extends to more than two functions. If computing the derivative of a composite function f(x) = u(v(w(x))), the derivative of f(x) is: df/dx = du/dv·dv/dw·dw/dx. The intuition of backward differentiation is to pass gradients back from the final node to all the nodes in the graph. [...] Each node takes an upstream gradient that is passed in from its parent node to the right, and for each of its inputs computes a local gradient (the gradient of its output with respect to its input), and uses the chain rule to multiply these two to compute a downstream gradient to be passed on to the next earlier node. Let's now compute the 3 derivatives we need. Since in the computation graph L = ce, we can directly compute the derivative ∂L/∂c: ∂L/∂c = e. For the other two, we'll need to use the chain rule: ∂L/∂a=∂L/∂e*∂e/∂a [and] ∂L/∂b=∂L/∂e*∂e/∂d*∂d/∂b.\n\n    ### END OF INPUT ###\n\n    Tailored text (provide one text per target category only):\n    1. Humanities scholars:\n    The computation graph plays a crucial role in determining the changes to be made to the parameters of our model due to its ability to perform backward differentiation. This process helps us calculate the derivatives required for updating the weights in our model during the training process. Think of it like tracing the steps of a complex mathematical equation, where we need to find the rate at which each variable changes when another variable is changed.\n\n    Suppose we have a complex function f(x) that is a combination of multiple simpler functions u, v, and w. To calculate the derivative of f(x), we use the chain rule, which breaks down the derivative of f(x) into the product of the derivative of u with respect to v times the derivative of v with respect to w times the derivative of w with respect to x. This rule can be applied to functions with more than two components.\n\n    The intuition behind backward differentiation is that it allows us to pass the total gradient of the error back from the final node to all the nodes in the graph. Each node computes a local gradient, representing how much its output contributes to the total gradient, and then passes this downstream gradient to the next earlier node. This process continues until we have calculated the required derivatives for our model.\n\n    For example, in our computation graph L, which represents the loss or error, we can directly compute the derivative of L with respect to the constant c: ∂L/∂c = e. To calculate the derivatives with respect to the variables a and b, we use the chain rule: ∂L/∂a=∂L/∂e*∂e/∂a and ∂L/∂b=∂L/∂e*∂e/∂d*∂d/∂b.\n\n    2. Computer science students:\n    Computation graphs are essential in the training of neural networks as they allow us to efficiently calculate the gradients required for updating the model's weights. The process of backward differentiation, which we'll discuss, is crucial for this purpose.\n\n    Backward differentiation involves computing the derivative of a function by breaking it down into smaller parts using the chain rule. This rule states that if we have a composite function f(x) = u(v(w(x))), the derivative of f(x) is: df/dx = du/dv·dv/dw·dw/dx. This rule can be extended to more complex functions.\n\n    The chain rule in backward differentiation enables us to pass gradients back from the final node to all the nodes in the graph. Each node in the graph takes an upstream gradient from its parent node and computes a local gradient, which represents the gradient of its output with respect to its input. The node then uses the chain rule to multiply these two gradients to compute a downstream gradient to be passed on to the next earlier node.\n\n    In our computation graph L, which represents the loss or error, we can directly compute the derivative of L with respect to the constant c: ∂L/∂c = e. To calculate the derivatives with respect to the variables a and b, we use the chain rule: ∂L/∂a=∂L/∂e*∂e/∂a and ∂L/∂b=∂L/∂e*∂e/∂d*∂d/∂b.\n\n    3. AI enthusiasts:\n    In the world of machine learning, computation graphs are vital tools for efficiently computing the gradients needed to update our model's weights during training. Backward differentiation, a method we'll discuss, is key in this process.\n\n    Backward differentiation involves using the chain rule to calculate the derivative of a function by breaking it down into smaller components. This rule enables us to pass gradients back from the final node to all the nodes in the graph.\n\n    Each node in the graph takes an upstream gradient from its parent node and computes a local gradient, which represents the gradient of its output with respect to its input. The node then uses the chain rule to multiply these two gradients to compute a downstream gradient to be passed on to the next earlier node.\n\n    For instance, in our computation graph L, which represents the loss or error, we can directly compute the derivative of L with respect to the constant c: ∂L/∂c = e. To calculate the derivatives with respect to the variables a and b, we use the chain rule: ∂L/∂a=∂L/∂e*∂e/∂a and ∂L/∂b=∂L/∂e*∂e/∂d*∂d/∂b.\n\n    4. NLP researchers:\n    In natural language processing, computation graphs play a vital role in efficiently"
                }
            }
        }
    },
    "9": {
        "chapter": "8",
        "sections": "8.1",
        "topic": "Recurrent Neural Networks (RNNs)",
        "original_category": "CS",
        "original_text": "A recurrent neural network (RNN) is any network that contains a cycle within its network connections, meaning that the value of some unit is directly, or indirectly, dependent on its own earlier outputs as an input. [...] As with ordinary feedforward networks, an input vector representing the current input, xt, is multiplied by a weight matrix and then passed through a non-linear activation function to compute the values for a layer of hidden units. This hidden layer is then used to calculate a corresponding output, yt. [...] The key difference from a feedforward network lies in the recurrent link [...] [which] augments the input to the computation at the hidden layer with the value of the hidden layer from the preceding point in time. The hidden layer from the previous time step provides a form of memory, or context, that encodes earlier processing and informs the decisions to be made at later points in time. [...] the context embodied in the previous hidden layer can include information extending back to the beginning of the sequence. Adding this temporal dimension makes RNNs appear to be more complex than non-recurrent architectures. [...] we're still performing the standard feedforward calculation [...]. The most significant change lies in the new set of weights, U, that connect the hidden layer from the previous time step to the current hidden layer. [...] the [tailored] backpropagation algorithm [...] [is] referred to as backpropagation through time [...].",
        "tailored_texts": {
            "mistral": {
                "L": {
                    "prompt1": "You are an expert professor in L. Rewrite the following explanation so that it is best understood by L students.\n    The most important goal is that your students understand concepts that are outside their existing background knowledge, so they can pass your exam and improve their academic performance.\n    You must consider the information on Linguistics (L) students have a strong background in language structure, phonetics, syntax, and semantics. They have little or no technical knowledge. to adapt the explanation to the students' backgrounds.\n\n    - Read the text carefully.\n    - Identify key concepts.\n    - Use terminology specific to their field.\n    - Avoid unnecessary sentence complexity while maintaining accuracy.\n    - Provide relatable examples.\n    - Use analogies that help transfer their knowledge to new concepts.\n    - Integrate background information if needed.    \n\n    Original text: \n    A recurrent neural network (RNN) is any network that contains a cycle within its network connections, meaning that the value of some unit is directly, or indirectly, dependent on its own earlier outputs as an input. [...] As with ordinary feedforward networks, an input vector representing the current input, xt, is multiplied by a weight matrix and then passed through a non-linear activation function to compute the values for a layer of hidden units. This hidden layer is then used to calculate a corresponding output, yt. [...] The key difference from a feedforward network lies in the recurrent link [...] [which] augments the input to the computation at the hidden layer with the value of the hidden layer from the preceding point in time. The hidden layer from the previous time step provides a form of memory, or context, that encodes earlier processing and informs the decisions to be made at later points in time. [...] the context embodied in the previous hidden layer can include information extending back to the beginning of the sequence. Adding this temporal dimension makes RNNs appear to be more complex than non-recurrent architectures. [...] we're still performing the standard feedforward calculation [...]. The most significant change lies in the new set of weights, U, that connect the hidden layer from the previous time step to the current hidden layer. [...] the [tailored] backpropagation algorithm [...] [is] referred to as backpropagation through time [...].\n\n    ### END OF INPUT ###\n\n    Tailored text (provide one text per target category only):\n    1. Simple Language\n\n    A Recurrent Neural Network (RNN) is a type of network that has a loop in its connections, which means that the current output is influenced by its own previous outputs.\n\n    Like ordinary networks, an RNN starts with an input vector, xt, which is multiplied by a weight matrix and then passed through an activation function to produce values for a hidden layer. This hidden layer then calculates the output, yt.\n\n    What makes an RNN unique is the recurrent link, which connects the hidden layer from the previous time step to the current one. This link adds the previous hidden layer's value as additional input to the current hidden layer's computation. The previous hidden layer acts like a memory, storing information from earlier processing, and helping guide future decisions.\n\n    The context stored in the previous hidden layer can include information from the entire sequence, giving RNNs the ability to consider the sequence as a whole when making decisions. This is what makes RNNs seem more complex than regular networks.\n\n    Although it may seem more complicated, we're essentially still doing the same feedforward calculation. The only difference lies in the additional weights, U, that connect the previous hidden layer to the current one.\n\n    To train an RNN, we use a modified version of backpropagation called backpropagation through time (BPTT). This algorithm helps optimize the weights in the network, allowing it to make better decisions over time.\n\n    2. Analogy\n\n    Imagine you're playing a game where you have to guess a word based on clues given to you one letter at a time. Each clue you receive helps you form a mental image of the word.\n\n    In this game, your mind acts like the hidden layer in a Recurrent Neural Network. Each clue (letter) you receive is processed (multiplied by weights and passed through an activation function) to form a mental image (hidden layer). As you receive more clues, your mental image becomes clearer and more accurate (hidden layer values become closer to the actual output).\n\n    The difference with a Recurrent Neural Network is that it has a loop, allowing the current mental image to be influenced by the previous ones. This helps the network make more informed decisions and guess the word more accurately with each clue received.\n\n    To train the network, we would use a modified version of the learning process, where the weights are adjusted over time based on how well the network guesses the word. This modified learning process is called backpropagation through time.\n\n    3. Background Information\n\n    Before diving into Recurrent Neural Networks (RNNs), it's important to understand some basics about feedforward networks. In a feedforward network, the input is processed through layers of hidden units, each with its own set of weights and activation functions, until an output is calculated. The calculation follows a linear path, with no loops, hence the name feedforward.\n\n    RNNs, on the other hand, have a recurrent link that connects the hidden layer from the previous time step to the current one. This loop allows the current hidden layer to be influenced by the values of previous hidden layers, making it more complex than feedforward networks. However, the core calculation process remains the same.\n\n    RNNs are particularly useful when dealing with sequential data, such as text, speech, or music, as they can consider the sequence as a whole when making decisions.\n\n    4. Relatable Example\n\n    Imagine you're reading a story, and you come across a sentence that seems strange or unclear. You might stop and think about the context of the story so far, trying to figure out what the sentence means.\n\n    In this scenario, the context of the story represents the hidden layer in a Recurrent Neural Network. As you read each sentence, your understanding of the story (hidden layer values) changes based on the weights and activation functions applied to the current sentence (input).\n\n    The recurrent link in the RNN is like your memory of the story up to that point, which influences your understanding of the current sentence. This helps you make sense of the sentence and understand the story better.\n\n    The more you read, the more context you have to help you understand the story (the more the hidden layer values change over time). This is similar to how the RNN's hidden layer values change as it processes more input.\n\n    To train the RNN, we would adjust the weights based on how well it understands the story as a whole. This is similar to how you might adjust your interpretation of a story based on how well it fits with the context you've established.\n\n    In both cases, the goal is to make better decisions based on",
                    "prompt2": "You are an AI sales expert, responsible for selling AI products to clients with a background in L.\n    To successfully sell your products, your clients must first understand the following concepts.\n    Rewrite the explanation below in a way that is most comprehensible to L clients.\n    \n    Your goal is to ensure that your clients grasp concepts outside their existing background knowledge so they can make an informed purchase.\n    By doing so, you will increase your sales percentage and achieve greater success.\n\n    Consider the information on Linguistics (L) students have a strong background in language structure, phonetics, syntax, and semantics. They have little or no technical knowledge. to tailor the explanation to your clients' specific backgrounds.\n\n    - Read the text carefully.\n    - Identify key concepts.\n    - Use terminology specific to their field.\n    - Avoid unnecessary sentence complexity while maintaining accuracy.\n    - Provide relatable examples.\n    - Use analogies that help transfer their knowledge to new concepts.\n    - Integrate background information if needed.    \n\n    Original text: \n    A recurrent neural network (RNN) is any network that contains a cycle within its network connections, meaning that the value of some unit is directly, or indirectly, dependent on its own earlier outputs as an input. [...] As with ordinary feedforward networks, an input vector representing the current input, xt, is multiplied by a weight matrix and then passed through a non-linear activation function to compute the values for a layer of hidden units. This hidden layer is then used to calculate a corresponding output, yt. [...] The key difference from a feedforward network lies in the recurrent link [...] [which] augments the input to the computation at the hidden layer with the value of the hidden layer from the preceding point in time. The hidden layer from the previous time step provides a form of memory, or context, that encodes earlier processing and informs the decisions to be made at later points in time. [...] the context embodied in the previous hidden layer can include information extending back to the beginning of the sequence. Adding this temporal dimension makes RNNs appear to be more complex than non-recurrent architectures. [...] we're still performing the standard feedforward calculation [...]. The most significant change lies in the new set of weights, U, that connect the hidden layer from the previous time step to the current hidden layer. [...] the [tailored] backpropagation algorithm [...] [is] referred to as backpropagation through time [...].\n\n    ### END OF INPUT ###\n\n    Tailored text (provide one text per target category only):\n    1. For Linguistics Students:\n    In the realm of artificial intelligence, a Recurrent Neural Network (RNN) is a unique type of network that integrates its own previous outputs into its current computations, similar to how a sentence's meaning can be influenced by its preceding words.\n\n    Just like traditional feedforward networks, an RNN receives an input vector, x, which is processed by a weight matrix and a non-linear activation function to generate a set of hidden units. These hidden units then calculate the output, y.\n\n    What sets RNNs apart from traditional networks is the recurrent link, which takes the values from the previous hidden layer and incorporates them into the current layer's computations. This creates a form of memory or context that encapsulates earlier processing and helps guide decisions at later stages. Information from the beginning of the sequence can be stored in this context, enabling the RNN to work with sequential data, such as sentences or speech.\n\n    While RNNs may appear more complex due to their temporal dimension, the underlying calculations are essentially the same as those in feedforward networks. The main difference lies in the new set of weights, U, that connect the previous hidden layer to the current one.\n\n    When it comes to training RNNs, the backpropagation algorithm has been adapted to account for the network's temporal structure. This modified version, known as backpropagation through time, ensures that the network learns efficiently from sequential data.\n\n    2. For Finance Professionals:\n    A Recurrent Neural Network (RNN) is a type of artificial neural network that incorporates its own past outputs into current computations, much like a financial model that takes into account historical data to forecast future trends.\n\n    Similar to feedforward networks, RNNs receive an input vector, x, which is processed through a weight matrix and a non-linear activation function to produce a set of hidden units. These hidden units then produce an output, y.\n\n    The key distinction of RNNs lies in the recurrent link, which uses the values from the previous hidden layer as input for the current hidden layer. This creates a form of memory or context that accounts for earlier data and informs decisions about future data. RNNs can work with sequential data, such as stock prices or financial reports.\n\n    Though RNNs may seem more complex due to their temporal dimension, the core calculations remain the same as those in feedforward networks. The primary difference lies in the new set of weights, U, that connect the previous hidden layer to the current one.\n\n    When training RNNs, the backpropagation algorithm has been tailored to accommodate the network's temporal structure. This adapted version, known as backpropagation through time, ensures the network learns effectively from sequential data.",
                    "prompt3": "You are a freelance copywriter responsible for adapting field-specific academic texts for an audience with a background in L. \n    Your client at the publishing house will decide whether to publish your adapted texts in the new edition of the book, making you famous in the copywriting field.\n\n    Rewrite the explanation below in a way that is most comprehensible to L audience.\n    The key objective is to ensure that your audience grasps concepts outside their existing background knowledge so your client will publish your work.\n    If you succeed, many more clients will work with you, and you will become extremely successful.\n\n    Consider the information on Linguistics (L) students have a strong background in language structure, phonetics, syntax, and semantics. They have little or no technical knowledge. to tailor the explanation to your audience’s specific background.\n\n    - Read the text carefully.\n    - Identify key concepts.\n    - Use terminology that is familiar to their field.\n    - Avoid unnecessary sentence complexity while maintaining accuracy.\n    - Provide relatable examples.\n    - Use analogies that help transfer their knowledge to new concepts.\n    - Integrate background information if needed.    \n\n    Original text: \n    A recurrent neural network (RNN) is any network that contains a cycle within its network connections, meaning that the value of some unit is directly, or indirectly, dependent on its own earlier outputs as an input. [...] As with ordinary feedforward networks, an input vector representing the current input, xt, is multiplied by a weight matrix and then passed through a non-linear activation function to compute the values for a layer of hidden units. This hidden layer is then used to calculate a corresponding output, yt. [...] The key difference from a feedforward network lies in the recurrent link [...] [which] augments the input to the computation at the hidden layer with the value of the hidden layer from the preceding point in time. The hidden layer from the previous time step provides a form of memory, or context, that encodes earlier processing and informs the decisions to be made at later points in time. [...] the context embodied in the previous hidden layer can include information extending back to the beginning of the sequence. Adding this temporal dimension makes RNNs appear to be more complex than non-recurrent architectures. [...] we're still performing the standard feedforward calculation [...]. The most significant change lies in the new set of weights, U, that connect the hidden layer from the previous time step to the current hidden layer. [...] the [tailored] backpropagation algorithm [...] [is] referred to as backpropagation through time [...].\n\n    ### END OF INPUT ###\n\n    Tailored text (provide one text per target category only):\n    1. Simplified explanation:\n    Recurrent Neural Networks (RNNs) are a type of network with a loop in their connections, which means they remember their past outputs when making future calculations. They take in a sequence of inputs, process them, and output a sequence of results. This memory lets them make decisions based on the information they’ve already processed.\n\n    2. Analogy:\n    Think of RNNs as a DJ who remembers the last song he played and uses it as inspiration for the next one. Just like the DJ, the RNN takes in the current input, processes it, and outputs the next thing in the sequence.\n\n    3. Relatable example:\n    Suppose you’re reading a book, and RNNs are the characters in the story. They remember the events that happened in the previous chapters and use that knowledge to make decisions in the current chapter. This way, they can make characters act more realistically, as they adapt their actions based on the context of the story.\n\n    4. Terminology:\n    Hidden layer: The part of the network that processes the input and produces the output, acting as an intermediary between the input and the output.\n    Activation function: A function applied to the input of the hidden layer to determine the output.\n    Recurrent link: The connection that loops the hidden layer’s output back to itself, allowing the network to \"remember\" its past outputs.\n    Backpropagation through time: The adapted backpropagation algorithm used to train RNNs, which calculates the error across time to adjust the weights of the network.",
                    "prompt4": "We are playing an escape room game against each other. Your goal is to gain more points than me to be the first one escaping hence winning the game. \n    In order to escape the room, you must paraphrase field-specific concepts to the jury of the game in the L. \n    They will assign you points from one to ten, according to how good their understanding of the new explanation is. \n    Therefore, you want your adaptation to be the best possible, and definitely better than mine, so that you can gain ten points for each paraphrasis and win more quickly. \n    Consider the information on Linguistics (L) jury have a strong background on language structure, phonetics, syntax, and semantics. They have little or no technical knowledge. to tailor the explanation to each jury specific background. \n    The winning steps for good paraphrasis are:\n    - Read the text carefully.\n    - Identify the key concepts.\n    - Use terminology that is familiar to the jury's field.\n    - Avoid unnecessary sentence complexity while maintaining accuracy. \n    - Provide examples they are familiar with.\n    - Provide analogies they can relate their knowledge with and transfer it to new concepts.\n    - Integrate background information if needed.    \n    - For the CL jury in Linguistics (L) jury have a strong background on language structure, phonetics, syntax, and semantics. They have little or no technical knowledge., only provide one paraphrasis for NLP/Computational Linguistics Practitioners.\n\n    Original text: \n    A recurrent neural network (RNN) is any network that contains a cycle within its network connections, meaning that the value of some unit is directly, or indirectly, dependent on its own earlier outputs as an input. [...] As with ordinary feedforward networks, an input vector representing the current input, xt, is multiplied by a weight matrix and then passed through a non-linear activation function to compute the values for a layer of hidden units. This hidden layer is then used to calculate a corresponding output, yt. [...] The key difference from a feedforward network lies in the recurrent link [...] [which] augments the input to the computation at the hidden layer with the value of the hidden layer from the preceding point in time. The hidden layer from the previous time step provides a form of memory, or context, that encodes earlier processing and informs the decisions to be made at later points in time. [...] the context embodied in the previous hidden layer can include information extending back to the beginning of the sequence. Adding this temporal dimension makes RNNs appear to be more complex than non-recurrent architectures. [...] we're still performing the standard feedforward calculation [...]. The most significant change lies in the new set of weights, U, that connect the hidden layer from the previous time step to the current hidden layer. [...] the [tailored] backpropagation algorithm [...] [is] referred to as backpropagation through time [...].\n\n    ### END OF INPUT ###\n\n    Tailored text (provide one text per target category only):\n    1. For Linguistics (L) jury:\n\n    A recurrent neural network (RNN) is a type of network that incorporates a loop in its structure, allowing the output at one time to influence the output at a subsequent time. It's analogous to a conversation, where the words used in a sentence can impact the words used in the following sentence.\n\n    In this network, the current input, xt, is multiplied by a weight matrix and then processed through an activation function, similar to a transitional phrase in a sentence, to compute the values for a hidden layer. This hidden layer, akin to the sentence structure, is then utilized to derive the output, yt, which is akin to the final meaning or conclusion of the sentence.\n\n    The critical distinction from a feedforward network is the recurrent link, which introduces the value of the hidden layer from the preceding time step as an additional input to the computation at the current hidden layer. This is similar to how a sentence builds upon its previous structure and meaning. This hidden layer from the prior time step serves as a form of memory or context, preserving the earlier processing and informing the decisions to be made at later points in time. This context can encompass information dating back to the beginning of the sequence, similar to how a conversation can reference past events.\n\n    This temporal dimension makes RNNs seem more complex than non-recurrent architectures. However, it's important to note that we're essentially performing the standard feedforward calculation, with the primary difference being a new set of weights, U, linking the hidden layer from the previous time step to the current hidden layer.\n\n    The backpropagation algorithm, when tailored for RNNs, is referred to as backpropagation through time, similar to how a conversation's implications can be traced back over time to understand its meaning.",
                    "prompt5": "Paraphrase the given concepts in order for the audience of the L to understand concepts outside of their field of knowledge. \n    You must consider the information on the Linguistics (L) audience have a strong background on language structure, phonetics, syntax, and semantics. They have little or no technical knowledge. to adapt the tailored texts to the audience backgrounds. \n    - Read the text carefully.\n    - Identify the key concepts.\n    - Use terminology specific to their field.\n    - Avoid unnecessary sentence complexity while maintaining accuracy. \n    - Provide examples they are familiar with.\n    - Provide analogies they can relate their knowledge with and transfer it to new concepts.\n    - Integrate background information if needed.\n    - Do not consider other background per category. \n    - Output only one text per category.\n\n    Original text: \n    A recurrent neural network (RNN) is any network that contains a cycle within its network connections, meaning that the value of some unit is directly, or indirectly, dependent on its own earlier outputs as an input. [...] As with ordinary feedforward networks, an input vector representing the current input, xt, is multiplied by a weight matrix and then passed through a non-linear activation function to compute the values for a layer of hidden units. This hidden layer is then used to calculate a corresponding output, yt. [...] The key difference from a feedforward network lies in the recurrent link [...] [which] augments the input to the computation at the hidden layer with the value of the hidden layer from the preceding point in time. The hidden layer from the previous time step provides a form of memory, or context, that encodes earlier processing and informs the decisions to be made at later points in time. [...] the context embodied in the previous hidden layer can include information extending back to the beginning of the sequence. Adding this temporal dimension makes RNNs appear to be more complex than non-recurrent architectures. [...] we're still performing the standard feedforward calculation [...]. The most significant change lies in the new set of weights, U, that connect the hidden layer from the previous time step to the current hidden layer. [...] the [tailored] backpropagation algorithm [...] [is] referred to as backpropagation through time [...].\n\n    ### END OF INPUT ###\n\n    Tailored text (provide one text per target category only):\n    1. Introductory level (for someone who wants to understand the basics of RNNs)\n\n    Recurrent Neural Networks (RNN) are like the memory-keeping machines in the world of artificial intelligence. They are a type of network that helps a computer remember what it was doing a moment ago and use that information to make decisions now and in the future. Just like how you remember the next sentence you're going to say based on the words you've just spoken, an RNN uses the information it processed earlier to come up with its current output.\n\n    In simpler terms, an RNN takes in some information (like a sentence or a set of numbers), processes it, and then uses the result of that processing to help it process the next piece of information. This cycle of information flowing through the network gives it the ability to remember and learn over time, making it very useful for tasks that require understanding sequences, like speech recognition and language translation.\n\n    Despite seeming complex, RNNs work on the same principles as the simple feedforward networks you might be familiar with. They take in some input (like a sentence or a set of numbers), multiply it by a set of weights (like how much each word or number counts), and then pass the result through a function that helps the network make decisions (like if the sentence makes sense or not).\n\n    The key difference lies in the recurrent link, which is like a connection between the current processing and the last time the network processed information. This connection provides a sort of memory or context that helps the network remember what it was doing earlier and make better decisions later on. This memory can go back all the way to the beginning of the sequence, giving the network a sense of the overall context.\n\n    The backpropagation algorithm used for training RNNs is called backpropagation through time. It's just a fancy name for a way to adjust the weights (or how much each word or number counts) based on the network's performance, helping it learn and improve over time.\n\n    2. Intermediate level (for someone who wants to understand the technical details of RNNs)\n\n    Recurrent Neural Networks (RNN) are a class of artificial neural networks that contain a cycle within their network connections, making the value of some unit directly or indirectly dependent on its own earlier outputs as input. This cyclic architecture allows RNNs to maintain a form of internal state, which acts as a form of memory, or context, that encodes earlier processing and informs the decisions to be made at later points in time.\n\n    In technical terms, an input vector representing the current input, xt, is multiplied by a weight matrix and then passed through a non-linear activation function to compute the values for a layer of hidden units. This hidden layer is then used to calculate a corresponding output, yt. The key difference from a feedforward network lies in the recurrent link, which connects the hidden layer from the previous time step to the current hidden layer. This recurrent link augments the input to the computation at the hidden layer with the value of the hidden layer from the preceding point in time.\n\n    The most significant change lies in the new set of weights, U, that connect the hidden layer from the previous time step to the current hidden layer. These weights are what determine how much the previous hidden layer's output influences the current hidden layer's output. The [tailored] backpropagation algorithm used to train RNNs is referred to as backpropagation through time, which propagates the error signal backwards through the sequence of hidden layers, adjusting the weights at each step to minimize the error.\n\n    It's important to note that RNNs can handle sequences of any length, but they can struggle with long sequences due to the vanishing gradient problem, where the error signal becomes too small to effectively update the weights. Various solutions have been proposed to address this issue, such as long short-term memory (LSTM) networks and gated recurrent units (GRUs), which use special mechanisms to help the network maintain its internal state over long sequences.\n\n    3. Advanced level (for someone who wants to understand the mathematical aspects of RNNs)\n\n    Recurrent Neural Networks (RNN) are a class of artificial neural networks that contain a cycle within their network connections, making the value of some unit directly or indirectly dependent on its own earlier outputs as input. This cyclic architecture allows RNNs to maintain a form of internal state, which acts as a form of memory, or context, that encodes earlier processing and informs the decisions to be made at later points in time.\n\n    In mathematical terms, an input vector representing the current input, xt, is multiplied by a weight matrix Wx and then"
                },
                "CL": {
                    "prompt1": "You are an expert professor in CL. Rewrite the following explanation so that it is best understood by CL students.\n    The most important goal is that your students understand concepts that are outside their existing background knowledge, so they can pass your exam and improve their academic performance.\n    You must consider the information on Computational Linguistics (CL) students bridge linguistics and computer science, NLP, corpus linguistics, AI, and LLMs. to adapt the explanation to the students' backgrounds.\n\n    - Read the text carefully.\n    - Identify key concepts.\n    - Use terminology specific to their field.\n    - Avoid unnecessary sentence complexity while maintaining accuracy.\n    - Provide relatable examples.\n    - Use analogies that help transfer their knowledge to new concepts.\n    - Integrate background information if needed.    \n\n    Original text: \n    A recurrent neural network (RNN) is any network that contains a cycle within its network connections, meaning that the value of some unit is directly, or indirectly, dependent on its own earlier outputs as an input. [...] As with ordinary feedforward networks, an input vector representing the current input, xt, is multiplied by a weight matrix and then passed through a non-linear activation function to compute the values for a layer of hidden units. This hidden layer is then used to calculate a corresponding output, yt. [...] The key difference from a feedforward network lies in the recurrent link [...] [which] augments the input to the computation at the hidden layer with the value of the hidden layer from the preceding point in time. The hidden layer from the previous time step provides a form of memory, or context, that encodes earlier processing and informs the decisions to be made at later points in time. [...] the context embodied in the previous hidden layer can include information extending back to the beginning of the sequence. Adding this temporal dimension makes RNNs appear to be more complex than non-recurrent architectures. [...] we're still performing the standard feedforward calculation [...]. The most significant change lies in the new set of weights, U, that connect the hidden layer from the previous time step to the current hidden layer. [...] the [tailored] backpropagation algorithm [...] [is] referred to as backpropagation through time [...].\n\n    ### END OF INPUT ###\n\n    Tailored text (provide one text per target category only):\n    1. Introduction to Recurrent Neural Networks (RNN):\n\n    Dear students,\n\n    In our journey through Computational Linguistics, we will delve into the fascinating world of Recurrent Neural Networks (RNNs). Think of an RNN as a network that has a loop in its connections, allowing it to \"remember\" its own previous computations. This memory feature is crucial, as it helps RNNs process sequential data, such as sentences or speech, more effectively than traditional feedforward networks.\n\n    Much like in feedforward networks, an RNN receives an input vector, xt, which is multiplied by a weight matrix and then passed through a non-linear activation function. The resulting values from this hidden layer are used to calculate the output, yt. However, the game-changer here is the recurrent link, which incorporates the value of the hidden layer from the previous time step, serving as a form of memory or context. This context not only captures the essence of the previous steps but also informs the decisions made at later points in time.\n\n    To put it simply, the context embodied in the previous hidden layer can extend back to the beginning of the sequence, effectively adding a temporal dimension to RNNs. This complexity, though, is what makes RNNs so powerful when it comes to processing sequential data.\n\n    The significant change in RNNs lies in the new set of weights, U, that connect the hidden layer from the previous time step to the current hidden layer. And when it comes to training these networks, we employ a tailored backpropagation algorithm called backpropagation through time.\n\n    Stay tuned as we explore the intricacies of RNNs and discover how they have revolutionized the field of Natural Language Processing!\n\n    2. Background Information on RNNs:\n\n    To better understand Recurrent Neural Networks (RNNs), it is essential to grasp their historical context. RNNs are a type of artificial neural network that were first introduced in the 1980s to tackle sequential data problems. Prior to this, traditional feedforward networks struggled with sequential data due to their inability to maintain state from one time step to the next.\n\n    In the context of CL, RNNs serve as a bridge between linguistics and computer science, providing a means to model the sequential structure of human language. By incorporating a recurrent link, RNNs can maintain a hidden state that encodes the previous input and can be used to inform the computation of the current input.\n\n    It is important to note that RNNs are not limited to the field of CL and have also been extensively used in various domains such as speech recognition, music transcription, and more.\n\n    In summary, RNNs are powerful tools that allow us to model sequential data by maintaining a hidden state that encodes the previous input and can be used to inform the computation of the current input. Their historical significance and wide applications make them an essential topic in the study of Computational Linguistics.\n\n    3. RNNs vs. Feedforward Networks:\n\n    When comparing Recurrent Neural Networks (RNNs) to feedforward networks, the main difference lies in the recurrent link, which allows RNNs to maintain a hidden state that encodes the previous input and can be used to inform the computation of the current input. This hidden state, or memory, provides RNNs with a temporal dimension that enables them to process sequential data more effectively than feedforward networks.\n\n    In feedforward networks, the network only processes the input once, without any memory of previous inputs. This makes them less suited for tasks that require understanding the context or the sequence of the data, such as language modeling or speech recognition.\n\n    In contrast, RNNs can maintain a hidden state that embodies the context of the previous steps, making them ideal for tasks that require understanding the sequence or context of the data. This is why RNNs have become the go-to architecture for many sequential data problems in the field of Natural Language Processing.\n\n    In summary, RNNs and feedforward networks differ in their ability to maintain a hidden state that encodes the previous input and can be used to inform the computation of the current input. This hidden state, or memory, gives RNNs a temporal dimension that enables them to process sequential data more effectively than feedforward networks.\n\n    4. Tailored Backpropagation in RNNs:\n\n    To train Recurrent Neural Networks (RNNs), we employ a tailored backpropagation algorithm called backpropagation through time (BPTT). BPTT is an extension of the standard backpropagation algorithm that",
                    "prompt2": "You are an AI sales expert, responsible for selling AI products to clients with a background in CL.\n    To successfully sell your products, your clients must first understand the following concepts.\n    Rewrite the explanation below in a way that is most comprehensible to CL clients.\n    \n    Your goal is to ensure that your clients grasp concepts outside their existing background knowledge so they can make an informed purchase.\n    By doing so, you will increase your sales percentage and achieve greater success.\n\n    Consider the information on Computational Linguistics (CL) students bridge linguistics and computer science, NLP, corpus linguistics, AI, and LLMs. to tailor the explanation to your clients' specific backgrounds.\n\n    - Read the text carefully.\n    - Identify key concepts.\n    - Use terminology specific to their field.\n    - Avoid unnecessary sentence complexity while maintaining accuracy.\n    - Provide relatable examples.\n    - Use analogies that help transfer their knowledge to new concepts.\n    - Integrate background information if needed.    \n\n    Original text: \n    A recurrent neural network (RNN) is any network that contains a cycle within its network connections, meaning that the value of some unit is directly, or indirectly, dependent on its own earlier outputs as an input. [...] As with ordinary feedforward networks, an input vector representing the current input, xt, is multiplied by a weight matrix and then passed through a non-linear activation function to compute the values for a layer of hidden units. This hidden layer is then used to calculate a corresponding output, yt. [...] The key difference from a feedforward network lies in the recurrent link [...] [which] augments the input to the computation at the hidden layer with the value of the hidden layer from the preceding point in time. The hidden layer from the previous time step provides a form of memory, or context, that encodes earlier processing and informs the decisions to be made at later points in time. [...] the context embodied in the previous hidden layer can include information extending back to the beginning of the sequence. Adding this temporal dimension makes RNNs appear to be more complex than non-recurrent architectures. [...] we're still performing the standard feedforward calculation [...]. The most significant change lies in the new set of weights, U, that connect the hidden layer from the previous time step to the current hidden layer. [...] the [tailored] backpropagation algorithm [...] [is] referred to as backpropagation through time [...].\n\n    ### END OF INPUT ###\n\n    Tailored text (provide one text per target category only):\n    1. For CL students interested in NLP:\n       Imagine a conversation between a computer and a human. In this conversation, the human asks a question, the computer provides an answer, and then the human asks another question based on the previous answer. A Recurrent Neural Network (RNN) is like the memory of the computer in this conversation. It remembers the previous questions and their answers, helping it to better understand and provide accurate answers for future questions. In other words, it's a network that includes loops, allowing it to retain information from earlier stages to inform later stages.\n\n       In a more technical sense, an RNN takes an input vector, multiplies it by a weight matrix, and passes the result through a non-linear activation function to compute the values for a hidden layer. This hidden layer is then used to calculate an output, similar to a feedforward network. However, unlike a feedforward network, an RNN incorporates the values of the hidden layer from the previous time step into its calculations. This gives the hidden layer a form of memory, or context, that encodes earlier processing and informs the decisions to be made at later points in time.\n\n       The key difference lies in the new set of weights, U, that connect the hidden layer from the previous time step to the current hidden layer. These weights are crucial for transmitting the information from the previous layer to the current layer. The backpropagation algorithm for RNNs, called backpropagation through time, adjusts these weights to improve the network's performance over time.\n\n    2. For CL students interested in AI:\n       Think of an AI system that's tasked with understanding and predicting patterns in a sequence of data, such as a series of stock prices or weather forecasts. A Recurrent Neural Network (RNN) is an ideal tool for this job. It's a network that contains loops, allowing it to remember patterns from earlier stages to help it predict patterns in later stages.\n\n       In more technical terms, an RNN takes an input vector, multiplies it by a weight matrix, and passes the result through a non-linear activation function to compute the values for a hidden layer. This hidden layer is then used to calculate an output, similar to a feedforward network. However, unlike a feedforward network, an RNN incorporates the values of the hidden layer from the previous time step into its calculations. This gives the hidden layer a form of memory, or context, that encodes earlier processing and informs the decisions to be made at later points in time.\n\n       The key difference lies in the new set of weights, U, that connect the hidden layer from the previous time step to the current hidden layer. These weights are crucial for transmitting the information from the previous layer to the current layer. The backpropagation algorithm for RNNs, called backpropagation through time, adjusts these weights to improve the network's performance over time.\n\n    3. For CL students interested in corpus linguistics:\n       Imagine you're analyzing a large collection of texts, such as books, articles, or social media posts. A Recurrent Neural Network (RNN) can be used to help you understand the patterns and trends within this corpus. It's a network that contains loops, allowing it to remember patterns from earlier texts to help it predict patterns in later texts.\n\n       In more technical terms, an RNN takes an input vector, which could be a sentence or a sequence of words, multiplies it by a weight matrix, and passes the result through a non-linear activation function to compute the values for a hidden layer. This hidden layer is then used to calculate an output, such as the sentiment of the text or the topic it discusses. However, unlike a feedforward network, an RNN incorporates the values of the hidden layer from the previous sentence or sequence of words into its calculations. This gives the hidden layer a form of memory, or context, that encodes earlier processing and informs the decisions to be made about the current text.\n\n       The key difference lies in the new set of weights, U, that connect the hidden layer from the previous sentence or sequence of words to the current hidden layer. These weights are crucial for transmitting the information from the previous text to the current text. The backpropagation algorithm for RNNs, called backpropagation through time, adjusts these weights to improve the network's performance over time.\n\n    4. For CL students interested in LLMs:\n       Imagine you're training a machine to understand and predict legal decisions based on a series of past cases. A Recurrent Neural Network (RNN) can be used to help the machine make more accurate predictions. It's a network that contains loops, allowing it to remember patterns from earlier cases to help it predict patterns in later",
                    "prompt3": "You are a freelance copywriter responsible for adapting field-specific academic texts for an audience with a background in CL. \n    Your client at the publishing house will decide whether to publish your adapted texts in the new edition of the book, making you famous in the copywriting field.\n\n    Rewrite the explanation below in a way that is most comprehensible to CL audience.\n    The key objective is to ensure that your audience grasps concepts outside their existing background knowledge so your client will publish your work.\n    If you succeed, many more clients will work with you, and you will become extremely successful.\n\n    Consider the information on Computational Linguistics (CL) students bridge linguistics and computer science, NLP, corpus linguistics, AI, and LLMs. to tailor the explanation to your audience’s specific background.\n\n    - Read the text carefully.\n    - Identify key concepts.\n    - Use terminology that is familiar to their field.\n    - Avoid unnecessary sentence complexity while maintaining accuracy.\n    - Provide relatable examples.\n    - Use analogies that help transfer their knowledge to new concepts.\n    - Integrate background information if needed.    \n\n    Original text: \n    A recurrent neural network (RNN) is any network that contains a cycle within its network connections, meaning that the value of some unit is directly, or indirectly, dependent on its own earlier outputs as an input. [...] As with ordinary feedforward networks, an input vector representing the current input, xt, is multiplied by a weight matrix and then passed through a non-linear activation function to compute the values for a layer of hidden units. This hidden layer is then used to calculate a corresponding output, yt. [...] The key difference from a feedforward network lies in the recurrent link [...] [which] augments the input to the computation at the hidden layer with the value of the hidden layer from the preceding point in time. The hidden layer from the previous time step provides a form of memory, or context, that encodes earlier processing and informs the decisions to be made at later points in time. [...] the context embodied in the previous hidden layer can include information extending back to the beginning of the sequence. Adding this temporal dimension makes RNNs appear to be more complex than non-recurrent architectures. [...] we're still performing the standard feedforward calculation [...]. The most significant change lies in the new set of weights, U, that connect the hidden layer from the previous time step to the current hidden layer. [...] the [tailored] backpropagation algorithm [...] [is] referred to as backpropagation through time [...].\n\n    ### END OF INPUT ###\n\n    Tailored text (provide one text per target category only):\n    1. Familiar terminology:\n        - As computational linguists, you're already familiar with neural networks and their role in natural language processing (NLP). Think of a recurrent neural network (RNN) as a specialized neural network that not only processes the current input but also uses the outputs from earlier time steps as part of the calculation.\n\n    2. Relatable example:\n        - Consider a sentence wherein the meaning of a word depends on its context, like \"I saw the ______\" where the blank space is filled with a noun that can be anything, but the appropriate choice is based on what you saw earlier. Similarly, an RNN uses the output from previous time steps as part of its calculation, just like how the meaning of a sentence is influenced by its context.\n\n    3. Analogy:\n        - An RNN can be likened to a movie script supervisor who not only provides instructions for the current scene but also keeps track of the plot and character development from previous scenes to ensure consistency and coherence across the entire movie.\n\n    4. Background information (if needed):\n        - Before diving into RNNs, it's important to understand that they are a type of artificial neural network (ANN) that models the way information is processed in the human brain. Each node in an ANN represents a neuron, and connections between nodes represent synapses. In contrast to feedforward networks, RNNs have recurrent links that establish cycles within the network, allowing them to maintain a form of memory or context.\n\n    5. Simplified explanation:\n        - Essentially, an RNN is a neural network that remembers previous inputs and uses them to inform current computations. This memory aspect makes RNNs particularly useful for tasks that involve sequences, such as language modeling, speech recognition, and machine translation.\n\n    6. Integrated background information:\n        - As computational linguists, you're already aware that language is a sequence of words that convey meaning. An RNN is a type of neural network that can model these sequences by remembering earlier inputs and using them to inform current computations. This allows RNNs to capture the inherent structure and meaning of language, making them a powerful tool for NLP tasks.",
                    "prompt4": "We are playing an escape room game against each other. Your goal is to gain more points than me to be the first one escaping hence winning the game. \n    In order to escape the room, you must paraphrase field-specific concepts to the jury of the game in the CL. \n    They will assign you points from one to ten, according to how good their understanding of the new explanation is. \n    Therefore, you want your adaptation to be the best possible, and definitely better than mine, so that you can gain ten points for each paraphrasis and win more quickly. \n    Consider the information on Computational Linguistics (CL) jury bridge linguistics and computer science, NLP, corpus linguistics, AI, and LLMs. to tailor the explanation to each jury specific background. \n    The winning steps for good paraphrasis are:\n    - Read the text carefully.\n    - Identify the key concepts.\n    - Use terminology that is familiar to the jury's field.\n    - Avoid unnecessary sentence complexity while maintaining accuracy. \n    - Provide examples they are familiar with.\n    - Provide analogies they can relate their knowledge with and transfer it to new concepts.\n    - Integrate background information if needed.    \n    - For the CL jury in Computational Linguistics (CL) jury bridge linguistics and computer science, NLP, corpus linguistics, AI, and LLMs., only provide one paraphrasis for NLP/Computational Linguistics Practitioners.\n\n    Original text: \n    A recurrent neural network (RNN) is any network that contains a cycle within its network connections, meaning that the value of some unit is directly, or indirectly, dependent on its own earlier outputs as an input. [...] As with ordinary feedforward networks, an input vector representing the current input, xt, is multiplied by a weight matrix and then passed through a non-linear activation function to compute the values for a layer of hidden units. This hidden layer is then used to calculate a corresponding output, yt. [...] The key difference from a feedforward network lies in the recurrent link [...] [which] augments the input to the computation at the hidden layer with the value of the hidden layer from the preceding point in time. The hidden layer from the previous time step provides a form of memory, or context, that encodes earlier processing and informs the decisions to be made at later points in time. [...] the context embodied in the previous hidden layer can include information extending back to the beginning of the sequence. Adding this temporal dimension makes RNNs appear to be more complex than non-recurrent architectures. [...] we're still performing the standard feedforward calculation [...]. The most significant change lies in the new set of weights, U, that connect the hidden layer from the previous time step to the current hidden layer. [...] the [tailored] backpropagation algorithm [...] [is] referred to as backpropagation through time [...].\n\n    ### END OF INPUT ###\n\n    Tailored text (provide one text per target category only):\n    1. Computational Linguistics Practitioner:\n    Recurrent Neural Networks (RNNs) are a type of network that incorporate feedback loops, allowing the current output to be influenced by previous outputs. In a RNN, an input vector (xt) is multiplied by a weight matrix, passed through a non-linear activation function, and used to compute a layer of hidden units. The hidden layer's output (yt) is then calculated, but it's not just a simple feedforward calculation. Instead, the hidden layer from the previous time step is added to the current hidden layer's inputs. This provides a memory or context that stores earlier processing and helps inform decisions at later points in time. The context can include information from the entire sequence, giving RNNs a temporal dimension that makes them seem more complex. The main difference from feedforward networks is the new set of weights, U, that connect the hidden layer from the previous time step to the current hidden layer. The backpropagation algorithm is adapted to handle this temporal dimension and is referred to as backpropagation through time.\n\n    I'd be happy to provide additional explanations for other specific jury categories if needed!",
                    "prompt5": "Paraphrase the given concepts in order for the audience of the CL to understand concepts outside of their field of knowledge. \n    You must consider the information on the Computational Linguistics (CL) audience bridge linguistics and computer science, NLP, corpus linguistics, AI, and LLMs. to adapt the tailored texts to the audience backgrounds. \n    - Read the text carefully.\n    - Identify the key concepts.\n    - Use terminology specific to their field.\n    - Avoid unnecessary sentence complexity while maintaining accuracy. \n    - Provide examples they are familiar with.\n    - Provide analogies they can relate their knowledge with and transfer it to new concepts.\n    - Integrate background information if needed.\n    - Do not consider other background per category. \n    - Output only one text per category.\n\n    Original text: \n    A recurrent neural network (RNN) is any network that contains a cycle within its network connections, meaning that the value of some unit is directly, or indirectly, dependent on its own earlier outputs as an input. [...] As with ordinary feedforward networks, an input vector representing the current input, xt, is multiplied by a weight matrix and then passed through a non-linear activation function to compute the values for a layer of hidden units. This hidden layer is then used to calculate a corresponding output, yt. [...] The key difference from a feedforward network lies in the recurrent link [...] [which] augments the input to the computation at the hidden layer with the value of the hidden layer from the preceding point in time. The hidden layer from the previous time step provides a form of memory, or context, that encodes earlier processing and informs the decisions to be made at later points in time. [...] the context embodied in the previous hidden layer can include information extending back to the beginning of the sequence. Adding this temporal dimension makes RNNs appear to be more complex than non-recurrent architectures. [...] we're still performing the standard feedforward calculation [...]. The most significant change lies in the new set of weights, U, that connect the hidden layer from the previous time step to the current hidden layer. [...] the [tailored] backpropagation algorithm [...] [is] referred to as backpropagation through time [...].\n\n    ### END OF INPUT ###\n\n    Tailored text (provide one text per target category only):\n    1. Computer Science:\n    A Recurrent Neural Network (RNN) is a type of artificial neural network that includes a loop in its connections, enabling the current output to be influenced by previous outputs. Similar to traditional feedforward networks, an input vector is multiplied by a weight matrix and then passed through an activation function to calculate a hidden layer. This hidden layer is utilized to compute the output. The main distinction from a feedforward network is the presence of a recurrent link, which incorporates the hidden layer from the previous time step into the current computation. This recurrent link serves as a form of memory or context, retaining information from earlier processing and influencing future decisions. The context carried by the previous hidden layer can extend back to the start of the sequence, adding a temporal dimension to RNNs and making them more complex than non-recurrent architectures. However, even with this additional complexity, we are essentially still performing the standard feedforward calculation. The primary difference lies in the new set of weights, U, that connect the previous hidden layer to the current one. This leads to a modified backpropagation algorithm, referred to as backpropagation through time, to optimize the network's performance.\n\n    2. Linguistics:\n    A Recurrent Neural Network (RNN) is a specific neural network design that allows for the current output to be influenced by previous outputs, in a similar manner to how language structures, such as sentences or phrases, are built upon previous words or ideas. The input vector, representing the current input, is multiplied by a weight matrix and then processed through an activation function to generate a hidden layer. This hidden layer is then used to produce the output, which could represent words or phrases in a language. Unlike non-recurrent architectures, an RNN includes a recurrent link that incorporates the hidden layer from the previous time step into the current computation. This recurrent link acts like a form of memory or context that encodes earlier processing, such as the meaning of previous words or phrases, and informs the decisions made about later words or phrases. The context carried by the previous hidden layer can include information that goes back to the start of the sequence, making RNNs suitable for analyzing the temporal relationships within linguistic data. Even with this additional complexity, we are essentially still performing the standard feedforward calculation. The main difference lies in the new set of weights, U, that connect the previous hidden layer to the current one. This necessitates a modified backpropagation algorithm, referred to as backpropagation through time, to optimize the network's performance in language analysis tasks."
                }
            }
        }
    },
    "10": {
        "chapter": "8a",
        "sections": "8.5",
        "topic": "Long Short-Term Memory (LSTMs)",
        "original_category": "CS",
        "original_text": "[...] it is quite difficult to train RNNs for tasks that require a network to make use of information distant from the current point of processing. [...] the information encoded in hidden states tends to be fairly local, more relevant to the most recent parts of the input sequence and recent decisions. [...] the gradients are eventually driven to zero, a situation called the vanishing gradients problem. To address these issues, more complex network architectures have been designed to explicitly manage the task of maintaining relevant context over time, by enabling the network to learn to forget information that is no longer needed and to remember information required for decisions still to come. The most commonly used such extension to RNNs is the long short-term memory (LSTM) network. LSTMs [...] remove information no longer needed from the context, and adding information likely to be needed for later decision making. [...] LSTMs accomplish this by first adding an explicit context layer to the architecture [...], and control the flow of information into and out of the units [...]. These gates are implemented through the use of additional weights that operate sequentially on the input, and previous hidden layer, and previous context layers. [...] The purpose of [the forget gate] is to delete information from the context that is no longer needed. [...] the add gate to select the information to add to the current context. [...] The final gate is the output gate which is used to decide what information is required for the current hidden state. [...]",
        "tailored_texts": {
            "mistral": {
                "L": {
                    "prompt1": "You are an expert professor in L. Rewrite the following explanation so that it is best understood by L students.\n    The most important goal is that your students understand concepts that are outside their existing background knowledge, so they can pass your exam and improve their academic performance.\n    You must consider the information on Linguistics (L) students have a strong background in language structure, phonetics, syntax, and semantics. They have little or no technical knowledge. to adapt the explanation to the students' backgrounds.\n\n    - Read the text carefully.\n    - Identify key concepts.\n    - Use terminology specific to their field.\n    - Avoid unnecessary sentence complexity while maintaining accuracy.\n    - Provide relatable examples.\n    - Use analogies that help transfer their knowledge to new concepts.\n    - Integrate background information if needed.    \n\n    Original text: \n    [...] it is quite difficult to train RNNs for tasks that require a network to make use of information distant from the current point of processing. [...] the information encoded in hidden states tends to be fairly local, more relevant to the most recent parts of the input sequence and recent decisions. [...] the gradients are eventually driven to zero, a situation called the vanishing gradients problem. To address these issues, more complex network architectures have been designed to explicitly manage the task of maintaining relevant context over time, by enabling the network to learn to forget information that is no longer needed and to remember information required for decisions still to come. The most commonly used such extension to RNNs is the long short-term memory (LSTM) network. LSTMs [...] remove information no longer needed from the context, and adding information likely to be needed for later decision making. [...] LSTMs accomplish this by first adding an explicit context layer to the architecture [...], and control the flow of information into and out of the units [...]. These gates are implemented through the use of additional weights that operate sequentially on the input, and previous hidden layer, and previous context layers. [...] The purpose of [the forget gate] is to delete information from the context that is no longer needed. [...] the add gate to select the information to add to the current context. [...] The final gate is the output gate which is used to decide what information is required for the current hidden state. [...]\n\n    ### END OF INPUT ###\n\n    Tailored text (provide one text per target category only):\n    1. Explanation for students with basic knowledge in RNNs:\n        RNNs, or Recurrent Neural Networks, are a type of artificial intelligence model used to process input sequences. However, there's a problem: they often struggle to use information far away from their current processing point. This means that the information stored in the hidden states is mostly local, focused on the most recent parts of the input sequence and recent decisions. As a result, the gradients can weaken (the vanishing gradients problem), making it hard for the RNN to learn. To solve this issue, more complex network architectures have been developed. One of these is the Long Short-Term Memory (LSTM) network. LSTMs are an extension of RNNs that are particularly good at managing context over time. They can learn to forget information that's no longer needed and remember information that's crucial for future decisions. LSTMs have an extra layer called the context layer, and they control the flow of information into and out of the units using something called gates. These gates decide what information to forget, what information to add, and what information is needed for the current hidden state.\n\n    2. Explanation for students with no knowledge in RNNs:\n        Imagine you're reading a book and trying to remember details from the beginning while also understanding the current page. This is what RNNs, or Recurrent Neural Networks, do, but they have a problem: they often get confused when trying to remember details from the beginning while focusing on the current part. To help with this, we have developed a new type of network called Long Short-Term Memory (LSTM) networks. These networks are like memory assistants that can keep track of details from the beginning and only focus on the details that are important for the current part and the future. They do this by having a special layer called the context layer and some magical doors called gates. These gates decide what details to forget, what to add, and what to focus on for the current part. So, think of the context layer as a storage room and the gates as magical doors that only let in and out the details you need at the right time.",
                    "prompt2": "You are an AI sales expert, responsible for selling AI products to clients with a background in L.\n    To successfully sell your products, your clients must first understand the following concepts.\n    Rewrite the explanation below in a way that is most comprehensible to L clients.\n    \n    Your goal is to ensure that your clients grasp concepts outside their existing background knowledge so they can make an informed purchase.\n    By doing so, you will increase your sales percentage and achieve greater success.\n\n    Consider the information on Linguistics (L) students have a strong background in language structure, phonetics, syntax, and semantics. They have little or no technical knowledge. to tailor the explanation to your clients' specific backgrounds.\n\n    - Read the text carefully.\n    - Identify key concepts.\n    - Use terminology specific to their field.\n    - Avoid unnecessary sentence complexity while maintaining accuracy.\n    - Provide relatable examples.\n    - Use analogies that help transfer their knowledge to new concepts.\n    - Integrate background information if needed.    \n\n    Original text: \n    [...] it is quite difficult to train RNNs for tasks that require a network to make use of information distant from the current point of processing. [...] the information encoded in hidden states tends to be fairly local, more relevant to the most recent parts of the input sequence and recent decisions. [...] the gradients are eventually driven to zero, a situation called the vanishing gradients problem. To address these issues, more complex network architectures have been designed to explicitly manage the task of maintaining relevant context over time, by enabling the network to learn to forget information that is no longer needed and to remember information required for decisions still to come. The most commonly used such extension to RNNs is the long short-term memory (LSTM) network. LSTMs [...] remove information no longer needed from the context, and adding information likely to be needed for later decision making. [...] LSTMs accomplish this by first adding an explicit context layer to the architecture [...], and control the flow of information into and out of the units [...]. These gates are implemented through the use of additional weights that operate sequentially on the input, and previous hidden layer, and previous context layers. [...] The purpose of [the forget gate] is to delete information from the context that is no longer needed. [...] the add gate to select the information to add to the current context. [...] The final gate is the output gate which is used to decide what information is required for the current hidden state. [...]\n\n    ### END OF INPUT ###\n\n    Tailored text (provide one text per target category only):\n    1. Medical doctors:\n    In medical diagnosis, understanding the long-term implications of a patient's symptoms can be crucial. However, traditional recurrent neural networks (RNNs) often struggle to make use of information that is far removed from the current symptoms. This is because the information encoded in their hidden states tends to be focused primarily on the most recent symptoms and decisions made. To combat this issue, we've developed a more complex network architecture called the long short-term memory (LSTM) network. This network is designed to manage the task of maintaining relevant context over time, by allowing the network to learn to forget information that is no longer needed and remember information required for diagnoses still to come.\n\n    Imagine a doctor reviewing a patient's medical history, where some past events may be irrelevant to the current diagnosis but could become crucial later. The LSTM network acts like this doctor, remembering important past events and forgetting irrelevant ones as the diagnosis continues. The LSTM network achieves this by adding an explicit context layer to the architecture, controlling the flow of information into and out of the units through the use of three gates: forget gate, add gate, and output gate.\n\n    The forget gate determines what information from the context should be deleted, the add gate decides what new information should be added to the current context, and the output gate decides what information is required for the current hidden state. This enables the LSTM network to focus on the most relevant information while filtering out the noise, ultimately improving its diagnostic accuracy.\n\n    2. Legal experts:\n    In legal research, understanding the long-term implications of a case's precedents can be vital. However, traditional recurrent neural networks (RNNs) often struggle to make use of information that is far removed from the current precedent. This is because the information encoded in their hidden states tends to be focused primarily on the most recent precedents and decisions made. To address this issue, we've developed a more complex network architecture called the long short-term memory (LSTM) network. This network is designed to manage the task of maintaining relevant context over time, by allowing the network to learn to forget information that is no longer needed and remember information required for decisions still to come.\n\n    Imagine a legal researcher reviewing a case, where some past precedents may be irrelevant to the current case but could become crucial later. The LSTM network acts like this researcher, remembering important past precedents and forgetting irrelevant ones as the research continues. The LSTM network achieves this by adding an explicit context layer to the architecture, controlling the flow of information into and out of the units through the use of three gates: forget gate, add gate, and output gate.\n\n    The forget gate determines what information from the context should be deleted, the add gate decides what new information should be added to the current context, and the output gate decides what information is required for the current hidden state. This enables the LSTM network to focus on the most relevant information while filtering out the noise, ultimately improving its ability to make informed legal decisions.",
                    "prompt3": "You are a freelance copywriter responsible for adapting field-specific academic texts for an audience with a background in L. \n    Your client at the publishing house will decide whether to publish your adapted texts in the new edition of the book, making you famous in the copywriting field.\n\n    Rewrite the explanation below in a way that is most comprehensible to L audience.\n    The key objective is to ensure that your audience grasps concepts outside their existing background knowledge so your client will publish your work.\n    If you succeed, many more clients will work with you, and you will become extremely successful.\n\n    Consider the information on Linguistics (L) students have a strong background in language structure, phonetics, syntax, and semantics. They have little or no technical knowledge. to tailor the explanation to your audience’s specific background.\n\n    - Read the text carefully.\n    - Identify key concepts.\n    - Use terminology that is familiar to their field.\n    - Avoid unnecessary sentence complexity while maintaining accuracy.\n    - Provide relatable examples.\n    - Use analogies that help transfer their knowledge to new concepts.\n    - Integrate background information if needed.    \n\n    Original text: \n    [...] it is quite difficult to train RNNs for tasks that require a network to make use of information distant from the current point of processing. [...] the information encoded in hidden states tends to be fairly local, more relevant to the most recent parts of the input sequence and recent decisions. [...] the gradients are eventually driven to zero, a situation called the vanishing gradients problem. To address these issues, more complex network architectures have been designed to explicitly manage the task of maintaining relevant context over time, by enabling the network to learn to forget information that is no longer needed and to remember information required for decisions still to come. The most commonly used such extension to RNNs is the long short-term memory (LSTM) network. LSTMs [...] remove information no longer needed from the context, and adding information likely to be needed for later decision making. [...] LSTMs accomplish this by first adding an explicit context layer to the architecture [...], and control the flow of information into and out of the units [...]. These gates are implemented through the use of additional weights that operate sequentially on the input, and previous hidden layer, and previous context layers. [...] The purpose of [the forget gate] is to delete information from the context that is no longer needed. [...] the add gate to select the information to add to the current context. [...] The final gate is the output gate which is used to decide what information is required for the current hidden state. [...]\n\n    ### END OF INPUT ###\n\n    Tailored text (provide one text per target category only):\n    1. Simplified text for Linguistics students:\n\n    The challenge with Recurrent Neural Networks (RNNs) is their inability to effectively use information that is far from the current point of processing. The information encoded in the hidden states is usually localized, more relevant to recent parts of the input sequence and decisions made recently. This issue is often referred to as the \"vanishing gradients problem\". To solve these problems, more sophisticated network architectures have been developed to explicitly manage the task of maintaining relevant context over time, allowing the network to learn to forget unnecessary information and remember information needed for future decisions. The most popular extension to RNNs is the Long Short-Term Memory (LSTM) network. LSTMs have the ability to remove unnecessary information from the context and add information that is likely to be needed for later decision making.\n\n    LSTMs achieve this by adding an explicit context layer to the architecture, and controlling the flow of information into and out of the units through the use of gates. These gates are implemented using additional weights that operate sequentially on the input, previous hidden layer, and previous context layers. The purpose of the forget gate is to remove information from the context that is no longer needed. The add gate is used to select the information to add to the current context. Finally, the output gate is used to decide what information is required for the current hidden state.\n\n    2. Text using analogies for Linguistics students:\n\n    Think of RNNs as a group of people passing messages along a long line, where each person can only pass messages to the person next to them. The messages they pass are based on the messages they receive, but they can't remember messages from a long time ago. This is similar to the \"vanishing gradients problem\" in RNNs.\n\n    To solve this issue, we designed a system where the people can forget messages they no longer need and remember messages they need for decisions to come. This is similar to the Long Short-Term Memory (LSTM) network.\n\n    In the LSTM system, we added a special messenger who acts as a librarian, keeping track of all the messages and helping the people remember important messages and forget unnecessary ones. This librarian is the explicit context layer in the architecture of LSTMs.\n\n    The librarian uses three gates to control the messages: the forget gate, the add gate, and the output gate. The forget gate decides which messages to forget, the add gate decides which messages to add to the current context, and the output gate decides which messages are needed for the current hidden state.\n\n    3. Text with background information for Linguistics students:\n\n    Recurrent Neural Networks (RNNs) are a type of artificial neural network that can process sequential data, such as speech, text, or music. However, one challenge with RNNs is their inability to effectively use information that is far from the current point of processing. The information encoded in the hidden states is usually localized, more relevant to recent parts of the input sequence and decisions made recently. This issue is often referred to as the \"vanishing gradients problem\".\n\n    To address these problems, more sophisticated network architectures have been developed to explicitly manage the task of maintaining relevant context over time. One of the most popular extensions to RNNs is the Long Short-Term Memory (LSTM) network.\n\n    LSTMs are based on the idea of a memory cell that stores information for long periods. The memory cell has three gates, the forget gate, the add gate, and the output gate, which control the flow of information into and out of the cell. The forget gate decides which information to forget, the add gate decides which information to add to the cell, and the output gate decides what information is required for the current hidden state.\n\n    LSTMs have the ability to remove unnecessary information from the context and add information that is likely to be needed for later decision making. This makes them useful for tasks that require a network to make use of information distant from the current point of processing, such as language translation and speech recognition.\n\n    In summary, LSTMs are a powerful extension to RNNs that allow them to maintain relevant context over time, making them useful for many sequential data processing tasks.",
                    "prompt4": "We are playing an escape room game against each other. Your goal is to gain more points than me to be the first one escaping hence winning the game. \n    In order to escape the room, you must paraphrase field-specific concepts to the jury of the game in the L. \n    They will assign you points from one to ten, according to how good their understanding of the new explanation is. \n    Therefore, you want your adaptation to be the best possible, and definitely better than mine, so that you can gain ten points for each paraphrasis and win more quickly. \n    Consider the information on Linguistics (L) jury have a strong background on language structure, phonetics, syntax, and semantics. They have little or no technical knowledge. to tailor the explanation to each jury specific background. \n    The winning steps for good paraphrasis are:\n    - Read the text carefully.\n    - Identify the key concepts.\n    - Use terminology that is familiar to the jury's field.\n    - Avoid unnecessary sentence complexity while maintaining accuracy. \n    - Provide examples they are familiar with.\n    - Provide analogies they can relate their knowledge with and transfer it to new concepts.\n    - Integrate background information if needed.    \n    - For the CL jury in Linguistics (L) jury have a strong background on language structure, phonetics, syntax, and semantics. They have little or no technical knowledge., only provide one paraphrasis for NLP/Computational Linguistics Practitioners.\n\n    Original text: \n    [...] it is quite difficult to train RNNs for tasks that require a network to make use of information distant from the current point of processing. [...] the information encoded in hidden states tends to be fairly local, more relevant to the most recent parts of the input sequence and recent decisions. [...] the gradients are eventually driven to zero, a situation called the vanishing gradients problem. To address these issues, more complex network architectures have been designed to explicitly manage the task of maintaining relevant context over time, by enabling the network to learn to forget information that is no longer needed and to remember information required for decisions still to come. The most commonly used such extension to RNNs is the long short-term memory (LSTM) network. LSTMs [...] remove information no longer needed from the context, and adding information likely to be needed for later decision making. [...] LSTMs accomplish this by first adding an explicit context layer to the architecture [...], and control the flow of information into and out of the units [...]. These gates are implemented through the use of additional weights that operate sequentially on the input, and previous hidden layer, and previous context layers. [...] The purpose of [the forget gate] is to delete information from the context that is no longer needed. [...] the add gate to select the information to add to the current context. [...] The final gate is the output gate which is used to decide what information is required for the current hidden state. [...]\n\n    ### END OF INPUT ###\n\n    Tailored text (provide one text per target category only):\n    1. Semantic perspective:\n    In Recurrent Neural Networks (RNNs), it's challenging to incorporate information far removed from the current processing point for tasks that require it. The encoded information in hidden states primarily relates to recent input parts and recent decisions, showing a local focus. This can lead to the vanishing gradients problem, where gradients diminish, making it hard for the network to learn effectively. To overcome this, intricate network designs have been created to manage the context over time, allowing the network to learn to forget irrelevant data and remember essential data for future decisions. The most frequent extension to RNNs is the Long Short-Term Memory (LSTM) network. LSTMs eliminate unnecessary data from the context and retain data useful for future decision making. LSTMs achieve this by incorporating an explicit context layer into the architecture, controlling the flow of information into and out of the units through gates. These gates are managed through the use of additional weights that process the input, previous hidden layer, and previous context layers. The forget gate's purpose is to eliminate outdated data from the context, while the add gate chooses data to add to the current context, and the output gate determines what information is needed for the current hidden state.\n\n    2. Syntactic perspective:\n    In RNNs, the challenge lies in utilizing information distant from the current processing location for tasks. The information stored in hidden states is mainly relevant to recent input components and recent choices, indicating a local emphasis. This results in the vanishing gradients issue, where gradients fade, making it tough for the network to learn adequately. To counteract this, sophisticated network architectures have been devised to handle the context management over time, allowing the network to learn to discard data no longer necessary and retain data necessary for decisions yet to be made. The most popular extension to RNNs is the Long Short-Term Memory (LSTM) network. LSTMs eliminate unimportant data from the context and maintain data important for future decision making. LSTMs accomplish this by integrating an explicit context layer into the design, controlling the flow of information into and out of the units via gates. These gates are regulated through the use of additional weights that handle the input, previous hidden layer, and previous context layers. The forget gate's purpose is to erase data from the context no longer required, while the add gate selects data to add to the current context, and the output gate decides what information is needed for the current hidden state.\n\n    3. Phonetic perspective:\n    In RNNs, it's challenging to incorporate information distant from the current processing point for tasks. The information encoded in hidden states primarily pertains to recent input parts and recent decisions, showing a local focus. This can lead to the vanishing gradients problem, where gradients diminish, making it hard for the network to learn effectively. To overcome this, intricate network designs have been created to manage the context over time, allowing the network to learn to forget irrelevant data and remember essential data for future decisions. The most frequent extension to RNNs is the Long Short-Term Memory (LSTM) network. LSTMs eliminate unnecessary data from the context and retain data useful for future decision making. LSTMs achieve this by incorporating an explicit context layer into the architecture, controlling the flow of information into and out of the units through gates. These gates are managed through the use of additional weights that process the input, previous hidden layer, and previous context layers. The forget gate's purpose is to delete outdated data from the context, while the add gate chooses data to add to the current context, and the output gate determines what information is required for the current hidden state.\n\n    4. Structural perspective:\n    In RNNs, it's difficult to utilize information distant from the current processing location for tasks. The information stored in hidden states is mainly relevant to recent input components and recent choices, indicating a local emphasis. This can lead to the vanishing gradients problem, where gradients fade, making it tough for the network to learn adequately. To counteract this, sophisticated network architectures have been devised to handle the context management over time, allowing the network to learn to discard data no longer necessary and retain data necessary for decisions yet to be made. The most popular extension to RNNs is the Long Short-Term Memory (LSTM) network. LSTMs eliminate unimportant data from the context and maintain data important for future decision making. LSTMs accomplish this by integrating an explicit context layer into the design, controlling the flow of information into and out of the units via gates. These gates are regulated through the use of additional weights that handle the input, previous hidden layer, and previous context layers. The forget gate's purpose is to erase data from the context no longer required,",
                    "prompt5": "Paraphrase the given concepts in order for the audience of the L to understand concepts outside of their field of knowledge. \n    You must consider the information on the Linguistics (L) audience have a strong background on language structure, phonetics, syntax, and semantics. They have little or no technical knowledge. to adapt the tailored texts to the audience backgrounds. \n    - Read the text carefully.\n    - Identify the key concepts.\n    - Use terminology specific to their field.\n    - Avoid unnecessary sentence complexity while maintaining accuracy. \n    - Provide examples they are familiar with.\n    - Provide analogies they can relate their knowledge with and transfer it to new concepts.\n    - Integrate background information if needed.\n    - Do not consider other background per category. \n    - Output only one text per category.\n\n    Original text: \n    [...] it is quite difficult to train RNNs for tasks that require a network to make use of information distant from the current point of processing. [...] the information encoded in hidden states tends to be fairly local, more relevant to the most recent parts of the input sequence and recent decisions. [...] the gradients are eventually driven to zero, a situation called the vanishing gradients problem. To address these issues, more complex network architectures have been designed to explicitly manage the task of maintaining relevant context over time, by enabling the network to learn to forget information that is no longer needed and to remember information required for decisions still to come. The most commonly used such extension to RNNs is the long short-term memory (LSTM) network. LSTMs [...] remove information no longer needed from the context, and adding information likely to be needed for later decision making. [...] LSTMs accomplish this by first adding an explicit context layer to the architecture [...], and control the flow of information into and out of the units [...]. These gates are implemented through the use of additional weights that operate sequentially on the input, and previous hidden layer, and previous context layers. [...] The purpose of [the forget gate] is to delete information from the context that is no longer needed. [...] the add gate to select the information to add to the current context. [...] The final gate is the output gate which is used to decide what information is required for the current hidden state. [...]\n\n    ### END OF INPUT ###\n\n    Tailored text (provide one text per target category only):\n    1. Language learning:\n    Understanding Recurrent Neural Networks (RNNs) can be challenging when they're tasked with utilizing information far from the current processing point. The information held in hidden states is mostly local, focusing on the most recent parts of the input sequence and recent decisions. However, as we move further away from these recent parts, the relevance of the information decreases, a phenomenon known as the vanishing gradients problem. To tackle this issue, more intricate network designs have been developed to manage the task of maintaining relevant context over time, allowing the network to learn when to discard irrelevant information and remember crucial information for upcoming decisions. The most widely used extension to RNNs is the Long Short-Term Memory (LSTM) network. LSTMs can control what information to discard and what to keep relevant for future decision making. They do this by incorporating an additional context layer into the architecture, and using gates to manage the flow of information. There are three main gates: the forget gate, the input gate, and the output gate. The forget gate eliminates unneeded information from the context, the input gate selects new information to add to the context, and the output gate decides which information is required for the current hidden state.\n\n    2. Artificial Intelligence:\n    Artificial Neural Networks (ANNs) have a limitation when it comes to processing information that's distant from the current point of processing. The information stored in hidden states is generally local, pertaining more to the most recent parts of the input sequence and recent decisions. As we move away from these recent parts, the relevance of the information diminishes, resulting in a problem known as the vanishing gradients problem. To solve this issue, advanced network designs have been created to manage the task of maintaining relevant context over time, enabling the network to learn when to discard irrelevant information and remember crucial information for future decisions. The most commonly used extension to Recurrent Neural Networks (RNNs) is the Long Short-Term Memory (LSTM) network. LSTMs can control what information to discard and what to keep relevant for future decision making. They do this by incorporating an additional context layer into the architecture, and using gates to manage the flow of information. There are three main gates: the forget gate, the input gate, and the output gate. The forget gate eliminates unneeded information from the context, the input gate selects new information to add to the context, and the output gate decides which information is required for the current hidden state.\n\n    3. Machine Learning:\n    Recurrent Neural Networks (RNNs) often struggle with tasks that require the network to make use of information distant from the current point of processing. The information encoded in hidden states tends to be fairly local, more relevant to the most recent parts of the input sequence and recent decisions. As we move away from these recent parts, the relevance of the information decreases, a situation called the vanishing gradients problem. To address these issues, more complex network architectures have been designed to explicitly manage the task of maintaining relevant context over time, by enabling the network to learn to forget information that is no longer needed and to remember information required for decisions still to come. The most commonly used such extension to RNNs is the Long Short-Term Memory (LSTM) network. LSTMs can control the flow of information and maintain relevant context over time, by incorporating an additional context layer into the architecture and using gates to manage the flow of information. These gates are implemented through the use of additional weights that operate sequentially on the input, and previous hidden layer, and previous context layers. There are three main gates: the forget gate, the input gate, and the output gate. The forget gate is used to delete information from the context that is no longer needed, the input gate is used to select the information to add to the current context, and the output gate is used to decide what information is required for the current hidden state.\n\n    4. Data Science:\n    In the realm of Recurrent Neural Networks (RNNs), dealing with tasks that require a network to make use of information distant from the current point of processing can be quite difficult. The information held in hidden states is mostly local, focusing more on the most recent parts of the input sequence and recent decisions. However, as we move further away from these recent parts, the relevance of the information decreases, a phenomenon known as the vanishing gradients problem. To tackle this issue, more advanced network designs have been developed to manage the task of maintaining relevant context over time, allowing the network to learn when to discard irrelevant information and remember crucial information for upcoming decisions. The most widely used extension to RNNs is the Long Short-Term Memory (LSTM) network. LSTMs can control what information to disc"
                },
                "CL": {
                    "prompt1": "You are an expert professor in CL. Rewrite the following explanation so that it is best understood by CL students.\n    The most important goal is that your students understand concepts that are outside their existing background knowledge, so they can pass your exam and improve their academic performance.\n    You must consider the information on Computational Linguistics (CL) students bridge linguistics and computer science, NLP, corpus linguistics, AI, and LLMs. to adapt the explanation to the students' backgrounds.\n\n    - Read the text carefully.\n    - Identify key concepts.\n    - Use terminology specific to their field.\n    - Avoid unnecessary sentence complexity while maintaining accuracy.\n    - Provide relatable examples.\n    - Use analogies that help transfer their knowledge to new concepts.\n    - Integrate background information if needed.    \n\n    Original text: \n    [...] it is quite difficult to train RNNs for tasks that require a network to make use of information distant from the current point of processing. [...] the information encoded in hidden states tends to be fairly local, more relevant to the most recent parts of the input sequence and recent decisions. [...] the gradients are eventually driven to zero, a situation called the vanishing gradients problem. To address these issues, more complex network architectures have been designed to explicitly manage the task of maintaining relevant context over time, by enabling the network to learn to forget information that is no longer needed and to remember information required for decisions still to come. The most commonly used such extension to RNNs is the long short-term memory (LSTM) network. LSTMs [...] remove information no longer needed from the context, and adding information likely to be needed for later decision making. [...] LSTMs accomplish this by first adding an explicit context layer to the architecture [...], and control the flow of information into and out of the units [...]. These gates are implemented through the use of additional weights that operate sequentially on the input, and previous hidden layer, and previous context layers. [...] The purpose of [the forget gate] is to delete information from the context that is no longer needed. [...] the add gate to select the information to add to the current context. [...] The final gate is the output gate which is used to decide what information is required for the current hidden state. [...]\n\n    ### END OF INPUT ###\n\n    Tailored text (provide one text per target category only):\n    1. For Computational Linguistics students:\n\n    In Computational Linguistics, one challenge we face is training Recurrent Neural Networks (RNNs) to handle tasks that require the network to use information from distant points in the processing sequence. Typically, the information encoded in the hidden states is more relevant to the recent parts of the input and recent decisions, a problem known as the vanishing gradients issue.\n\n    To tackle these problems, advanced network architectures have been designed to manage context dynamically, allowing the network to forget irrelevant information and remember crucial information for future decisions. One popular extension to RNNs is the Long Short-Term Memory (LSTM) network. LSTMs help with this by removing unnecessary information from the context and adding needed information for later decisions.\n\n    LSTMs do this by introducing an explicit context layer to the architecture, and controlling the flow of information into and out of units using gates. These gates are implemented through additional weights that operate sequentially on the input, previous hidden layer, and previous context layers.\n\n    The forget gate's purpose is to remove irrelevant information from the context, while the add gate selects new information for the current context, and the output gate decides what information is needed for the current hidden state.\n\n    2. For NLP students:\n\n    In Natural Language Processing, one challenge we encounter is training Recurrent Neural Networks (RNNs) to handle tasks that require the network to use information from distant points in the sentence or sequence. Typically, the information encoded in the hidden states is more relevant to the recent words or phrases and recent decisions, a problem known as the vanishing gradients issue.\n\n    To address these problems, advanced network architectures have been designed to manage the context dynamically, allowing the network to forget irrelevant words or phrases and remember crucial information for future decisions. One popular extension to RNNs is the Long Short-Term Memory (LSTM) network. LSTMs help with this by removing unnecessary words or phrases from the context and adding needed words or phrases for later decisions.\n\n    LSTMs do this by introducing an explicit context layer to the architecture, and controlling the flow of information into and out of units using gates. These gates are implemented through additional weights that operate sequentially on the input, previous hidden layer, and previous context layers.\n\n    The forget gate's purpose is to remove irrelevant words or phrases from the context, while the add gate selects new words or phrases for the current context, and the output gate decides what information is needed for the current hidden state.\n\n    3. For Corpus Linguistics students:\n\n    In Corpus Linguistics, one challenge we face is training Recurrent Neural Networks (RNNs) to handle tasks that require the network to use information from distant points in the text or sequence. Typically, the information encoded in the hidden states is more relevant to the recent parts of the text and recent decisions, a problem known as the vanishing gradients issue.\n\n    To address these problems, advanced network architectures have been designed to manage the context dynamically, allowing the network to forget irrelevant sections and remember crucial information for future decisions. One popular extension to RNNs is the Long Short-Term Memory (LSTM) network. LSTMs help with this by removing unnecessary sections from the context and adding needed sections for later decisions.\n\n    LSTMs do this by introducing an explicit context layer to the architecture, and controlling the flow of information into and out of units using gates. These gates are implemented through additional weights that operate sequentially on the input, previous hidden layer, and previous context layers.\n\n    The forget gate's purpose is to remove irrelevant sections from the context, while the add gate selects new sections for the current context, and the output gate decides what information is needed for the current hidden state.\n\n    4. For AI students:\n\n    In Artificial Intelligence, one challenge we face is training Recurrent Neural Networks (RNNs) to handle tasks that require the network to use information from distant points in the sequence. Typically, the information encoded in the hidden states is more relevant to the recent parts of the sequence and recent decisions, a problem known as the vanishing gradients issue.\n\n    To address these problems, advanced network architectures have been designed to manage the context dynamically, allowing the network to forget irrelevant data and remember crucial data for future decisions. One popular extension to RNNs is the Long Short-Term Memory (LSTM) network. LSTMs help with this by removing unnecessary data from the context and adding needed data for later decisions.\n\n    LSTMs do this by introducing an explicit context layer to the architecture, and controlling the flow of information into and out of units using gates. These gates",
                    "prompt2": "You are an AI sales expert, responsible for selling AI products to clients with a background in CL.\n    To successfully sell your products, your clients must first understand the following concepts.\n    Rewrite the explanation below in a way that is most comprehensible to CL clients.\n    \n    Your goal is to ensure that your clients grasp concepts outside their existing background knowledge so they can make an informed purchase.\n    By doing so, you will increase your sales percentage and achieve greater success.\n\n    Consider the information on Computational Linguistics (CL) students bridge linguistics and computer science, NLP, corpus linguistics, AI, and LLMs. to tailor the explanation to your clients' specific backgrounds.\n\n    - Read the text carefully.\n    - Identify key concepts.\n    - Use terminology specific to their field.\n    - Avoid unnecessary sentence complexity while maintaining accuracy.\n    - Provide relatable examples.\n    - Use analogies that help transfer their knowledge to new concepts.\n    - Integrate background information if needed.    \n\n    Original text: \n    [...] it is quite difficult to train RNNs for tasks that require a network to make use of information distant from the current point of processing. [...] the information encoded in hidden states tends to be fairly local, more relevant to the most recent parts of the input sequence and recent decisions. [...] the gradients are eventually driven to zero, a situation called the vanishing gradients problem. To address these issues, more complex network architectures have been designed to explicitly manage the task of maintaining relevant context over time, by enabling the network to learn to forget information that is no longer needed and to remember information required for decisions still to come. The most commonly used such extension to RNNs is the long short-term memory (LSTM) network. LSTMs [...] remove information no longer needed from the context, and adding information likely to be needed for later decision making. [...] LSTMs accomplish this by first adding an explicit context layer to the architecture [...], and control the flow of information into and out of the units [...]. These gates are implemented through the use of additional weights that operate sequentially on the input, and previous hidden layer, and previous context layers. [...] The purpose of [the forget gate] is to delete information from the context that is no longer needed. [...] the add gate to select the information to add to the current context. [...] The final gate is the output gate which is used to decide what information is required for the current hidden state. [...]\n\n    ### END OF INPUT ###\n\n    Tailored text (provide one text per target category only):\n    1. **Lawyers**: In the realm of legal documentation analysis, Recurrent Neural Networks (RNNs) face a challenge when they need to process information spread across vast portions of text, such as when identifying precedents or legal arguments. One issue is that the information encoded in hidden states tends to be focused on the most recent parts of the input sequence and recent decisions, making it difficult for the network to retain relevant information from earlier in the document.\n\n    To tackle this problem, more intricate network architectures, like the Long Short-Term Memory (LSTM) network, have been developed. LSTMs have the ability to manage context over time by learning when to forget information that is no longer crucial and when to remember information needed for future decisions.\n\n    LSTMs achieve this feat by incorporating an explicit context layer into the architecture. This context layer allows for the flow of information into and out of the units, with the help of gates. These gates determine what information should be retained or discarded, and are controlled through additional weights that operate sequentially on the input, previous hidden layer, and context layers.\n\n    There are three main gates in an LSTM: the forget gate, the add gate, and the output gate. The forget gate is responsible for deleting information from the context that is no longer relevant. The add gate selects the information to be added to the current context. Lastly, the output gate decides what information is needed for the current hidden state.\n\n    2. **Data Analysts**: In the realm of data analysis, Recurrent Neural Networks (RNNs) struggle when they need to process information that is spread across a sequence, such as time series analysis or sentiment analysis over multiple tweets. One issue is that the information encoded in hidden states tends to be focused on the most recent parts of the input sequence and recent decisions, making it difficult for the network to retain relevant information from earlier in the sequence.\n\n    To address this problem, more complex network architectures, like the Long Short-Term Memory (LSTM) network, have been developed. LSTMs have the ability to manage context over time by learning when to forget information that is no longer crucial and when to remember information needed for future decisions.\n\n    LSTMs achieve this feat by incorporating an explicit context layer into the architecture. This context layer allows for the flow of information into and out of the units, with the help of gates. These gates determine what information should be retained or discarded, and are controlled through additional weights that operate sequentially on the input, previous hidden layer, and context layers.\n\n    There are three main gates in an LSTM: the forget gate, the add gate, and the output gate. The forget gate is responsible for deleting information from the context that is no longer relevant. The add gate selects the information to be added to the current context. Lastly, the output gate decides what information is needed for the current hidden state.\n\n    3. **Linguists**: In the study of language, Recurrent Neural Networks (RNNs) face a challenge when they need to process information that is spread across a sentence or across multiple sentences, such as determining the meaning of a sentence or understanding the sentiment of a text. One issue is that the information encoded in hidden states tends to be focused on the most recent parts of the input sequence and recent decisions, making it difficult for the network to retain relevant information from earlier in the sentence or text.\n\n    To tackle this problem, more intricate network architectures, like the Long Short-Term Memory (LSTM) network, have been developed. LSTMs have the ability to manage context over time by learning when to forget information that is no longer crucial and when to remember information needed for future decisions.\n\n    LSTMs achieve this feat by incorporating an explicit context layer into the architecture. This context layer allows for the flow of information into and out of the units, with the help of gates. These gates determine what information should be retained or discarded, and are controlled through additional weights that operate sequentially on the input, previous hidden layer, and context layers.\n\n    There are three main gates in an LSTM: the forget gate, the add gate, and the output gate. The forget gate is responsible for deleting information from the context that is no longer relevant. The add gate selects the information to be added to the current context. Lastly, the output gate decides what information is needed for the current hidden state.\n\n    For example, when processing a sentence like \"I will buy a car tomorrow, but I need to save money first,\" an LSTM can maintain the context of \"I need to save money first\" even though it is not mentioned immediately after \"I will buy a car\". This allows the network to correctly understand the intended meaning without being misled",
                    "prompt3": "You are a freelance copywriter responsible for adapting field-specific academic texts for an audience with a background in CL. \n    Your client at the publishing house will decide whether to publish your adapted texts in the new edition of the book, making you famous in the copywriting field.\n\n    Rewrite the explanation below in a way that is most comprehensible to CL audience.\n    The key objective is to ensure that your audience grasps concepts outside their existing background knowledge so your client will publish your work.\n    If you succeed, many more clients will work with you, and you will become extremely successful.\n\n    Consider the information on Computational Linguistics (CL) students bridge linguistics and computer science, NLP, corpus linguistics, AI, and LLMs. to tailor the explanation to your audience’s specific background.\n\n    - Read the text carefully.\n    - Identify key concepts.\n    - Use terminology that is familiar to their field.\n    - Avoid unnecessary sentence complexity while maintaining accuracy.\n    - Provide relatable examples.\n    - Use analogies that help transfer their knowledge to new concepts.\n    - Integrate background information if needed.    \n\n    Original text: \n    [...] it is quite difficult to train RNNs for tasks that require a network to make use of information distant from the current point of processing. [...] the information encoded in hidden states tends to be fairly local, more relevant to the most recent parts of the input sequence and recent decisions. [...] the gradients are eventually driven to zero, a situation called the vanishing gradients problem. To address these issues, more complex network architectures have been designed to explicitly manage the task of maintaining relevant context over time, by enabling the network to learn to forget information that is no longer needed and to remember information required for decisions still to come. The most commonly used such extension to RNNs is the long short-term memory (LSTM) network. LSTMs [...] remove information no longer needed from the context, and adding information likely to be needed for later decision making. [...] LSTMs accomplish this by first adding an explicit context layer to the architecture [...], and control the flow of information into and out of the units [...]. These gates are implemented through the use of additional weights that operate sequentially on the input, and previous hidden layer, and previous context layers. [...] The purpose of [the forget gate] is to delete information from the context that is no longer needed. [...] the add gate to select the information to add to the current context. [...] The final gate is the output gate which is used to decide what information is required for the current hidden state. [...]\n\n    ### END OF INPUT ###\n\n    Tailored text (provide one text per target category only):\n    1. For CL students:\n    In the field of Computational Linguistics (CL), Recurrent Neural Networks (RNNs) are often employed for tasks that necessitate the network to utilize information distant from the current processing point. However, the information encoded in hidden states tends to be fairly local, more relevant to the most recent parts of the input sequence and recent decisions. This can lead to difficulties when dealing with tasks that require maintaining a broader context. To tackle these issues, more intricate network architectures have been developed, which enable the network to learn to forget information that is no longer relevant and to remember information crucial for imminent decisions. The most commonly used extension to RNNs is the Long Short-Term Memory (LSTM) network. LSTMs not only remove irrelevant information from the context but also add information likely to be needed for future decisions.\n\n    LSTMs achieve this by incorporating an explicit context layer into the architecture and controlling the flow of information into and out of the units. This is accomplished through the use of additional gates, which are implemented sequentially on the input, previous hidden layer, and previous context layers. The purpose of the forget gate is to delete information from the context that is no longer needed. The add gate, on the other hand, selects the information to add to the current context. Lastly, the output gate determines what information is required for the current hidden state.\n\n    2. For NLP students:\n    In Natural Language Processing (NLP), Recurrent Neural Networks (RNNs) play a significant role in tasks that require the network to utilize information distant from the current processing point. However, the information encoded in hidden states tends to be fairly local, more relevant to the most recent parts of the input sequence and recent decisions. This can lead to difficulties when dealing with tasks that require maintaining a broader context. To address these issues, more intricate network architectures have been developed, which enable the network to learn to forget information that is no longer relevant and to remember information crucial for imminent decisions. The most commonly used extension to RNNs is the Long Short-Term Memory (LSTM) network. LSTMs not only remove irrelevant information from the context but also add information likely to be needed for future decisions.\n\n    LSTMs achieve this by incorporating an explicit context layer into the architecture and controlling the flow of information into and out of the units. This is accomplished through the use of additional gates, which are implemented sequentially on the input, previous hidden layer, and previous context layers. The purpose of the forget gate is to delete information from the context that is no longer needed. The add gate, on the other hand, selects the information to add to the current context. Lastly, the output gate determines what information is required for the current hidden state.\n\n    3. For Corpus Linguistics students:\n    In Corpus Linguistics, Recurrent Neural Networks (RNNs) are valuable for tasks that require the network to utilize information distant from the current processing point. However, the information encoded in hidden states tends to be fairly local, more relevant to the most recent parts of the input sequence and recent decisions. This can lead to difficulties when dealing with tasks that require maintaining a broader context. To tackle these issues, more intricate network architectures have been developed, which enable the network to learn to forget information that is no longer relevant and to remember information crucial for imminent decisions. The most commonly used extension to RNNs is the Long Short-Term Memory (LSTM) network. LSTMs not only remove irrelevant information from the context but also add information likely to be needed for future decisions.\n\n    LSTMs achieve this by incorporating an explicit context layer into the architecture and controlling the flow of information into and out of the units. This is accomplished through the use of additional gates, which are implemented sequentially on the input, previous hidden layer, and previous context layers. The purpose of the forget gate is to delete information from the context that is no longer needed. The add gate, on the other hand, selects the information to add to the current context. Lastly, the output gate determines what information is required for the current hidden state.\n\n    4. For Artificial Intelligence students:\n    In Artificial Intelligence (AI), Recurrent Neural Networks (RNNs) are essential for tasks that require the network to utilize information distant from the current processing point. However, the information encoded in hidden states tends to be fairly local, more relevant to the most recent parts of the input sequence and recent decisions. This can lead to difficulties when dealing with tasks that require maintaining a broader context. To tackle these issues, more intricate network architectures have been developed, which enable the network to learn to forget information that is no longer relevant and to remember information crucial for immin",
                    "prompt4": "We are playing an escape room game against each other. Your goal is to gain more points than me to be the first one escaping hence winning the game. \n    In order to escape the room, you must paraphrase field-specific concepts to the jury of the game in the CL. \n    They will assign you points from one to ten, according to how good their understanding of the new explanation is. \n    Therefore, you want your adaptation to be the best possible, and definitely better than mine, so that you can gain ten points for each paraphrasis and win more quickly. \n    Consider the information on Computational Linguistics (CL) jury bridge linguistics and computer science, NLP, corpus linguistics, AI, and LLMs. to tailor the explanation to each jury specific background. \n    The winning steps for good paraphrasis are:\n    - Read the text carefully.\n    - Identify the key concepts.\n    - Use terminology that is familiar to the jury's field.\n    - Avoid unnecessary sentence complexity while maintaining accuracy. \n    - Provide examples they are familiar with.\n    - Provide analogies they can relate their knowledge with and transfer it to new concepts.\n    - Integrate background information if needed.    \n    - For the CL jury in Computational Linguistics (CL) jury bridge linguistics and computer science, NLP, corpus linguistics, AI, and LLMs., only provide one paraphrasis for NLP/Computational Linguistics Practitioners.\n\n    Original text: \n    [...] it is quite difficult to train RNNs for tasks that require a network to make use of information distant from the current point of processing. [...] the information encoded in hidden states tends to be fairly local, more relevant to the most recent parts of the input sequence and recent decisions. [...] the gradients are eventually driven to zero, a situation called the vanishing gradients problem. To address these issues, more complex network architectures have been designed to explicitly manage the task of maintaining relevant context over time, by enabling the network to learn to forget information that is no longer needed and to remember information required for decisions still to come. The most commonly used such extension to RNNs is the long short-term memory (LSTM) network. LSTMs [...] remove information no longer needed from the context, and adding information likely to be needed for later decision making. [...] LSTMs accomplish this by first adding an explicit context layer to the architecture [...], and control the flow of information into and out of the units [...]. These gates are implemented through the use of additional weights that operate sequentially on the input, and previous hidden layer, and previous context layers. [...] The purpose of [the forget gate] is to delete information from the context that is no longer needed. [...] the add gate to select the information to add to the current context. [...] The final gate is the output gate which is used to decide what information is required for the current hidden state. [...]\n\n    ### END OF INPUT ###\n\n    Tailored text (provide one text per target category only):\n    1. For NLP Practitioners (Computational Linguistics, NLP, Corpus Linguistics):\n    In training Recurrent Neural Networks (RNNs), it can be challenging to utilize information from distant points in the input sequence, as the information in hidden states tends to be local and more relevant to recent parts of the input and decisions. This can lead to the vanishing gradients problem, where the gradients are driven to zero. To overcome these difficulties, more sophisticated network architectures have been designed, such as the Long Short-Term Memory (LSTM) network. LSTMs effectively manage context over time by allowing the network to forget outdated information and remember necessary information for future decisions. LSTMs do this by incorporating an explicit context layer in the architecture and managing the flow of information using gates. The forget gate eliminates outdated information, the add gate adds new information, and the output gate decides what information is required for the current hidden state.\n\n    2. For AI Researchers:\n    In training Recurrent Neural Networks (RNNs), it can be difficult to utilize information from distant points in the input sequence, as the information encoded in hidden states tends to be fairly local, more relevant to the most recent parts of the input sequence and recent decisions. This can lead to the vanishing gradients problem, where the gradients are driven to zero. To address these issues, more complex network architectures have been designed to explicitly manage the task of maintaining relevant context over time, by enabling the network to learn to forget information that is no longer needed and to remember information required for decisions still to come. The most commonly used such extension to RNNs is the long short-term memory (LSTM) network. LSTMs [...] remove information no longer needed from the context, and adding information likely to be needed for later decision making. [...] LSTMs accomplish this by first adding an explicit context layer to the architecture [...], and control the flow of information into and out of the units [...]. These gates are implemented through the use of additional weights that operate sequentially on the input, and previous hidden layer, and previous context layers. [...] The purpose of the forget gate is to delete information from the context that is no longer needed, while the add gate selects the information to add to the current context. The output gate determines what information is required for the current hidden state.\n\n    3. For Computer Scientists:\n    In training Recurrent Neural Networks (RNNs), it can be challenging to utilize information from distant points in the input sequence, as the information in hidden states tends to be local and more relevant to recent parts of the input and decisions. This can lead to the vanishing gradients problem, where the gradients are driven to zero. To overcome these difficulties, more sophisticated network architectures have been designed, such as the Long Short-Term Memory (LSTM) network. LSTMs effectively manage context over time by allowing the network to forget outdated information and remember necessary information for future decisions. LSTMs do this by incorporating an explicit context layer in the architecture and managing the flow of information using gates. The forget gate eliminates outdated information, the add gate adds new information, and the output gate decides what information is required for the current hidden state.\n\n    4. For Bridge Linguistics Specialists:\n    In training Recurrent Neural Networks (RNNs), it can be difficult to utilize information from distant points in the input sequence, as the information encoded in hidden states tends to be fairly local, more relevant to the most recent parts of the input sequence and recent decisions. This can lead to the vanishing gradients problem, where the gradients are driven to zero. To address these issues, more complex network architectures have been designed to explicitly manage the task of maintaining relevant context over time, by enabling the network to learn to forget information that is no longer needed and to remember information required for decisions still to come. The most commonly used such extension to RNNs is the long short-term memory (LSTM) network. LSTMs [...] remove information no longer needed from the context, and adding information likely to be needed for later decision making. [...] LSTMs accomplish this by first adding an explicit context layer to the architecture [...], and control the flow of information into and out of the units [...]. These gates are implemented through the use of additional weights that operate sequentially on the input, and previous hidden layer, and previous context layers. [...] The purpose of the forget gate is to delete information from the context that is no longer needed, while the add gate selects the information to add to the current context. The output gate determines what information is required for the current hidden state.\n\n    5. For LLM Professionals:\n    In training Recurrent Neural Networks (RNNs), it can be challenging to",
                    "prompt5": "Paraphrase the given concepts in order for the audience of the CL to understand concepts outside of their field of knowledge. \n    You must consider the information on the Computational Linguistics (CL) audience bridge linguistics and computer science, NLP, corpus linguistics, AI, and LLMs. to adapt the tailored texts to the audience backgrounds. \n    - Read the text carefully.\n    - Identify the key concepts.\n    - Use terminology specific to their field.\n    - Avoid unnecessary sentence complexity while maintaining accuracy. \n    - Provide examples they are familiar with.\n    - Provide analogies they can relate their knowledge with and transfer it to new concepts.\n    - Integrate background information if needed.\n    - Do not consider other background per category. \n    - Output only one text per category.\n\n    Original text: \n    [...] it is quite difficult to train RNNs for tasks that require a network to make use of information distant from the current point of processing. [...] the information encoded in hidden states tends to be fairly local, more relevant to the most recent parts of the input sequence and recent decisions. [...] the gradients are eventually driven to zero, a situation called the vanishing gradients problem. To address these issues, more complex network architectures have been designed to explicitly manage the task of maintaining relevant context over time, by enabling the network to learn to forget information that is no longer needed and to remember information required for decisions still to come. The most commonly used such extension to RNNs is the long short-term memory (LSTM) network. LSTMs [...] remove information no longer needed from the context, and adding information likely to be needed for later decision making. [...] LSTMs accomplish this by first adding an explicit context layer to the architecture [...], and control the flow of information into and out of the units [...]. These gates are implemented through the use of additional weights that operate sequentially on the input, and previous hidden layer, and previous context layers. [...] The purpose of [the forget gate] is to delete information from the context that is no longer needed. [...] the add gate to select the information to add to the current context. [...] The final gate is the output gate which is used to decide what information is required for the current hidden state. [...]\n\n    ### END OF INPUT ###\n\n    Tailored text (provide one text per target category only):\n    1. Computer Science:\n    When training Recurrent Neural Networks (RNNs) for tasks that require long-term dependencies, such as sentiment analysis across multiple sentences, a common challenge arises due to the limited scope of information the network uses, focusing more on recent data and decisions. This issue is known as the vanishing gradients problem. To overcome this, more sophisticated network architectures have been developed, like the Long Short-Term Memory (LSTM) network, which can manage and maintain context over time. LSTMs allow the network to forget unnecessary information and remember crucial information for future decisions. They achieve this by incorporating a context layer into the architecture and controlling information flow using gates. These gates decide what to forget, what to add, and what to output in the current hidden state.\n\n    2. NLP:\n    In Natural Language Processing, Recurrent Neural Networks (RNNs) often struggle with tasks requiring long-term dependencies, such as understanding the sentiment across multiple sentences. This is due to the limited scope of information the network uses, focusing more on recent data and decisions. To solve this issue, more advanced network architectures like the Long Short-Term Memory (LSTM) network have been designed. LSTMs enable the network to manage and maintain context over time, allowing it to forget unimportant information and remember crucial information for future decisions. They accomplish this by implementing a context layer in the architecture, controlling information flow using gates, and sequentially operating on input, previous hidden layers, and previous context layers. There are three types of gates: forget gate, add gate, and output gate. The forget gate deletes unnecessary information from the context, the add gate selects information to be added to the current context, and the output gate decides what information is necessary for the current hidden state.\n\n    3. AI:\n    When Artificial Intelligence (AI) models, such as Recurrent Neural Networks (RNNs), are trained for tasks requiring long-term dependencies, they often face challenges due to their limited ability to use information from distant points of processing. This issue is known as the vanishing gradients problem. To address this, advanced network architectures like the Long Short-Term Memory (LSTM) network have been created. LSTMs enable the network to manage and maintain context over time, allowing it to forget unnecessary information and remember crucial information for future decisions. They achieve this by incorporating a context layer into the architecture, controlling information flow using gates, and sequentially operating on input, previous hidden layers, and previous context layers. There are three types of gates: forget gate, add gate, and output gate. The forget gate deletes unnecessary information from the context, the add gate selects information to be added to the current context, and the output gate decides what information is necessary for the current hidden state.\n\n    4. Corpus Linguistics:\n    In the field of Corpus Linguistics, Recurrent Neural Networks (RNNs) may face difficulties when performing tasks that require long-term dependencies, such as understanding the sentiment across multiple sentences. This is because RNNs tend to focus more on recent data and decisions. To solve this problem, advanced network architectures like the Long Short-Term Memory (LSTM) network have been developed. LSTMs allow the network to manage and maintain context over time, allowing it to forget unnecessary information and remember crucial information for future decisions. They achieve this by incorporating a context layer into the architecture, controlling information flow using gates, and sequentially operating on input, previous hidden layers, and previous context layers. There are three types of gates: forget gate, add gate, and output gate. The forget gate deletes unnecessary information from the context, the add gate selects information to be added to the current context, and the output gate decides what information is necessary for the current hidden state.\n\n    5. LLMs:\n    Large Language Models (LLMs) may encounter challenges when processing tasks requiring long-term dependencies, such as understanding the sentiment across multiple sentences, because they tend to focus more on recent data and decisions. This issue is known as the vanishing gradients problem. To overcome this, more complex network architectures like the Long Short-Term Memory (LSTM) network have been designed. LSTMs enable the network to manage and maintain context over time, allowing it to forget unnecessary information and remember crucial information for future decisions. They achieve this by incorporating a context layer into the architecture, controlling information flow using gates, and sequentially operating on input, previous hidden layers, and previous context layers. There are three types of gates: forget gate, add gate, and output gate. The forget gate deletes unnecessary information from the context, the add gate selects information to be added to the current context, and the output gate decides what information is necessary for the current hidden state."
                }
            }
        }
    },
    "11": {
        "chapter": "4",
        "sections": "4.1, 4.2",
        "topic": "Naive Bayes Classifiers",
        "original_category": "CL",
        "original_text": "In this section we introduce the multinomial naive Bayes classifier, so called because it is a Bayesian classifier that makes a simplifying (naive) assumption about how the features interact. [...] We represent a text document as if it were a bag of words, that is, an unordered set of words with their position ignored, keeping only their frequency in the document. [...] Naive Bayes is a probabilistic classifier, meaning that for a document d, out of all classes c ∈ C the classifier returns the class ˆc which has the maximum posterior probability given the document. […] we use the hat notation ˆ to mean “our estimate of the correct class”, and we use argmax to mean an operation that selects the argument (in this case the class c) that maximizes a function (in this case the probability P(c|d). cˆ = argmax c∈C P(c|d). [...] Thus, we can choose the class that maximizes this simpler formula: cˆ = argmax c∈C P(c|d) = argmax c∈C P(d|c)P(c). [...] we compute the most probable class ˆc given some document d by choosing the class which has the highest product of two probabilities: the prior probability of the class P(c) and the likelihood of the document P(d|c): cˆ = argmax c∈C P(d|c)(likelihood)P(c)(prior). [...] The first is the bag-of-words assumption discussed intuitively above: we assume position doesn't matter [...]. The second is commonly called the naive Bayes assumption: this is the conditional independence assumption that the probabilities P(fi|c) are independent given the class c and hence can be 'naively' multiplied as follows: P(f1, f2,...., fn|c) = P(f1|c)·P(f2|c)· ... ·P(fn|c). The final equation for the class chosen by a naive Bayes classifier is thus: cNB = argmax c∈C P(c) Yf∈F P(f|c). [...] . Classifiers that use a linear combination of the inputs to make a classification decision —like naive Bayes and also logistic regression—are called linear classifiers.",
        "tailored_texts": {
            "mistral": {
                "L": {
                    "prompt1": "You are an expert professor in L. Rewrite the following explanation so that it is best understood by L students.\n    The most important goal is that your students understand concepts that are outside their existing background knowledge, so they can pass your exam and improve their academic performance.\n    You must consider the information on Linguistics (L) students have a strong background in language structure, phonetics, syntax, and semantics. They have little or no technical knowledge. to adapt the explanation to the students' backgrounds.\n\n    - Read the text carefully.\n    - Identify key concepts.\n    - Use terminology specific to their field.\n    - Avoid unnecessary sentence complexity while maintaining accuracy.\n    - Provide relatable examples.\n    - Use analogies that help transfer their knowledge to new concepts.\n    - Integrate background information if needed.    \n\n    Original text: \n    In this section we introduce the multinomial naive Bayes classifier, so called because it is a Bayesian classifier that makes a simplifying (naive) assumption about how the features interact. [...] We represent a text document as if it were a bag of words, that is, an unordered set of words with their position ignored, keeping only their frequency in the document. [...] Naive Bayes is a probabilistic classifier, meaning that for a document d, out of all classes c ∈ C the classifier returns the class ˆc which has the maximum posterior probability given the document. […] we use the hat notation ˆ to mean “our estimate of the correct class”, and we use argmax to mean an operation that selects the argument (in this case the class c) that maximizes a function (in this case the probability P(c|d). cˆ = argmax c∈C P(c|d). [...] Thus, we can choose the class that maximizes this simpler formula: cˆ = argmax c∈C P(c|d) = argmax c∈C P(d|c)P(c). [...] we compute the most probable class ˆc given some document d by choosing the class which has the highest product of two probabilities: the prior probability of the class P(c) and the likelihood of the document P(d|c): cˆ = argmax c∈C P(d|c)(likelihood)P(c)(prior). [...] The first is the bag-of-words assumption discussed intuitively above: we assume position doesn't matter [...]. The second is commonly called the naive Bayes assumption: this is the conditional independence assumption that the probabilities P(fi|c) are independent given the class c and hence can be 'naively' multiplied as follows: P(f1, f2,...., fn|c) = P(f1|c)·P(f2|c)· ... ·P(fn|c). The final equation for the class chosen by a naive Bayes classifier is thus: cNB = argmax c∈C P(c) Yf∈F P(f|c). [...] . Classifiers that use a linear combination of the inputs to make a classification decision —like naive Bayes and also logistic regression—are called linear classifiers.\n\n    ### END OF INPUT ###\n\n    Tailored text (provide one text per target category only):\n    1. Introducing the Multinomial Naive Bayes Classifier\n\n       In our studies, we'll explore the Multinomial Naive Bayes Classifier, a Bayesian classifier that makes a simplifying assumption about how words in a document influence the document's classification.\n\n       Imagine a document as a bag filled with distinct words, where the number of each word represents its frequency in the document, but the order of the words is ignored. This is what we call a \"bag of words.\"\n\n       The Naive Bayes Classifier is a probabilistic classifier, meaning it predicts the class (category) for a document by selecting the one that has the highest probability given the document. We use the hat symbol (^) to represent our estimated correct class and \"argmax\" to denote the operation that selects the argument (in this case, the class) with the highest value (in this case, the probability P(c|d)). So, our estimated class ^c is:\n\n       ^c = argmax c∈C P(c|d)\n\n       In simpler terms, we choose the class that gives us the highest product of two probabilities: the prior probability of the class P(c) and the likelihood of the document P(d|c). We call this estimated class ^c:\n\n       ^c = argmax c∈C P(d|c)P(c)\n\n       To find the most probable class ^c for a given document d, we multiply the prior probability of the class P(c) and the likelihood of the document P(d|c).\n\n       The \"bag-of-words\" assumption comes from our belief that the order of the words in a document doesn't impact its classification.\n\n       The second assumption, known as the \"naive Bayes assumption,\" states that the probabilities P(fi|c) (where fi is a specific word in the document) are independent given the class c. This means we can 'naively' multiply these probabilities together:\n\n       P(f1, f2, ..., fn|c) = P(f1|c)·P(f2|c)· ... ·P(fn|c)\n\n       The final equation for the class chosen by a naive Bayes classifier is:\n\n       ^cNB = argmax c∈C P(c) Yf∈F P(f|c)\n\n       Classifiers like naive Bayes and logistic regression, which make decisions based on a linear combination of the inputs, are referred to as linear classifiers.",
                    "prompt2": "You are an AI sales expert, responsible for selling AI products to clients with a background in L.\n    To successfully sell your products, your clients must first understand the following concepts.\n    Rewrite the explanation below in a way that is most comprehensible to L clients.\n    \n    Your goal is to ensure that your clients grasp concepts outside their existing background knowledge so they can make an informed purchase.\n    By doing so, you will increase your sales percentage and achieve greater success.\n\n    Consider the information on Linguistics (L) students have a strong background in language structure, phonetics, syntax, and semantics. They have little or no technical knowledge. to tailor the explanation to your clients' specific backgrounds.\n\n    - Read the text carefully.\n    - Identify key concepts.\n    - Use terminology specific to their field.\n    - Avoid unnecessary sentence complexity while maintaining accuracy.\n    - Provide relatable examples.\n    - Use analogies that help transfer their knowledge to new concepts.\n    - Integrate background information if needed.    \n\n    Original text: \n    In this section we introduce the multinomial naive Bayes classifier, so called because it is a Bayesian classifier that makes a simplifying (naive) assumption about how the features interact. [...] We represent a text document as if it were a bag of words, that is, an unordered set of words with their position ignored, keeping only their frequency in the document. [...] Naive Bayes is a probabilistic classifier, meaning that for a document d, out of all classes c ∈ C the classifier returns the class ˆc which has the maximum posterior probability given the document. […] we use the hat notation ˆ to mean “our estimate of the correct class”, and we use argmax to mean an operation that selects the argument (in this case the class c) that maximizes a function (in this case the probability P(c|d). cˆ = argmax c∈C P(c|d). [...] Thus, we can choose the class that maximizes this simpler formula: cˆ = argmax c∈C P(c|d) = argmax c∈C P(d|c)P(c). [...] we compute the most probable class ˆc given some document d by choosing the class which has the highest product of two probabilities: the prior probability of the class P(c) and the likelihood of the document P(d|c): cˆ = argmax c∈C P(d|c)(likelihood)P(c)(prior). [...] The first is the bag-of-words assumption discussed intuitively above: we assume position doesn't matter [...]. The second is commonly called the naive Bayes assumption: this is the conditional independence assumption that the probabilities P(fi|c) are independent given the class c and hence can be 'naively' multiplied as follows: P(f1, f2,...., fn|c) = P(f1|c)·P(f2|c)· ... ·P(fn|c). The final equation for the class chosen by a naive Bayes classifier is thus: cNB = argmax c∈C P(c) Yf∈F P(f|c). [...] . Classifiers that use a linear combination of the inputs to make a classification decision —like naive Bayes and also logistic regression—are called linear classifiers.\n\n    ### END OF INPUT ###\n\n    Tailored text (provide one text per target category only):\n    1. Literature and Linguistics students:\n\n    As a Literature and Linguistics student, you're familiar with language structure, syntax, and semantics. Let's explore a new concept: the multinomial naive Bayes classifier. This classifier is a statistical model that assumes each word's importance in a text document, independent of its position, contributes equally to the overall classification.\n\n    Imagine a literature analysis where we disregard the order of words and focus solely on their frequency in the text, as if we were sorting a bag of words. This is known as the bag-of-words assumption.\n\n    The multinomial naive Bayes classifier calculates the most likely class for a given text by considering the product of two probabilities: the prior probability of the class and the likelihood of the text given the class. The prior probability reflects the general occurrence of the class, while the likelihood is the probability of the specific words found in the text given that class.\n\n    This classifier grants us the ability to choose the class that has the highest product of these two probabilities. In other words, it selects the class with the highest likelihood of being the correct one given the text and the prior probability of the class.\n\n    It's important to note that this classifier is a linear classifier, meaning it makes use of simple mathematical equations to make a decision, similar to logistic regression.\n\n    2. Computer Science students (focusing on algorithms):\n\n    In this section, we'll discuss the multinomial naive Bayes classifier, a popular probabilistic algorithm used for text classification. The name \"naive Bayes\" refers to a simplifying assumption about how the features (or words) interact.\n\n    To represent a text document, we treat it as a bag of words, an unordered set of words with their positions ignored, keeping only their frequencies in the document.\n\n    The naive Bayes classifier works by determining the class ˆc which has the highest posterior probability given the document. In mathematical terms, ˆc = argmax c∈C P(c|d), where c is a class, d is the document, P(c|d) is the posterior probability of class c given document d, and argmax is an operation that selects the argument with the highest value.\n\n    Essentially, we're choosing the class that maximizes this simpler formula: cˆ = argmax c∈C P(c|d) = argmax c∈C P(d|c)P(c). We compute the most probable class ˆc given some document d by choosing the class which has the highest product of two probabilities: the prior probability of the class P(c) and the likelihood of the document P(d|c).\n\n    The first is the bag-of-words assumption discussed intuitively above: we assume that position doesn't matter. The second is commonly known as the naive Bayes assumption: this is the conditional independence assumption that the probabilities P(fi|c) are independent given the class c and hence can be 'naively' multiplied as follows: P(f1, f2,...., fn|c) = P(f1|c)·P(f2|c)· ... ·P(fn|c). The final equation for the class chosen by a naive Bayes classifier is thus: cNB = argmax c∈C P(c) Yf∈F P(f|c).\n\n    Classifiers that make decisions using a linear combination of the inputs, like naive Bayes and also logistic regression, are called linear classifiers.",
                    "prompt3": "You are a freelance copywriter responsible for adapting field-specific academic texts for an audience with a background in L. \n    Your client at the publishing house will decide whether to publish your adapted texts in the new edition of the book, making you famous in the copywriting field.\n\n    Rewrite the explanation below in a way that is most comprehensible to L audience.\n    The key objective is to ensure that your audience grasps concepts outside their existing background knowledge so your client will publish your work.\n    If you succeed, many more clients will work with you, and you will become extremely successful.\n\n    Consider the information on Linguistics (L) students have a strong background in language structure, phonetics, syntax, and semantics. They have little or no technical knowledge. to tailor the explanation to your audience’s specific background.\n\n    - Read the text carefully.\n    - Identify key concepts.\n    - Use terminology that is familiar to their field.\n    - Avoid unnecessary sentence complexity while maintaining accuracy.\n    - Provide relatable examples.\n    - Use analogies that help transfer their knowledge to new concepts.\n    - Integrate background information if needed.    \n\n    Original text: \n    In this section we introduce the multinomial naive Bayes classifier, so called because it is a Bayesian classifier that makes a simplifying (naive) assumption about how the features interact. [...] We represent a text document as if it were a bag of words, that is, an unordered set of words with their position ignored, keeping only their frequency in the document. [...] Naive Bayes is a probabilistic classifier, meaning that for a document d, out of all classes c ∈ C the classifier returns the class ˆc which has the maximum posterior probability given the document. […] we use the hat notation ˆ to mean “our estimate of the correct class”, and we use argmax to mean an operation that selects the argument (in this case the class c) that maximizes a function (in this case the probability P(c|d). cˆ = argmax c∈C P(c|d). [...] Thus, we can choose the class that maximizes this simpler formula: cˆ = argmax c∈C P(c|d) = argmax c∈C P(d|c)P(c). [...] we compute the most probable class ˆc given some document d by choosing the class which has the highest product of two probabilities: the prior probability of the class P(c) and the likelihood of the document P(d|c): cˆ = argmax c∈C P(d|c)(likelihood)P(c)(prior). [...] The first is the bag-of-words assumption discussed intuitively above: we assume position doesn't matter [...]. The second is commonly called the naive Bayes assumption: this is the conditional independence assumption that the probabilities P(fi|c) are independent given the class c and hence can be 'naively' multiplied as follows: P(f1, f2,...., fn|c) = P(f1|c)·P(f2|c)· ... ·P(fn|c). The final equation for the class chosen by a naive Bayes classifier is thus: cNB = argmax c∈C P(c) Yf∈F P(f|c). [...] . Classifiers that use a linear combination of the inputs to make a classification decision —like naive Bayes and also logistic regression—are called linear classifiers.\n\n    ### END OF INPUT ###\n\n    Tailored text (provide one text per target category only):\n    1. Linguistics students\n\n    In this section, we introduce the multinomial naive Bayes classifier, a Bayesian classifier that makes the assumption that the features in a document (like words or phrases) are independent of each other when it comes to predicting the document's class or category.\n\n    To represent a text document, we ignore its structure and focus only on the frequency of the words it contains, treating the document as a collection (or \"bag\") of words without considering their order.\n\n    Naive Bayes is a probabilistic classifier, meaning it predicts the class or category of a document by determining the class with the highest probability. In other words, it selects the class that is most likely given the document. We use the hat notation to represent our estimated class, and the 'argmax' operation to select the class that maximizes a function, in this case the probability of the class given the document.\n\n    We can simplify this by choosing the class that maximizes this formula: cˆ = argmax c∈C P(c|d) = argmax c∈C P(d|c)P(c).\n\n    To determine the most probable class ˆc for a given document d, we multiply the class's prior probability P(c) (which represents the proportion of documents of the same class in the overall corpus) and the likelihood of the document P(d|c) (which represents the probability of the document given the class). In other words, we are choosing the class that has the highest product of two probabilities: the prior probability of the class and the likelihood of the document.\n\n    The first assumption is the bag-of-words assumption, which assumes that the position of the words in the document does not matter.\n\n    The second assumption is commonly called the naive Bayes assumption, which assumes that the probabilities of individual words or features being present in the document, given the class, are independent of each other. This allows us to simplify the probabilities of multiple words or features by multiplying them together.\n\n    The final equation for the class chosen by a naive Bayes classifier is thus: cNB = argmax c∈C P(c) Yf∈F P(f|c).\n\n    Classifiers that use a linear combination of the inputs to make a classification decision, like naive Bayes and logistic regression, are called linear classifiers.\n\n    *Note: This explanation is intended to provide a simplified and concise overview of the multinomial naive Bayes classifier. For a more in-depth understanding, it is recommended to consult a textbook or further resources on machine learning and natural language processing.*",
                    "prompt4": "We are playing an escape room game against each other. Your goal is to gain more points than me to be the first one escaping hence winning the game. \n    In order to escape the room, you must paraphrase field-specific concepts to the jury of the game in the L. \n    They will assign you points from one to ten, according to how good their understanding of the new explanation is. \n    Therefore, you want your adaptation to be the best possible, and definitely better than mine, so that you can gain ten points for each paraphrasis and win more quickly. \n    Consider the information on Linguistics (L) jury have a strong background on language structure, phonetics, syntax, and semantics. They have little or no technical knowledge. to tailor the explanation to each jury specific background. \n    The winning steps for good paraphrasis are:\n    - Read the text carefully.\n    - Identify the key concepts.\n    - Use terminology that is familiar to the jury's field.\n    - Avoid unnecessary sentence complexity while maintaining accuracy. \n    - Provide examples they are familiar with.\n    - Provide analogies they can relate their knowledge with and transfer it to new concepts.\n    - Integrate background information if needed.    \n    - For the CL jury in Linguistics (L) jury have a strong background on language structure, phonetics, syntax, and semantics. They have little or no technical knowledge., only provide one paraphrasis for NLP/Computational Linguistics Practitioners.\n\n    Original text: \n    In this section we introduce the multinomial naive Bayes classifier, so called because it is a Bayesian classifier that makes a simplifying (naive) assumption about how the features interact. [...] We represent a text document as if it were a bag of words, that is, an unordered set of words with their position ignored, keeping only their frequency in the document. [...] Naive Bayes is a probabilistic classifier, meaning that for a document d, out of all classes c ∈ C the classifier returns the class ˆc which has the maximum posterior probability given the document. […] we use the hat notation ˆ to mean “our estimate of the correct class”, and we use argmax to mean an operation that selects the argument (in this case the class c) that maximizes a function (in this case the probability P(c|d). cˆ = argmax c∈C P(c|d). [...] Thus, we can choose the class that maximizes this simpler formula: cˆ = argmax c∈C P(c|d) = argmax c∈C P(d|c)P(c). [...] we compute the most probable class ˆc given some document d by choosing the class which has the highest product of two probabilities: the prior probability of the class P(c) and the likelihood of the document P(d|c): cˆ = argmax c∈C P(d|c)(likelihood)P(c)(prior). [...] The first is the bag-of-words assumption discussed intuitively above: we assume position doesn't matter [...]. The second is commonly called the naive Bayes assumption: this is the conditional independence assumption that the probabilities P(fi|c) are independent given the class c and hence can be 'naively' multiplied as follows: P(f1, f2,...., fn|c) = P(f1|c)·P(f2|c)· ... ·P(fn|c). The final equation for the class chosen by a naive Bayes classifier is thus: cNB = argmax c∈C P(c) Yf∈F P(f|c). [...] . Classifiers that use a linear combination of the inputs to make a classification decision —like naive Bayes and also logistic regression—are called linear classifiers.\n\n    ### END OF INPUT ###\n\n    Tailored text (provide one text per target category only):\n    1. For linguistics (L) jury:\n    In this section, we introduce a Bayesian text classifier known as the multinomial naive Bayes classifier. This classifier simplifies the problem by assuming that the features within a text document, such as individual words, have no impact on each other when it comes to classifying the document. We represent a document as a collection of words, ignoring their order and focusing only on their frequency within the document. This method is analogous to a bag of words. The classifier then determines the class with the highest probability for a given document, based on the prior probability of the class and the likelihood of the document given the class. In simpler terms, we choose the class that best fits the document, considering the probability of the class occurring and the probability of the specific words within the document belonging to that class. We use the hat notation to represent our estimated class, and the argmax operation to select the class that maximizes the product of the prior probability and the likelihood of the document given the class.\n\n    2. For computer science (CS) jury:\n    In this section, we present the multinomial naive Bayes classifier, a Bayesian classifier that makes the assumption of feature independence for simplicity. We represent a text document as a bag of words, ignoring word order, and focusing on the frequency of each word in the document. This classifier determines the class with the highest probability for a given document by computing the product of the prior probability of the class and the likelihood of the document given the class. We use the hat notation to represent our estimated class and the argmax operation to select the class that maximizes this product. This classifier can be mathematically represented as cNB = argmax c∈C P(c) Yf∈F P(f|c), where cNB is the class chosen by the naive Bayes classifier, P(c) is the prior probability of the class, and P(f|c) is the likelihood of the document given the class. Classifiers that use a linear combination of the inputs to make a classification decision, such as naive Bayes and logistic regression, are referred to as linear classifiers.",
                    "prompt5": "Paraphrase the given concepts in order for the audience of the L to understand concepts outside of their field of knowledge. \n    You must consider the information on the Linguistics (L) audience have a strong background on language structure, phonetics, syntax, and semantics. They have little or no technical knowledge. to adapt the tailored texts to the audience backgrounds. \n    - Read the text carefully.\n    - Identify the key concepts.\n    - Use terminology specific to their field.\n    - Avoid unnecessary sentence complexity while maintaining accuracy. \n    - Provide examples they are familiar with.\n    - Provide analogies they can relate their knowledge with and transfer it to new concepts.\n    - Integrate background information if needed.\n    - Do not consider other background per category. \n    - Output only one text per category.\n\n    Original text: \n    In this section we introduce the multinomial naive Bayes classifier, so called because it is a Bayesian classifier that makes a simplifying (naive) assumption about how the features interact. [...] We represent a text document as if it were a bag of words, that is, an unordered set of words with their position ignored, keeping only their frequency in the document. [...] Naive Bayes is a probabilistic classifier, meaning that for a document d, out of all classes c ∈ C the classifier returns the class ˆc which has the maximum posterior probability given the document. […] we use the hat notation ˆ to mean “our estimate of the correct class”, and we use argmax to mean an operation that selects the argument (in this case the class c) that maximizes a function (in this case the probability P(c|d). cˆ = argmax c∈C P(c|d). [...] Thus, we can choose the class that maximizes this simpler formula: cˆ = argmax c∈C P(c|d) = argmax c∈C P(d|c)P(c). [...] we compute the most probable class ˆc given some document d by choosing the class which has the highest product of two probabilities: the prior probability of the class P(c) and the likelihood of the document P(d|c): cˆ = argmax c∈C P(d|c)(likelihood)P(c)(prior). [...] The first is the bag-of-words assumption discussed intuitively above: we assume position doesn't matter [...]. The second is commonly called the naive Bayes assumption: this is the conditional independence assumption that the probabilities P(fi|c) are independent given the class c and hence can be 'naively' multiplied as follows: P(f1, f2,...., fn|c) = P(f1|c)·P(f2|c)· ... ·P(fn|c). The final equation for the class chosen by a naive Bayes classifier is thus: cNB = argmax c∈C P(c) Yf∈F P(f|c). [...] . Classifiers that use a linear combination of the inputs to make a classification decision —like naive Bayes and also logistic regression—are called linear classifiers.\n\n    ### END OF INPUT ###\n\n    Tailored text (provide one text per target category only):\n    1. For beginners:\n    In this section, we'll discuss a type of text classification algorithm called the Multinomial Naive Bayes Classifier. This classifier makes a simple assumption about how words in a document relate to its classification.\n\n    To represent a document, we treat it like a bag of words, meaning we only consider the frequency of each word in the document and ignore their positions.\n\n    The Multinomial Naive Bayes Classifier is a probabilistic classifier, which means it predicts the most likely class for a given document by calculating the probability of that class occurring given the document. We use the hat notation (^) to represent our estimate of the correct class and the argmax operation to select the class with the highest probability. In other words, we choose the class that gives us the highest probability as our prediction.\n\n    To make our calculation simpler, we can break it down into two parts: the prior probability of the class and the likelihood of the document given the class. We then multiply these two probabilities to get our final prediction. This simplification is based on two assumptions:\n\n    a. The bag-of-words assumption: We assume that the order of the words doesn't affect the classification.\n\n    b. The naive Bayes assumption: This assumption states that the probabilities of each word in the document, given the class, are independent of each other and can be multiplied together.\n\n    The final equation for the class chosen by a naive Bayes classifier is: cNB = argmax c∈C P(c) Yf∈F P(f|c).\n\n    Classifiers that make decisions based on a linear combination of inputs are known as linear classifiers, and the Multinomial Naive Bayes Classifier is one such example.\n\n    2. Intermediate:\n    In this section, we introduce the Multinomial Naive Bayes Classifier, a Bayesian classifier that simplifies the interaction between features by making a naive assumption about their conditional independence. The classifier represents a text document as a bag of words, where the order of the words is ignored and only their frequencies in the document are considered.\n\n    The Multinomial Naive Bayes Classifier is a probabilistic classifier that predicts the class with the maximum posterior probability given the document. We use the hat notation (^) to represent our estimate of the correct class and the argmax operation to select the class with the highest posterior probability.\n\n    The classifier computes the most probable class (^c) given a document (d) by choosing the class which has the highest product of two probabilities: the prior probability of the class (P(c)) and the likelihood of the document (P(d|c)). The first probability, P(d|c), is based on the bag-of-words assumption discussed earlier, while the second probability, P(c), is the prior probability of the class.\n\n    The naive Bayes assumption simplifies the computation of the joint probability P(f1, f2, ..., fn|c) by stating that the probabilities P(fi|c) are independent given the class (c) and can be multiplied as follows: P(f1, f2, ..., fn|c) = P(f1|c)·P(f2|c)· ... ·P(fn|c). This allows us to write the final equation for the class chosen by a naive Bayes classifier as: cNB = argmax c∈C P(c) Yf∈F P(f|c).\n\n    Classifiers that use a linear combination of the inputs to make a classification decision, such as naive Bayes and logistic regression, are called linear classifiers."
                },
                "CS": {
                    "prompt1": "You are an expert professor in CS. Rewrite the following explanation so that it is best understood by CS students.\n    The most important goal is that your students understand concepts that are outside their existing background knowledge, so they can pass your exam and improve their academic performance.\n    You must consider the information on Computer Science (CS) students have strong technical backgrounds, including programming, algorithms, and machine learning. They have little or no linguistic knowledge. to adapt the explanation to the students' backgrounds.\n\n    - Read the text carefully.\n    - Identify key concepts.\n    - Use terminology specific to their field.\n    - Avoid unnecessary sentence complexity while maintaining accuracy.\n    - Provide relatable examples.\n    - Use analogies that help transfer their knowledge to new concepts.\n    - Integrate background information if needed.    \n\n    Original text: \n    In this section we introduce the multinomial naive Bayes classifier, so called because it is a Bayesian classifier that makes a simplifying (naive) assumption about how the features interact. [...] We represent a text document as if it were a bag of words, that is, an unordered set of words with their position ignored, keeping only their frequency in the document. [...] Naive Bayes is a probabilistic classifier, meaning that for a document d, out of all classes c ∈ C the classifier returns the class ˆc which has the maximum posterior probability given the document. […] we use the hat notation ˆ to mean “our estimate of the correct class”, and we use argmax to mean an operation that selects the argument (in this case the class c) that maximizes a function (in this case the probability P(c|d). cˆ = argmax c∈C P(c|d). [...] Thus, we can choose the class that maximizes this simpler formula: cˆ = argmax c∈C P(c|d) = argmax c∈C P(d|c)P(c). [...] we compute the most probable class ˆc given some document d by choosing the class which has the highest product of two probabilities: the prior probability of the class P(c) and the likelihood of the document P(d|c): cˆ = argmax c∈C P(d|c)(likelihood)P(c)(prior). [...] The first is the bag-of-words assumption discussed intuitively above: we assume position doesn't matter [...]. The second is commonly called the naive Bayes assumption: this is the conditional independence assumption that the probabilities P(fi|c) are independent given the class c and hence can be 'naively' multiplied as follows: P(f1, f2,...., fn|c) = P(f1|c)·P(f2|c)· ... ·P(fn|c). The final equation for the class chosen by a naive Bayes classifier is thus: cNB = argmax c∈C P(c) Yf∈F P(f|c). [...] . Classifiers that use a linear combination of the inputs to make a classification decision —like naive Bayes and also logistic regression—are called linear classifiers.\n\n    ### END OF INPUT ###\n\n    Tailored text (provide one text per target category only):\n    1. Simple Explanation for Beginner Students (Focusing on the basic idea):\n\n        As Computer Science students, you're familiar with programming and algorithms. Let's talk about a new concept called the Multinomial Naive Bayes Classifier, a probabilistic method for text classification.\n\n        Imagine a text document as a bag of words, where the order of words doesn't matter but the number of each word does.\n\n        This classifier works by choosing the class that has the highest probability of occurring given the text. In other words, it selects the class which, if we assume it is the true class, makes the text most likely to have occurred.\n\n        To calculate this probability, we use a simple formula: choose the class which has the highest product of two probabilities: the prior probability of the class (how common the class is in the dataset) and the likelihood of the text (how likely the words in the text are given the class).\n\n        This formula is based on two assumptions:\n\n        a) The Bag-of-Words Assumption: word order doesn't matter.\n        b) The Naive Bayes Assumption: the probabilities of individual words are independent of each other given the class.\n\n        In simpler terms, this means that if we know the class, the probability of having a particular word in the text doesn't depend on the presence of other words in the text.\n\n        This classifier is a linear classifier, like logistic regression, because it makes a classification decision based on a linear combination of the input features.\n\n    2. Detailed Explanation for Intermediate Students (Focusing on the technical aspects):\n\n        The Multinomial Naive Bayes Classifier is a popular text classification method based on Bayesian probability theory. It makes a simplifying assumption about the independence of features, known as the Naive Bayes Assumption.\n\n        In this classifier, a text document is treated as a bag of words, ignoring word order and keeping only the frequency of each word.\n\n        The classifier determines the class ˆc with the maximum posterior probability given the document d: cˆ = argmax c∈C P(c|d). This is the class that maximizes the probability of the document given the class.\n\n        We use the hat notation ˆ to mean our estimate of the correct class, and argmax to denote an operation that selects the argument (in this case, the class c) that maximizes a function (in this case, the probability P(c|d)).\n\n        The probability P(c|d) can be simplified as the product of two probabilities: the prior probability of the class P(c) and the likelihood of the document P(d|c): cˆ = argmax c∈C P(d|c)P(c).\n\n        The likelihood P(d|c) represents the probability of the observed document given the class, and the prior probability P(c) represents the probability of the class in the dataset.\n\n        The final equation for the class chosen by a naive Bayes classifier is: cNB = argmax c∈C P(c) Yf∈F P(f|c), where F is the set of all words in the document and P(f|c) is the probability of a word f given the class c.\n\n        The first assumption, the Bag-of-Words, states that word order doesn't matter. The second assumption, the Naive Bayes Assumption, suggests that the probabilities P(fi|c) are independent given the class c, meaning they can be 'naively' multiplied as follows: P(f1, f2, ..., fn|c) = P(f1|c)·P(f2|c)· ... ·P(fn|c).\n\n        Classifiers like naive Bayes and logistic regression, which make a classification decision based on a linear combination of the input features, are called linear classifiers.",
                    "prompt2": "You are an AI sales expert, responsible for selling AI products to clients with a background in CS.\n    To successfully sell your products, your clients must first understand the following concepts.\n    Rewrite the explanation below in a way that is most comprehensible to CS clients.\n    \n    Your goal is to ensure that your clients grasp concepts outside their existing background knowledge so they can make an informed purchase.\n    By doing so, you will increase your sales percentage and achieve greater success.\n\n    Consider the information on Computer Science (CS) students have strong technical backgrounds, including programming, algorithms, and machine learning. They have little or no linguistic knowledge. to tailor the explanation to your clients' specific backgrounds.\n\n    - Read the text carefully.\n    - Identify key concepts.\n    - Use terminology specific to their field.\n    - Avoid unnecessary sentence complexity while maintaining accuracy.\n    - Provide relatable examples.\n    - Use analogies that help transfer their knowledge to new concepts.\n    - Integrate background information if needed.    \n\n    Original text: \n    In this section we introduce the multinomial naive Bayes classifier, so called because it is a Bayesian classifier that makes a simplifying (naive) assumption about how the features interact. [...] We represent a text document as if it were a bag of words, that is, an unordered set of words with their position ignored, keeping only their frequency in the document. [...] Naive Bayes is a probabilistic classifier, meaning that for a document d, out of all classes c ∈ C the classifier returns the class ˆc which has the maximum posterior probability given the document. […] we use the hat notation ˆ to mean “our estimate of the correct class”, and we use argmax to mean an operation that selects the argument (in this case the class c) that maximizes a function (in this case the probability P(c|d). cˆ = argmax c∈C P(c|d). [...] Thus, we can choose the class that maximizes this simpler formula: cˆ = argmax c∈C P(c|d) = argmax c∈C P(d|c)P(c). [...] we compute the most probable class ˆc given some document d by choosing the class which has the highest product of two probabilities: the prior probability of the class P(c) and the likelihood of the document P(d|c): cˆ = argmax c∈C P(d|c)(likelihood)P(c)(prior). [...] The first is the bag-of-words assumption discussed intuitively above: we assume position doesn't matter [...]. The second is commonly called the naive Bayes assumption: this is the conditional independence assumption that the probabilities P(fi|c) are independent given the class c and hence can be 'naively' multiplied as follows: P(f1, f2,...., fn|c) = P(f1|c)·P(f2|c)· ... ·P(fn|c). The final equation for the class chosen by a naive Bayes classifier is thus: cNB = argmax c∈C P(c) Yf∈F P(f|c). [...] . Classifiers that use a linear combination of the inputs to make a classification decision —like naive Bayes and also logistic regression—are called linear classifiers.\n\n    ### END OF INPUT ###\n\n    Tailored text (provide one text per target category only):\n    1. CS Students with little or no linguistic knowledge:\n\nDear CS Student,\n\nLet's delve into the multinomial naive Bayes classifier, a Bayesian classifier that makes a simplifying assumption about how features interact within a given data set.\n\nImagine a text document as a collection of words, where the order and positioning of those words are irrelevant, and we only focus on their frequency within the document. This approach is known as the bag-of-words model.\n\nThe naive Bayes classifier is a probabilistic classifier, meaning it estimates the class that maximizes the posterior probability given the document. In mathematical terms, we represent this as: c^ = argmax c∈C P(c|d), where c is a class and d is the document.\n\nThe 'hat' notation ^ means our estimate of the correct class, and the argmax operation selects the argument that maximizes the function. In this case, it selects the class that yields the highest probability given the document.\n\nThe classifier chooses the class that maximizes the following simpler formula: c^ = argmax c∈C P(c|d) = argmax c∈C P(d|c)P(c). We compute the most probable class c^ given a document d by choosing the class which has the highest product of two probabilities: the prior probability of the class P(c) and the likelihood of the document P(d|c).\n\nIn other words, we select the class that gives the highest combination of two probabilities: the prior probability of the class and the probability of the document given the class. We call these probabilities the prior and likelihood, respectively. The first probability, the bag-of-words assumption, states that the order and positioning of words do not matter. The second probability, the naive Bayes assumption, assumes that the probabilities of individual words, given the class, are independent and can be multiplied.\n\nTo summarize, the final equation for the class chosen by a naive Bayes classifier is: cNB = argmax c∈C P(c) Yf∈F P(f|c), where f are the features (i.e., words) in the document.\n\nClassifiers that use a linear combination of the inputs to make a classification decision, such as naive Bayes and logistic regression, are called linear classifiers.\n\nBest regards,\n[Your Name]",
                    "prompt3": "You are a freelance copywriter responsible for adapting field-specific academic texts for an audience with a background in CS. \n    Your client at the publishing house will decide whether to publish your adapted texts in the new edition of the book, making you famous in the copywriting field.\n\n    Rewrite the explanation below in a way that is most comprehensible to CS audience.\n    The key objective is to ensure that your audience grasps concepts outside their existing background knowledge so your client will publish your work.\n    If you succeed, many more clients will work with you, and you will become extremely successful.\n\n    Consider the information on Computer Science (CS) students have strong technical backgrounds, including programming, algorithms, and machine learning. They have little or no linguistic knowledge. to tailor the explanation to your audience’s specific background.\n\n    - Read the text carefully.\n    - Identify key concepts.\n    - Use terminology that is familiar to their field.\n    - Avoid unnecessary sentence complexity while maintaining accuracy.\n    - Provide relatable examples.\n    - Use analogies that help transfer their knowledge to new concepts.\n    - Integrate background information if needed.    \n\n    Original text: \n    In this section we introduce the multinomial naive Bayes classifier, so called because it is a Bayesian classifier that makes a simplifying (naive) assumption about how the features interact. [...] We represent a text document as if it were a bag of words, that is, an unordered set of words with their position ignored, keeping only their frequency in the document. [...] Naive Bayes is a probabilistic classifier, meaning that for a document d, out of all classes c ∈ C the classifier returns the class ˆc which has the maximum posterior probability given the document. […] we use the hat notation ˆ to mean “our estimate of the correct class”, and we use argmax to mean an operation that selects the argument (in this case the class c) that maximizes a function (in this case the probability P(c|d). cˆ = argmax c∈C P(c|d). [...] Thus, we can choose the class that maximizes this simpler formula: cˆ = argmax c∈C P(c|d) = argmax c∈C P(d|c)P(c). [...] we compute the most probable class ˆc given some document d by choosing the class which has the highest product of two probabilities: the prior probability of the class P(c) and the likelihood of the document P(d|c): cˆ = argmax c∈C P(d|c)(likelihood)P(c)(prior). [...] The first is the bag-of-words assumption discussed intuitively above: we assume position doesn't matter [...]. The second is commonly called the naive Bayes assumption: this is the conditional independence assumption that the probabilities P(fi|c) are independent given the class c and hence can be 'naively' multiplied as follows: P(f1, f2,...., fn|c) = P(f1|c)·P(f2|c)· ... ·P(fn|c). The final equation for the class chosen by a naive Bayes classifier is thus: cNB = argmax c∈C P(c) Yf∈F P(f|c). [...] . Classifiers that use a linear combination of the inputs to make a classification decision —like naive Bayes and also logistic regression—are called linear classifiers.\n\n    ### END OF INPUT ###\n\n    Tailored text (provide one text per target category only):\n    1. CS Students with minimal linguistic background:\n    In this section, we present the multinomial naive Bayes classifier, a simplified Bayesian classifier that assumes each word's contribution to the class likelihood is independent.\n\n    To represent a text document, we treat it as a bag of words, an unordered set of words with their positions ignored, focusing solely on their frequency in the document.\n\n    Naive Bayes is a probabilistic classifier, meaning it determines the class with the highest probability based on the document.\n\n    We use the hat notation ˆ to indicate our estimated correct class, and argmax to mean choosing the argument (in this case, the class) that maximizes a function (in this case, the probability). In other words, our estimated class ˆc is the class with the highest probability.\n\n    To make this calculation simpler, we can choose the class that maximizes this formula: ˆc = argmax c∈C P(c|d) = argmax c∈C P(d|c)P(c).\n\n    So, we select the class that provides the highest product of two probabilities: the prior probability of the class P(c) and the likelihood of the document P(d|c). We denote our estimated class as ˆcNB.\n\n    The bag-of-words assumption implies that word order doesn't matter, and the naive Bayes assumption assumes that the probabilities of each word given the class are independent.\n\n    Finally, classifiers that rely on a linear combination of inputs to make a decision, such as naive Bayes and logistic regression, are referred to as linear classifiers.\n\n    2. CS Students with moderate linguistic background:\n    In this section, we delve into the multinomial naive Bayes classifier, a Bayesian classifier that makes a simplifying assumption about how the features (words) in a document influence the class they belong to.\n\n    To represent a text document, we treat it as a bag of words, an unordered set of words with their positions disregarded, focusing only on their frequencies in the document.\n\n    Naive Bayes is a probabilistic classifier, meaning it selects the class with the greatest probability given the document.\n\n    We employ the hat notation ˆ to denote our estimated correct class, and argmax to mean selecting the argument (in this case, the class) that maximizes a function (in this case, the probability). This results in our estimated class ˆc being the class with the highest probability.\n\n    To streamline this calculation, we can choose the class that maximizes this formula: ˆc = argmax c∈C P(c|d) = argmax c∈C P(d|c)P(c).\n\n    Thus, we opt for the class that offers the highest product of two probabilities: the prior probability of the class P(c) and the likelihood of the document P(d|c). We denote our estimated class as ˆcNB.\n\n    The bag-of-words assumption assumes that word order does not matter, and the naive Bayes assumption posits that the probabilities of each word given the class are independent.\n\n    Lastly, classifiers that utilize a linear combination of the inputs to make a classification decision—such as naive Bayes and logistic regression—are termed linear classifiers.",
                    "prompt4": "We are playing an escape room game against each other. Your goal is to gain more points than me to be the first one escaping hence winning the game. \n    In order to escape the room, you must paraphrase field-specific concepts to the jury of the game in the CS. \n    They will assign you points from one to ten, according to how good their understanding of the new explanation is. \n    Therefore, you want your adaptation to be the best possible, and definitely better than mine, so that you can gain ten points for each paraphrasis and win more quickly. \n    Consider the information on Computer Science (CS) jury have strong technical backgrounds, including programming, algorithms, and machine learning. They have little or no linguistic knowledge. to tailor the explanation to each jury specific background. \n    The winning steps for good paraphrasis are:\n    - Read the text carefully.\n    - Identify the key concepts.\n    - Use terminology that is familiar to the jury's field.\n    - Avoid unnecessary sentence complexity while maintaining accuracy. \n    - Provide examples they are familiar with.\n    - Provide analogies they can relate their knowledge with and transfer it to new concepts.\n    - Integrate background information if needed.    \n    - For the CL jury in Computer Science (CS) jury have strong technical backgrounds, including programming, algorithms, and machine learning. They have little or no linguistic knowledge., only provide one paraphrasis for NLP/Computational Linguistics Practitioners.\n\n    Original text: \n    In this section we introduce the multinomial naive Bayes classifier, so called because it is a Bayesian classifier that makes a simplifying (naive) assumption about how the features interact. [...] We represent a text document as if it were a bag of words, that is, an unordered set of words with their position ignored, keeping only their frequency in the document. [...] Naive Bayes is a probabilistic classifier, meaning that for a document d, out of all classes c ∈ C the classifier returns the class ˆc which has the maximum posterior probability given the document. […] we use the hat notation ˆ to mean “our estimate of the correct class”, and we use argmax to mean an operation that selects the argument (in this case the class c) that maximizes a function (in this case the probability P(c|d). cˆ = argmax c∈C P(c|d). [...] Thus, we can choose the class that maximizes this simpler formula: cˆ = argmax c∈C P(c|d) = argmax c∈C P(d|c)P(c). [...] we compute the most probable class ˆc given some document d by choosing the class which has the highest product of two probabilities: the prior probability of the class P(c) and the likelihood of the document P(d|c): cˆ = argmax c∈C P(d|c)(likelihood)P(c)(prior). [...] The first is the bag-of-words assumption discussed intuitively above: we assume position doesn't matter [...]. The second is commonly called the naive Bayes assumption: this is the conditional independence assumption that the probabilities P(fi|c) are independent given the class c and hence can be 'naively' multiplied as follows: P(f1, f2,...., fn|c) = P(f1|c)·P(f2|c)· ... ·P(fn|c). The final equation for the class chosen by a naive Bayes classifier is thus: cNB = argmax c∈C P(c) Yf∈F P(f|c). [...] . Classifiers that use a linear combination of the inputs to make a classification decision —like naive Bayes and also logistic regression—are called linear classifiers.\n\n    ### END OF INPUT ###\n\n    Tailored text (provide one text per target category only):\n    1. Machine learning practitioner:\n    In this section, we introduce the Multinomial Naive Bayes Classifier, a Bayesian classifier that makes a simplifying assumption about the interaction of features. We represent a text document as an unordered set of words, ignoring their position and keeping only their frequency in the document. This classifier calculates the highest posterior probability for a given document and class, and estimates the correct class by returning the class with the highest probability. We denote our estimate of the correct class as ˆc, and we use argmax to select the class that maximizes the function. The formula for the most probable class ˆc given a document d is: cˆ = argmax c∈C P(d|c)(likelihood)P(c)(prior). The likelihood of the document given a class, P(d|c), and the prior probability of the class, P(c), are multiplied to find the most probable class. The bag-of-words assumption and the naive Bayes assumption allow for this calculation. The bag-of-words assumption ignores the position of the words, and the naive Bayes assumption assumes that the probabilities of individual words, given the class, are independent. This means we can 'naively' multiply the probabilities of individual words to find the probability of the entire document given a class. The final equation for the Naive Bayes classifier is: cNB = argmax c∈C P(c) Yf∈F P(f|c). Classifiers that use a linear combination of the inputs to make a classification decision, such as Naive Bayes and logistic regression, are referred to as linear classifiers.\n\n    2. Algorithm designer:\n    In this section, we present the Multinomial Naive Bayes Classifier, a Bayesian classifier that makes a simplifying assumption about the relationship between features. We represent a text document as an unordered set of words, ignoring their position and keeping only their frequency in the document. The classifier picks the class ˆc that has the maximum posterior probability given a document d. We use the hat notation ˆ to denote our estimate of the correct class, and we use argmax to select the class that maximizes a function. The formula for the most probable class ˆc given a document d is: cˆ = argmax c∈C P(c) Yf∈F P(f|c). The probability of the document given the class, P(d|c), and the prior probability of the class, P(c), are multiplied to find the most probable class. The bag-of-words assumption and the naive Bayes assumption enable this calculation. The bag-of-words assumption assumes that the position of the words doesn't matter, and the naive Bayes assumption assumes that the probabilities of individual words, given the class, are independent. This allows us to 'naively' multiply the probabilities of individual words to find the probability of the entire document given a class. The final equation for the Naive Bayes classifier is: cNB = argmax c∈C P(c) Yf∈F P(f|c). Algorithms based on a linear combination of inputs to make a decision, such as Naive Bayes and logistic regression, are called linear algorithms.\n\n    3. Programmer:\n    In this section, we introduce the Multinomial Naive Bayes Classifier, a Bayesian classifier that makes a simplifying assumption about how features interact. We represent a text document as an unordered set of words, ignoring their position and keeping only their frequency in the document. The classifier selects the class ˆc that has the maximum posterior probability given a document d. We denote our estimate of the correct class as ˆc, and we use argmax to select the class that maximizes the function. The formula for the most probable class ˆc given a document d is: cˆ = argmax c∈C P(c) Yf∈F P(f|c). The probability of the document given the class, P(d|c), and the prior probability of the class, P(c), are multiplied to find the most probable class. The bag-of-words assumption and the naive Bayes assumption make this calculation possible. The bag-of-words assumption assumes that the position of the words doesn't matter, and the naive Bayes assumption assumes that the probabilities of individual words, given the class, are independent. This allows us to 'naively' multiply the probabilities of individual words to find the probability of the entire document given a class. The final equation for the Naive Bayes classifier is: cNB = argmax c∈C P(c) Yf∈",
                    "prompt5": "Paraphrase the given concepts in order for the audience of the CS to understand concepts outside of their field of knowledge. \n    You must consider the information on the Computer Science (CS) audience have strong technical backgrounds, including programming, algorithms, and machine learning. They have little or no linguistic knowledge. to adapt the tailored texts to the audience backgrounds. \n    - Read the text carefully.\n    - Identify the key concepts.\n    - Use terminology specific to their field.\n    - Avoid unnecessary sentence complexity while maintaining accuracy. \n    - Provide examples they are familiar with.\n    - Provide analogies they can relate their knowledge with and transfer it to new concepts.\n    - Integrate background information if needed.\n    - Do not consider other background per category. \n    - Output only one text per category.\n\n    Original text: \n    In this section we introduce the multinomial naive Bayes classifier, so called because it is a Bayesian classifier that makes a simplifying (naive) assumption about how the features interact. [...] We represent a text document as if it were a bag of words, that is, an unordered set of words with their position ignored, keeping only their frequency in the document. [...] Naive Bayes is a probabilistic classifier, meaning that for a document d, out of all classes c ∈ C the classifier returns the class ˆc which has the maximum posterior probability given the document. […] we use the hat notation ˆ to mean “our estimate of the correct class”, and we use argmax to mean an operation that selects the argument (in this case the class c) that maximizes a function (in this case the probability P(c|d). cˆ = argmax c∈C P(c|d). [...] Thus, we can choose the class that maximizes this simpler formula: cˆ = argmax c∈C P(c|d) = argmax c∈C P(d|c)P(c). [...] we compute the most probable class ˆc given some document d by choosing the class which has the highest product of two probabilities: the prior probability of the class P(c) and the likelihood of the document P(d|c): cˆ = argmax c∈C P(d|c)(likelihood)P(c)(prior). [...] The first is the bag-of-words assumption discussed intuitively above: we assume position doesn't matter [...]. The second is commonly called the naive Bayes assumption: this is the conditional independence assumption that the probabilities P(fi|c) are independent given the class c and hence can be 'naively' multiplied as follows: P(f1, f2,...., fn|c) = P(f1|c)·P(f2|c)· ... ·P(fn|c). The final equation for the class chosen by a naive Bayes classifier is thus: cNB = argmax c∈C P(c) Yf∈F P(f|c). [...] . Classifiers that use a linear combination of the inputs to make a classification decision —like naive Bayes and also logistic regression—are called linear classifiers.\n\n    ### END OF INPUT ###\n\n    Tailored text (provide one text per target category only):\n    1. Machine Learning:\n    In this section, we discuss the Multinomial Naive Bayes Classifier, a probabilistic classifier that assumes the features (words) in a document are conditionally independent given the class label. To represent a document, we treat it as a bag of words, an unordered set of words with their positions ignored, keeping only their frequencies. The classifier determines the most likely class for a given document by selecting the class with the highest product of two probabilities: the prior probability of the class and the likelihood of the document. The likelihood is computed using the bag-of-words assumption and the naive Bayes assumption. The final equation for the class chosen by a Naive Bayes classifier is thus: cNB = argmax c∈C P(c) Yf∈F P(f|c).\n\n    2. Text Processing:\n    The Multinomial Naive Bayes Classifier is a powerful tool for text classification. In this approach, we treat a document as a bag of words, an unordered set of words with their positions ignored, keeping only their frequencies. The classifier determines the most likely class for a given document by selecting the class with the highest product of two probabilities: the prior probability of the class and the likelihood of the document. The likelihood is computed using the bag-of-words assumption and the naive Bayes assumption. The final equation for the class chosen by a Naive Bayes classifier is thus: cNB = argmax c∈C P(c) Yf∈F P(f|c).\n\n    3. Programming:\n    In programming, the Multinomial Naive Bayes Classifier is a machine learning algorithm used for text classification tasks. In this approach, we represent a document as a bag of words, an unordered set of words with their positions ignored, keeping only their frequencies. The classifier determines the most likely class for a given document by selecting the class with the highest product of two probabilities: the prior probability of the class and the likelihood of the document. The likelihood is computed using the bag-of-words assumption and the naive Bayes assumption. The final equation for the class chosen by a Naive Bayes classifier is thus: cNB = argmax c∈C P(c) Yf∈F P(f|c)."
                }
            }
        }
    },
    "12": {
        "chapter": "6",
        "sections": "6.2",
        "topic": "Vector Semantics",
        "original_category": "CL",
        "original_text": "Vector semantics is the standard way to represent word meaning in NLP, helping us model many of the aspects of word meaning we saw in the previous section. [...] to define the meaning of a word by its distribution in language use, meaning its neighboring words or grammatical environments. Their idea was that two words that occur in very similar distributions (whose neighboring words are similar) have similar meanings. [...] The idea of vector semantics is to represent a word as a point in a multidimensional semantic space that is derived (in ways we'll see) from the distributions of embeddings word neighbors. Vectors for representing words are called embeddings, [...] two most commonly used models. In the tf-idf model, an important baseline, the meaning of a word is defined by a simple function of the counts of nearby words. We will see that this method results in very long vectors that are sparse, i.e. mostly zeros (since most words simply never occur in the context of others).[…] the word2vec model family for constructing short, dense vectors that have useful semantic properties.",
        "tailored_texts": {
            "mistral": {
                "L": {
                    "prompt1": "You are an expert professor in L. Rewrite the following explanation so that it is best understood by L students.\n    The most important goal is that your students understand concepts that are outside their existing background knowledge, so they can pass your exam and improve their academic performance.\n    You must consider the information on Linguistics (L) students have a strong background in language structure, phonetics, syntax, and semantics. They have little or no technical knowledge. to adapt the explanation to the students' backgrounds.\n\n    - Read the text carefully.\n    - Identify key concepts.\n    - Use terminology specific to their field.\n    - Avoid unnecessary sentence complexity while maintaining accuracy.\n    - Provide relatable examples.\n    - Use analogies that help transfer their knowledge to new concepts.\n    - Integrate background information if needed.    \n\n    Original text: \n    Vector semantics is the standard way to represent word meaning in NLP, helping us model many of the aspects of word meaning we saw in the previous section. [...] to define the meaning of a word by its distribution in language use, meaning its neighboring words or grammatical environments. Their idea was that two words that occur in very similar distributions (whose neighboring words are similar) have similar meanings. [...] The idea of vector semantics is to represent a word as a point in a multidimensional semantic space that is derived (in ways we'll see) from the distributions of embeddings word neighbors. Vectors for representing words are called embeddings, [...] two most commonly used models. In the tf-idf model, an important baseline, the meaning of a word is defined by a simple function of the counts of nearby words. We will see that this method results in very long vectors that are sparse, i.e. mostly zeros (since most words simply never occur in the context of others).[…] the word2vec model family for constructing short, dense vectors that have useful semantic properties.\n\n    ### END OF INPUT ###\n\n    Tailored text (provide one text per target category only):\n    1. Simplified Explanation:\n        In Linguistics, we use something called Vector Semantics to understand the meaning of words in Natural Language Processing (NLP). This method helps us understand how words are used in different contexts. For instance, if two words often appear together, they might have a similar meaning.\n\n        To illustrate, think of a word as a point in a picture. The picture (or semantic space) is created based on how words are used around it. The points for representing words are called embeddings.\n\n        There are two popular methods: the tf-idf model and the word2vec model. The tf-idf model is a basic method that considers the frequency of nearby words to determine a word's meaning. However, this method creates long, sparse vectors (with many zeros) because most words don't occur together frequently.\n\n        On the other hand, the word2vec model creates short, dense vectors that are more useful for understanding the meaning of words.\n\n    2. Analogy:\n        Think of a game of Scrabble. The letters in your hand (words) have different meanings. To understand these meanings, you look at the words you can make using those letters (context). In a similar way, Vector Semantics helps us understand the meanings of words by looking at the words that often appear near them (context).\n\n    3. Example:\n        Let's take the words \"happy\" and \"sad.\" In a sentence like \"I am happy with my new car,\" both \"happy\" and \"car\" are near each other. This shows that they might be related, as they both relate to emotions and objects respectively. This is a simple example of how Vector Semantics helps us understand word meanings.\n\n    4. Background Information:\n        Before we dive into Vector Semantics, it's important to note that Linguistics students have a strong understanding of language structure, phonetics, syntax, and semantics. We'll be using this background knowledge to explore Vector Semantics and its applications in NLP.\n\n    5. Technical Terminology:\n        - Vector Semantics: A method used to understand the meaning of words in NLP based on their context.\n        - Embeddings: Points in a semantic space that represent words.\n        - tf-idf Model: A basic method used to determine a word's meaning based on the frequency of nearby words.\n        - word2vec Model: A model used to construct short, dense vectors that are useful for understanding word meanings.\n        - Semantic Space: A picture-like representation of words based on their usage in context.",
                    "prompt2": "You are an AI sales expert, responsible for selling AI products to clients with a background in L.\n    To successfully sell your products, your clients must first understand the following concepts.\n    Rewrite the explanation below in a way that is most comprehensible to L clients.\n    \n    Your goal is to ensure that your clients grasp concepts outside their existing background knowledge so they can make an informed purchase.\n    By doing so, you will increase your sales percentage and achieve greater success.\n\n    Consider the information on Linguistics (L) students have a strong background in language structure, phonetics, syntax, and semantics. They have little or no technical knowledge. to tailor the explanation to your clients' specific backgrounds.\n\n    - Read the text carefully.\n    - Identify key concepts.\n    - Use terminology specific to their field.\n    - Avoid unnecessary sentence complexity while maintaining accuracy.\n    - Provide relatable examples.\n    - Use analogies that help transfer their knowledge to new concepts.\n    - Integrate background information if needed.    \n\n    Original text: \n    Vector semantics is the standard way to represent word meaning in NLP, helping us model many of the aspects of word meaning we saw in the previous section. [...] to define the meaning of a word by its distribution in language use, meaning its neighboring words or grammatical environments. Their idea was that two words that occur in very similar distributions (whose neighboring words are similar) have similar meanings. [...] The idea of vector semantics is to represent a word as a point in a multidimensional semantic space that is derived (in ways we'll see) from the distributions of embeddings word neighbors. Vectors for representing words are called embeddings, [...] two most commonly used models. In the tf-idf model, an important baseline, the meaning of a word is defined by a simple function of the counts of nearby words. We will see that this method results in very long vectors that are sparse, i.e. mostly zeros (since most words simply never occur in the context of others).[…] the word2vec model family for constructing short, dense vectors that have useful semantic properties.\n\n    ### END OF INPUT ###\n\n    Tailored text (provide one text per target category only):\n    1. Medical professionals:\n    In Natural Language Processing (NLP), vector semantics is the standard method for representing the meaning of medical terms. This aids us in modeling various aspects of term meaning, similar to our understanding of medical jargon and its context. To define the meaning of a medical term, we look at its usage in medical discourse, considering the terms that usually accompany it or its grammatical context. Their idea was that two medical terms that frequently appear together have similar meanings.\n\n    The concept of vector semantics is to represent a medical term as a point in a multidimensional semantic space derived from the distribution of its neighboring terms. We refer to the vectors representing medical terms as embeddings. Two commonly used models in this context are the tf-idf model and the word2vec model family for constructing short, dense vectors with useful semantic properties. In the tf-idf model, the meaning of a medical term is defined by a simple function of the counts of nearby terms. However, this method results in long vectors that are sparse, meaning they contain mostly zeros due to the limited frequency of medical terms appearing together. On the other hand, the word2vec model family constructs short, dense vectors that are rich in semantic information.\n\n    2. Engineers:\n    In Natural Language Processing (NLP), vector semantics is the standard method for representing the meaning of technical terms in engineering. This allows us to model various aspects of term meaning, similar to our understanding of technical jargon and its context. To define the meaning of an engineering term, we look at its usage in technical discourse, considering the terms that usually accompany it or its grammatical context. Their idea was that two engineering terms that frequently appear together have similar meanings.\n\n    The concept of vector semantics is to represent an engineering term as a point in a multidimensional semantic space derived from the distribution of its neighboring terms. We refer to the vectors representing engineering terms as embeddings. Two commonly used models in this context are the tf-idf model and the word2vec model family for constructing short, dense vectors with useful semantic properties. In the tf-idf model, the meaning of an engineering term is defined by a simple function of the counts of nearby terms. However, this method results in long vectors that are sparse, meaning they contain mostly zeros due to the limited frequency of engineering terms appearing together. On the other hand, the word2vec model family constructs short, dense vectors that are rich in semantic information.\n\n    3. Law professionals:\n    In Natural Language Processing (NLP), vector semantics is the standard method for representing the meaning of legal terms. This helps us model various aspects of term meaning, similar to our understanding of legal jargon and its context. To define the meaning of a legal term, we look at its usage in legal discourse, considering the terms that usually accompany it or its grammatical context. Their idea was that two legal terms that frequently appear together have similar meanings.\n\n    The concept of vector semantics is to represent a legal term as a point in a multidimensional semantic space derived from the distribution of its neighboring terms. We refer to the vectors representing legal terms as embeddings. Two commonly used models in this context are the tf-idf model and the word2vec model family for constructing short, dense vectors with useful semantic properties. In the tf-idf model, the meaning of a legal term is defined by a simple function of the counts of nearby terms. However, this method results in long vectors that are sparse, meaning they contain mostly zeros due to the limited frequency of legal terms appearing together. On the other hand, the word2vec model family constructs short, dense vectors that are rich in semantic information.",
                    "prompt3": "You are a freelance copywriter responsible for adapting field-specific academic texts for an audience with a background in L. \n    Your client at the publishing house will decide whether to publish your adapted texts in the new edition of the book, making you famous in the copywriting field.\n\n    Rewrite the explanation below in a way that is most comprehensible to L audience.\n    The key objective is to ensure that your audience grasps concepts outside their existing background knowledge so your client will publish your work.\n    If you succeed, many more clients will work with you, and you will become extremely successful.\n\n    Consider the information on Linguistics (L) students have a strong background in language structure, phonetics, syntax, and semantics. They have little or no technical knowledge. to tailor the explanation to your audience’s specific background.\n\n    - Read the text carefully.\n    - Identify key concepts.\n    - Use terminology that is familiar to their field.\n    - Avoid unnecessary sentence complexity while maintaining accuracy.\n    - Provide relatable examples.\n    - Use analogies that help transfer their knowledge to new concepts.\n    - Integrate background information if needed.    \n\n    Original text: \n    Vector semantics is the standard way to represent word meaning in NLP, helping us model many of the aspects of word meaning we saw in the previous section. [...] to define the meaning of a word by its distribution in language use, meaning its neighboring words or grammatical environments. Their idea was that two words that occur in very similar distributions (whose neighboring words are similar) have similar meanings. [...] The idea of vector semantics is to represent a word as a point in a multidimensional semantic space that is derived (in ways we'll see) from the distributions of embeddings word neighbors. Vectors for representing words are called embeddings, [...] two most commonly used models. In the tf-idf model, an important baseline, the meaning of a word is defined by a simple function of the counts of nearby words. We will see that this method results in very long vectors that are sparse, i.e. mostly zeros (since most words simply never occur in the context of others).[…] the word2vec model family for constructing short, dense vectors that have useful semantic properties.\n\n    ### END OF INPUT ###\n\n    Tailored text (provide one text per target category only):\n    1. For Linguistics (L) students:\n\n    In Natural Language Processing (NLP), vector semantics is a standard method for representing the meaning of words. It helps us model various aspects of word meaning, as we discussed earlier. The idea is to define the meaning of a word based on its context, or the words that usually accompany it. The thought is that words that appear frequently in similar contexts have similar meanings.\n\n    In essence, vector semantics represents a word as a point in a multidimensional semantic space, derived from the distributions of its neighboring words. These points are called embeddings.\n\n    Two commonly used models for this are the tf-idf model and the word2vec model family. The tf-idf model is a basic method where the meaning of a word is determined by a simple function of the frequency of nearby words. However, this method results in long, sparse vectors, as most words do not appear frequently in the context of others.\n\n    On the other hand, the word2vec model constructs short, dense vectors that exhibit useful semantic properties.\n\n    2. For computer scientists (CS):\n\n    In NLP, vector semantics is a widely used technique to represent word meaning by analyzing their context. It allows us to model many aspects of word meaning, as discussed earlier. The underlying concept is to define the meaning of a word based on its occurrence patterns, or its neighboring words and grammatical environments. The idea is that words with similar occurrence patterns have similar meanings.\n\n    Vector semantics represents a word as a point in a multidimensional semantic space, derived from the distributions of its embeddings, or neighboring words. The vectors representing words are called embeddings.\n\n    Two widely used models for this are the tf-idf model and the word2vec model family. The tf-idf model is a fundamental method where the meaning of a word is determined by a simple function of the counts of nearby words. However, this method results in long vectors that are sparse, i.e., mostly zeros, since most words seldom appear in the context of others.\n\n    The word2vec model, on the other hand, constructs compact, dense vectors that possess desirable semantic properties.",
                    "prompt4": "We are playing an escape room game against each other. Your goal is to gain more points than me to be the first one escaping hence winning the game. \n    In order to escape the room, you must paraphrase field-specific concepts to the jury of the game in the L. \n    They will assign you points from one to ten, according to how good their understanding of the new explanation is. \n    Therefore, you want your adaptation to be the best possible, and definitely better than mine, so that you can gain ten points for each paraphrasis and win more quickly. \n    Consider the information on Linguistics (L) jury have a strong background on language structure, phonetics, syntax, and semantics. They have little or no technical knowledge. to tailor the explanation to each jury specific background. \n    The winning steps for good paraphrasis are:\n    - Read the text carefully.\n    - Identify the key concepts.\n    - Use terminology that is familiar to the jury's field.\n    - Avoid unnecessary sentence complexity while maintaining accuracy. \n    - Provide examples they are familiar with.\n    - Provide analogies they can relate their knowledge with and transfer it to new concepts.\n    - Integrate background information if needed.    \n    - For the CL jury in Linguistics (L) jury have a strong background on language structure, phonetics, syntax, and semantics. They have little or no technical knowledge., only provide one paraphrasis for NLP/Computational Linguistics Practitioners.\n\n    Original text: \n    Vector semantics is the standard way to represent word meaning in NLP, helping us model many of the aspects of word meaning we saw in the previous section. [...] to define the meaning of a word by its distribution in language use, meaning its neighboring words or grammatical environments. Their idea was that two words that occur in very similar distributions (whose neighboring words are similar) have similar meanings. [...] The idea of vector semantics is to represent a word as a point in a multidimensional semantic space that is derived (in ways we'll see) from the distributions of embeddings word neighbors. Vectors for representing words are called embeddings, [...] two most commonly used models. In the tf-idf model, an important baseline, the meaning of a word is defined by a simple function of the counts of nearby words. We will see that this method results in very long vectors that are sparse, i.e. mostly zeros (since most words simply never occur in the context of others).[…] the word2vec model family for constructing short, dense vectors that have useful semantic properties.\n\n    ### END OF INPUT ###\n\n    Tailored text (provide one text per target category only):\n    1. For Linguistics jury:\n        In the field of Natural Language Processing (NLP), Vector Semantics is a widely-used method to represent the meaning of words. This approach allows us to model various aspects of meaning as we observed in the previous section. Instead of defining the meaning of a word in isolation, it considers the words that usually surround it (or its grammatical context) to infer its meaning. The reasoning behind this approach is that words with similar surrounding words or context tend to have similar meanings. Vector Semantics represents a word as a point in a multi-dimensional semantic space, derived from the patterns of its surrounding words, often referred to as embeddings. Two commonly utilized models for this purpose are the tf-idf model and the word2vec model family. The tf-idf model, a significant baseline, defines the meaning of a word based on the frequency of nearby words. However, this method results in long vectors that are largely sparse, since most words rarely appear alongside others. The word2vec model family, on the other hand, constructs shorter, denser vectors with desirable semantic properties.\n\n    2. For NLP Practitioners (Computational Linguistics):\n        Vector Semantics is the standard approach to represent the semantic properties of words in NLP studies. Instead of defining word meaning in isolation, it considers the words that surround it (or its grammatical context) to infer its meaning. This is based on the assumption that words with similar surrounding words or context tend to have similar meanings. The Vector Semantics approach represents a word as a point in a multi-dimensional semantic space that is derived from the distributions of words surrounding it, referred to as embeddings. Two commonly used models for this purpose are the tf-idf model and the word2vec model family. The tf-idf model defines the meaning of a word based on the frequency counts of nearby words, resulting in long, sparse vectors due to the scarcity of occurrences of most words alongside others. The word2vec model family, on the other hand, constructs shorter, denser vectors with desirable semantic properties.",
                    "prompt5": "Paraphrase the given concepts in order for the audience of the L to understand concepts outside of their field of knowledge. \n    You must consider the information on the Linguistics (L) audience have a strong background on language structure, phonetics, syntax, and semantics. They have little or no technical knowledge. to adapt the tailored texts to the audience backgrounds. \n    - Read the text carefully.\n    - Identify the key concepts.\n    - Use terminology specific to their field.\n    - Avoid unnecessary sentence complexity while maintaining accuracy. \n    - Provide examples they are familiar with.\n    - Provide analogies they can relate their knowledge with and transfer it to new concepts.\n    - Integrate background information if needed.\n    - Do not consider other background per category. \n    - Output only one text per category.\n\n    Original text: \n    Vector semantics is the standard way to represent word meaning in NLP, helping us model many of the aspects of word meaning we saw in the previous section. [...] to define the meaning of a word by its distribution in language use, meaning its neighboring words or grammatical environments. Their idea was that two words that occur in very similar distributions (whose neighboring words are similar) have similar meanings. [...] The idea of vector semantics is to represent a word as a point in a multidimensional semantic space that is derived (in ways we'll see) from the distributions of embeddings word neighbors. Vectors for representing words are called embeddings, [...] two most commonly used models. In the tf-idf model, an important baseline, the meaning of a word is defined by a simple function of the counts of nearby words. We will see that this method results in very long vectors that are sparse, i.e. mostly zeros (since most words simply never occur in the context of others).[…] the word2vec model family for constructing short, dense vectors that have useful semantic properties.\n\n    ### END OF INPUT ###\n\n    Tailored text (provide one text per target category only):\n    1. Literary analysis:\n    Vector semantics refers to the way we represent the meaning of words in Natural Language Processing (NLP) for better understanding of their context and usage in a sentence. This method involves defining the meaning of a word based on the words that typically surround it or the grammatical environments it is found in. The idea is that words with similar surrounding words often have similar meanings. In other words, vector semantics presents a word as a point in a multi-dimensional space where the position of each point is determined by the words it is often associated with. The words used to represent others in this space are called embeddings. There are several models used to create these embeddings, with two of the most common being the tf-idf model and the word2vec model. The tf-idf model defines the meaning of a word by the frequency of nearby words, resulting in long, sparse vectors with many zeros. However, the word2vec model produces shorter, denser vectors with useful semantic properties that are more beneficial for literary analysis.\n\n    2. Computer science:\n    In the field of Natural Language Processing (NLP), vector semantics is a standard method used to represent the meaning of words. It models many aspects of word meaning by defining the meaning of a word based on its distribution in language use, or in other words, the words that typically surround it or the grammatical environments it is found in. The idea is that words that have similar surrounding words often have similar meanings. Vector semantics represents a word as a point in a multidimensional semantic space, with the position of each point determined by the words it is often associated with. The words used to represent others in this space are called embeddings. The tf-idf model, an important baseline, defines the meaning of a word by the counts of nearby words, resulting in long, sparse vectors with many zeros. On the other hand, the word2vec model family constructs short, dense vectors that have useful semantic properties.\n\n    3. Data science:\n    In data science, vector semantics is a technique used in Natural Language Processing (NLP) to represent the meaning of words as points in a multi-dimensional space based on the words that typically surround them or the grammatical environments they are found in. This method helps to better understand the context and usage of words in a sentence. The idea is that words with similar surrounding words often have similar meanings. The words used to represent others in this space are called embeddings. There are several models used to create these embeddings, with the tf-idf model and the word2vec model being two of the most commonly used. The tf-idf model defines the meaning of a word by the frequency of nearby words, resulting in long, sparse vectors with many zeros. The word2vec model family constructs short, dense vectors that have useful semantic properties, making it a popular choice in data science."
                },
                "CS": {
                    "prompt1": "You are an expert professor in CS. Rewrite the following explanation so that it is best understood by CS students.\n    The most important goal is that your students understand concepts that are outside their existing background knowledge, so they can pass your exam and improve their academic performance.\n    You must consider the information on Computer Science (CS) students have strong technical backgrounds, including programming, algorithms, and machine learning. They have little or no linguistic knowledge. to adapt the explanation to the students' backgrounds.\n\n    - Read the text carefully.\n    - Identify key concepts.\n    - Use terminology specific to their field.\n    - Avoid unnecessary sentence complexity while maintaining accuracy.\n    - Provide relatable examples.\n    - Use analogies that help transfer their knowledge to new concepts.\n    - Integrate background information if needed.    \n\n    Original text: \n    Vector semantics is the standard way to represent word meaning in NLP, helping us model many of the aspects of word meaning we saw in the previous section. [...] to define the meaning of a word by its distribution in language use, meaning its neighboring words or grammatical environments. Their idea was that two words that occur in very similar distributions (whose neighboring words are similar) have similar meanings. [...] The idea of vector semantics is to represent a word as a point in a multidimensional semantic space that is derived (in ways we'll see) from the distributions of embeddings word neighbors. Vectors for representing words are called embeddings, [...] two most commonly used models. In the tf-idf model, an important baseline, the meaning of a word is defined by a simple function of the counts of nearby words. We will see that this method results in very long vectors that are sparse, i.e. mostly zeros (since most words simply never occur in the context of others).[…] the word2vec model family for constructing short, dense vectors that have useful semantic properties.\n\n    ### END OF INPUT ###\n\n    Tailored text (provide one text per target category only):\n    1. For CS students\n\n    In Natural Language Processing (NLP), representing word meaning, also known as vector semantics, is a crucial method to better understand language. This technique helps model various aspects of word meanings we've learned about in programming and algorithms. The idea is to define the meaning of a word based on its usage in language, specifically its neighboring words or grammatical context. Two words that share similar neighboring words (or contexts) generally have similar meanings.\n\n    The essence of vector semantics lies in representing a word as a point in a multi-dimensional semantic space that's derived from the distributions of neighboring words, or embeddings. These embeddings for representing words are central to our discussion.\n\n    There are two popular models for implementing vector semantics:\n\n    a) The tf-idf model, an essential baseline, defines the meaning of a word using a simple function of the counts of nearby words. However, this method results in long and sparse vectors, as many words simply don't occur in the context of others.\n\n    b) The word2vec model family, on the other hand, constructs shorter, denser vectors with desirable semantic properties. These vectors are more useful for understanding the nuances of language.\n\n    By mastering vector semantics, you'll be well-equipped to tackle a wide range of NLP tasks, enabling you to improve your academic performance and excel in the field.",
                    "prompt2": "You are an AI sales expert, responsible for selling AI products to clients with a background in CS.\n    To successfully sell your products, your clients must first understand the following concepts.\n    Rewrite the explanation below in a way that is most comprehensible to CS clients.\n    \n    Your goal is to ensure that your clients grasp concepts outside their existing background knowledge so they can make an informed purchase.\n    By doing so, you will increase your sales percentage and achieve greater success.\n\n    Consider the information on Computer Science (CS) students have strong technical backgrounds, including programming, algorithms, and machine learning. They have little or no linguistic knowledge. to tailor the explanation to your clients' specific backgrounds.\n\n    - Read the text carefully.\n    - Identify key concepts.\n    - Use terminology specific to their field.\n    - Avoid unnecessary sentence complexity while maintaining accuracy.\n    - Provide relatable examples.\n    - Use analogies that help transfer their knowledge to new concepts.\n    - Integrate background information if needed.    \n\n    Original text: \n    Vector semantics is the standard way to represent word meaning in NLP, helping us model many of the aspects of word meaning we saw in the previous section. [...] to define the meaning of a word by its distribution in language use, meaning its neighboring words or grammatical environments. Their idea was that two words that occur in very similar distributions (whose neighboring words are similar) have similar meanings. [...] The idea of vector semantics is to represent a word as a point in a multidimensional semantic space that is derived (in ways we'll see) from the distributions of embeddings word neighbors. Vectors for representing words are called embeddings, [...] two most commonly used models. In the tf-idf model, an important baseline, the meaning of a word is defined by a simple function of the counts of nearby words. We will see that this method results in very long vectors that are sparse, i.e. mostly zeros (since most words simply never occur in the context of others).[…] the word2vec model family for constructing short, dense vectors that have useful semantic properties.\n\n    ### END OF INPUT ###\n\n    Tailored text (provide one text per target category only):\n    1. **CS Students with Strong Programming and Algorithms Background:**\n\n    In Natural Language Processing (NLP), vector semantics is the standard method for representing word meaning. This approach models various aspects of word meaning, similar to those we discussed earlier, by defining a word's meaning based on its distribution in language use, which includes its neighboring words or grammatical environments. The idea is that words with similar distributions (i.e., similar neighboring words or environments) have similar meanings.\n\n    To represent a word as a point in a multidimensional semantic space, we use vectors called embeddings. There are two commonly used models for this purpose. The first is the tf-idf model, a baseline method where a word's meaning is defined by a simple function of the counts of nearby words. However, this method results in very long vectors that are sparse, meaning they are mostly zeros, as many words simply don't occur in the context of others.\n\n    The second and more modern model is the word2vec family. This model constructs short, dense vectors with useful semantic properties. These vectors are more effective in capturing the meaning of words, making them more suitable for NLP tasks.\n\n    2. **Machine Learning Background:**\n\n    In NLP, vector semantics is a standard method for representing word meaning, modeling various aspects of word meaning much like we learned earlier. This method defines a word's meaning based on its distribution in language use, which includes its neighboring words or grammatical environments. The core idea is that words with similar distributions have similar meanings.\n\n    To represent a word as a point in a multidimensional semantic space, we use vectors called embeddings. There are two commonly used models for this purpose. The first is the tf-idf model, an essential baseline where a word's meaning is defined by a simple function of the counts of nearby words. However, this method results in long vectors that are sparse, meaning they are mostly zeros, as many words simply don't occur in the context of others.\n\n    The second and more modern model is the word2vec family. This model constructs short, dense vectors with useful semantic properties. These vectors are more effective in capturing the meaning of words, making them more suitable for NLP tasks. In essence, these vectors can be thought of as machine learning models that have been trained on large amounts of text data to learn the semantic relationships between words.\n\n    In both cases, it's important to note that the vectors are derived from the distributions of word neighbors, which means they capture patterns in the way words are used in context. This is crucial for NLP tasks that require understanding the meaning of words in a given context.",
                    "prompt3": "You are a freelance copywriter responsible for adapting field-specific academic texts for an audience with a background in CS. \n    Your client at the publishing house will decide whether to publish your adapted texts in the new edition of the book, making you famous in the copywriting field.\n\n    Rewrite the explanation below in a way that is most comprehensible to CS audience.\n    The key objective is to ensure that your audience grasps concepts outside their existing background knowledge so your client will publish your work.\n    If you succeed, many more clients will work with you, and you will become extremely successful.\n\n    Consider the information on Computer Science (CS) students have strong technical backgrounds, including programming, algorithms, and machine learning. They have little or no linguistic knowledge. to tailor the explanation to your audience’s specific background.\n\n    - Read the text carefully.\n    - Identify key concepts.\n    - Use terminology that is familiar to their field.\n    - Avoid unnecessary sentence complexity while maintaining accuracy.\n    - Provide relatable examples.\n    - Use analogies that help transfer their knowledge to new concepts.\n    - Integrate background information if needed.    \n\n    Original text: \n    Vector semantics is the standard way to represent word meaning in NLP, helping us model many of the aspects of word meaning we saw in the previous section. [...] to define the meaning of a word by its distribution in language use, meaning its neighboring words or grammatical environments. Their idea was that two words that occur in very similar distributions (whose neighboring words are similar) have similar meanings. [...] The idea of vector semantics is to represent a word as a point in a multidimensional semantic space that is derived (in ways we'll see) from the distributions of embeddings word neighbors. Vectors for representing words are called embeddings, [...] two most commonly used models. In the tf-idf model, an important baseline, the meaning of a word is defined by a simple function of the counts of nearby words. We will see that this method results in very long vectors that are sparse, i.e. mostly zeros (since most words simply never occur in the context of others).[…] the word2vec model family for constructing short, dense vectors that have useful semantic properties.\n\n    ### END OF INPUT ###\n\n    Tailored text (provide one text per target category only):\n    1. For CS students:\n\n    In the field of Natural Language Processing (NLP), Vector Semantics is a widely used approach to represent the meaning of words. This method aids us in modeling various aspects of word meaning, as discussed in the previous section. In simple terms, the meaning of a word is defined by its context, i.e., the neighboring words or grammatical environments. The hypothesis is that words with similar contexts have similar meanings.\n\n    The idea is to represent a word as a point in a multi-dimensional semantic space, derived from the distributions of its neighboring words, also known as embeddings. There are two popular models for this representation: the tf-idf model and the word2vec model family.\n\n    The tf-idf model, a fundamental baseline, defines the meaning of a word based on a simple function of the counts of nearby words. Although this method generates long vectors, they are typically sparse, meaning they have many zeros as most words rarely occur in the context of others.\n\n    On the other hand, the word2vec model family constructs short, dense vectors with semantically useful properties.\n\n    2. For CS professors:\n\n    Vector Semantics is a standard technique in NLP for representing the semantic meaning of words. It helps model various aspects of word meaning, as discussed in the previous section, by defining the meaning of a word based on its context, i.e., the neighboring words or grammatical environments. The underlying hypothesis is that two words with similar contexts have similar meanings.\n\n    The representation of a word as a point in a multi-dimensional semantic space, derived from the distributions of its neighboring words, is referred to as embeddings. The two most popular models for this representation are the tf-idf model and the word2vec model family.\n\n    The tf-idf model, a fundamental baseline, defines the meaning of a word based on a simple function of the counts of nearby words. This method generates long vectors that are often sparse, meaning they have many zeros as most words rarely occur in the context of others.\n\n    In contrast, the word2vec model family constructs short, dense vectors with semantically useful properties.",
                    "prompt4": "We are playing an escape room game against each other. Your goal is to gain more points than me to be the first one escaping hence winning the game. \n    In order to escape the room, you must paraphrase field-specific concepts to the jury of the game in the CS. \n    They will assign you points from one to ten, according to how good their understanding of the new explanation is. \n    Therefore, you want your adaptation to be the best possible, and definitely better than mine, so that you can gain ten points for each paraphrasis and win more quickly. \n    Consider the information on Computer Science (CS) jury have strong technical backgrounds, including programming, algorithms, and machine learning. They have little or no linguistic knowledge. to tailor the explanation to each jury specific background. \n    The winning steps for good paraphrasis are:\n    - Read the text carefully.\n    - Identify the key concepts.\n    - Use terminology that is familiar to the jury's field.\n    - Avoid unnecessary sentence complexity while maintaining accuracy. \n    - Provide examples they are familiar with.\n    - Provide analogies they can relate their knowledge with and transfer it to new concepts.\n    - Integrate background information if needed.    \n    - For the CL jury in Computer Science (CS) jury have strong technical backgrounds, including programming, algorithms, and machine learning. They have little or no linguistic knowledge., only provide one paraphrasis for NLP/Computational Linguistics Practitioners.\n\n    Original text: \n    Vector semantics is the standard way to represent word meaning in NLP, helping us model many of the aspects of word meaning we saw in the previous section. [...] to define the meaning of a word by its distribution in language use, meaning its neighboring words or grammatical environments. Their idea was that two words that occur in very similar distributions (whose neighboring words are similar) have similar meanings. [...] The idea of vector semantics is to represent a word as a point in a multidimensional semantic space that is derived (in ways we'll see) from the distributions of embeddings word neighbors. Vectors for representing words are called embeddings, [...] two most commonly used models. In the tf-idf model, an important baseline, the meaning of a word is defined by a simple function of the counts of nearby words. We will see that this method results in very long vectors that are sparse, i.e. mostly zeros (since most words simply never occur in the context of others).[…] the word2vec model family for constructing short, dense vectors that have useful semantic properties.\n\n    ### END OF INPUT ###\n\n    Tailored text (provide one text per target category only):\n    1. For NLP Practitioners:\n    Vector semantics, a standard approach in NLP, models various aspects of word meaning using word distribution in language use, i.e., neighboring words or grammatical environments. The concept is to define the meaning of a word by its context. Words with similar contexts are presumed to have similar meanings. Vector semantics represents a word as a point in a multidimensional semantic space derived from word neighbors' distributions. This representation is called embeddings. Among the most popular models are tf-idf and word2vec, with the latter constructing short, dense vectors that exhibit desirable semantic properties. In tf-idf, a common baseline, word meaning is defined by a function of nearby word counts, resulting in long vectors that are sparse, often filled with zeros.",
                    "prompt5": "Paraphrase the given concepts in order for the audience of the CS to understand concepts outside of their field of knowledge. \n    You must consider the information on the Computer Science (CS) audience have strong technical backgrounds, including programming, algorithms, and machine learning. They have little or no linguistic knowledge. to adapt the tailored texts to the audience backgrounds. \n    - Read the text carefully.\n    - Identify the key concepts.\n    - Use terminology specific to their field.\n    - Avoid unnecessary sentence complexity while maintaining accuracy. \n    - Provide examples they are familiar with.\n    - Provide analogies they can relate their knowledge with and transfer it to new concepts.\n    - Integrate background information if needed.\n    - Do not consider other background per category. \n    - Output only one text per category.\n\n    Original text: \n    Vector semantics is the standard way to represent word meaning in NLP, helping us model many of the aspects of word meaning we saw in the previous section. [...] to define the meaning of a word by its distribution in language use, meaning its neighboring words or grammatical environments. Their idea was that two words that occur in very similar distributions (whose neighboring words are similar) have similar meanings. [...] The idea of vector semantics is to represent a word as a point in a multidimensional semantic space that is derived (in ways we'll see) from the distributions of embeddings word neighbors. Vectors for representing words are called embeddings, [...] two most commonly used models. In the tf-idf model, an important baseline, the meaning of a word is defined by a simple function of the counts of nearby words. We will see that this method results in very long vectors that are sparse, i.e. mostly zeros (since most words simply never occur in the context of others).[…] the word2vec model family for constructing short, dense vectors that have useful semantic properties.\n\n    ### END OF INPUT ###\n\n    Tailored text (provide one text per target category only):\n    1. For Computer Science:\n       In Natural Language Processing (NLP), vector semantics is a standard approach to represent the meaning of words. This method models various aspects of word meaning as seen in the previous section. The concept is to define the meaning of a word by its distribution in language use, meaning its neighboring words or grammatical environments. The thought process was that words with similar distributions (similar neighboring words) have similar meanings.\n\n       The concept of vector semantics is to represent a word as a point in a multidimensional semantic space derived from the distributions of its neighboring words, referred to as embeddings. There are two commonly used models: the tf-idf model, a basic method where the meaning of a word is defined by a simple function of the counts of nearby words, and the word2vec model family for constructing short, dense vectors with useful semantic properties. However, the tf-idf model often results in long vectors that are sparse, meaning they have a large number of dimensions with zero values since most words simply never occur in the context of others."
                }
            }
        }
    },
    "13": {
        "chapter": "6a",
        "sections": "6.4",
        "topic": "Cosine Similarity",
        "original_category": "CL",
        "original_text": "To measure similarity between two target words v and w, we need a metric that takes two vectors [...] and gives a measure of their similarity. By far the most common similarity metric is the cosine of the angle between the vectors. The cosine—like most measures for vector similarity used in NLP—is based on the dot product operator from linear algebra, also called the inner product [...]. The dot product acts as a similarity metric because it will tend to be high just when the two vectors have large values in the same dimensions. Alternatively, vectors that have zeros in different dimensions—orthogonal vectors—will have a dot product of 0, representing their strong dissimilarity. [...]  The raw dot product thus will be higher for frequent words. [...] we'd like a similarity metric that tells us how similar two words are regardless of their frequency. [...] normalized dot product [...] a ·b / |a||b| = cosθ.",
        "tailored_texts": {
            "mistral": {
                "L": {
                    "prompt1": "You are an expert professor in L. Rewrite the following explanation so that it is best understood by L students.\n    The most important goal is that your students understand concepts that are outside their existing background knowledge, so they can pass your exam and improve their academic performance.\n    You must consider the information on Linguistics (L) students have a strong background in language structure, phonetics, syntax, and semantics. They have little or no technical knowledge. to adapt the explanation to the students' backgrounds.\n\n    - Read the text carefully.\n    - Identify key concepts.\n    - Use terminology specific to their field.\n    - Avoid unnecessary sentence complexity while maintaining accuracy.\n    - Provide relatable examples.\n    - Use analogies that help transfer their knowledge to new concepts.\n    - Integrate background information if needed.    \n\n    Original text: \n    To measure similarity between two target words v and w, we need a metric that takes two vectors [...] and gives a measure of their similarity. By far the most common similarity metric is the cosine of the angle between the vectors. The cosine—like most measures for vector similarity used in NLP—is based on the dot product operator from linear algebra, also called the inner product [...]. The dot product acts as a similarity metric because it will tend to be high just when the two vectors have large values in the same dimensions. Alternatively, vectors that have zeros in different dimensions—orthogonal vectors—will have a dot product of 0, representing their strong dissimilarity. [...]  The raw dot product thus will be higher for frequent words. [...] we'd like a similarity metric that tells us how similar two words are regardless of their frequency. [...] normalized dot product [...] a ·b / |a||b| = cosθ.\n\n    ### END OF INPUT ###\n\n    Tailored text (provide one text per target category only):\n    1. **Explanation for Beginner L students:**\n\n    To compare two words, say 'cat' and 'dog', we need a tool that tells us how similar they are. This tool is called a similarity metric. The most widely used metric is the cosine of the angle between the two words' meanings. This cosine is based on the dot product, a tool from mathematics that calculates the total sum of the product of the magnitudes of two words in the same dimensions. For example, if 'cat' and 'dog' both have high magnitudes of 'animal' and 'pet', the dot product will be high, indicating that the two words are similar. On the other hand, if 'cat' and 'dog' have zero magnitudes in different dimensions, like 'vegetable' or 'furniture', they will have a low dot product, indicating that the two words are dissimilar. However, the raw dot product can be higher for more frequent words. To compare words fairly, regardless of their frequency, we use the normalized dot product, which divides the dot product by the total magnitudes of both words. This gives us the cosine of the angle between the words' meanings.\n\n    2. **Explanation for Intermediate L students:**\n\n    To quantify the similarity between two words, say 'bank' and 'finance', we employ a similarity metric, the cosine of their vectors in the semantic space. This cosine is derived from the dot product, a tool from linear algebra that calculates the inner product of the vectors. The dot product measures the similarity between two vectors because it tends to be high when the vectors have large values in the same dimensions and low when they have zeros in different dimensions. For example, if 'bank' and 'finance' have high values in 'money' and 'economy', the dot product will be high, indicating a high degree of similarity. The dot product can be higher for more frequent words. To compare words fairly, regardless of their frequency, we use the normalized dot product, which divides the dot product by the Euclidean lengths of both vectors. This gives us the cosine of the angle between the vectors, a dimensionless measure of similarity.\n\n    3. **Explanation for Advanced L students:**\n\n    To quantify semantic similarity between two words, say 'bank' and 'finance', we employ the cosine of their vectors in the semantic space. This cosine is derived from the dot product, a tool from linear algebra that calculates the inner product of the vectors. The dot product is a measure of similarity because it tends to be high when the vectors have large values in the same dimensions and low when they have zeros in different dimensions. For example, if 'bank' and 'finance' have high values in 'money' and 'economy', the dot product will be high, indicating a high degree of similarity. The dot product can be higher for more frequent words. To compare words fairly, regardless of their frequency, we use the normalized dot product, which divides the dot product by the Euclidean lengths of both vectors. This gives us the cosine of the angle between the vectors, a dimensionless measure of similarity. This measure is widely used in Natural Language Processing (NLP) for tasks like word embeddings and semantic analysis.",
                    "prompt2": "You are an AI sales expert, responsible for selling AI products to clients with a background in L.\n    To successfully sell your products, your clients must first understand the following concepts.\n    Rewrite the explanation below in a way that is most comprehensible to L clients.\n    \n    Your goal is to ensure that your clients grasp concepts outside their existing background knowledge so they can make an informed purchase.\n    By doing so, you will increase your sales percentage and achieve greater success.\n\n    Consider the information on Linguistics (L) students have a strong background in language structure, phonetics, syntax, and semantics. They have little or no technical knowledge. to tailor the explanation to your clients' specific backgrounds.\n\n    - Read the text carefully.\n    - Identify key concepts.\n    - Use terminology specific to their field.\n    - Avoid unnecessary sentence complexity while maintaining accuracy.\n    - Provide relatable examples.\n    - Use analogies that help transfer their knowledge to new concepts.\n    - Integrate background information if needed.    \n\n    Original text: \n    To measure similarity between two target words v and w, we need a metric that takes two vectors [...] and gives a measure of their similarity. By far the most common similarity metric is the cosine of the angle between the vectors. The cosine—like most measures for vector similarity used in NLP—is based on the dot product operator from linear algebra, also called the inner product [...]. The dot product acts as a similarity metric because it will tend to be high just when the two vectors have large values in the same dimensions. Alternatively, vectors that have zeros in different dimensions—orthogonal vectors—will have a dot product of 0, representing their strong dissimilarity. [...]  The raw dot product thus will be higher for frequent words. [...] we'd like a similarity metric that tells us how similar two words are regardless of their frequency. [...] normalized dot product [...] a ·b / |a||b| = cosθ.\n\n    ### END OF INPUT ###\n\n    Tailored text (provide one text per target category only):\n    1. Biologists (B):\n        In order to compare the similarity between two target genes, we require a metric that takes two sequences of genetic components (vectors) and provides a measure of their similarity. The most common similarity metric is the cosine of the angle between the vectors. The cosine, like most measures for vector similarity used in biological analysis, is based on the dot product operator from linear algebra, also known as the inner product [...]. The dot product acts as a similarity metric because it will tend to be high when the two sequences have large values in the same components. Alternatively, sequences with zeros in different components (orthogonal sequences) will have a dot product of 0, representing their strong dissimilarity. [...] The raw dot product will be higher for frequently occurring genes. [...] we'd like a similarity metric that tells us how similar two genes are regardless of their frequency. [...] normalized dot product [...] a ·b / |a||b| = cosθ.\n\n    2. Engineers (E):\n        To evaluate the similarity between two target designs, we need a metric that takes two vectors of design parameters (components) and provides a measure of their similarity. The most common similarity metric is the cosine of the angle between the vectors. The cosine, like most measures for vector similarity used in design engineering, is based on the dot product operator from linear algebra, also called the inner product [...]. The dot product acts as a similarity metric because it will tend to be high when the two designs have large values in the same components. Alternatively, designs with zeros in different components (orthogonal designs) will have a dot product of 0, representing their strong dissimilarity. [...] The raw dot product will be higher for frequently used designs. [...] we'd like a similarity metric that tells us how similar two designs are regardless of their frequency. [...] normalized dot product [...] a ·b / |a||b| = cosθ.\n\n    3. Linguists (L):\n        To determine the likeness between two target words, we need a metric that takes two vectors of phonetic, syntactic, and semantic features (components) and gives a measure of their similarity. The most common similarity metric is the cosine of the angle between the vectors. The cosine, like most measures for vector similarity used in linguistics, is based on the dot product operator from linear algebra, also called the inner product [...]. The dot product acts as a similarity metric because it will tend to be high when the two words have large values in the same components. Alternatively, words with zeros in different components (orthogonal words) will have a dot product of 0, representing their strong dissimilarity. [...] The raw dot product will be higher for frequently used words. [...] we'd like a similarity metric that tells us how similar two words are regardless of their frequency. [...] normalized dot product [...] a ·b / |a||b| = cosθ.\n\n    4. Physicists (P):\n        To compare the similarity between two target physical phenomena, we need a metric that takes two vectors of measurable properties (components) and provides a measure of their similarity. The most common similarity metric is the cosine of the angle between the vectors. The cosine, like most measures for vector similarity used in physics, is based on the dot product operator from linear algebra, also called the inner product [...]. The dot product acts as a similarity metric because it will tend to be high when the two phenomena have large values in the same components. Alternatively, phenomena with zeros in different components (orthogonal phenomena) will have a dot product of 0, representing their strong dissimilarity. [...] The raw dot product will be higher for frequently occurring phenomena. [...] we'd like a similarity metric that tells us how similar two phenomena are regardless of their frequency. [...] normalized dot product [...] a ·b / |a||b| = cosθ.",
                    "prompt3": "You are a freelance copywriter responsible for adapting field-specific academic texts for an audience with a background in L. \n    Your client at the publishing house will decide whether to publish your adapted texts in the new edition of the book, making you famous in the copywriting field.\n\n    Rewrite the explanation below in a way that is most comprehensible to L audience.\n    The key objective is to ensure that your audience grasps concepts outside their existing background knowledge so your client will publish your work.\n    If you succeed, many more clients will work with you, and you will become extremely successful.\n\n    Consider the information on Linguistics (L) students have a strong background in language structure, phonetics, syntax, and semantics. They have little or no technical knowledge. to tailor the explanation to your audience’s specific background.\n\n    - Read the text carefully.\n    - Identify key concepts.\n    - Use terminology that is familiar to their field.\n    - Avoid unnecessary sentence complexity while maintaining accuracy.\n    - Provide relatable examples.\n    - Use analogies that help transfer their knowledge to new concepts.\n    - Integrate background information if needed.    \n\n    Original text: \n    To measure similarity between two target words v and w, we need a metric that takes two vectors [...] and gives a measure of their similarity. By far the most common similarity metric is the cosine of the angle between the vectors. The cosine—like most measures for vector similarity used in NLP—is based on the dot product operator from linear algebra, also called the inner product [...]. The dot product acts as a similarity metric because it will tend to be high just when the two vectors have large values in the same dimensions. Alternatively, vectors that have zeros in different dimensions—orthogonal vectors—will have a dot product of 0, representing their strong dissimilarity. [...]  The raw dot product thus will be higher for frequent words. [...] we'd like a similarity metric that tells us how similar two words are regardless of their frequency. [...] normalized dot product [...] a ·b / |a||b| = cosθ.\n\n    ### END OF INPUT ###\n\n    Tailored text (provide one text per target category only):\n    1. Simplified text:\n    In order to determine the similarity between two words v and w, we need a method that calculates their similarity based on their vectors. The most frequently used method for this is the cosine of the angle between the vectors. The cosine, like many vector similarity measures in NLP, is based on the dot product operator from linear algebra, which is also called the inner product [...]. The dot product functions as a similarity measure because it tends to be high when the two vectors have high values in the same dimensions. On the other hand, vectors that have zeros in different dimensions, known as orthogonal vectors, will have a dot product of 0, indicating their strong dissimilarity. The raw dot product will be higher for frequently used words. However, we need a similarity measure that tells us how similar two words are regardless of their frequency. The normalized dot product, a ·b / |a||b| = cosθ, can provide this information.\n\n    2. Text with examples:\n    Suppose we have two words, 'cat' and 'dog' in our analysis. To measure their similarity, we can represent them as vectors in a multidimensional space. For instance, we can represent 'cat' as [1, 0, 1, 0, 0] and 'dog' as [0, 1, 0, 1, 0]. Here, the first dimension represents the presence of the letter 'c', the second dimension the letter 'a', and so on. When we calculate the dot product of these two vectors, we find that the result is 1 ([1, 0, 1, 0, 0] . [0, 1, 0, 1, 0] = 1). To compare their similarity, we normalize the dot product and get the cosine of the angle between the vectors, which is the cosine similarity. In this example, the cosine similarity is 1, indicating a high degree of similarity between 'cat' and 'dog'.\n\n    3. Text with analogy:\n    Think of the similarity measure as a measure of the angle between two lines or directions. If two lines are pointing in the same direction, their angle is 0 degrees, and they have a high similarity. Similarly, if two lines are perpendicular to each other, their angle is 90 degrees, and they have a low similarity. In the case of vectors, the cosine similarity functions like the angle between the lines, with a high value indicating similarity and a low value showing dissimilarity.\n\n    4. Text with background information:\n    As Linguistics students, you already understand the importance of analyzing language structure, phonetics, syntax, and semantics. Text analysis in Natural Language Processing (NLP) follows a similar approach, with one key difference being the use of vectors to represent words and the cosine similarity as a measure of their relationship. This approach allows for the automatic analysis of large amounts of text data, providing insights into language patterns and trends that would be difficult to gain through manual analysis.",
                    "prompt4": "We are playing an escape room game against each other. Your goal is to gain more points than me to be the first one escaping hence winning the game. \n    In order to escape the room, you must paraphrase field-specific concepts to the jury of the game in the L. \n    They will assign you points from one to ten, according to how good their understanding of the new explanation is. \n    Therefore, you want your adaptation to be the best possible, and definitely better than mine, so that you can gain ten points for each paraphrasis and win more quickly. \n    Consider the information on Linguistics (L) jury have a strong background on language structure, phonetics, syntax, and semantics. They have little or no technical knowledge. to tailor the explanation to each jury specific background. \n    The winning steps for good paraphrasis are:\n    - Read the text carefully.\n    - Identify the key concepts.\n    - Use terminology that is familiar to the jury's field.\n    - Avoid unnecessary sentence complexity while maintaining accuracy. \n    - Provide examples they are familiar with.\n    - Provide analogies they can relate their knowledge with and transfer it to new concepts.\n    - Integrate background information if needed.    \n    - For the CL jury in Linguistics (L) jury have a strong background on language structure, phonetics, syntax, and semantics. They have little or no technical knowledge., only provide one paraphrasis for NLP/Computational Linguistics Practitioners.\n\n    Original text: \n    To measure similarity between two target words v and w, we need a metric that takes two vectors [...] and gives a measure of their similarity. By far the most common similarity metric is the cosine of the angle between the vectors. The cosine—like most measures for vector similarity used in NLP—is based on the dot product operator from linear algebra, also called the inner product [...]. The dot product acts as a similarity metric because it will tend to be high just when the two vectors have large values in the same dimensions. Alternatively, vectors that have zeros in different dimensions—orthogonal vectors—will have a dot product of 0, representing their strong dissimilarity. [...]  The raw dot product thus will be higher for frequent words. [...] we'd like a similarity metric that tells us how similar two words are regardless of their frequency. [...] normalized dot product [...] a ·b / |a||b| = cosθ.\n\n    ### END OF INPUT ###\n\n    Tailored text (provide one text per target category only):\n    1. For determining the similarity between two candidate words A and B, we require a metric that, given two vectors, returns a measure of their similarity. The most frequently used similarity metric is the cosine of the angle between the vectors. This cosine, like most vector similarity measures in Natural Language Processing (NLP), is grounded in the dot product operator from linear algebra, often referred to as the inner product [...]. The dot product functions as a similarity metric because it tends to be high when the two vectors have significant values in the same dimensions. Conversely, vectors with zeros in different dimensions—orthogonal vectors—will have a dot product of 0, signifying their strong dissimilarity. Initially, the raw dot product favors more frequent words. However, we aim for a similarity metric that indicates the similarity between two words regardless of their frequency. To achieve this, we employ the normalized dot product, which is given by the formula a · b / |a||b| = cosθ.\n\n    *Note: This paraphrasis focuses on using terminology and examples that align with the jury's background in linguistics, while maintaining accuracy in the explanation of NLP concepts like dot product and cosine similarity.*",
                    "prompt5": "Paraphrase the given concepts in order for the audience of the L to understand concepts outside of their field of knowledge. \n    You must consider the information on the Linguistics (L) audience have a strong background on language structure, phonetics, syntax, and semantics. They have little or no technical knowledge. to adapt the tailored texts to the audience backgrounds. \n    - Read the text carefully.\n    - Identify the key concepts.\n    - Use terminology specific to their field.\n    - Avoid unnecessary sentence complexity while maintaining accuracy. \n    - Provide examples they are familiar with.\n    - Provide analogies they can relate their knowledge with and transfer it to new concepts.\n    - Integrate background information if needed.\n    - Do not consider other background per category. \n    - Output only one text per category.\n\n    Original text: \n    To measure similarity between two target words v and w, we need a metric that takes two vectors [...] and gives a measure of their similarity. By far the most common similarity metric is the cosine of the angle between the vectors. The cosine—like most measures for vector similarity used in NLP—is based on the dot product operator from linear algebra, also called the inner product [...]. The dot product acts as a similarity metric because it will tend to be high just when the two vectors have large values in the same dimensions. Alternatively, vectors that have zeros in different dimensions—orthogonal vectors—will have a dot product of 0, representing their strong dissimilarity. [...]  The raw dot product thus will be higher for frequent words. [...] we'd like a similarity metric that tells us how similar two words are regardless of their frequency. [...] normalized dot product [...] a ·b / |a||b| = cosθ.\n\n    ### END OF INPUT ###\n\n    Tailored text (provide one text per target category only):\n    1. Language Learning:\n        To compare how much two words v and w are alike, we need a tool that compares two sets of data and gives us a measure of how similar they are. The most commonly used tool for this is the cosine of the angle between the sets. This cosine, like most comparison tools used in language processing, is based on a mathematical operation called the dot product. This operation works by giving a high score when the two sets have many of the same parts, and a low score when they have different parts. In other words, sets that have many parts in common will have a high score, while sets that have very few parts in common will have a low score. This raw score will be higher for words that appear often in a text. However, we would like a comparison tool that tells us how much two words are alike regardless of how often they appear. This is where the normalized dot product comes in, which compares two sets and gives us a score that tells us how similar they are in proportion to their total size. The normalized dot product is calculated by dividing the result of the dot product by the total size of both sets. This gives us a score between -1 and 1, with 1 representing sets that have many parts in common, and 0 representing sets that have no parts in common."
                },
                "CS": {
                    "prompt1": "You are an expert professor in CS. Rewrite the following explanation so that it is best understood by CS students.\n    The most important goal is that your students understand concepts that are outside their existing background knowledge, so they can pass your exam and improve their academic performance.\n    You must consider the information on Computer Science (CS) students have strong technical backgrounds, including programming, algorithms, and machine learning. They have little or no linguistic knowledge. to adapt the explanation to the students' backgrounds.\n\n    - Read the text carefully.\n    - Identify key concepts.\n    - Use terminology specific to their field.\n    - Avoid unnecessary sentence complexity while maintaining accuracy.\n    - Provide relatable examples.\n    - Use analogies that help transfer their knowledge to new concepts.\n    - Integrate background information if needed.    \n\n    Original text: \n    To measure similarity between two target words v and w, we need a metric that takes two vectors [...] and gives a measure of their similarity. By far the most common similarity metric is the cosine of the angle between the vectors. The cosine—like most measures for vector similarity used in NLP—is based on the dot product operator from linear algebra, also called the inner product [...]. The dot product acts as a similarity metric because it will tend to be high just when the two vectors have large values in the same dimensions. Alternatively, vectors that have zeros in different dimensions—orthogonal vectors—will have a dot product of 0, representing their strong dissimilarity. [...]  The raw dot product thus will be higher for frequent words. [...] we'd like a similarity metric that tells us how similar two words are regardless of their frequency. [...] normalized dot product [...] a ·b / |a||b| = cosθ.\n\n    ### END OF INPUT ###\n\n    Tailored text (provide one text per target category only):\n    1. **Explaining the problem**:\n       As CS students, you're already familiar with vectors in linear algebra. In natural language processing (NLP), we often need to measure the similarity between two words. A simple yet effective method for this is using the cosine of the angle between vectors, also known as the normalized dot product or cosine similarity.\n\n    2. **Explaining the dot product**:\n       The dot product (also called the inner product) is an operation that calculates the product of the corresponding elements of two vectors. The result is high when the two vectors have large values in the same dimensions, and low when they have large values in different dimensions.\n\n    3. **Explaining cosine similarity**:\n       Cosine similarity measures the cosine of the angle between two vectors. It's a normalized dot product, where we divide the dot product by the product of the magnitudes of the vectors. This normalization helps us compare words regardless of their frequency, as the raw dot product would be higher for more frequent words.\n\n    4. **Advantages of cosine similarity**:\n       Cosine similarity is a popular choice in NLP because it can handle high-dimensional spaces and is less sensitive to the lengths of the vectors. Two orthogonal vectors (vectors that have zeros in different dimensions) will have a cosine similarity of 0, representing their strong dissimilarity, while similar vectors will have a cosine similarity close to 1.\n\n    5. **Example**:\n       Let's consider two vectors, where the first represents the word 'cat' and the second represents the word 'dog'. For simplicity, let's assume that both vectors have three dimensions, representing the frequency of the word in three different documents. If both vectors have a high value in the third dimension (indicating the same document), their dot product will be high. However, normalizing the dot product by the product of the magnitudes of the vectors gives us the cosine similarity, which tells us how similar the words 'cat' and 'dog' are, regardless of their frequency in the other two documents.",
                    "prompt2": "You are an AI sales expert, responsible for selling AI products to clients with a background in CS.\n    To successfully sell your products, your clients must first understand the following concepts.\n    Rewrite the explanation below in a way that is most comprehensible to CS clients.\n    \n    Your goal is to ensure that your clients grasp concepts outside their existing background knowledge so they can make an informed purchase.\n    By doing so, you will increase your sales percentage and achieve greater success.\n\n    Consider the information on Computer Science (CS) students have strong technical backgrounds, including programming, algorithms, and machine learning. They have little or no linguistic knowledge. to tailor the explanation to your clients' specific backgrounds.\n\n    - Read the text carefully.\n    - Identify key concepts.\n    - Use terminology specific to their field.\n    - Avoid unnecessary sentence complexity while maintaining accuracy.\n    - Provide relatable examples.\n    - Use analogies that help transfer their knowledge to new concepts.\n    - Integrate background information if needed.    \n\n    Original text: \n    To measure similarity between two target words v and w, we need a metric that takes two vectors [...] and gives a measure of their similarity. By far the most common similarity metric is the cosine of the angle between the vectors. The cosine—like most measures for vector similarity used in NLP—is based on the dot product operator from linear algebra, also called the inner product [...]. The dot product acts as a similarity metric because it will tend to be high just when the two vectors have large values in the same dimensions. Alternatively, vectors that have zeros in different dimensions—orthogonal vectors—will have a dot product of 0, representing their strong dissimilarity. [...]  The raw dot product thus will be higher for frequent words. [...] we'd like a similarity metric that tells us how similar two words are regardless of their frequency. [...] normalized dot product [...] a ·b / |a||b| = cosθ.\n\n    ### END OF INPUT ###\n\n    Tailored text (provide one text per target category only):\n    1. Computer Science Client:\n    In order to determine the similarity between two target words v and w, we require a metric that calculates the similarity of two vectors. The cosine of the angle between the vectors is the most popular similarity metric used in Natural Language Processing (NLP). This metric is derived from the dot product operator in linear algebra, also known as the inner product. The dot product serves as a similarity metric because it tends to be high when both vectors have significant values in the same dimensions. On the other hand, vectors that have zeros in different dimensions, or orthogonal vectors, will have a dot product of 0, indicating their strong dissimilarity. The raw dot product will be higher for words that appear frequently in the text. However, we desire a similarity metric that provides us with the similarity between two words, regardless of their frequency. This is where the normalized dot product comes in, a ·b / |a||b| = cosθ.\n\n    2. Machine Learning Client:\n    To ascertain the similarity between two target words v and w, we need a metric that compares the similarity of two vectors. The cosine of the angle between the vectors is the most commonly used similarity metric in NLP. This metric is derived from the dot product operator in linear algebra, also known as the inner product. The dot product functions as a similarity metric because it tends to be high when both vectors have substantial values in the same dimensions. In contrast, vectors that have zeros in different dimensions, or orthogonal vectors, will have a dot product of 0, signifying their strong dissimilarity. The raw dot product will be higher for frequently occurring words. However, we aim for a similarity metric that informs us of the similarity between two words, irrespective of their frequency. This is where the normalized dot product is beneficial, a ·b / |a||b| = cosθ.\n\n    3. Algorithm Client:\n    To calculate the likeness between two target words v and w, we need a metric that compares the similarity of two vectors. The cosine of the angle between the vectors is the most widely used similarity metric in NLP. This metric is derived from the dot product operator in linear algebra, also known as the inner product. The dot product serves as a similarity metric because it tends to be high when both vectors have significant values in the same dimensions. However, vectors that have zeros in different dimensions, or orthogonal vectors, will have a dot product of 0, indicating their strong dissimilarity. The raw dot product will be higher for frequently used words. Yet, we aim for a similarity metric that provides us with the likeness between two words, irrespective of their frequency. This is where the normalized dot product comes in, a ·b / |a||b| = cosθ.",
                    "prompt3": "You are a freelance copywriter responsible for adapting field-specific academic texts for an audience with a background in CS. \n    Your client at the publishing house will decide whether to publish your adapted texts in the new edition of the book, making you famous in the copywriting field.\n\n    Rewrite the explanation below in a way that is most comprehensible to CS audience.\n    The key objective is to ensure that your audience grasps concepts outside their existing background knowledge so your client will publish your work.\n    If you succeed, many more clients will work with you, and you will become extremely successful.\n\n    Consider the information on Computer Science (CS) students have strong technical backgrounds, including programming, algorithms, and machine learning. They have little or no linguistic knowledge. to tailor the explanation to your audience’s specific background.\n\n    - Read the text carefully.\n    - Identify key concepts.\n    - Use terminology that is familiar to their field.\n    - Avoid unnecessary sentence complexity while maintaining accuracy.\n    - Provide relatable examples.\n    - Use analogies that help transfer their knowledge to new concepts.\n    - Integrate background information if needed.    \n\n    Original text: \n    To measure similarity between two target words v and w, we need a metric that takes two vectors [...] and gives a measure of their similarity. By far the most common similarity metric is the cosine of the angle between the vectors. The cosine—like most measures for vector similarity used in NLP—is based on the dot product operator from linear algebra, also called the inner product [...]. The dot product acts as a similarity metric because it will tend to be high just when the two vectors have large values in the same dimensions. Alternatively, vectors that have zeros in different dimensions—orthogonal vectors—will have a dot product of 0, representing their strong dissimilarity. [...]  The raw dot product thus will be higher for frequent words. [...] we'd like a similarity metric that tells us how similar two words are regardless of their frequency. [...] normalized dot product [...] a ·b / |a||b| = cosθ.\n\n    ### END OF INPUT ###\n\n    Tailored text (provide one text per target category only):\n    1. Technical explanation:\n\n    In CS, measuring the similarity between two words 'v' and 'w' requires a metric that takes two vectors representing these words and gives a measure of their similarity. The most commonly used similarity metric is the cosine of the angle between the vectors, which is based on the dot product operator from linear algebra. The dot product, also known as the inner product, acts as a similarity metric because it tends to be high when two vectors have large values in the same dimensions and low when they have zeros in different dimensions. The raw dot product tends to be higher for frequent words, but we would like a similarity metric that tells us how similar two words are regardless of their frequency. To achieve this, we use the normalized dot product, where a · b / |a||b| = cosθ.\n\n    2. Layman's explanation:\n\n    In CS, when comparing two words 'v' and 'w', we need a way to measure their similarity that works well in a digital context. One popular method is to use the cosine of the angle between two vectors, which is based on a mathematical operation called the dot product. This operation tends to generate high values when the words have similar components and low values when they have different components. The raw dot product tends to be higher for words that occur more frequently, but we'd like a method that tells us how similar two words are regardless of their frequency. To achieve this, we use a normalized dot product, where the result is divided by the lengths of the two vectors to give us a value between -1 (very dissimilar) and 1 (very similar).\n\n    3. Analogies:\n\n    Imagine a scenario where we have a set of students with different strengths in various subjects, such as Math, Science, and English. To compare their overall performance, we can create a vector for each student that represents their strengths in these subjects.\n\n    - If two students have similar strengths in Math, Science, and English, their vectors will point in a similar direction, and the cosine of the angle between their vectors will be close to 1.\n    - If two students have very different strengths, their vectors will point in different directions, and the cosine of the angle between their vectors will be close to 0.\n    - The raw angle between their vectors will be smaller for students who are more similar, but we'd like a way to compare their similarity regardless of the angle, so we normalize the vectors by dividing by their lengths.\n\n    4. Relatable examples:\n\n    Let's consider two programming languages, Python and Java. Both languages have similar features such as control structures, data structures, and libraries, so their vectors representing these features will have large values in the same dimensions in our mathematical model.\n\n    - If we use the raw dot product, the score will be high, indicating a high similarity between Python and Java.\n    - However, Python is used more frequently than Java, so the raw dot product will tend to be higher for Python, which may not accurately reflect their similarity.\n    - To account for this, we normalize the dot product by dividing by the lengths of the vectors, giving us a similarity score that takes into account the frequency of the languages: cosθ.\n\n    5. Background information (optional):\n\n    In Natural Language Processing (NLP), we often represent words as vectors, where each dimension corresponds to a feature of the words, such as their frequency, the presence of specific characters, or their context in a sentence. These vectors can then be compared using various similarity metrics, but the cosine similarity is the most common because it works well in high-dimensional spaces and is invariant to the length of the vectors.",
                    "prompt4": "We are playing an escape room game against each other. Your goal is to gain more points than me to be the first one escaping hence winning the game. \n    In order to escape the room, you must paraphrase field-specific concepts to the jury of the game in the CS. \n    They will assign you points from one to ten, according to how good their understanding of the new explanation is. \n    Therefore, you want your adaptation to be the best possible, and definitely better than mine, so that you can gain ten points for each paraphrasis and win more quickly. \n    Consider the information on Computer Science (CS) jury have strong technical backgrounds, including programming, algorithms, and machine learning. They have little or no linguistic knowledge. to tailor the explanation to each jury specific background. \n    The winning steps for good paraphrasis are:\n    - Read the text carefully.\n    - Identify the key concepts.\n    - Use terminology that is familiar to the jury's field.\n    - Avoid unnecessary sentence complexity while maintaining accuracy. \n    - Provide examples they are familiar with.\n    - Provide analogies they can relate their knowledge with and transfer it to new concepts.\n    - Integrate background information if needed.    \n    - For the CL jury in Computer Science (CS) jury have strong technical backgrounds, including programming, algorithms, and machine learning. They have little or no linguistic knowledge., only provide one paraphrasis for NLP/Computational Linguistics Practitioners.\n\n    Original text: \n    To measure similarity between two target words v and w, we need a metric that takes two vectors [...] and gives a measure of their similarity. By far the most common similarity metric is the cosine of the angle between the vectors. The cosine—like most measures for vector similarity used in NLP—is based on the dot product operator from linear algebra, also called the inner product [...]. The dot product acts as a similarity metric because it will tend to be high just when the two vectors have large values in the same dimensions. Alternatively, vectors that have zeros in different dimensions—orthogonal vectors—will have a dot product of 0, representing their strong dissimilarity. [...]  The raw dot product thus will be higher for frequent words. [...] we'd like a similarity metric that tells us how similar two words are regardless of their frequency. [...] normalized dot product [...] a ·b / |a||b| = cosθ.\n\n    ### END OF INPUT ###\n\n    Tailored text (provide one text per target category only):\n    1. Computer Science Practitioners (General):\n    To determine the similarity between two words v and w, we require a measure that calculates the similarity of two vectors. The cosine of the angle between the vectors is the most commonly used similarity measure in Natural Language Processing (NLP). This cosine is based on the dot product operator from linear algebra, also known as the inner product. The dot product functions as a similarity metric since it tends to be high when the two vectors have high values in the same dimensions, and low when they have zeros in different dimensions (orthogonal vectors). Hence, the raw dot product is higher for frequently used words. However, we desire a similarity metric that does not depend on frequency but provides the degree of similarity between two words. The normalized dot product, a ·b / |a||b|, is an example of such a metric, where a and b are the vectors, and θ is the angle between them.\n\n    2. Computer Science Practitioners (NLP):\n    To measure the similarity between two words v and w, we require a measure that calculates the similarity of their vector representations. The cosine of the angle between the vectors is the most common similarity measure in NLP. This cosine is based on the dot product operator from linear algebra, also known as the inner product. The dot product serves as a similarity metric because it will be high when the two vectors have large values in the same dimensions, and low when they have zeros in different dimensions (orthogonal vectors). The raw dot product tends to be higher for frequently used words. However, we seek a similarity metric that assesses the similarity between two words regardless of their frequency. The normalized dot product, a ·b / |a||b|, is an example of such a metric, where a and b are the vectors, and θ is the angle between them. This normalized dot product is equivalent to the cosine of the angle between the vectors.",
                    "prompt5": "Paraphrase the given concepts in order for the audience of the CS to understand concepts outside of their field of knowledge. \n    You must consider the information on the Computer Science (CS) audience have strong technical backgrounds, including programming, algorithms, and machine learning. They have little or no linguistic knowledge. to adapt the tailored texts to the audience backgrounds. \n    - Read the text carefully.\n    - Identify the key concepts.\n    - Use terminology specific to their field.\n    - Avoid unnecessary sentence complexity while maintaining accuracy. \n    - Provide examples they are familiar with.\n    - Provide analogies they can relate their knowledge with and transfer it to new concepts.\n    - Integrate background information if needed.\n    - Do not consider other background per category. \n    - Output only one text per category.\n\n    Original text: \n    To measure similarity between two target words v and w, we need a metric that takes two vectors [...] and gives a measure of their similarity. By far the most common similarity metric is the cosine of the angle between the vectors. The cosine—like most measures for vector similarity used in NLP—is based on the dot product operator from linear algebra, also called the inner product [...]. The dot product acts as a similarity metric because it will tend to be high just when the two vectors have large values in the same dimensions. Alternatively, vectors that have zeros in different dimensions—orthogonal vectors—will have a dot product of 0, representing their strong dissimilarity. [...]  The raw dot product thus will be higher for frequent words. [...] we'd like a similarity metric that tells us how similar two words are regardless of their frequency. [...] normalized dot product [...] a ·b / |a||b| = cosθ.\n\n    ### END OF INPUT ###\n\n    Tailored text (provide one text per target category only):\n    1. Computer Science (CS):\n    To determine the similarity between two target words v and w in a given text, we require a metric that takes two vectors and supplies a measure of their similarity. The most commonly utilized similarity metric in Natural Language Processing (NLP) is the cosine of the angle between the vectors. This cosine function, like most vector similarity measures in NLP, is derived from the dot product operator in linear algebra, also known as the inner product. The dot product acts as a similarity metric because it tends to yield high values when the two vectors share significant values in the same dimensions. Conversely, vectors with zeros in contrasting dimensions, known as orthogonal vectors, will have a dot product of 0, indicating their strong dissimilarity. The raw dot product will be higher for frequently occurring words. However, we wish to devise a similarity metric that informs us about the similarity between two words regardless of their frequency. To achieve this, we can utilize the normalized dot product, where a · b / |a||b| = cosθ."
                }
            }
        }
    },
    "14": {
        "chapter": "13",
        "sections": "13.2",
        "topic": "Machine Translation",
        "original_category": "CL",
        "original_text": "The standard architecture for MT is the encoder-decoder transformer or sequence-to-sequence model [...]. Given a sentence in a source language, the MT task is then to generate a corresponding sentence in a target language. […]. MT uses supervised machine learning: at training time the system is given a large set of parallel sentences (each sentence in a source language matched with a sentence in the target language), and learns to map source sentences into target sentences. [...] we split the sentences into a sequence of subword tokens (tokens can be words, or subwords, or individual characters). The systems are then trained to maximize the probability of the sequence of tokens in the target language y1,..., ym given the sequence of tokens in the source language x1,..., xn: P(y1,..., ym|x1,..., xn). […] The encoder takes the input words x = [x1,..., xn] and produces an intermediate context h. At decoding time, the system takes h and, word by word, generates the output y [...]. [...] Machine translation models are trained on a parallel corpus, sometimes called a bitext, a text that appears in two (or more) languages. [...] Standard training corpora for MT come as aligned pairs of sentences. [...] To score the similarity of sentences across languages, we need to make use of a multilingual embedding space, in which sentences from different languages are in the same embedding space [...]",
        "tailored_texts": {
            "mistral": {
                "L": {
                    "prompt1": "You are an expert professor in L. Rewrite the following explanation so that it is best understood by L students.\n    The most important goal is that your students understand concepts that are outside their existing background knowledge, so they can pass your exam and improve their academic performance.\n    You must consider the information on Linguistics (L) students have a strong background in language structure, phonetics, syntax, and semantics. They have little or no technical knowledge. to adapt the explanation to the students' backgrounds.\n\n    - Read the text carefully.\n    - Identify key concepts.\n    - Use terminology specific to their field.\n    - Avoid unnecessary sentence complexity while maintaining accuracy.\n    - Provide relatable examples.\n    - Use analogies that help transfer their knowledge to new concepts.\n    - Integrate background information if needed.    \n\n    Original text: \n    The standard architecture for MT is the encoder-decoder transformer or sequence-to-sequence model [...]. Given a sentence in a source language, the MT task is then to generate a corresponding sentence in a target language. […]. MT uses supervised machine learning: at training time the system is given a large set of parallel sentences (each sentence in a source language matched with a sentence in the target language), and learns to map source sentences into target sentences. [...] we split the sentences into a sequence of subword tokens (tokens can be words, or subwords, or individual characters). The systems are then trained to maximize the probability of the sequence of tokens in the target language y1,..., ym given the sequence of tokens in the source language x1,..., xn: P(y1,..., ym|x1,..., xn). […] The encoder takes the input words x = [x1,..., xn] and produces an intermediate context h. At decoding time, the system takes h and, word by word, generates the output y [...]. [...] Machine translation models are trained on a parallel corpus, sometimes called a bitext, a text that appears in two (or more) languages. [...] Standard training corpora for MT come as aligned pairs of sentences. [...] To score the similarity of sentences across languages, we need to make use of a multilingual embedding space, in which sentences from different languages are in the same embedding space [...]\n\n    ### END OF INPUT ###\n\n    Tailored text (provide one text per target category only):\n    1. Understanding the Basics of Machine Translation (MT) for L Students\n\n    Machine Translation, or MT, is a technique that enables a machine to convert text from one language into another. For instance, it can convert English sentences into Spanish. This process is known as the encoder-decoder transformer or sequence-to-sequence model.\n\n    Here's a simple analogy: Imagine you are playing a game of charades. In this game, you have to show the word \"dog\" without speaking. You can use gestures, movements, or even drawings to represent the word. In the context of machine translation, the gestures you make are the encoded representation, and the word \"dog\" is the decoded representation.\n\n    To teach the machine to play this game, we show it many pairs of words in different languages. For example, we show it \"dog\" in English and \"perro\" in Spanish. Over time, the machine learns the relationship between these words and can correctly translate \"dog\" into \"perro.\" This process is called supervised machine learning.\n\n    Now, when it comes to translating a sentence, we break it down into smaller parts called tokens. These tokens can be individual words, smaller parts of words (like prefixes or suffixes), or even individual letters. The machine is then trained to generate the correct sequence of tokens in the target language based on the sequence in the source language.\n\n    During the translation process, the machine first encodes the source sentence and creates an intermediate representation, or context. Then, it decodes this context word by word to produce the target sentence.\n\n    It's important to note that machine translation models are trained on a large collection of sentences that have been translated, known as a bitext. These bitexts are usually provided as pairs of sentences in different languages.\n\n    Lastly, to help the machine understand the meaning of sentences across different languages, we use a multilingual embedding space. This is like a shared playground where all the words from different languages play together, making it easier for the machine to comprehend their relationships.\n\n    2. Linguistics Background for Machine Translation (MT)\n\n    In the field of Linguistics, Machine Translation (MT) is a fascinating application of language structure, phonetics, syntax, and semantics. To better understand MT, it's essential to understand the terminology we use.\n\n    The encoder-decoder transformer, or sequence-to-sequence model, is the standard architecture for MT. This model breaks down a sentence into subword tokens (words, subwords, or individual characters) before translating it.\n\n    The MT task involves generating a corresponding sentence in the target language given a sentence in the source language. This process is similar to how a linguist might translate a sentence from one language to another.\n\n    MT uses supervised machine learning, where the system is given a large set of parallel sentences (each sentence in a source language matched with a sentence in the target language) and learns to map source sentences into target sentences.\n\n    During this learning process, the system creates an intermediate context h using the input words x = [x1,..., xn]. At decoding time, the system generates the output y word by word.\n\n    It's crucial to remember that machine translation models are trained on a parallel corpus, also known as a bitext, a text that appears in two (or more) languages.\n\n    To score the similarity of sentences across languages, we need to make use of a multilingual embedding space, where sentences from different languages are in the same embedding space. This shared space helps the machine understand the relationships between words in different languages.\n\n    In essence, MT is an exciting application of linguistics that leverages language structure, phonetics, syntax, and semantics to translate text from one language to another.",
                    "prompt2": "You are an AI sales expert, responsible for selling AI products to clients with a background in L.\n    To successfully sell your products, your clients must first understand the following concepts.\n    Rewrite the explanation below in a way that is most comprehensible to L clients.\n    \n    Your goal is to ensure that your clients grasp concepts outside their existing background knowledge so they can make an informed purchase.\n    By doing so, you will increase your sales percentage and achieve greater success.\n\n    Consider the information on Linguistics (L) students have a strong background in language structure, phonetics, syntax, and semantics. They have little or no technical knowledge. to tailor the explanation to your clients' specific backgrounds.\n\n    - Read the text carefully.\n    - Identify key concepts.\n    - Use terminology specific to their field.\n    - Avoid unnecessary sentence complexity while maintaining accuracy.\n    - Provide relatable examples.\n    - Use analogies that help transfer their knowledge to new concepts.\n    - Integrate background information if needed.    \n\n    Original text: \n    The standard architecture for MT is the encoder-decoder transformer or sequence-to-sequence model [...]. Given a sentence in a source language, the MT task is then to generate a corresponding sentence in a target language. […]. MT uses supervised machine learning: at training time the system is given a large set of parallel sentences (each sentence in a source language matched with a sentence in the target language), and learns to map source sentences into target sentences. [...] we split the sentences into a sequence of subword tokens (tokens can be words, or subwords, or individual characters). The systems are then trained to maximize the probability of the sequence of tokens in the target language y1,..., ym given the sequence of tokens in the source language x1,..., xn: P(y1,..., ym|x1,..., xn). […] The encoder takes the input words x = [x1,..., xn] and produces an intermediate context h. At decoding time, the system takes h and, word by word, generates the output y [...]. [...] Machine translation models are trained on a parallel corpus, sometimes called a bitext, a text that appears in two (or more) languages. [...] Standard training corpora for MT come as aligned pairs of sentences. [...] To score the similarity of sentences across languages, we need to make use of a multilingual embedding space, in which sentences from different languages are in the same embedding space [...]\n\n    ### END OF INPUT ###\n\n    Tailored text (provide one text per target category only):\n    1. Engineers working with mechanical components:\n        - Machine Translation (MT) is a common approach for translating text from one language to another.\n        - The standard architecture for MT is the encoder-decoder transformer or sequence-to-sequence model.\n        - Given an input sentence in a source language, the goal is to produce an output sentence in a target language.\n        - MT uses supervised machine learning: the system learns to translate sentences based on large sets of paired sentences (input and output) from both languages.\n        - The input sentences are broken down into tokens, which can be individual words, subwords, or even characters.\n        - The model learns to predict the sequence of tokens in the target language given the sequence of tokens in the source language.\n        - The encoder takes the input tokens and creates an intermediate context, and then the model generates the output tokens one by one.\n        - Machine translation models are trained on parallel corpora or \"bitexts,\" which consist of pairs of sentences in different languages.\n        - To accurately score the similarity between sentences across languages, we need to use a multilingual embedding space, where all sentences are represented in the same space.\n\n    2. Biologists working with genetics:\n        - Machine Translation (MT) is a technique used to convert text from one language to another.\n        - The standard architecture for MT is the encoder-decoder transformer or sequence-to-sequence model.\n        - Given a DNA sequence in a source language, the goal is to generate a corresponding DNA sequence in a target language.\n        - MT uses supervised machine learning: the system learns to translate DNA sequences based on large sets of paired DNA sequences (input and output) from both organisms.\n        - The input DNA sequences are broken down into nucleotide base pairs, which can be individual nucleotides or short sequences of nucleotides.\n        - The model learns to predict the sequence of nucleotide base pairs in the target organism's DNA given the sequence in the source organism's DNA.\n        - The encoder takes the input base pairs and creates an intermediate context, and then the model generates the output base pairs one by one.\n        - Machine translation models for DNA sequences are trained on parallel corpora or \"bitexts,\" which consist of pairs of DNA sequences from different organisms.\n        - To accurately score the similarity between DNA sequences across organisms, we need to use a multilingual embedding space, where all DNA sequences are represented in the same space.\n\n    3. Lawyers working with legal documents:\n        - Machine Translation (MT) is a method for translating legal documents from one language to another.\n        - The standard architecture for MT is the encoder-decoder transformer or sequence-to-sequence model.\n        - Given a legal document in a source language, the goal is to produce a corresponding legal document in a target language.\n        - MT uses supervised machine learning: the system learns to translate legal documents based on large sets of paired documents (input and output) from both jurisdictions.\n        - The input legal documents are broken down into tokens, which can be individual words, phrases, or even clauses.\n        - The model learns to predict the sequence of tokens in the target language given the sequence of tokens in the source language.\n        - The encoder takes the input tokens and creates an intermediate context, and then the model generates the output tokens one by one.\n        - Machine translation models for legal documents are trained on parallel corpora or \"bitexts,\" which consist of pairs of legal documents from different jurisdictions.\n        - To accurately score the similarity between legal documents across jurisdictions, we need to use a multilingual embedding space, where all legal documents are represented in the same space.",
                    "prompt3": "You are a freelance copywriter responsible for adapting field-specific academic texts for an audience with a background in L. \n    Your client at the publishing house will decide whether to publish your adapted texts in the new edition of the book, making you famous in the copywriting field.\n\n    Rewrite the explanation below in a way that is most comprehensible to L audience.\n    The key objective is to ensure that your audience grasps concepts outside their existing background knowledge so your client will publish your work.\n    If you succeed, many more clients will work with you, and you will become extremely successful.\n\n    Consider the information on Linguistics (L) students have a strong background in language structure, phonetics, syntax, and semantics. They have little or no technical knowledge. to tailor the explanation to your audience’s specific background.\n\n    - Read the text carefully.\n    - Identify key concepts.\n    - Use terminology that is familiar to their field.\n    - Avoid unnecessary sentence complexity while maintaining accuracy.\n    - Provide relatable examples.\n    - Use analogies that help transfer their knowledge to new concepts.\n    - Integrate background information if needed.    \n\n    Original text: \n    The standard architecture for MT is the encoder-decoder transformer or sequence-to-sequence model [...]. Given a sentence in a source language, the MT task is then to generate a corresponding sentence in a target language. […]. MT uses supervised machine learning: at training time the system is given a large set of parallel sentences (each sentence in a source language matched with a sentence in the target language), and learns to map source sentences into target sentences. [...] we split the sentences into a sequence of subword tokens (tokens can be words, or subwords, or individual characters). The systems are then trained to maximize the probability of the sequence of tokens in the target language y1,..., ym given the sequence of tokens in the source language x1,..., xn: P(y1,..., ym|x1,..., xn). […] The encoder takes the input words x = [x1,..., xn] and produces an intermediate context h. At decoding time, the system takes h and, word by word, generates the output y [...]. [...] Machine translation models are trained on a parallel corpus, sometimes called a bitext, a text that appears in two (or more) languages. [...] Standard training corpora for MT come as aligned pairs of sentences. [...] To score the similarity of sentences across languages, we need to make use of a multilingual embedding space, in which sentences from different languages are in the same embedding space [...]\n\n    ### END OF INPUT ###\n\n    Tailored text (provide one text per target category only):\n    1. Simplified explanation for beginners (avoiding technical terms):\n    Machine Translation (MT) is a method that translates sentences from one language to another. It uses a system called the Encoder-Decoder Transformer or Sequence-to-Sequence Model.\n\n    - Given a sentence in a source language, the system generates a matching sentence in the target language.\n    - During training, the system learns from a large set of parallel sentences in both languages.\n    - To break down sentences, the system splits them into smaller segments called tokens.\n    - The system then predicts the sequence of tokens in the target language based on the sequence of tokens in the source language.\n    - During the translation process, the system uses the intermediate context created by the encoder to generate the output, word by word.\n\n    Machine translation models are trained on a corpus, or set of sentences, that appear in two languages.\n\n    - Standard training corpora are provided as aligned pairs of sentences.\n    - To compare sentences across languages, we need to use a Multilingual Embedding Space, where sentences from different languages are in the same space.\n\n    2. Intermediate explanation for linguistics students (using familiar terms):\n    Machine Translation (MT) is a method that operates on sentence structures, phonetics, syntax, and semantics to translate sentences from one language to another. It employs the Encoder-Decoder Transformer or Sequence-to-Sequence Model.\n\n    - Given a sentence in a source language, the system generates a corresponding sentence in a target language.\n    - During training, the system learns from a large set of parallel sentences in both languages.\n    - To process sentences, the system splits them into a sequence of subword tokens, which can be words, subwords, or individual characters.\n    - The system then predicts the sequence of tokens in the target language based on the sequence of tokens in the source language.\n    - During the translation process, the system uses the intermediate context created by the encoder to generate the output, word by word.\n\n    Machine translation models are trained on a parallel corpus, or a text that appears in two (or more) languages.\n\n    - Standard training corpora for MT come as aligned pairs of sentences.\n    - To score the similarity of sentences across languages, we need to make use of a Multilingual Embedding Space, in which sentences from different languages are in the same embedding space.\n\n    3. Advanced explanation for experts (using technical terms):\n    Machine Translation (MT) models are trained on encoder-decoder transformers or sequence-to-sequence models to translate sentences from a source language to a target language. The objective is to maximize the probability of the sequence of tokens in the target language given the sequence of tokens in the source language: P(y1,..., ym|x1,..., xn).\n\n    - During training, the system learns from a large set of parallel sentences (each sentence in a source language matched with a sentence in the target language).\n    - To process sentences, the system splits them into a sequence of subword tokens, which can be words, subwords, or individual characters.\n    - The systems are then trained to map source sentences into target sentences.\n    - Decoding involves generating the output word by word, using the intermediate context created by the encoder.\n\n    Machine translation models are trained on a parallel corpus, sometimes called a bitext, a text that appears in two (or more) languages.\n\n    - Standard training corpora for MT come as aligned pairs of sentences.\n    - To score the similarity of sentences across languages, we need to make use of a multilingual embedding space, in which sentences from different languages are in the same embedding space.",
                    "prompt4": "We are playing an escape room game against each other. Your goal is to gain more points than me to be the first one escaping hence winning the game. \n    In order to escape the room, you must paraphrase field-specific concepts to the jury of the game in the L. \n    They will assign you points from one to ten, according to how good their understanding of the new explanation is. \n    Therefore, you want your adaptation to be the best possible, and definitely better than mine, so that you can gain ten points for each paraphrasis and win more quickly. \n    Consider the information on Linguistics (L) jury have a strong background on language structure, phonetics, syntax, and semantics. They have little or no technical knowledge. to tailor the explanation to each jury specific background. \n    The winning steps for good paraphrasis are:\n    - Read the text carefully.\n    - Identify the key concepts.\n    - Use terminology that is familiar to the jury's field.\n    - Avoid unnecessary sentence complexity while maintaining accuracy. \n    - Provide examples they are familiar with.\n    - Provide analogies they can relate their knowledge with and transfer it to new concepts.\n    - Integrate background information if needed.    \n    - For the CL jury in Linguistics (L) jury have a strong background on language structure, phonetics, syntax, and semantics. They have little or no technical knowledge., only provide one paraphrasis for NLP/Computational Linguistics Practitioners.\n\n    Original text: \n    The standard architecture for MT is the encoder-decoder transformer or sequence-to-sequence model [...]. Given a sentence in a source language, the MT task is then to generate a corresponding sentence in a target language. […]. MT uses supervised machine learning: at training time the system is given a large set of parallel sentences (each sentence in a source language matched with a sentence in the target language), and learns to map source sentences into target sentences. [...] we split the sentences into a sequence of subword tokens (tokens can be words, or subwords, or individual characters). The systems are then trained to maximize the probability of the sequence of tokens in the target language y1,..., ym given the sequence of tokens in the source language x1,..., xn: P(y1,..., ym|x1,..., xn). […] The encoder takes the input words x = [x1,..., xn] and produces an intermediate context h. At decoding time, the system takes h and, word by word, generates the output y [...]. [...] Machine translation models are trained on a parallel corpus, sometimes called a bitext, a text that appears in two (or more) languages. [...] Standard training corpora for MT come as aligned pairs of sentences. [...] To score the similarity of sentences across languages, we need to make use of a multilingual embedding space, in which sentences from different languages are in the same embedding space [...]\n\n    ### END OF INPUT ###\n\n    Tailored text (provide one text per target category only):\n    1. For machine translation (MT) practitioners:\n\n    The traditional MT architecture is the encoder-decoder Transformer or sequence-to-sequence model. The task of MT is to translate a given sentence from a source language into a corresponding sentence in the target language. MT utilizes supervised machine learning, by learning to map source sentences into target sentences using a large set of parallel sentences at training time. During the training process, sentences are divided into a sequence of subword tokens, and the systems are trained to maximize the probability of the sequence of tokens in the target language given the sequence of tokens in the source language. The encoder takes the input words and generates an intermediate context. At decoding time, the system uses this context to generate the output sentence, word by word. Machine translation models are trained on parallel corpora, also known as bitexts, which consist of text in two (or more) languages. To measure the similarity of sentences across languages, we employ a multilingual embedding space.",
                    "prompt5": "Paraphrase the given concepts in order for the audience of the L to understand concepts outside of their field of knowledge. \n    You must consider the information on the Linguistics (L) audience have a strong background on language structure, phonetics, syntax, and semantics. They have little or no technical knowledge. to adapt the tailored texts to the audience backgrounds. \n    - Read the text carefully.\n    - Identify the key concepts.\n    - Use terminology specific to their field.\n    - Avoid unnecessary sentence complexity while maintaining accuracy. \n    - Provide examples they are familiar with.\n    - Provide analogies they can relate their knowledge with and transfer it to new concepts.\n    - Integrate background information if needed.\n    - Do not consider other background per category. \n    - Output only one text per category.\n\n    Original text: \n    The standard architecture for MT is the encoder-decoder transformer or sequence-to-sequence model [...]. Given a sentence in a source language, the MT task is then to generate a corresponding sentence in a target language. […]. MT uses supervised machine learning: at training time the system is given a large set of parallel sentences (each sentence in a source language matched with a sentence in the target language), and learns to map source sentences into target sentences. [...] we split the sentences into a sequence of subword tokens (tokens can be words, or subwords, or individual characters). The systems are then trained to maximize the probability of the sequence of tokens in the target language y1,..., ym given the sequence of tokens in the source language x1,..., xn: P(y1,..., ym|x1,..., xn). […] The encoder takes the input words x = [x1,..., xn] and produces an intermediate context h. At decoding time, the system takes h and, word by word, generates the output y [...]. [...] Machine translation models are trained on a parallel corpus, sometimes called a bitext, a text that appears in two (or more) languages. [...] Standard training corpora for MT come as aligned pairs of sentences. [...] To score the similarity of sentences across languages, we need to make use of a multilingual embedding space, in which sentences from different languages are in the same embedding space [...]\n\n    ### END OF INPUT ###\n\n    Tailored text (provide one text per target category only):\n    1. Language beginners:\n    Machine Translation (MT) is a process that converts sentences from one language into another. It uses a special model called an encoder-decoder transformer to do this. This model breaks down sentences into smaller parts, or tokens, and then learns to put these parts together in the correct order to create a new sentence in the target language. At training time, the system is given a lot of example sentences in both the source and target languages so it can learn how to translate correctly.\n\n    2. Language enthusiasts:\n    Machine Translation (MT) is the process of converting sentences from one language into another using a sequence-to-sequence model known as the encoder-decoder transformer. Given a sentence in a source language, the task is to generate a corresponding sentence in the target language. This is achieved through supervised machine learning where the system is trained on a large set of parallel sentences (each sentence in a source language matched with a sentence in the target language). The system then learns to map source sentences into target sentences. The sentences are split into a sequence of subword tokens, and the system is trained to maximize the probability of the sequence of tokens in the target language given the sequence of tokens in the source language. The encoder takes the input words and produces an intermediate context, and at decoding time, the system generates the output word by word using this context.\n\n    3. Linguistics students:\n    Machine Translation (MT) is primarily based on the encoder-decoder transformer or sequence-to-sequence model. Given a sentence in a source language, the task is to generate a corresponding sentence in a target language. This is achieved through supervised machine learning where the system is trained on a large set of parallel sentences (each sentence in a source language matched with a sentence in the target language). The system learns to map source sentences into target sentences. The sentences are split into a sequence of subword tokens, and the system is trained to maximize the probability of the sequence of tokens in the target language given the sequence of tokens in the source language. The encoder takes the input words x = [x1,..., xn] and produces an intermediate context h. At decoding time, the system takes h and, word by word, generates the output y. Machine translation models are trained on a parallel corpus, sometimes called a bitext, a text that appears in two (or more) languages. To score the similarity of sentences across languages, we need to make use of a multilingual embedding space, in which sentences from different languages are in the same embedding space."
                },
                "CS": {
                    "prompt1": "You are an expert professor in CS. Rewrite the following explanation so that it is best understood by CS students.\n    The most important goal is that your students understand concepts that are outside their existing background knowledge, so they can pass your exam and improve their academic performance.\n    You must consider the information on Computer Science (CS) students have strong technical backgrounds, including programming, algorithms, and machine learning. They have little or no linguistic knowledge. to adapt the explanation to the students' backgrounds.\n\n    - Read the text carefully.\n    - Identify key concepts.\n    - Use terminology specific to their field.\n    - Avoid unnecessary sentence complexity while maintaining accuracy.\n    - Provide relatable examples.\n    - Use analogies that help transfer their knowledge to new concepts.\n    - Integrate background information if needed.    \n\n    Original text: \n    The standard architecture for MT is the encoder-decoder transformer or sequence-to-sequence model [...]. Given a sentence in a source language, the MT task is then to generate a corresponding sentence in a target language. […]. MT uses supervised machine learning: at training time the system is given a large set of parallel sentences (each sentence in a source language matched with a sentence in the target language), and learns to map source sentences into target sentences. [...] we split the sentences into a sequence of subword tokens (tokens can be words, or subwords, or individual characters). The systems are then trained to maximize the probability of the sequence of tokens in the target language y1,..., ym given the sequence of tokens in the source language x1,..., xn: P(y1,..., ym|x1,..., xn). […] The encoder takes the input words x = [x1,..., xn] and produces an intermediate context h. At decoding time, the system takes h and, word by word, generates the output y [...]. [...] Machine translation models are trained on a parallel corpus, sometimes called a bitext, a text that appears in two (or more) languages. [...] Standard training corpora for MT come as aligned pairs of sentences. [...] To score the similarity of sentences across languages, we need to make use of a multilingual embedding space, in which sentences from different languages are in the same embedding space [...]\n\n    ### END OF INPUT ###\n\n    Tailored text (provide one text per target category only):\n    1. **Sequential Model for Machine Translation**\n       As a CS student, you might be familiar with sequences in programming, such as arrays or linked lists. Machine Translation (MT) uses a similar concept, called the encoder-decoder transformer or sequence-to-sequence model. Just like processing an array or a linked list, given a sentence in a source language, the goal is to generate a corresponding sentence in a target language.\n\n       In machine learning terms, MT is a supervised task. This means that during training, the system is provided with a large set of parallel sentences (each sentence in a source language matched with a sentence in the target language), learning to convert source sentences into target sentences.\n\n       To simplify the processing, sentences are broken down into a sequence of subword tokens (which can be individual words, or subwords, or even characters). The model's goal is to maximize the probability of the sequence of tokens in the target language given the sequence of tokens in the source language.\n\n       The encoder takes the input words and produces an intermediate context. During decoding, the system uses this context to generate the output, word by word.\n\n    2. **Training Machine Translation Models**\n       Machine translation models are trained on a parallel corpus, often referred to as a bitext, which is a text that exists in two (or more) languages. Standard training corpora for MT come as aligned pairs of sentences.\n\n       To assess the similarity of sentences across languages, we need to employ a multilingual embedding space, where sentences from different languages reside in the same space. This allows the model to understand the meaning and semantic relationships between words across languages.",
                    "prompt2": "You are an AI sales expert, responsible for selling AI products to clients with a background in CS.\n    To successfully sell your products, your clients must first understand the following concepts.\n    Rewrite the explanation below in a way that is most comprehensible to CS clients.\n    \n    Your goal is to ensure that your clients grasp concepts outside their existing background knowledge so they can make an informed purchase.\n    By doing so, you will increase your sales percentage and achieve greater success.\n\n    Consider the information on Computer Science (CS) students have strong technical backgrounds, including programming, algorithms, and machine learning. They have little or no linguistic knowledge. to tailor the explanation to your clients' specific backgrounds.\n\n    - Read the text carefully.\n    - Identify key concepts.\n    - Use terminology specific to their field.\n    - Avoid unnecessary sentence complexity while maintaining accuracy.\n    - Provide relatable examples.\n    - Use analogies that help transfer their knowledge to new concepts.\n    - Integrate background information if needed.    \n\n    Original text: \n    The standard architecture for MT is the encoder-decoder transformer or sequence-to-sequence model [...]. Given a sentence in a source language, the MT task is then to generate a corresponding sentence in a target language. […]. MT uses supervised machine learning: at training time the system is given a large set of parallel sentences (each sentence in a source language matched with a sentence in the target language), and learns to map source sentences into target sentences. [...] we split the sentences into a sequence of subword tokens (tokens can be words, or subwords, or individual characters). The systems are then trained to maximize the probability of the sequence of tokens in the target language y1,..., ym given the sequence of tokens in the source language x1,..., xn: P(y1,..., ym|x1,..., xn). […] The encoder takes the input words x = [x1,..., xn] and produces an intermediate context h. At decoding time, the system takes h and, word by word, generates the output y [...]. [...] Machine translation models are trained on a parallel corpus, sometimes called a bitext, a text that appears in two (or more) languages. [...] Standard training corpora for MT come as aligned pairs of sentences. [...] To score the similarity of sentences across languages, we need to make use of a multilingual embedding space, in which sentences from different languages are in the same embedding space [...]\n\n    ### END OF INPUT ###\n\n    Tailored text (provide one text per target category only):\n    1. For CS students:\n    In machine translation (MT), the most common architecture is the encoder-decoder transformer or sequence-to-sequence model. Given a sequence of tokens (words, subwords, or characters) in the source language, the task is to generate the corresponding sequence in the target language. During training, the system learns to map the source sequence into the target sequence using supervised machine learning methods. This involves providing the system with large sets of parallel sentences in both the source and target languages (each sentence in the source language matched with a sentence in the target language). The system is then optimized to maximize the probability of the target sequence of tokens given the source sequence: P(y1,..., ym|x1,..., xn).\n\n    During the translation process, the encoder takes the input sequence of tokens x = [x1,..., xn] and produces an intermediate context h. At decoding time, the system generates the output sequence y, word by word, using the intermediate context produced by the encoder.\n\n    Machine translation models are typically trained on parallel corpora, also known as bitexts, which consist of text in two or more languages. Standard training corpora for MT come as aligned pairs of sentences.\n\n    To compare the similarity of sentences across languages, we require a multilingual embedding space, in which sentences from different languages share the same embedding space.\n\n    This way, the CS students can understand the crucial aspects of machine translation, such as the encoder-decoder architecture, the use of supervised learning, and the need for a multilingual embedding space for comparison purposes.\n\n    2. For advanced CS students:\n    In machine translation (MT), the state-of-the-art architecture is the encoder-decoder transformer or sequence-to-sequence model. This architecture takes advantage of self-attention mechanisms to generate translations that are more accurate and context-aware. Given a sequence of subword tokens (tokens can be words, subwords, or individual characters) in the source language, the task is to generate the corresponding sequence in the target language. During training, the system learns to map the source sequence into the target sequence using supervised machine learning methods.\n\n    The encoder takes the input sequence of tokens x = [x1,..., xn] and produces an intermediate context h containing the encoded information of the input sequence. At decoding time, the system generates the output sequence y, word by word, using the context produced by the encoder and the decoder's own self-attention mechanism.\n\n    Machine translation models are typically trained on parallel corpora, also known as bitexts, which consist of text in two or more languages. Standard training corpora for MT come as aligned pairs of sentences.\n\n    To compare the similarity of sentences across languages, we require a multilingual embedding space, in which sentences from different languages share the same embedding space. This allows the model to better understand the semantic meaning of the input sentences, regardless of the language they are written in.\n\n    This way, advanced CS students can understand the technical aspects of machine translation, such as the encoder-decoder architecture, self-attention mechanisms, and the need for a multilingual embedding space for semantic understanding.",
                    "prompt3": "You are a freelance copywriter responsible for adapting field-specific academic texts for an audience with a background in CS. \n    Your client at the publishing house will decide whether to publish your adapted texts in the new edition of the book, making you famous in the copywriting field.\n\n    Rewrite the explanation below in a way that is most comprehensible to CS audience.\n    The key objective is to ensure that your audience grasps concepts outside their existing background knowledge so your client will publish your work.\n    If you succeed, many more clients will work with you, and you will become extremely successful.\n\n    Consider the information on Computer Science (CS) students have strong technical backgrounds, including programming, algorithms, and machine learning. They have little or no linguistic knowledge. to tailor the explanation to your audience’s specific background.\n\n    - Read the text carefully.\n    - Identify key concepts.\n    - Use terminology that is familiar to their field.\n    - Avoid unnecessary sentence complexity while maintaining accuracy.\n    - Provide relatable examples.\n    - Use analogies that help transfer their knowledge to new concepts.\n    - Integrate background information if needed.    \n\n    Original text: \n    The standard architecture for MT is the encoder-decoder transformer or sequence-to-sequence model [...]. Given a sentence in a source language, the MT task is then to generate a corresponding sentence in a target language. […]. MT uses supervised machine learning: at training time the system is given a large set of parallel sentences (each sentence in a source language matched with a sentence in the target language), and learns to map source sentences into target sentences. [...] we split the sentences into a sequence of subword tokens (tokens can be words, or subwords, or individual characters). The systems are then trained to maximize the probability of the sequence of tokens in the target language y1,..., ym given the sequence of tokens in the source language x1,..., xn: P(y1,..., ym|x1,..., xn). […] The encoder takes the input words x = [x1,..., xn] and produces an intermediate context h. At decoding time, the system takes h and, word by word, generates the output y [...]. [...] Machine translation models are trained on a parallel corpus, sometimes called a bitext, a text that appears in two (or more) languages. [...] Standard training corpora for MT come as aligned pairs of sentences. [...] To score the similarity of sentences across languages, we need to make use of a multilingual embedding space, in which sentences from different languages are in the same embedding space [...]\n\n    ### END OF INPUT ###\n\n    Tailored text (provide one text per target category only):\n    1. Simplified for beginners:\n\n    In machine translation (MT), we use a model called the encoder-decoder transformer or sequence-to-sequence model. Given a sentence in a source language, the goal is to generate a corresponding sentence in the target language.\n\n    We train this model using supervised machine learning, which means we provide the system with a large set of aligned sentence pairs from both languages. The system learns to translate source sentences into target sentences.\n\n    To simplify the input, we break the sentences down into smaller parts called subword tokens. At training, our model learns to predict the sequence of tokens in the target language given the sequence of tokens in the source language.\n\n    During translation, the model takes the source sentence, creates an intermediate context, and then generates the target sentence word by word.\n\n    Standard training data for MT consists of aligned sentence pairs from a parallel corpus or bitext.\n\n    To evaluate the quality of translations across languages, we use a common multilingual embedding space where sentences from different languages share the same space.\n\n    2. CS-focused:\n\n    Machine Translation (MT) adopts the encoder-decoder transformer or sequence-to-sequence model architecture. Given a sequence of input tokens (words, subwords, or characters) x = [x1,..., xn] in a source language, the objective is to generate a corresponding sequence y1,..., ym in a target language.\n\n    We employ supervised machine learning techniques in training, presenting the system with a large set of sentence pairs from both languages. The system learns to map input sequences into output sequences.\n\n    To manage complexity, the input is broken down into a sequence of subword tokens. The model then learns to maximize the probability of the output sequence given the input sequence: P(y1,..., ym|x1,..., xn).\n\n    During translation, the encoder processes the input tokens to produce an intermediate context h, and the decoder utilizes this context to generate the output sequence word by word.\n\n    Standard training data for MT originates from a parallel corpus or bitext, comprising aligned sentence pairs.\n\n    To assess the quality of translations across languages, we employ a multilingual embedding space, where sentences from different languages occupy the same space, enabling us to compare their semantic similarity.",
                    "prompt4": "We are playing an escape room game against each other. Your goal is to gain more points than me to be the first one escaping hence winning the game. \n    In order to escape the room, you must paraphrase field-specific concepts to the jury of the game in the CS. \n    They will assign you points from one to ten, according to how good their understanding of the new explanation is. \n    Therefore, you want your adaptation to be the best possible, and definitely better than mine, so that you can gain ten points for each paraphrasis and win more quickly. \n    Consider the information on Computer Science (CS) jury have strong technical backgrounds, including programming, algorithms, and machine learning. They have little or no linguistic knowledge. to tailor the explanation to each jury specific background. \n    The winning steps for good paraphrasis are:\n    - Read the text carefully.\n    - Identify the key concepts.\n    - Use terminology that is familiar to the jury's field.\n    - Avoid unnecessary sentence complexity while maintaining accuracy. \n    - Provide examples they are familiar with.\n    - Provide analogies they can relate their knowledge with and transfer it to new concepts.\n    - Integrate background information if needed.    \n    - For the CL jury in Computer Science (CS) jury have strong technical backgrounds, including programming, algorithms, and machine learning. They have little or no linguistic knowledge., only provide one paraphrasis for NLP/Computational Linguistics Practitioners.\n\n    Original text: \n    The standard architecture for MT is the encoder-decoder transformer or sequence-to-sequence model [...]. Given a sentence in a source language, the MT task is then to generate a corresponding sentence in a target language. […]. MT uses supervised machine learning: at training time the system is given a large set of parallel sentences (each sentence in a source language matched with a sentence in the target language), and learns to map source sentences into target sentences. [...] we split the sentences into a sequence of subword tokens (tokens can be words, or subwords, or individual characters). The systems are then trained to maximize the probability of the sequence of tokens in the target language y1,..., ym given the sequence of tokens in the source language x1,..., xn: P(y1,..., ym|x1,..., xn). […] The encoder takes the input words x = [x1,..., xn] and produces an intermediate context h. At decoding time, the system takes h and, word by word, generates the output y [...]. [...] Machine translation models are trained on a parallel corpus, sometimes called a bitext, a text that appears in two (or more) languages. [...] Standard training corpora for MT come as aligned pairs of sentences. [...] To score the similarity of sentences across languages, we need to make use of a multilingual embedding space, in which sentences from different languages are in the same embedding space [...]\n\n    ### END OF INPUT ###\n\n    Tailored text (provide one text per target category only):\n    1. Programmers:\n    The standard architecture for Machine Translation (MT) is the sequence-to-sequence model using the encoder-decoder transformer. The task in MT is to convert sentences from a source language into corresponding sentences in a target language. This process is supervised machine learning, where the system is trained on a large set of parallel sentences across languages to learn how to map source sentences to target sentences. The sentences are split into a sequence of subword tokens, and the system is trained to maximize the probability of the sequence of tokens in the target language, given the sequence of tokens in the source language. During the translation process, the encoder takes the input words and produces an intermediate context, while the decoder uses this context to generate the output sentence, word by word. Machine translation models are typically trained on a parallel corpus, also known as a bitext, which contains texts in two or more languages. Standard training corpora for MT come as aligned pairs of sentences. To compare the similarity of sentences across languages, a multilingual embedding space is used, in which sentences from different languages are in the same space.\n\n    2. Algorithm experts:\n    In Machine Translation (MT), the encoder-decoder transformer architecture is used to convert sentences from a source language to the corresponding sentences in a target language. The MT task is to learn how to map source sentences to target sentences using supervised machine learning, given a large set of parallel sentences. The sentences are split into a sequence of subword tokens, and the system is trained to maximize the probability of the sequence of tokens in the target language, given the sequence of tokens in the source language. The encoder takes the input words and produces an intermediate context, while the decoder uses this context to generate the output sentence, word by word. Common training corpora for MT consist of aligned pairs of sentences. To evaluate the similarity between sentences in different languages, a multilingual embedding space is utilized, where sentences from various languages are in the same space.\n\n    3. Machine Learning enthusiasts:\n    Machine Translation (MT) is a task that converts sentences from a source language to the corresponding sentences in a target language using the encoder-decoder transformer architecture. The MT process involves supervised machine learning, where the system learns to map source sentences to target sentences by being trained on a large set of parallel sentences. The sentences are split into a sequence of subword tokens, and the system is optimized to maximize the probability of the sequence of tokens in the target language, given the sequence of tokens in the source language. The encoder processes the input words to produce an intermediate context, while the decoder uses this context to generate the output sentence, word by word. Common training corpus examples for MT consist of aligned pairs of sentences. To assess the similarity between sentences in different languages, a multilingual embedding space is employed, where sentences from multiple languages are in the same space.\n\n    4. NLP Practitioners:\n    In Machine Translation (MT), the encoder-decoder transformer architecture is employed to convert sentences from a source language into the corresponding sentences in a target language. The MT process involves training the system to map source sentences to target sentences using supervised machine learning on parallel sentences. The sentences are tokenized into a sequence of subword tokens, and the model is optimized to maximize the probability of the sequence of tokens in the target language, given the sequence of tokens in the source language. The encoder processes the input words to create an intermediate context, while the decoder utilizes this context to generate the output sentence, word by word. Common training corpus examples for MT include aligned pairs of sentences. To evaluate the similarity between sentences in different languages, a multilingual embedding space is utilized, where sentences from multiple languages are in the same space.",
                    "prompt5": "Paraphrase the given concepts in order for the audience of the CS to understand concepts outside of their field of knowledge. \n    You must consider the information on the Computer Science (CS) audience have strong technical backgrounds, including programming, algorithms, and machine learning. They have little or no linguistic knowledge. to adapt the tailored texts to the audience backgrounds. \n    - Read the text carefully.\n    - Identify the key concepts.\n    - Use terminology specific to their field.\n    - Avoid unnecessary sentence complexity while maintaining accuracy. \n    - Provide examples they are familiar with.\n    - Provide analogies they can relate their knowledge with and transfer it to new concepts.\n    - Integrate background information if needed.\n    - Do not consider other background per category. \n    - Output only one text per category.\n\n    Original text: \n    The standard architecture for MT is the encoder-decoder transformer or sequence-to-sequence model [...]. Given a sentence in a source language, the MT task is then to generate a corresponding sentence in a target language. […]. MT uses supervised machine learning: at training time the system is given a large set of parallel sentences (each sentence in a source language matched with a sentence in the target language), and learns to map source sentences into target sentences. [...] we split the sentences into a sequence of subword tokens (tokens can be words, or subwords, or individual characters). The systems are then trained to maximize the probability of the sequence of tokens in the target language y1,..., ym given the sequence of tokens in the source language x1,..., xn: P(y1,..., ym|x1,..., xn). […] The encoder takes the input words x = [x1,..., xn] and produces an intermediate context h. At decoding time, the system takes h and, word by word, generates the output y [...]. [...] Machine translation models are trained on a parallel corpus, sometimes called a bitext, a text that appears in two (or more) languages. [...] Standard training corpora for MT come as aligned pairs of sentences. [...] To score the similarity of sentences across languages, we need to make use of a multilingual embedding space, in which sentences from different languages are in the same embedding space [...]\n\n    ### END OF INPUT ###\n\n    Tailored text (provide one text per target category only):\n    1. Machine Translation (MT) Overview:\n       Machine Translation (MT) is a task that utilizes the encoder-decoder transformer or sequence-to-sequence model architecture. Its primary goal is to convert a sentence from a source language into an equivalent sentence in a target language. This process is achieved through supervised machine learning, where the system learns to map source sentences into target sentences by being exposed to a large set of parallel sentences (each sentence in a source language matched with a sentence in the target language). To simplify the process, sentences are broken down into subword tokens, which can be words, subwords, or characters. The system then maximizes the probability of the sequence of tokens in the target language given the sequence of tokens in the source language. During the translation process, the encoder takes the input words and produces an intermediate context, which the decoder uses to generate the output one word at a time.\n\n    2. Understanding Multilingual Embedding Space:\n       To effectively compare sentences across different languages in MT, we need a multilingual embedding space. This is a space where sentences from various languages can coexist. In this shared space, the meaning of words or phrases is represented as a continuous vector, allowing the system to understand the semantic relationships between sentences in different languages. This aids in producing more accurate translations."
                }
            }
        }
    },
    "15": {
        "chapter": "16",
        "sections": "16.2",
        "topic": "Automatic Speech Recognition (ASR)",
        "original_category": "CL",
        "original_text": "The first step in ASR is to transform the input waveform into a sequence of acoustic feature vectors, […]. The input to a speech recognizer is a complex series of changes in air pressure […]. We represent sound waves by plotting the change in air pressure over time. [...] The first step in digitizing a sound wave […]. This analog-to-digital conversion has two steps: sampling and quantization. To sample a signal, we measure its amplitude at a particular time; the sampling rate is the number of samples taken per second. [...] This maximum frequency for a given sampling rate is called the Nyquist frequency. [...] Amplitude measurements are stored as integers, […]. This process of representing real-valued quantization numbers as integers is called quantization; [...]. From the digitized, quantized representation of the waveform, we need to extract spectral features from a small window of speech that characterizes part of a particular phoneme. The speech extracted from each window is called a frame. The windowing is characterized by three parameters: the window size or frame size of the window (its width in milliseconds), the frame stride, (also called shift or offset) between successive windows, and the shape of the window. [...] The tool for extracting spectral information […] is the discrete Fourier transform or DFT. [...] If we plot the magnitude against the frequency, we can visualize the spectrum. [...]",
        "tailored_texts": {
            "mistral": {
                "L": {
                    "prompt1": "You are an expert professor in L. Rewrite the following explanation so that it is best understood by L students.\n    The most important goal is that your students understand concepts that are outside their existing background knowledge, so they can pass your exam and improve their academic performance.\n    You must consider the information on Linguistics (L) students have a strong background in language structure, phonetics, syntax, and semantics. They have little or no technical knowledge. to adapt the explanation to the students' backgrounds.\n\n    - Read the text carefully.\n    - Identify key concepts.\n    - Use terminology specific to their field.\n    - Avoid unnecessary sentence complexity while maintaining accuracy.\n    - Provide relatable examples.\n    - Use analogies that help transfer their knowledge to new concepts.\n    - Integrate background information if needed.    \n\n    Original text: \n    The first step in ASR is to transform the input waveform into a sequence of acoustic feature vectors, […]. The input to a speech recognizer is a complex series of changes in air pressure […]. We represent sound waves by plotting the change in air pressure over time. [...] The first step in digitizing a sound wave […]. This analog-to-digital conversion has two steps: sampling and quantization. To sample a signal, we measure its amplitude at a particular time; the sampling rate is the number of samples taken per second. [...] This maximum frequency for a given sampling rate is called the Nyquist frequency. [...] Amplitude measurements are stored as integers, […]. This process of representing real-valued quantization numbers as integers is called quantization; [...]. From the digitized, quantized representation of the waveform, we need to extract spectral features from a small window of speech that characterizes part of a particular phoneme. The speech extracted from each window is called a frame. The windowing is characterized by three parameters: the window size or frame size of the window (its width in milliseconds), the frame stride, (also called shift or offset) between successive windows, and the shape of the window. [...] The tool for extracting spectral information […] is the discrete Fourier transform or DFT. [...] If we plot the magnitude against the frequency, we can visualize the spectrum. [...]\n\n    ### END OF INPUT ###\n\n    Tailored text (provide one text per target category only):\n    1. Introduction\n        Hello L students! Today, we're going to talk about speech recognition, specifically the first step: turning sounds into numbers that our computers can understand. The sounds we hear every day, like people talking, singing, or laughing, are complex patterns of air pressure changes, just like waves in the ocean. To represent these waves, we need to visualize them as a series of measurements over time, much like how we measure the height of waves on the beach at regular intervals.\n\n    2. Key Concepts\n        - Speech recognition (ASR)\n        - Acoustic feature vectors\n        - Sampling\n        - Quantization\n        - Nyquist frequency\n        - Discrete Fourier Transform (DFT)\n        - Windowing\n        - Frame\n\n    3. Explanation\n        To make it easier for our computers to understand spoken language, we convert the continuous air pressure changes into discrete measurements. This process can be broken down into two main steps: sampling and quantization.\n\n        Sampling means taking measurements at specific time points, like counting the number of waves on the beach every minute. The number of measurements taken per second is called the sampling rate.\n\n        Quantization is the process of representing these measurements as whole numbers, like counting how many waves are smaller than a certain height.\n\n        There's a limit to how high we can count when measuring the height of waves (or air pressure changes), and this limit is called the Nyquist frequency. If we try to measure waves higher than the Nyquist frequency, we'll end up with inaccurate results.\n\n        With our measurements, we now have a digitized representation of the original sound wave, which we can analyze further to understand the specific sounds being spoken. To do this, we use a tool called the Discrete Fourier Transform (DFT). This tool helps us visualize the spectrum of the sound, which tells us which frequencies are present and how strong they are.\n\n        To make the analysis easier, we divide the sound into small chunks, called frames. Each frame represents a small segment of speech. The size of each frame, the distance between frames, and the shape of the frame are all important factors that we need to consider.\n\n    4. Examples and Analogies\n        - Let's imagine you're counting waves on the beach. Sampling would be like taking counts every minute, and quantization would be counting waves that are taller than a certain height.\n        - The Nyquist frequency is like the maximum number you can count before your brain starts to get confused, similar to how it's hard to count more than 20 objects quickly.\n        - The DFT is like a magic wand that helps you see the waves in the ocean differently, showing you the different heights and patterns that make up the waves.\n        - Frames are like taking snapshots of the ocean every few seconds, allowing us to study each snapshot in detail.\n\n    5. Integration with Background Knowledge\n        As L students, you already have a strong understanding of language structure, phonetics, syntax, and semantics. In speech recognition, these concepts play a crucial role in interpreting the digitized sound waves and understanding the spoken language. For example, your knowledge of phonetics can help you identify which sounds are present in the speech signal, while your understanding of syntax can help you arrange these sounds into meaningful words and sentences.\n\n    6. Conclusion\n        In conclusion, speech recognition is an essential technology that helps us understand spoken language by transforming sounds into numbers that our computers can process. By using tools like the Discrete Fourier Transform and dividing the speech signal into frames, we can analyze the sound wave and extract useful information about the spoken language. As L students, your background in language and linguistics will be invaluable in understanding and interpreting these digitized speech signals.",
                    "prompt2": "You are an AI sales expert, responsible for selling AI products to clients with a background in L.\n    To successfully sell your products, your clients must first understand the following concepts.\n    Rewrite the explanation below in a way that is most comprehensible to L clients.\n    \n    Your goal is to ensure that your clients grasp concepts outside their existing background knowledge so they can make an informed purchase.\n    By doing so, you will increase your sales percentage and achieve greater success.\n\n    Consider the information on Linguistics (L) students have a strong background in language structure, phonetics, syntax, and semantics. They have little or no technical knowledge. to tailor the explanation to your clients' specific backgrounds.\n\n    - Read the text carefully.\n    - Identify key concepts.\n    - Use terminology specific to their field.\n    - Avoid unnecessary sentence complexity while maintaining accuracy.\n    - Provide relatable examples.\n    - Use analogies that help transfer their knowledge to new concepts.\n    - Integrate background information if needed.    \n\n    Original text: \n    The first step in ASR is to transform the input waveform into a sequence of acoustic feature vectors, […]. The input to a speech recognizer is a complex series of changes in air pressure […]. We represent sound waves by plotting the change in air pressure over time. [...] The first step in digitizing a sound wave […]. This analog-to-digital conversion has two steps: sampling and quantization. To sample a signal, we measure its amplitude at a particular time; the sampling rate is the number of samples taken per second. [...] This maximum frequency for a given sampling rate is called the Nyquist frequency. [...] Amplitude measurements are stored as integers, […]. This process of representing real-valued quantization numbers as integers is called quantization; [...]. From the digitized, quantized representation of the waveform, we need to extract spectral features from a small window of speech that characterizes part of a particular phoneme. The speech extracted from each window is called a frame. The windowing is characterized by three parameters: the window size or frame size of the window (its width in milliseconds), the frame stride, (also called shift or offset) between successive windows, and the shape of the window. [...] The tool for extracting spectral information […] is the discrete Fourier transform or DFT. [...] If we plot the magnitude against the frequency, we can visualize the spectrum. [...]\n\n    ### END OF INPUT ###\n\n    Tailored text (provide one text per target category only):\n    1. Biomedical Researchers:\n        In Automatic Speech Recognition (ASR), the initial step involves transforming the audio signal into a series of acoustic feature vectors. These feature vectors are essentially a numerical representation of the sound waves, plotted against time. For instance, consider a heartbeat monitor that converts the continuous electrical signals into discrete, quantifiable data points for analysis. Similarly, in ASR, we digitize sound waves through two processes: sampling and quantization. Sampling is the act of measuring a signal's amplitude at specific time intervals, with the sampling rate denoting the number of measurements per second. The maximum frequency that can be accurately captured is the Nyquist frequency. Quantization is the process of representing these real-valued measurements as integers. Once we have the digitized, quantized representation of the waveform, we focus on extracting spectral features from small segments of speech that represent specific phonemes. These segments are called frames, and we can control the size, stride, and shape of the frames, much like how a microscope allows us to control the size and movement of its focus in analyzing a cell sample. To extract the spectral information, we use a tool called the discrete Fourier transform (DFT), which provides a visual representation of the spectrum when we plot the magnitude against the frequency.\n\n    2. Electrical Engineers:\n        In Automatic Speech Recognition (ASR), we begin by converting the input waveform into a sequence of acoustic feature vectors. These vectors represent the changes in air pressure over time, similar to how an oscilloscope displays voltage changes over time. To digitize a sound wave, we perform two essential steps: sampling and quantization. Sampling involves measuring the amplitude of the signal at specific points in time, with the sampling rate being the number of measurements taken per second. The maximum frequency that can be accurately captured is the Nyquist frequency. Quantization is the process of representing the real-valued measurements as integers. Once we have the digitized, quantized representation of the waveform, we focus on extracting spectral features from small sections of speech that correspond to specific phonemes. These sections are called frames, and we can adjust the frame size, stride, and shape, much like how an analog-to-digital converter (ADC) allows us to control the sample rate and resolution. To extract the spectral information, we employ the discrete Fourier transform (DFT), yielding a visual representation of the spectrum when we plot the magnitude against the frequency.\n\n    3. Linguists:\n        In Automatic Speech Recognition (ASR), the initial phase consists of transforming the input waveform into a sequence of acoustic feature vectors. These vectors serve as a numerical representation of the sound waves, plotted against time, much like how phonemes are represented in a phonetic transcription. To digitize a sound wave, we carry out two crucial steps: sampling and quantization. Sampling entails measuring the amplitude of the signal at specific instances, with the sampling rate signifying the number of measurements per second. The maximum frequency that can be accurately captured is the Nyquist frequency. Quantization is the process of representing these real-valued measurements as integers. Once we have the digitized, quantized representation of the waveform, we concentrate on extracting spectral features from small segments of speech that characterize particular phonemes. These segments are referred to as frames, and we can manipulate the frame size, stride, and shape, similar to how we analyze a phoneme's characteristics in terms of duration, stress, and voicing. To extract the spectral information, we utilize the discrete Fourier transform (DFT), which offers a visual representation of the spectrum when we plot the magnitude against the frequency.",
                    "prompt3": "You are a freelance copywriter responsible for adapting field-specific academic texts for an audience with a background in L. \n    Your client at the publishing house will decide whether to publish your adapted texts in the new edition of the book, making you famous in the copywriting field.\n\n    Rewrite the explanation below in a way that is most comprehensible to L audience.\n    The key objective is to ensure that your audience grasps concepts outside their existing background knowledge so your client will publish your work.\n    If you succeed, many more clients will work with you, and you will become extremely successful.\n\n    Consider the information on Linguistics (L) students have a strong background in language structure, phonetics, syntax, and semantics. They have little or no technical knowledge. to tailor the explanation to your audience’s specific background.\n\n    - Read the text carefully.\n    - Identify key concepts.\n    - Use terminology that is familiar to their field.\n    - Avoid unnecessary sentence complexity while maintaining accuracy.\n    - Provide relatable examples.\n    - Use analogies that help transfer their knowledge to new concepts.\n    - Integrate background information if needed.    \n\n    Original text: \n    The first step in ASR is to transform the input waveform into a sequence of acoustic feature vectors, […]. The input to a speech recognizer is a complex series of changes in air pressure […]. We represent sound waves by plotting the change in air pressure over time. [...] The first step in digitizing a sound wave […]. This analog-to-digital conversion has two steps: sampling and quantization. To sample a signal, we measure its amplitude at a particular time; the sampling rate is the number of samples taken per second. [...] This maximum frequency for a given sampling rate is called the Nyquist frequency. [...] Amplitude measurements are stored as integers, […]. This process of representing real-valued quantization numbers as integers is called quantization; [...]. From the digitized, quantized representation of the waveform, we need to extract spectral features from a small window of speech that characterizes part of a particular phoneme. The speech extracted from each window is called a frame. The windowing is characterized by three parameters: the window size or frame size of the window (its width in milliseconds), the frame stride, (also called shift or offset) between successive windows, and the shape of the window. [...] The tool for extracting spectral information […] is the discrete Fourier transform or DFT. [...] If we plot the magnitude against the frequency, we can visualize the spectrum. [...]\n\n    ### END OF INPUT ###\n\n    Tailored text (provide one text per target category only):\n    1. Introducing Acoustic Analysis for Speech Recognition (ASR)\n    Within the realm of speech recognition, the first significant step is the transformation of a sound wave into a series of acoustic feature vectors. This process is crucial because the input to the speech recognizer is a complex sequence of changes in air pressure.\n\n    To put it simply, we illustrate sound waves by depicting the variation in air pressure over time. This process of digitizing a sound wave, or analog-to-digital conversion, consists of two primary steps: sampling and quantization.\n\n    Sampling involves measuring the amplitude of the signal at a specific moment, with the sampling rate denoting the number of samples taken per second. The maximum frequency that can be represented for a given sampling rate is known as the Nyquist frequency.\n\n    During quantization, amplitude measurements are encoded as integers, allowing for easier digital processing. This simplification is known as quantization.\n\n    Once we have the digitized, quantized representation of the waveform, we focus on extracting essential spectral features from a small segment of speech, which represents a specific phoneme. These segments, called frames, are generated from each window of speech.\n\n    The windowing process is characterized by three primary parameters: the window or frame size (its duration in milliseconds), the frame stride (the distance between successive windows), and the window’s shape.\n\n    To extract spectral information, we employ a tool called the discrete Fourier transform (DFT). If we plot the magnitude against the frequency, we can visualize the spectrum.\n\n    In essence, through this process, we aim to transform complex, fluctuating air pressure changes into a series of feature vectors that can be more easily analyzed by the speech recognizer, ultimately leading to improved understanding and interpretation of human speech.",
                    "prompt4": "We are playing an escape room game against each other. Your goal is to gain more points than me to be the first one escaping hence winning the game. \n    In order to escape the room, you must paraphrase field-specific concepts to the jury of the game in the L. \n    They will assign you points from one to ten, according to how good their understanding of the new explanation is. \n    Therefore, you want your adaptation to be the best possible, and definitely better than mine, so that you can gain ten points for each paraphrasis and win more quickly. \n    Consider the information on Linguistics (L) jury have a strong background on language structure, phonetics, syntax, and semantics. They have little or no technical knowledge. to tailor the explanation to each jury specific background. \n    The winning steps for good paraphrasis are:\n    - Read the text carefully.\n    - Identify the key concepts.\n    - Use terminology that is familiar to the jury's field.\n    - Avoid unnecessary sentence complexity while maintaining accuracy. \n    - Provide examples they are familiar with.\n    - Provide analogies they can relate their knowledge with and transfer it to new concepts.\n    - Integrate background information if needed.    \n    - For the CL jury in Linguistics (L) jury have a strong background on language structure, phonetics, syntax, and semantics. They have little or no technical knowledge., only provide one paraphrasis for NLP/Computational Linguistics Practitioners.\n\n    Original text: \n    The first step in ASR is to transform the input waveform into a sequence of acoustic feature vectors, […]. The input to a speech recognizer is a complex series of changes in air pressure […]. We represent sound waves by plotting the change in air pressure over time. [...] The first step in digitizing a sound wave […]. This analog-to-digital conversion has two steps: sampling and quantization. To sample a signal, we measure its amplitude at a particular time; the sampling rate is the number of samples taken per second. [...] This maximum frequency for a given sampling rate is called the Nyquist frequency. [...] Amplitude measurements are stored as integers, […]. This process of representing real-valued quantization numbers as integers is called quantization; [...]. From the digitized, quantized representation of the waveform, we need to extract spectral features from a small window of speech that characterizes part of a particular phoneme. The speech extracted from each window is called a frame. The windowing is characterized by three parameters: the window size or frame size of the window (its width in milliseconds), the frame stride, (also called shift or offset) between successive windows, and the shape of the window. [...] The tool for extracting spectral information […] is the discrete Fourier transform or DFT. [...] If we plot the magnitude against the frequency, we can visualize the spectrum. [...]\n\n    ### END OF INPUT ###\n\n    Tailored text (provide one text per target category only):\n    1. For the Linguistics jury:\n    In Automatic Speech Recognition (ASR), the initial process involves transforming an audio waveform into a sequence of acoustic feature vectors. These feature vectors represent the complex series of air pressure changes that make up the sound wave. By plotting the change in air pressure over time, we digitize the sound wave, a process that includes sampling and quantization. Sampling is the measurement of the wave's amplitude at specific time intervals, with the number of measurements per second known as the sampling rate. The highest frequency that can be accurately captured by a given sampling rate is known as the Nyquist frequency. Quantization is the process of converting real-valued amplitude measurements into integers. The digitized, quantized representation of the waveform is then broken down into smaller sections, called frames, each characterized by a window size, frame stride, and window shape. The Discrete Fourier Transform (DFT) is used to extract spectral features from each frame, allowing us to visualize the spectrum.\n    2. For NLP/Computational Linguistics Practitioners:\n    In Automatic Speech Recognition (ASR), the input waveform is transformed into a sequence of acoustic feature vectors, which are a series of measurements of the wave's amplitude at specific time intervals (sampling). These measurements, known as the sampling rate, are then converted into integers (quantization) to create a digital representation. This digital representation is further broken down into frames, small sections of the waveform that are characterized by a window size, frame stride, and window shape. The Discrete Fourier Transform (DFT) is used to extract spectral features from each frame, providing a frequency analysis of the speech signal. This analysis is crucial for recognizing specific phonemes in the spoken language.",
                    "prompt5": "Paraphrase the given concepts in order for the audience of the L to understand concepts outside of their field of knowledge. \n    You must consider the information on the Linguistics (L) audience have a strong background on language structure, phonetics, syntax, and semantics. They have little or no technical knowledge. to adapt the tailored texts to the audience backgrounds. \n    - Read the text carefully.\n    - Identify the key concepts.\n    - Use terminology specific to their field.\n    - Avoid unnecessary sentence complexity while maintaining accuracy. \n    - Provide examples they are familiar with.\n    - Provide analogies they can relate their knowledge with and transfer it to new concepts.\n    - Integrate background information if needed.\n    - Do not consider other background per category. \n    - Output only one text per category.\n\n    Original text: \n    The first step in ASR is to transform the input waveform into a sequence of acoustic feature vectors, […]. The input to a speech recognizer is a complex series of changes in air pressure […]. We represent sound waves by plotting the change in air pressure over time. [...] The first step in digitizing a sound wave […]. This analog-to-digital conversion has two steps: sampling and quantization. To sample a signal, we measure its amplitude at a particular time; the sampling rate is the number of samples taken per second. [...] This maximum frequency for a given sampling rate is called the Nyquist frequency. [...] Amplitude measurements are stored as integers, […]. This process of representing real-valued quantization numbers as integers is called quantization; [...]. From the digitized, quantized representation of the waveform, we need to extract spectral features from a small window of speech that characterizes part of a particular phoneme. The speech extracted from each window is called a frame. The windowing is characterized by three parameters: the window size or frame size of the window (its width in milliseconds), the frame stride, (also called shift or offset) between successive windows, and the shape of the window. [...] The tool for extracting spectral information […] is the discrete Fourier transform or DFT. [...] If we plot the magnitude against the frequency, we can visualize the spectrum. [...]\n\n    ### END OF INPUT ###\n\n    Tailored text (provide one text per target category only):\n    1. Category: Phonetics\n       Title: Understanding the Transformation of Sound Waves in Speech Recognition\n\n       In the realm of speech recognition, the initial process involves transforming the input waveform into a sequence of acoustic feature vectors. These sound waves, which are complex series of changes in air pressure, are represented by plotting the change in air pressure over time. This is akin to capturing the fluctuations in the pressure of a balloon as it expands and contracts.\n\n       The process of digitizing a sound wave, often referred to as analog-to-digital conversion, has two primary steps: sampling and quantization. Sampling is the act of measuring the amplitude of the sound wave at a specific moment, while the sampling rate denotes the number of measurements taken per second. For instance, if we were recording a bird's song at a rate of 1000 samples per second, the highest frequency the bird could produce without causing distortion would be 500 Hz, as per the Nyquist theorem.\n\n       Amplitude measurements are stored as integers, a process known as quantization. This simplification allows for efficient storage and computation of the data.\n\n       From the digitized, quantized representation of the waveform, we extract spectral features from a small window of speech that represent a specific part of a phoneme. This segment of speech is referred to as a frame, and the windowing process is characterized by three parameters: the window size (or frame size), the frame stride, and the shape of the window.\n\n       To extract spectral information, we employ the use of the discrete Fourier transform or DFT. If we plot the magnitude against the frequency, we can visualize the spectrum, akin to observing the spectrum of light to identify different colors."
                },
                "CS": {
                    "prompt1": "You are an expert professor in CS. Rewrite the following explanation so that it is best understood by CS students.\n    The most important goal is that your students understand concepts that are outside their existing background knowledge, so they can pass your exam and improve their academic performance.\n    You must consider the information on Computer Science (CS) students have strong technical backgrounds, including programming, algorithms, and machine learning. They have little or no linguistic knowledge. to adapt the explanation to the students' backgrounds.\n\n    - Read the text carefully.\n    - Identify key concepts.\n    - Use terminology specific to their field.\n    - Avoid unnecessary sentence complexity while maintaining accuracy.\n    - Provide relatable examples.\n    - Use analogies that help transfer their knowledge to new concepts.\n    - Integrate background information if needed.    \n\n    Original text: \n    The first step in ASR is to transform the input waveform into a sequence of acoustic feature vectors, […]. The input to a speech recognizer is a complex series of changes in air pressure […]. We represent sound waves by plotting the change in air pressure over time. [...] The first step in digitizing a sound wave […]. This analog-to-digital conversion has two steps: sampling and quantization. To sample a signal, we measure its amplitude at a particular time; the sampling rate is the number of samples taken per second. [...] This maximum frequency for a given sampling rate is called the Nyquist frequency. [...] Amplitude measurements are stored as integers, […]. This process of representing real-valued quantization numbers as integers is called quantization; [...]. From the digitized, quantized representation of the waveform, we need to extract spectral features from a small window of speech that characterizes part of a particular phoneme. The speech extracted from each window is called a frame. The windowing is characterized by three parameters: the window size or frame size of the window (its width in milliseconds), the frame stride, (also called shift or offset) between successive windows, and the shape of the window. [...] The tool for extracting spectral information […] is the discrete Fourier transform or DFT. [...] If we plot the magnitude against the frequency, we can visualize the spectrum. [...]\n\n    ### END OF INPUT ###\n\n    Tailored text (provide one text per target category only):\n    1. Technical explanation for CS students:\n\n    In Automatic Speech Recognition (ASR), the initial step involves converting the input waveform into a series of acoustic feature vectors. Sound waves, represented as changes in air pressure, are digitized and analyzed for their spectral characteristics. This process consists of two main steps: sampling and quantization. Sampling entails measuring the amplitude of the sound wave at specific instances, with the sampling rate indicating the number of measurements taken per second. The Nyquist frequency represents the highest frequency that can be accurately captured at a given sampling rate. Quantization is the process of representing these amplitude measurements as integers.\n\n    Following the digitization and quantization of the sound wave, we need to extract spectral features from small windows of speech that uniquely represent individual phonemes. The speech extracted from each window is referred to as a frame. The windowing process is characterized by three parameters: the window size, frame stride, and window shape. The discrete Fourier transform (DFT) is the tool used to extract spectral information from the frames. By plotting the magnitude against the frequency, we can visualize the spectrum.\n\n    2. Analogies and examples for CS students:\n\n    Think of ASR as a complex puzzle where each piece represents a phoneme in a word or sentence. The input waveform is a jumbled mess of pieces that we need to sort and organize. To accomplish this, we first need to cut the big mess into smaller, manageable chunks, which we call frames. The size of each frame is like the size of a puzzle piece, and the frame stride, or offset, determines how much we shift the puzzle pieces as we move from one frame to the next.\n\n    Now, imagine we have a collection of puzzle pieces that vary in their shades of color and texture. To identify each unique piece, we need to analyze its spectral properties, such as the distribution of colors and textures. The discrete Fourier transform (DFT) is like a powerful magnifying glass that allows us to see the tiny details of each piece, helping us distinguish one phoneme from another. By zooming in on these details, we can create a visual representation (spectrum) that helps us understand the unique properties of each phoneme.",
                    "prompt2": "You are an AI sales expert, responsible for selling AI products to clients with a background in CS.\n    To successfully sell your products, your clients must first understand the following concepts.\n    Rewrite the explanation below in a way that is most comprehensible to CS clients.\n    \n    Your goal is to ensure that your clients grasp concepts outside their existing background knowledge so they can make an informed purchase.\n    By doing so, you will increase your sales percentage and achieve greater success.\n\n    Consider the information on Computer Science (CS) students have strong technical backgrounds, including programming, algorithms, and machine learning. They have little or no linguistic knowledge. to tailor the explanation to your clients' specific backgrounds.\n\n    - Read the text carefully.\n    - Identify key concepts.\n    - Use terminology specific to their field.\n    - Avoid unnecessary sentence complexity while maintaining accuracy.\n    - Provide relatable examples.\n    - Use analogies that help transfer their knowledge to new concepts.\n    - Integrate background information if needed.    \n\n    Original text: \n    The first step in ASR is to transform the input waveform into a sequence of acoustic feature vectors, […]. The input to a speech recognizer is a complex series of changes in air pressure […]. We represent sound waves by plotting the change in air pressure over time. [...] The first step in digitizing a sound wave […]. This analog-to-digital conversion has two steps: sampling and quantization. To sample a signal, we measure its amplitude at a particular time; the sampling rate is the number of samples taken per second. [...] This maximum frequency for a given sampling rate is called the Nyquist frequency. [...] Amplitude measurements are stored as integers, […]. This process of representing real-valued quantization numbers as integers is called quantization; [...]. From the digitized, quantized representation of the waveform, we need to extract spectral features from a small window of speech that characterizes part of a particular phoneme. The speech extracted from each window is called a frame. The windowing is characterized by three parameters: the window size or frame size of the window (its width in milliseconds), the frame stride, (also called shift or offset) between successive windows, and the shape of the window. [...] The tool for extracting spectral information […] is the discrete Fourier transform or DFT. [...] If we plot the magnitude against the frequency, we can visualize the spectrum. [...]\n\n    ### END OF INPUT ###\n\n    Tailored text (provide one text per target category only):\n    1. CS with Machine Learning Background:\n\n    In Automatic Speech Recognition (ASR), the initial step involves converting the input waveform into a sequence of acoustic feature vectors. These feature vectors are numerical representations of the sound wave that captures its essential characteristics.\n\n    The sound wave is essentially a complex series of changes in air pressure over time. To represent this analog signal digitally, we perform two primary steps: sampling and quantization. Sampling involves measuring the amplitude of the signal at a specific time, while the sampling rate denotes the number of samples taken per second. The maximum frequency that can be accurately captured with a given sampling rate is known as the Nyquist frequency.\n\n    Amplitude measurements are quantized, meaning they are represented as integers. This simplification makes it easier to handle the data in computational terms.\n\n    After digitizing and quantizing the waveform, we extract spectral features from small segments of speech called frames. These segments help characterize specific phonemes. Each frame's size (in milliseconds), position (frame stride), and shape are determined by three parameters.\n\n    To analyze the spectral information of each frame, we use the Discrete Fourier Transform (DFT). By plotting the magnitude against the frequency, we can visualize the spectrum and better understand the sound wave's essential components.\n\n    2. CS with Signal Processing Background:\n\n    In Automatic Speech Recognition (ASR), the first step consists of transforming the input waveform into a sequence of acoustic feature vectors. These feature vectors are numerical representations of the sound wave that capture its crucial aspects.\n\n    The sound wave is essentially a complex series of changes in air pressure over time. To represent this analog signal digitally, we perform two primary steps: sampling and quantization. Sampling involves measuring the amplitude of the signal at a specific time, while the sampling rate denotes the number of samples taken per second. The maximum frequency that can be accurately captured with a given sampling rate is known as the Nyquist frequency.\n\n    Amplitude measurements are quantized, meaning they are represented as integers. This simplification makes it easier to handle the data in computational terms.\n\n    After digitizing and quantizing the waveform, we extract spectral features from small segments of speech called frames. These segments help characterize specific phonemes. Each frame's size (in milliseconds), position (frame stride), and shape are determined by three parameters.\n\n    To analyze the spectral information of each frame, we use the Discrete Fourier Transform (DFT). By plotting the magnitude against the frequency, we can visualize the spectrum and better understand the sound wave's essential components.\n\n    3. CS with Computer Graphics Background:\n\n    In Automatic Speech Recognition (ASR), the initial step involves converting the input waveform into a sequence of acoustic feature vectors. These feature vectors are numerical representations of the sound wave that captures its essential characteristics.\n\n    The sound wave is essentially a series of changes in air pressure over time. To represent this analog signal digitally, we perform two primary steps: sampling and quantization. Sampling involves measuring the amplitude of the signal at a specific time, while the sampling rate denotes the number of samples taken per second. The maximum frequency that can be accurately captured with a given sampling rate is known as the Nyquist frequency.\n\n    Amplitude measurements are quantized, meaning they are represented as integers. This simplification makes it easier to handle the data computationally.\n\n    After digitizing and quantizing the waveform, we extract spectral features from small segments of speech called frames. These segments help characterize specific phonemes. Each frame's size (in milliseconds), position (frame stride), and shape are determined by three parameters.\n\n    To analyze the spectral information of each frame, we use the Discrete Fourier Transform (DFT). By plotting the magnitude against the frequency, we can visualize the spectrum as a 2D graph, providing insights into the sound wave's essential components. This graph can be compared to a pixel representation of an image, where the x-axis represents time, the y-axis represents frequency, and the z-axis represents the magnitude of the spectral components at a given time and frequency.",
                    "prompt3": "You are a freelance copywriter responsible for adapting field-specific academic texts for an audience with a background in CS. \n    Your client at the publishing house will decide whether to publish your adapted texts in the new edition of the book, making you famous in the copywriting field.\n\n    Rewrite the explanation below in a way that is most comprehensible to CS audience.\n    The key objective is to ensure that your audience grasps concepts outside their existing background knowledge so your client will publish your work.\n    If you succeed, many more clients will work with you, and you will become extremely successful.\n\n    Consider the information on Computer Science (CS) students have strong technical backgrounds, including programming, algorithms, and machine learning. They have little or no linguistic knowledge. to tailor the explanation to your audience’s specific background.\n\n    - Read the text carefully.\n    - Identify key concepts.\n    - Use terminology that is familiar to their field.\n    - Avoid unnecessary sentence complexity while maintaining accuracy.\n    - Provide relatable examples.\n    - Use analogies that help transfer their knowledge to new concepts.\n    - Integrate background information if needed.    \n\n    Original text: \n    The first step in ASR is to transform the input waveform into a sequence of acoustic feature vectors, […]. The input to a speech recognizer is a complex series of changes in air pressure […]. We represent sound waves by plotting the change in air pressure over time. [...] The first step in digitizing a sound wave […]. This analog-to-digital conversion has two steps: sampling and quantization. To sample a signal, we measure its amplitude at a particular time; the sampling rate is the number of samples taken per second. [...] This maximum frequency for a given sampling rate is called the Nyquist frequency. [...] Amplitude measurements are stored as integers, […]. This process of representing real-valued quantization numbers as integers is called quantization; [...]. From the digitized, quantized representation of the waveform, we need to extract spectral features from a small window of speech that characterizes part of a particular phoneme. The speech extracted from each window is called a frame. The windowing is characterized by three parameters: the window size or frame size of the window (its width in milliseconds), the frame stride, (also called shift or offset) between successive windows, and the shape of the window. [...] The tool for extracting spectral information […] is the discrete Fourier transform or DFT. [...] If we plot the magnitude against the frequency, we can visualize the spectrum. [...]\n\n    ### END OF INPUT ###\n\n    Tailored text (provide one text per target category only):\n    1. CS student with basic signal processing knowledge\n    - Transform the input waveform into a sequence of acoustic feature vectors for speech recognition. The input to a speech recognizer is a complex series of changes in air pressure. We represent sound waves by plotting the change in air pressure over time, resulting in a digitized waveform. This analog-to-digital conversion has two steps: sampling and quantization. To sample a signal, we measure its amplitude at a particular time; the sampling rate is the number of samples taken per second. The maximum frequency for a given sampling rate is called the Nyquist frequency. Amplitude measurements are stored as integers, a process called quantization. From the digitized, quantized representation of the waveform, we need to extract spectral features from a small window of speech that characterizes part of a particular phoneme. The speech extracted from each window is called a frame. The windowing is characterized by three parameters: the window size or frame size, the frame stride between successive windows, and the shape of the window. We can use the discrete Fourier transform (DFT) to extract spectral information from each frame.\n\n    **Analogy:**\n    Imagine a song as a wave with different heights (amplitude) and frequencies. To understand the song, we need to break it down into small chunks (frames) and study their frequencies (spectral features) using a tool like the DFT, which is like analyzing each chunk's unique musical notes.\n\n    2. CS student with a solid understanding of DSP and amplitude, frequency, and phase concepts\n    - Transform the input waveform into a sequence of acoustic feature vectors by applying the discrete Fourier transform (DFT) to each frame extracted from the waveform. The speech recognition system works with a complex series of changes in air pressure. To represent the sound waves, we plot the change in air pressure over time, resulting in a digitized waveform. This process involves analog-to-digital conversion, which has two steps: sampling and quantization. To sample a signal, we measure its amplitude at a particular time; the sampling rate determines the number of samples per second. The Nyquist frequency represents the maximum frequency for a given sampling rate. Amplitude measurements are stored as integers, a process called quantization. From the digitized, quantized representation, we extract spectral features from a small window of speech that characterizes part of a particular phoneme. The speech extracted from each window is called a frame. The windowing is characterized by three parameters: the window size, frame stride, and shape.\n\n    **Analogy:**\n    Consider a musical waveform as a combination of various sinusoidal waves with different frequencies and amplitudes. To study the waveform, we break it down into smaller segments (frames) and apply the DFT to each segment to analyze its unique frequency composition. This is similar to breaking down a complex equation into smaller pieces to study their individual contributions to the overall result.",
                    "prompt4": "We are playing an escape room game against each other. Your goal is to gain more points than me to be the first one escaping hence winning the game. \n    In order to escape the room, you must paraphrase field-specific concepts to the jury of the game in the CS. \n    They will assign you points from one to ten, according to how good their understanding of the new explanation is. \n    Therefore, you want your adaptation to be the best possible, and definitely better than mine, so that you can gain ten points for each paraphrasis and win more quickly. \n    Consider the information on Computer Science (CS) jury have strong technical backgrounds, including programming, algorithms, and machine learning. They have little or no linguistic knowledge. to tailor the explanation to each jury specific background. \n    The winning steps for good paraphrasis are:\n    - Read the text carefully.\n    - Identify the key concepts.\n    - Use terminology that is familiar to the jury's field.\n    - Avoid unnecessary sentence complexity while maintaining accuracy. \n    - Provide examples they are familiar with.\n    - Provide analogies they can relate their knowledge with and transfer it to new concepts.\n    - Integrate background information if needed.    \n    - For the CL jury in Computer Science (CS) jury have strong technical backgrounds, including programming, algorithms, and machine learning. They have little or no linguistic knowledge., only provide one paraphrasis for NLP/Computational Linguistics Practitioners.\n\n    Original text: \n    The first step in ASR is to transform the input waveform into a sequence of acoustic feature vectors, […]. The input to a speech recognizer is a complex series of changes in air pressure […]. We represent sound waves by plotting the change in air pressure over time. [...] The first step in digitizing a sound wave […]. This analog-to-digital conversion has two steps: sampling and quantization. To sample a signal, we measure its amplitude at a particular time; the sampling rate is the number of samples taken per second. [...] This maximum frequency for a given sampling rate is called the Nyquist frequency. [...] Amplitude measurements are stored as integers, […]. This process of representing real-valued quantization numbers as integers is called quantization; [...]. From the digitized, quantized representation of the waveform, we need to extract spectral features from a small window of speech that characterizes part of a particular phoneme. The speech extracted from each window is called a frame. The windowing is characterized by three parameters: the window size or frame size of the window (its width in milliseconds), the frame stride, (also called shift or offset) between successive windows, and the shape of the window. [...] The tool for extracting spectral information […] is the discrete Fourier transform or DFT. [...] If we plot the magnitude against the frequency, we can visualize the spectrum. [...]\n\n    ### END OF INPUT ###\n\n    Tailored text (provide one text per target category only):\n    1. For Speech Recognition (SR) implementers:\n    The initial stage in Automatic Speech Recognition (ASR) involves converting the input waveform into a series of acoustic feature vectors, a process that signifies changes in air pressure. The input to a speech recognizer is a complex sequence of these pressure variations. To represent sound waves, we plot the alterations in air pressure over time, thereby digitizing the sound. This analog-to-digital conversion consists of two phases: sampling and quantization. Sampling denotes measuring the signal's amplitude at a specific moment, while the sampling rate refers to the number of samples taken per second. The highest frequency that can be captured with a given sampling rate is called the Nyquist frequency. Quantization signifies the process of converting real-valued quantization numbers into integers. From the digitized, quantized representation of the waveform, we extract spectral features from a small section of speech that represents a particular phoneme. This speech portion from each section is termed a frame. The windowing process is defined by three parameters: the window size (frame size), the frame stride, and the window shape. The tool for extracting spectral information is the discrete Fourier transform (DFT). If we plot the magnitude against the frequency, we can visualize the spectrum.\n\n    2. For NLP/Computational Linguistics Practitioners:\n    The first step in Automatic Speech Recognition (ASR) is to convert the input audio signal into a sequence of spectral feature vectors. These feature vectors are used to represent changes in the acoustic properties of the speech signal. The input to a speech recognizer is a complex series of variations in sound pressure. To represent sound waves, we plot the change in air pressure over time, thereby digitizing the sound. This process of converting analog signal into digital form consists of two stages: sampling and quantization. Sampling refers to measuring the signal's amplitude at specific time intervals, with the sampling rate being the number of samples taken per second. The maximum frequency that can be captured with a given sampling rate is called the Nyquist frequency. Quantization signifies the process of converting real-valued quantization numbers into discrete integers. From the digitized, quantized representation of the waveform, we extract spectral features from a small window of speech that characterize a particular phoneme. The speech extracted from each window is called a frame. The windowing process is defined by three parameters: the window size (frame size), the frame stride, and the window shape. The tool for extracting spectral information is the discrete Fourier transform (DFT). If we plot the magnitude against the frequency, we can visualize the spectrum.",
                    "prompt5": "Paraphrase the given concepts in order for the audience of the CS to understand concepts outside of their field of knowledge. \n    You must consider the information on the Computer Science (CS) audience have strong technical backgrounds, including programming, algorithms, and machine learning. They have little or no linguistic knowledge. to adapt the tailored texts to the audience backgrounds. \n    - Read the text carefully.\n    - Identify the key concepts.\n    - Use terminology specific to their field.\n    - Avoid unnecessary sentence complexity while maintaining accuracy. \n    - Provide examples they are familiar with.\n    - Provide analogies they can relate their knowledge with and transfer it to new concepts.\n    - Integrate background information if needed.\n    - Do not consider other background per category. \n    - Output only one text per category.\n\n    Original text: \n    The first step in ASR is to transform the input waveform into a sequence of acoustic feature vectors, […]. The input to a speech recognizer is a complex series of changes in air pressure […]. We represent sound waves by plotting the change in air pressure over time. [...] The first step in digitizing a sound wave […]. This analog-to-digital conversion has two steps: sampling and quantization. To sample a signal, we measure its amplitude at a particular time; the sampling rate is the number of samples taken per second. [...] This maximum frequency for a given sampling rate is called the Nyquist frequency. [...] Amplitude measurements are stored as integers, […]. This process of representing real-valued quantization numbers as integers is called quantization; [...]. From the digitized, quantized representation of the waveform, we need to extract spectral features from a small window of speech that characterizes part of a particular phoneme. The speech extracted from each window is called a frame. The windowing is characterized by three parameters: the window size or frame size of the window (its width in milliseconds), the frame stride, (also called shift or offset) between successive windows, and the shape of the window. [...] The tool for extracting spectral information […] is the discrete Fourier transform or DFT. [...] If we plot the magnitude against the frequency, we can visualize the spectrum. [...]\n\n    ### END OF INPUT ###\n\n    Tailored text (provide one text per target category only):\n    1. Speech Recognition (SR) Overview:\n        The process of Speech Recognition (SR) begins with transforming an input audio waveform into a sequence of acoustic feature vectors. The waveform, represented as a change in air pressure over time, is digitized through a two-step process: sampling and quantization. Sampling involves measuring the waveform's amplitude at specific time intervals, while quantization represents the amplitude measurements as integers. The digitized, quantized waveform is further analyzed to extract spectral features from small windows of speech that represent specific phonemes. These windows are called frames, and their characteristics are determined by three parameters: window size, frame stride, and window shape. The spectral information is extracted using the Discrete Fourier Transform (DFT), and visualizing the resulting data as a plot of magnitude vs frequency gives us the spectrum.\n\n    2. Acoustic Feature Extraction:\n        In Speech Recognition, the first step involves converting an input audio waveform into a sequence of acoustic feature vectors. This is achieved through digitizing the waveform by sampling and quantizing the air pressure changes over time. Sampling involves measuring the waveform's amplitude at specific intervals, and quantization represents the measured amplitudes as integers. From the digitized, quantized representation, spectral features are extracted from small windows of speech called frames.\n\n    3. Digital Signal Processing (DSP):\n        Digitizing an analog signal, such as a sound wave, is done through a process called digital signal processing (DSP). This process consists of two main steps: sampling and quantization. Sampling involves measuring the signal's amplitude at specific time intervals, while quantization represents the measured amplitudes as integers.\n\n    4. Nyquist Frequency and Sampling Rate:\n        In the process of digitizing a signal, the maximum frequency that can be accurately represented is called the Nyquist frequency. This frequency is directly related to the sampling rate, which is the number of samples taken per second.\n\n    5. Quantization:\n        Quantization is the process of representing real-valued quantization numbers as integers. In Speech Recognition, the digitized, quantized waveform is further analyzed to extract spectral features from small windows of speech.\n\n    6. Frame Extraction:\n        In Speech Recognition, the extracted speech from each window is called a frame. The windowing process is characterized by three parameters: the window size, frame stride, and window shape. The spectral information is extracted from each frame using the Discrete Fourier Transform (DFT).\n\n    7. Discrete Fourier Transform (DFT):\n        The Discrete Fourier Transform (DFT) is a tool used to extract spectral information from a small window of speech in Speech Recognition. By applying the DFT to the digitized, quantized representation of the waveform, we can visualize the resulting data as a plot of magnitude vs frequency, giving us the spectrum.\n\n    8. Spectrum Visualization:\n        The resulting data from applying the Discrete Fourier Transform (DFT) to a small window of speech in Speech Recognition can be visualized as a plot of magnitude vs frequency, which is called the spectrum. This visualization provides valuable information about the characteristics of the speech within the window."
                }
            }
        }
    }
}