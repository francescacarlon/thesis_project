
            <!DOCTYPE html>
            <html lang="en">
            <head>
                <meta charset="UTF-8">
                <meta name="viewport" content="width=device-width, initial-scale=1.0">
                <title>Naive Bayes Classifiers</title>
                <style>
                    body { font-family: Arial, sans-serif; margin: 40px; padding: 20px; line-height: 1.6; }
                    .container { max-width: 800px; margin: auto; }
                    h1, h2 { color: #333; }
                    .box { background: #f4f4f4; padding: 15px; margin-bottom: 20px; border-radius: 5px; }
                    .category { font-weight: bold; color: #0056b3; }
                </style>
            </head>
            <body>
                <div class="container">
                    <h1>Naive Bayes Classifiers</h1>
                    <p><strong>Instance Code:</strong> T11</p>
                    <p><strong>Original Category:</strong> CL</p>
                    <p><strong>Target Category:</strong> CS</p>
                    <div class="box">
                        <h2>Original Text</h2>
                        <p>In this section we introduce the multinomial naive Bayes classifier, so called because it is a Bayesian classifier that makes a simplifying (naive) assumption about how the features interact. [...] We represent a text document as if it were a bag of words, that is, an unordered set of words with their position ignored, keeping only their frequency in the document. [...] Naive Bayes is a probabilistic classifier, meaning that for a document d, out of all classes c ∈ C the classifier returns the class ˆc which has the maximum posterior probability given the document. […] we use the hat notation ˆ to mean “our estimate of the correct class”, and we use argmax to mean an operation that selects the argument (in this case the class c) that maximizes a function (in this case the probability P(c|d). cˆ = argmax c∈C P(c|d). [...] Thus, we can choose the class that maximizes this simpler formula: cˆ = argmax c∈C P(c|d) = argmax c∈C P(d|c)P(c). [...] we compute the most probable class ˆc given some document d by choosing the class which has the highest product of two probabilities: the prior probability of the class P(c) and the likelihood of the document P(d|c): cˆ = argmax c∈C P(d|c)(likelihood)P(c)(prior). [...] The first is the bag-of-words assumption discussed intuitively above: we assume position doesn't matter [...]. The second is commonly called the naive Bayes assumption: this is the conditional independence assumption that the probabilities P(fi|c) are independent given the class c and hence can be 'naively' multiplied as follows: P(f1, f2,...., fn|c) = P(f1|c)·P(f2|c)· ... ·P(fn|c). The final equation for the class chosen by a naive Bayes classifier is thus: cNB = argmax c∈C P(c) Yf∈F P(f|c). [...] . Classifiers that use a linear combination of the inputs to make a classification decision —like naive Bayes and also logistic regression—are called linear classifiers.</p>
                    </div>
                    <h2>CS Tailored Explanations</h2>
            
                    <div class="box">
                        <p class="category">mistral - CS (CS_mistral_prompt1)</p>
                        <p>1. In this section, we'll discuss the Multinomial Naive Bayes classifier, a Bayesian classifier that inherently simplifies interactions between features assuming they are independent.

    2. To represent a text document, we treat it as a bag of words — an unordered collection of words with their positions ignored, preserving only their frequencies within the document.

    3. As a probabilistic classifier, the Multinomial Naive Bayes classifier aims to find the class ˆc with the highest probability given a document d, among all classes c ∈ C.

    4. Using the hat notation ˆ to signify our estimated correct class, and argmax to denote selecting the argument that maximizes a function, we write this as: cˆ = argmax c∈C P(c|d).

    5. Essentially, we select the class that maximizes this simplified formula: cˆ = argmax c∈C P(c|d) = argmax c∈C P(d|c)P(c).

    6. To determine the most probable class ˆc for a given document d, we choose the class with the highest product of two probabilities: the prior probability of the class P(c) and the likelihood of the document P(d|c). This can be written as: cˆ = argmax c∈C P(d|c)(likelihood)P(c)(prior).

    7. The Multinomial Naive Bayes classifier makes two main assumptions:
        a. The bag-of-words assumption, which states that word positions don't matter.
        b. The naive Bayes assumption, which assumes that the probabilities P(fi|c) are independent given the class c and can be multiplied as follows: P(f1, f2,...., fn|c) = P(f1|c)·P(f2|c)· ... ·P(fn|c). The final equation for the class chosen by a naive Bayes classifier is thus: cNB = argmax c∈C P(c) Yf∈F P(f|c).

    8. Classifiers that employ a linear combination of inputs to make a classification decision, like naive Bayes and logistic regression, are known as linear classifiers.</p>
                    </div>
                    
                    <div class="box">
                        <p class="category">llama - CS (CS_llama_prompt5)</p>
                        <p>The multinomial naive Bayes classifier is similar to a decision tree in machine learning, where each node represents a feature or attribute. However, in naive Bayes, we make a simplifying assumption that each feature is conditionally independent of the others given the class. Think of it like a bag of words, where the order of words doesn't matter, just their frequency. This is analogous to a term frequency-inverse document frequency (TF-IDF) vector in text analysis, where each word is represented by its frequency and importance in the document. 

    To classify a document, the naive Bayes algorithm calculates the posterior probability of each class given the document, and returns the class with the highest probability. This is similar to the argmax function in optimization problems, where we select the argument that maximizes a function. In this case, the function is the probability of the class given the document, P(c|d). We can simplify this calculation by using Bayes' theorem, which states that P(c|d) = P(d|c)P(c). 

    The naive Bayes assumption is that the features are conditionally independent given the class, which means we can multiply the probabilities of each feature given the class. This is like calculating the probability of a sequence of independent events, where the probability of the sequence is the product of the probabilities of each event. The final equation for the class chosen by a naive Bayes classifier is: cNB = argmax c∈C P(c) ∏f∈F P(f|c). 

    This type of classifier is called a linear classifier, because it uses a linear combination of the inputs to make a classification decision. Other examples of linear classifiers include logistic regression. The key idea is that the decision boundary is a hyperplane, which is a linear subspace that separates the classes. In the case of naive Bayes, the decision boundary is defined by the product of the probabilities of each feature given the class. 

    For example, consider a spam filter that uses naive Bayes to classify emails as spam or not spam. The features might be the frequency of certain words in the email, such as "free" or "discount". The naive Bayes algorithm would calculate the probability of each class given the email, and return the class with the highest probability. If the probability of the email being spam given the features is higher than the probability of it not being spam, then the email is classified as spam. 

    In summary, the multinomial naive Bayes classifier is a type of linear classifier that uses a simplifying assumption of conditional independence to calculate the posterior probability of each class given a document. It is similar to other linear classifiers, such as logistic regression, and is commonly used in text classification tasks, such as spam filtering.</p>
                    </div>
                    
                    <div class="box">
                        <p class="category">gpt4o - CS (CS_gpt4o_prompt4)</p>
                        <p>The multinomial naive Bayes classifier is a type of probabilistic model used for classification tasks. It's called "naive" because it assumes that the features in a dataset are independent of each other given the class, which simplifies the calculations. This model is particularly useful for text classification, where we treat a document as a "bag of words." This means we consider only the frequency of each word in the document, ignoring their order.

In practice, for any given document \(d\), the classifier seeks to identify the class \( \hat{c} \) that maximizes the probability \( P(c|d) \). This is achieved using the argmax operation, which finds the class \(c\) that maximizes the function. The key probability formula is \( \hat{c} = \arg\max_{c \in C} P(d|c)P(c) \), where \(P(c)\) is the prior probability of class \(c\), and \(P(d|c)\) is the likelihood of document \(d\) given class \(c\).

The naive Bayes assumption allows us to treat each word's contribution to the class probability as independent, meaning we can multiply each word's probability given the class: \( P(f_1|c) \cdot P(f_2|c) \cdot \ldots \cdot P(f_n|c) \). This assumption simplifies computations considerably. Naive Bayes, along with logistic regression, falls under the category of linear classifiers because they both use a linear combination of input features to make decisions.</p>
                    </div>
                    
                    <div class="box">
                        <p class="category">claude - CS (CS_claude_prompt2)</p>
                        <p>Here's a CS-friendly explanation of Multinomial Naive Bayes Classification:

Think of Multinomial Naive Bayes as a probabilistic algorithm similar to how you'd implement a simple recommendation system. It's a linear classifier that makes classification decisions based on feature vectors, much like how you'd classify data points in machine learning.

The algorithm treats a document as an array of word frequencies, ignoring their sequential order (similar to converting a sequence into a hashmap of counts). This representation is called a "bag of words" - imagine dumping all words into an unordered data structure where only frequency matters.

The core algorithm follows this pattern:
1. For input document d, we want to find class c that maximizes P(c|d)
2. Using Bayes' theorem, we optimize: argmax(P(d|c) * P(c))
   where P(c) is our prior probability (like class distribution in training data)
   and P(d|c) is our likelihood

The "naive" part comes from a simplifying assumption in our probability calculation: we treat features (words) as independent variables, similar to treating array elements as independent in parallel processing. This allows us to multiply individual feature probabilities instead of calculating complex dependencies:

P(features|class) = P(feature1|class) * P(feature2|class) * ... * P(featureN|class)

The final implementation reduces to:
classifierOutput = argmax(P(class) * product(P(feature|class)))

This makes it computationally efficient, O(n) complexity, similar to linear search algorithms you're familiar with. Like other linear classifiers (logistic regression), it creates a decision boundary through feature space using a linear combination of inputs.</p>
                    </div>
                    
                    <div class="box">
                        <p class="category">deepseek - CS (CS_deepseek_prompt1)</p>
                        <p>Here's the CS-focused explanation:

The multinomial naive Bayes classifier is a probabilistic algorithm for categorizing data points (like text documents). Think of it as an optimized way to compute the most likely class label by combining prior knowledge with observed feature patterns.

Key mechanics:

1. **Data Representation**: Documents are encoded as *feature vectors* counting word frequencies (like a hashmap storing word:count pairs). Positional info is discarded - similar to building a histogram of tokens.

2. **Bayesian Core**: For a document **d**, we calculate:
   ĉ = argmax[P(c) * Π P(wordᵢ|c)]  
   Where:
   - P(c) = prior probability (class frequency in training data)
   - P(wordᵢ|c) = likelihood (word frequency in class c)

3. **Naive Assumption = Conditional Independence**: Features (words) are treated as independent given the class - like assuming pixels in an image classifier don't spatially interact. This allows efficient probability multiplication instead of calculating complex joint probabilities.

4. **Optimization Perspective**: The argmax operation makes this an optimization problem - we're finding the class that maximizes the product of prior and likelihood terms. In practice, we use log probabilities to convert products to sums (avoiding underflow, similar to numerical stability in neural networks).

5. **Linear Classification**: Like logistic regression, this is a linear classifier - decision boundaries are hyperplanes in the feature space. The "linearity" becomes clear when taking logs:  
   log(P(c|d)) ∝ log(P(c)) + Σ count(wordᵢ) * log(P(wordᵢ|c))

Example: Spam detection  
- Features = word counts  
- P(spam) = % of spam emails in training set  
- P("free"|spam) = frequency of "free" in spam vs non-spam  
- New email classified by multiplying all word probabilities with prior

This approach is computationally efficient (O(n) complexity) and works well with high-dimensional data, making it popular for NLP tasks despite its simplifying assumptions.</p>
                    </div>
                    </div></body></html>