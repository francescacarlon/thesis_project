
            <!DOCTYPE html>
            <html lang="en">
            <head>
                <meta charset="UTF-8">
                <meta name="viewport" content="width=device-width, initial-scale=1.0">
                <title>Automatic Speech Recognition (ASR)</title>
                <style>
                    body { font-family: Arial, sans-serif; margin: 40px; padding: 20px; line-height: 1.6; }
                    .container { max-width: 800px; margin: auto; }
                    h1, h2 { color: #333; }
                    .box { background: #f4f4f4; padding: 15px; margin-bottom: 20px; border-radius: 5px; }
                    .category { font-weight: bold; color: #0056b3; }
                </style>
            </head>
            <body>
                <div class="container">
                    <h1>Automatic Speech Recognition (ASR)</h1>
                    <p><strong>Instance Code:</strong> T15</p>
                    <p><strong>Original Category:</strong> CL</p>
                    <p><strong>Target Category:</strong> CS</p>
                    <div class="box">
                        <h2>Original Text</h2>
                        <p>The first step in ASR is to transform the input waveform into a sequence of acoustic feature vectors, […]. The input to a speech recognizer is a complex series of changes in air pressure […]. We represent sound waves by plotting the change in air pressure over time. [...] The first step in digitizing a sound wave […]. This analog-to-digital conversion has two steps: sampling and quantization. To sample a signal, we measure its amplitude at a particular time; the sampling rate is the number of samples taken per second. [...] This maximum frequency for a given sampling rate is called the Nyquist frequency. [...] Amplitude measurements are stored as integers, […]. This process of representing real-valued quantization numbers as integers is called quantization; [...]. From the digitized, quantized representation of the waveform, we need to extract spectral features from a small window of speech that characterizes part of a particular phoneme. The speech extracted from each window is called a frame. The windowing is characterized by three parameters: the window size or frame size of the window (its width in milliseconds), the frame stride, (also called shift or offset) between successive windows, and the shape of the window. [...] The tool for extracting spectral information […] is the discrete Fourier transform or DFT. [...] If we plot the magnitude against the frequency, we can visualize the spectrum. [...]</p>
                    </div>
                    <h2>CS Tailored Explanations</h2>
            
                    <div class="box">
                        <p class="category">mistral - CS (CS_mistral_prompt2)</p>
                        <p>1. To begin with our Automatic Speech Recognition (ASR) solution, we transform the input sound wave into a collection of acoustic feature vectors. A sound wave is represented as a graph of air pressure changes over time.

    2. For digitizing the sound wave, we perform two main steps: sampling and quantization. Sampling involves measuring the wave's amplitude at specific instances, and the sampling rate is the number of samples taken per second. The highest frequency that can be accurately represented by a given sampling rate is known as the Nyquist frequency.

    3. Quantization is the process of representing the amplitude measurements as integers.

    4. From the digitized and quantized representation, we extract spectral features from a small segment of speech, called a frame. Each frame characterizes part of a specific phoneme. The windowing process for creating frames comprises three parameters: the frame size (window width in milliseconds), the frame stride (the distance between successive windows), and the window shape.

    5. To extract spectral information from each frame, we utilize the Discrete Fourier Transform (DFT) tool. This analysis provides a visual representation of the spectrum by plotting the magnitude against the frequency.</p>
                    </div>
                    
                    <div class="box">
                        <p class="category">llama - CS (CS_llama_prompt4)</p>
                        <p>The process of converting audio input into a digital signal can be likened to compiling code from a high-level language to machine code. The initial step involves capturing the audio waveform, akin to reading source code, and transforming it into a sequence of feature vectors, similar to how an interpreter breaks down code into executable instructions. The audio signal is represented as a series of changes in air pressure over time, comparable to how a time series dataset is plotted. To digitize this signal, we employ an analog-to-digital conversion, which consists of sampling, where we measure the amplitude at discrete time intervals, and quantization, where we assign integer values to these amplitudes, much like how floating-point numbers are represented as integers in a computer. The sampling rate determines the maximum frequency that can be captured, known as the Nyquist frequency, analogous to the clock speed of a CPU. We then extract spectral features from small segments of the audio, called frames, using a windowing technique with parameters such as size, stride, and shape, similar to how a sliding window algorithm processes data. The discrete Fourier transform (DFT) is used to extract spectral information, analogous to performing a frequency analysis on a signal, allowing us to visualize the spectrum by plotting magnitude against frequency, much like visualizing a dataset's frequency distribution.</p>
                    </div>
                    
                    <div class="box">
                        <p class="category">gpt4o - CS (CS_gpt4o_prompt4)</p>
                        <p>In automatic speech recognition (ASR), the first step is converting the input sound wave into a series of acoustic feature vectors. This process begins with the analog-to-digital conversion of the sound wave, which involves two main steps: sampling and quantization. Sampling means measuring the wave's amplitude at specific intervals, with the sampling rate being the number of measurements per second. The highest frequency that can be accurately captured at a given sampling rate is known as the Nyquist frequency. During quantization, these amplitude measurements are represented as integers.

After digitizing and quantizing the waveform, we extract spectral features from small segments of speech known as frames. These frames are characterized by parameters such as window size (duration in milliseconds), frame stride (the gap between successive frames), and window shape. To derive spectral information from these frames, we use the discrete Fourier transform (DFT), which helps analyze the frequency components of the signal. By plotting the magnitude of these components against frequency, we obtain a spectrum that represents the sound's characteristics.</p>
                    </div>
                    
                    <div class="box">
                        <p class="category">claude - CS (CS_claude_prompt5)</p>
                        <p>Here's the CS-adapted explanation:

Think of speech recognition's first phase as signal processing, similar to handling any digital input stream. The raw audio input is essentially a time series of amplitude values - like an array of floating-point numbers representing air pressure changes over time.

The conversion process is analogous to image processing, where we convert analog signals to digital data. We perform sampling (like setting a frame rate) at regular intervals, typically 16,000 times per second, following the Nyquist theorem - which is similar to how we need sufficient FPS in video to avoid temporal aliasing.

The quantization step maps continuous values to discrete integers, much like converting floating-point numbers to fixed-point representation in digital signal processing. Think of it as reducing a 32-bit float to an 8-bit integer for efficient storage.

For feature extraction, we implement a sliding window approach (similar to convolution operations in CNNs) where we process fixed-size chunks of samples called frames. Each frame overlaps with its neighbors, controlled by parameters like:
- Window size (buffer length in ms)
- Stride (step size between consecutive windows)
- Window function (weighting matrix)

The spectral analysis uses the Discrete Fourier Transform (DFT) algorithm - essentially converting time-domain data into frequency-domain features, similar to how image processing converts spatial data to frequency components. The output is comparable to a feature vector in machine learning, where each dimension represents the energy at a specific frequency band.

This transforms our raw input array into a matrix of feature vectors that can be fed into subsequent machine learning models.</p>
                    </div>
                    
                    <div class="box">
                        <p class="category">deepseek - CS (CS_deepseek_prompt4)</p>
                        <p>In ASR, raw audio is processed like a time-series dataset. First, **analog-to-digital conversion** samples the continuous sound wave at fixed intervals (sampling rate), akin to capturing periodic sensor readings in IoT systems. The Nyquist theorem ensures no aliasing, mirroring how data pipelines avoid undersampling. Next, **quantization** maps amplitude values to integers, similar to casting floating-point sensor data to 8-bit representations for efficiency.  

The digitized signal is then split into overlapping **frames** using a sliding window (e.g., 25ms chunks with 10ms shifts), analogous to batch processing in ML or convolutional kernel operations. Each frame undergoes **spectral analysis via DFT/FFT**, decomposing the temporal signal into frequency components—like projecting a vector into an orthogonal basis. The resulting magnitude-frequency plot (spectrum) acts as a feature vector, comparable to extracting edge detectors in CNN layers before classification.  

This pipeline transforms raw waveforms into structured, frequency-domain inputs suitable for sequence models (e.g., RNNs/Transformers), aligning with preprocessing steps in vision or time-series tasks.</p>
                    </div>
                    </div></body></html>