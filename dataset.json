{
    "1": {
        "chapter": "17",
        "sections": "17.1",
        "topic": "English Word Classes",
        "original_category": "L",
        "original_text": "2 CHAPTER17 • SEQUENCELABELINGFORPARTSOFSPEECHANDNAMEDENTITIES\n17.1 (Mostly) English Word Classes\nUntilnowwehavebeenusingpart-of-speechtermslikenounandverbratherfreely.\nIn this section we give more complete definitions. While word classes do have\nsemantictendencies—adjectives, forexample, oftendescribepropertiesandnouns\npeople—partsofspeecharedefinedinsteadbasedontheirgrammaticalrelationship\nwithneighboringwordsorthemorphologicalpropertiesabouttheiraffixes.\nTag Description Example\nssalCnepO\nADJ Adjective:nounmodifiersdescribingproperties red,young,awesome\nADV Adverb:verbmodifiersoftime,place,manner very,slowly,home,yesterday\nNOUN wordsforpersons,places,things,etc. algorithm,cat,mango,beauty\nVERB wordsforactionsandprocesses draw,provide,go\nPROPN Propernoun:nameofaperson,organization,place,etc.. Regina,IBM,Colorado\nINTJ Interjection:exclamation,greeting,yes/noresponse,etc. oh,um,yes,hello\nsdroWssalCdesolC\nADP Adposition (Preposition/Postposition): marks a noun’s in,on,by,under\nspacial,temporal,orotherrelation\nAUX Auxiliary:helpingverbmarkingtense,aspect,mood,etc., can,may,should,are\nCCONJ CoordinatingConjunction:joinstwophrases/clauses and,or,but\nDET Determiner:marksnounphraseproperties a,an,the,this\nNUM Numeral one,two,2026,11:00,hundred\nPART Particle: afunctionwordthatmustbeassociatedwithan- ’s,not,(infinitive)to\notherword\nPRON Pronoun:ashorthandforreferringtoanentityorevent she,who,I,others\nSCONJ Subordinating Conjunction: joins a main clause with a whether,because\nsubordinateclausesuchasasententialcomplement\nrehtO\nPUNCT Punctuation , ,()\n˙\nSYM Symbolslike$oremoji $,%\nX Other asdf,qwfg\nFigure17.1 The17partsofspeechintheUniversalDependenciestagset(deMarneffeetal.,2021).Features\ncanbeaddedtomakefiner-graineddistinctions(withpropertieslikenumber,case,definiteness,andsoon).\nclosedclass Parts of speech fall into two broad categories: closed class and open class.\nopenclass Closed classes are those with relatively fixed membership, such as prepositions—\nnewprepositionsarerarelycoined. Bycontrast,nounsandverbsareopenclasses—\nnewnounsandverbslikeiPhoneortofaxarecontinuallybeingcreatedorborrowed.\nfunctionword Closedclasswordsaregenerallyfunctionwordslikeof,it,and,oryou,whichtend\ntobeveryshort,occurfrequently,andoftenhavestructuringusesingrammar.\nFourmajoropenclassesoccurinthelanguagesoftheworld: nouns(including\npropernouns),verbs,adjectives,andadverbs,aswellasthesmalleropenclassof\ninterjections. Englishhasallfive,althoughnoteverylanguagedoes.\nnoun Nounsarewordsforpeople,places,orthings,butincludeothersaswell. Com-\ncommonnoun monnounsincludeconcretetermslikecatandmango,abstractionslikealgorithm\nandbeauty,andverb-liketermslikepacingasinHispacingtoandfrobecamequite\nannoying. Nouns in English can occur with determiners (a goat, this bandwidth)\ntakepossessives(IBM’sannualrevenue),andmayoccurintheplural(goats,abaci).\ncountnoun Many languages, including English, divide common nouns into count nouns and\nmassnoun mass nouns. Count nouns can occur in the singular and plural (goat/goats, rela-\ntionship/relationships) and can be counted (one goat, two goats). Mass nouns are\nusedwhensomethingisconceptualizedasahomogeneousgroup.Sosnow,salt,and\npropernoun communismarenotcounted(i.e.,*twosnowsor*twocommunisms).Propernouns,\nlikeRegina,Colorado,andIBM,arenamesofspecificpersonsorentities. 17.1 • (MOSTLY)ENGLISHWORDCLASSES 3\nverb Verbs refer to actions and processes, including main verbs like draw, provide,\nandgo.Englishverbshaveinflections(non-third-person-singular(eat),third-person-\nsingular (eats), progressive (eating), past participle (eaten)). While many scholars\nbelievethatallhumanlanguageshavethecategoriesofnounandverb,othershave\narguedthatsomelanguages,suchasRiauIndonesianandTongan,don’tevenmake\nthisdistinction(Broschart1997;Evans2000;Gil2000).\nadjective Adjectives often describe properties or qualities of nouns, like color (white,\nblack), age (old, young), and value (good, bad), but there are languages without\nadjectives. In Korean, for example, the words corresponding to English adjectives\nact as a subclass of verbs, so what is in English an adjective “beautiful” acts in\nKoreanlikeaverbmeaning“tobebeautiful”.\nadverb Adverbsareahodge-podge.Alltheitalicizedwordsinthisexampleareadverbs:\nActually,Iranhomeextremelyquicklyyesterday\nAdverbs generally modify something (often verbs, hence the name “adverb”, but\nlocative also other adverbs and entire verb phrases). Directional adverbs or locative ad-\ndegree verbs(home,here,downhill)specifythedirectionorlocationofsomeaction;degree\nadverbs(extremely,very,somewhat)specifytheextentofsomeaction,process,or\nmanner property;manneradverbs(slowly,slinkily,delicately)describethemannerofsome\ntemporal actionorprocess;andtemporaladverbsdescribethetimethatsomeactionorevent\ntookplace(yesterday,Monday).\ninterjection Interjections(oh,hey,alas,uh,um)areasmalleropenclassthatalsoincludes\ngreetings(hello,goodbye)andquestionresponses(yes,no,uh-huh).\npreposition Englishadpositionsoccurbeforenouns,hencearecalledprepositions.Theycan\nindicatespatialortemporalrelations,whetherliteral(onit,beforethen,bythehouse)\normetaphorical(ontime,withgusto,besideherself),andrelationslikemarkingthe\nagentinHamletwaswrittenbyShakespeare.\nparticle Aparticleresemblesaprepositionoranadverbandisusedincombinationwith\na verb. Particles often have extended meanings that aren’t quite the same as the\nprepositionstheyresemble, asintheparticleoverinsheturnedthepaperover. A\nphrasalverb verb and a particle acting as a single unit is called a phrasal verb. The meaning\nof phrasal verbs is often non-compositional—not predictable from the individual\nmeanings of the verb and the particle. Thus, turn down means ‘reject’, rule out\n‘eliminate’,andgoon‘continue’.\ndeterminer Determinerslikethisandthat(thischapter,thatpage)canmarkthestartofan\narticle Englishnounphrase. Articleslikea,an,andthe,areatypeofdeterminerthatmark\ndiscourse properties of the noun and are quite frequent; the is the most common\nwordinwrittenEnglish,withaandanrightbehind.\nconjunction Conjunctions join two phrases, clauses, or sentences. Coordinating conjunc-\ntionslikeand,or,andbutjointwoelementsofequalstatus. Subordinatingconjunc-\ntions are used when one of the elements has some embedded status. For example,\nthesubordinatingconjunctionthatin“Ithoughtthatyoumightlikesomemilk”links\nthemainclauseIthoughtwiththesubordinateclauseyoumightlikesomemilk.This\nclause is called subordinate because this entire clause is the “content” of the main\nverbthought. Subordinatingconjunctionslikethatwhichlinkaverbtoitsargument\ncomplementizer inthiswayarealsocalledcomplementizers.\npronoun Pronouns act as a shorthand for referring to an entity or event. Personal pro-\nnounsrefertopersonsorentities(you,she,I,it,me,etc.). Possessivepronounsare\nformsofpersonalpronounsthatindicateeitheractualpossessionormoreoftenjust\nanabstractrelationbetweenthepersonandsomeobject(my,your,his,her,its,one’s,\nwh our,their). Wh-pronouns(what,who,whom,whoever)areusedincertainquestion 4 CHAPTER17 • SEQUENCELABELINGFORPARTSOFSPEECHANDNAMEDENTITIES\nforms,oractascomplementizers(Frida,whomarriedDiego...).\nauxiliary Auxiliaryverbsmarksemanticfeaturesofamainverbsuchasitstense,whether\nit is completed (aspect), whether it is negated (polarity), and whether an action is\nnecessary, possible, suggested, or desired (mood). English auxiliaries include the\ncopula copulaverbbe,thetwoverbsdoandhave,forms,aswellasmodalverbsusedto\nmodal markthemoodassociatedwiththeeventdepictedbythemainverb: canindicates\nabilityorpossibility,maypermissionorpossibility,mustnecessity.\nAnEnglish-specifictagset,thePennTreebanktagset(Marcusetal.,1993),shown\nin Fig. 17.2, has been used to label many syntactically annotated corpora like the\nPennTreebankcorpora,soitisworthknowingabout.\nTag Description Example Tag Description Example Tag Description Example\nCC coord.conj. and,but,or NNP propernoun,sing. IBM TO infinitiveto to\nCD cardinalnumber one,two NNPS propernoun,plu. Carolinas UH interjection ah,oops\nDT determiner a,the NNS noun,plural llamas VB verbbase eat\nEX existential‘there’ there PDT predeterminer all,both VBD verbpasttense ate\nFW foreignword meaculpa POS possessiveending ’s VBG verbgerund eating\nIN preposition/ of,in,by PRP personalpronoun I,you,he VBN verb past partici- eaten\nsubordin-conj ple\nJJ adjective yellow PRP$ possess.pronoun your VBP verbnon-3sg-pr eat\nJJR comparativeadj bigger RB adverb quickly VBZ verb3sgpres eats\nJJS superlativeadj wildest RBR comparativeadv faster WDT wh-determ. which,that\nLS listitemmarker 1,2,One RBS superlatv.adv fastest WP wh-pronoun what,who\nMD modal can,should RP particle up,off WP$ wh-possess. whose\nNN singormassnoun llama SYM symbol +,%,& WRB wh-adverb how,where\nFigure17.2 PennTreebankcore36part-of-speechtags.\nBelowweshowsomeexampleswitheachwordtaggedaccordingtoboththeUD\n(in blue) and Penn (in red) tagsets. Notice that the Penn tagset distinguishes tense\nandparticiplesonverbs,andhasaspecialtagfortheexistentialthereconstructionin\nEnglish. NotethatsinceLondonJournalofMedicineisapropernoun,bothtagsets\nmarkitscomponentnounsasPROPN/NNP,includingjournalandmedicine,which\nmightotherwisebelabeledascommonnouns(NOUN/NN).\n(17.1) There/PRON/EXare/VERB/VBP70/NUM/CDchildren/NOUN/NNS\nthere/ADV/RB./PUNC/.\n(17.2) Preliminary/ADJ/JJfindings/NOUN/NNSwere/AUX/VBD\nreported/VERB/VBNin/ADP/INtoday/NOUN/NN’s/PART/POS\nLondon/PROPN/NNPJournal/PROPN/NNPof/ADP/INMedicine/PROPN/NNP\n17.2 Part-of-Speech Tagging\npart-of-speech Part-of-speechtaggingistheprocessofassigningapart-of-speechtoeachwordin\ntagging\na text. The input is a sequence x ,x ,...,x of (tokenized) words and a tagset, and\n1 2 n\ntheoutputisasequencey ,y ,...,y oftags,eachoutputy correspondingexactlyto\n1 2 n i\noneinputx,asshownintheintuitioninFig.17.3.\ni\nambiguous Taggingisadisambiguationtask;wordsareambiguous—havemorethanone\npossible part-of-speech—and the goal is to find the correct tag for the situation.\nFor example, book can be a verb (book that flight) or a noun (hand me that book).\nThat can be a determiner (Does that flight serve dinner) or a complementizer (I 24 CHAPTER17 • SEQUENCELABELINGFORPARTSOFSPEECHANDNAMEDENTITIES\nLog-linear models for POS tagging were introduced by Ratnaparkhi (1996),\nwhointroducedasystemcalledMXPOSTwhichimplementedamaximumentropy\nMarkov model (MEMM), a slightly simpler version of a CRF. Around the same\ntime,sequencelabelerswereappliedtothetaskofnamedentitytagging,firstwith\nHMMs (Bikel et al., 1997) and MEMMs (McCallum et al., 2000), and then once\nCRFs were developed (Lafferty et al. 2001), they were also applied to NER (Mc-\nCallumandLi,2003). Awideexplorationoffeaturesfollowed(Zhouetal.,2005).\nNeural approaches to NER mainly follow from the pioneering results of Collobert\netal.(2011),whoappliedaCRFontopofaconvolutionalnet. BiLSTMswithword\nand character-based embeddings as input followed shortly and became a standard\nneural algorithm for NER (Huang et al. 2015, Ma and Hovy 2016, Lample et al.\n2016)followedbythemorerecentuseofTransformersandBERT.\nTheideaofusinglettersuffixesforunknownwordsisquiteold;theearlyKlein\nand Simmons (1963) system checked all final letter suffixes of lengths 1-5. The\nunknownwordfeaturesdescribedonpage17comemainlyfromRatnaparkhi(1996),\nwithaugmentationsfromToutanovaetal.(2003)andManning(2011).\nStateoftheartPOStaggersuseneuralalgorithms,eitherbidirectionalRNNsor\nTransformerslikeBERT;seeChapter8toChapter11. HMM(Brants2000;Thede\nandHarper1999)andCRFtaggeraccuraciesarelikelyjustatadlower.\nManning(2011)investigatestheremaining2.7%oferrorsinahigh-performing\ntagger(Toutanovaetal.,2003). Hesuggeststhatathirdorhalfoftheseremaining\nerrorsareduetoerrorsorinconsistenciesinthetrainingdata,athirdmightbesolv-\nablewithricherlinguisticmodels, andfortheremainderthetaskisunderspecified\norunclear.\nSupervised tagging relies heavily on in-domain training data hand-labeled by\nexperts. Waystorelaxthisassumptionincludeunsupervisedalgorithmsforcluster-\ningwordsintopart-of-speech-likeclasses,summarizedinChristodoulopoulosetal.\n(2010),andwaystocombinelabeledandunlabeleddata,forexamplebyco-training\n(Clarketal.2003;Søgaard2010).\nSee Householder (1995) for historical notes on parts of speech, and Sampson\n(1987)andGarsideetal.(1997)ontheprovenanceoftheBrownandothertagsets.\nExercises\n17.1 Findonetaggingerrorineachofthefollowingsentencesthataretaggedwith\nthePennTreebanktagset:\n1. I/PRPneed/VBPa/DTflight/NNfrom/INAtlanta/NN\n2. Does/VBZthis/DTflight/NNserve/VBdinner/NNS\n3. I/PRPhave/VBa/DTfriend/NNliving/VBGin/INDenver/NNP\n4. Can/VBPyou/PRPlist/VBthe/DTnonstop/JJafternoon/NNflights/NNS\n17.2 Use the Penn Treebank tagset to tag each word in the following sentences\nfrom Damon Runyon’s short stories. You may ignore punctuation. Some of\nthesearequitedifficult;doyourbest.\n1. Itisanicenight.\n2. ThiscrapgameisoveragarageinFifty-secondStreet...\n3. ...Nobodyevertakesthenewspapersshesells...\n4. He is a tall, skinny guy with a long, sad, mean-looking kisser, and a\nmournfulvoice."
    },
    "2": {
        "chapter": "18",
        "sections": "18.1, 18.2",
        "topic": "Constituency and Context-Free Grammars",
        "original_category": "L",
        "original_text": "2 CHAPTER18 • CONTEXT-FREEGRAMMARSANDCONSTITUENCYPARSING\n18.1 Constituency\nSyntactic constituency is the idea that groups of words can behave as single units,\norconstituents. Partofdevelopingagrammarinvolvesbuildinganinventoryofthe\nconstituents in the language. How do words group together in English? Consider\nnounphrase thenounphrase,asequenceofwordssurroundingatleastonenoun.Herearesome\nexamplesofnounphrases(thankstoDamonRunyon):\nHarrytheHorse ahigh-classspotsuchasMindy’s\ntheBroadwaycoppers thereasonhecomesintotheHotBox\nthey threepartiesfromBrooklyn\nWhatevidencedowehavethatthesewordsgrouptogether(or“formconstituents”)?\nOnepieceofevidenceisthattheycanallappearinsimilarsyntacticenvironments,\nforexample,beforeaverb.\nthreepartiesfromBrooklynarrive...\nahigh-classspotsuchasMindy’sattracts...\ntheBroadwaycopperslove...\ntheysit\nButwhilethewholenounphrasecanoccurbeforeaverb,thisisnottrueofeach\noftheindividualwordsthatmakeupanounphrase.Thefollowingarenotgrammat-\nicalsentencesofEnglish(recallthatweuseanasterisk(*)tomarkfragmentsthat\narenotgrammaticalEnglishsentences):\n*fromarrive... *asattracts...\n*theis... *spotsat...\nThus, to correctly describe facts about the ordering of these words in English, we\nmustbeabletosaythingslike“NounPhrasescanoccurbeforeverbs”. Let’snow\nseehowtodothisinamoreformalway!\n18.2 Context-Free Grammars\nA widely used formal system for modeling constituent structure in natural lan-\nCFG guageisthecontext-freegrammar,orCFG.Context-freegrammarsarealsocalled\nphrase-structuregrammars,andtheformalismisequivalenttoBackus-Naurform,\norBNF.Theideaofbasingagrammaronconstituentstructuredatesbacktothepsy-\nchologistWilhelmWundt(1900)butwasnotformalizeduntilChomsky(1956)and,\nindependently,Backus(1959).\nrules Acontext-freegrammarconsistsofasetofrulesorproductions,eachofwhich\nexpresses the ways that symbols of the language can be grouped and ordered to-\nlexicon gether,andalexiconofwordsandsymbols.Forexample,thefollowingproductions\nNP express that an NP (or noun phrase) can be composed of either a ProperNoun or\nadeterminer(Det)followedbyaNominal; aNominalinturncanconsistofoneor 2 CHAPTER18 • CONTEXT-FREEGRAMMARSANDCONSTITUENCYPARSING\n18.1 Constituency\nSyntactic constituency is the idea that groups of words can behave as single units,\norconstituents. Partofdevelopingagrammarinvolvesbuildinganinventoryofthe\nconstituents in the language. How do words group together in English? Consider\nnounphrase thenounphrase,asequenceofwordssurroundingatleastonenoun.Herearesome\nexamplesofnounphrases(thankstoDamonRunyon):\nHarrytheHorse ahigh-classspotsuchasMindy’s\ntheBroadwaycoppers thereasonhecomesintotheHotBox\nthey threepartiesfromBrooklyn\nWhatevidencedowehavethatthesewordsgrouptogether(or“formconstituents”)?\nOnepieceofevidenceisthattheycanallappearinsimilarsyntacticenvironments,\nforexample,beforeaverb.\nthreepartiesfromBrooklynarrive...\nahigh-classspotsuchasMindy’sattracts...\ntheBroadwaycopperslove...\ntheysit\nButwhilethewholenounphrasecanoccurbeforeaverb,thisisnottrueofeach\noftheindividualwordsthatmakeupanounphrase.Thefollowingarenotgrammat-\nicalsentencesofEnglish(recallthatweuseanasterisk(*)tomarkfragmentsthat\narenotgrammaticalEnglishsentences):\n*fromarrive... *asattracts...\n*theis... *spotsat...\nThus, to correctly describe facts about the ordering of these words in English, we\nmustbeabletosaythingslike“NounPhrasescanoccurbeforeverbs”. Let’snow\nseehowtodothisinamoreformalway!\n18.2 Context-Free Grammars\nA widely used formal system for modeling constituent structure in natural lan-\nCFG guageisthecontext-freegrammar,orCFG.Context-freegrammarsarealsocalled\nphrase-structuregrammars,andtheformalismisequivalenttoBackus-Naurform,\norBNF.Theideaofbasingagrammaronconstituentstructuredatesbacktothepsy-\nchologistWilhelmWundt(1900)butwasnotformalizeduntilChomsky(1956)and,\nindependently,Backus(1959).\nrules Acontext-freegrammarconsistsofasetofrulesorproductions,eachofwhich\nexpresses the ways that symbols of the language can be grouped and ordered to-\nlexicon gether,andalexiconofwordsandsymbols.Forexample,thefollowingproductions\nNP express that an NP (or noun phrase) can be composed of either a ProperNoun or\nadeterminer(Det)followedbyaNominal; aNominalinturncanconsistofoneor 18.2 • CONTEXT-FREEGRAMMARS 3\nmoreNouns.1\nNP → DetNominal\nNP → ProperNoun\nNominal → Noun | NominalNoun\nContext-freerulescanbehierarchicallyembedded,sowecancombinetheprevious\nruleswithothers,likethefollowing,thatexpressfactsaboutthelexicon:\nDet → a\nDet → the\nNoun → flight\nThe symbols that are used in a CFG are divided into two classes. The symbols\nterminal that correspond to words in the language (“the”, “nightclub”) are called terminal\nsymbols; thelexiconisthesetofrulesthatintroducetheseterminalsymbols. The\nnon-terminal symbolsthatexpressabstractionsovertheseterminalsarecallednon-terminals. In\neachcontext-freerule,theitemtotherightofthearrow(→)isanorderedlistofone\normoreterminalsandnon-terminals;totheleftofthearrowisasinglenon-terminal\nsymbolexpressingsomeclusterorgeneralization.Thenon-terminalassociatedwith\neachwordinthelexiconisitslexicalcategory,orpartofspeech.\nA CFG can be thought of in two ways: as a device for generating sentences\nand as a device for assigning a structure to a given sentence. Viewing a CFG as a\ngenerator,wecanreadthe→arrowas“rewritethesymbolontheleftwiththestring\nofsymbolsontheright”.\nSostartingfromthesymbol: NP\nwecanuseourfirstruletorewriteNPas: DetNominal\nandthenrewriteNominalas: Noun\nandfinallyrewritetheseparts-of-speechas: aflight\nWesaythestringaflightcanbederivedfromthenon-terminalNP.Thus,aCFG\ncanbeusedtogenerateasetofstrings. Thissequenceofruleexpansionsiscalleda\nderivation derivationofthestringofwords. Itiscommontorepresentaderivationbyaparse\nparsetree tree (commonlyshowninvertedwiththerootatthetop).Figure18.1showsthetree\nrepresentationofthisderivation.\nNP\nDet Nom\na Noun\nflight\nFigure18.1 Aparsetreefor“aflight”.\ndominates In the parse tree shown in Fig. 18.1, we can say that the node NP dominates\nall the nodes in the tree (Det, Nom, Noun, a, flight). We can say further that it\nimmediatelydominatesthenodesDetandNom.\nThe formal language defined by a CFG is the set of strings that are derivable\nstartsymbol from the designated start symbol. Each grammar must have one designated start\n1 Whentalkingabouttheseruleswecanpronouncetherightarrow→as“goesto”,andsowemight\nreadthefirstruleaboveas“NPgoestoDetNominal”. 18.2 • CONTEXT-FREEGRAMMARS 3\nmoreNouns.1\nNP → DetNominal\nNP → ProperNoun\nNominal → Noun | NominalNoun\nContext-freerulescanbehierarchicallyembedded,sowecancombinetheprevious\nruleswithothers,likethefollowing,thatexpressfactsaboutthelexicon:\nDet → a\nDet → the\nNoun → flight\nThe symbols that are used in a CFG are divided into two classes. The symbols\nterminal that correspond to words in the language (“the”, “nightclub”) are called terminal\nsymbols; thelexiconisthesetofrulesthatintroducetheseterminalsymbols. The\nnon-terminal symbolsthatexpressabstractionsovertheseterminalsarecallednon-terminals. In\neachcontext-freerule,theitemtotherightofthearrow(→)isanorderedlistofone\normoreterminalsandnon-terminals;totheleftofthearrowisasinglenon-terminal\nsymbolexpressingsomeclusterorgeneralization.Thenon-terminalassociatedwith\neachwordinthelexiconisitslexicalcategory,orpartofspeech.\nA CFG can be thought of in two ways: as a device for generating sentences\nand as a device for assigning a structure to a given sentence. Viewing a CFG as a\ngenerator,wecanreadthe→arrowas“rewritethesymbolontheleftwiththestring\nofsymbolsontheright”.\nSostartingfromthesymbol: NP\nwecanuseourfirstruletorewriteNPas: DetNominal\nandthenrewriteNominalas: Noun\nandfinallyrewritetheseparts-of-speechas: aflight\nWesaythestringaflightcanbederivedfromthenon-terminalNP.Thus,aCFG\ncanbeusedtogenerateasetofstrings. Thissequenceofruleexpansionsiscalleda\nderivation derivationofthestringofwords. Itiscommontorepresentaderivationbyaparse\nparsetree tree (commonlyshowninvertedwiththerootatthetop).Figure18.1showsthetree\nrepresentationofthisderivation.\nNP\nDet Nom\na Noun\nflight\nFigure18.1 Aparsetreefor“aflight”.\ndominates In the parse tree shown in Fig. 18.1, we can say that the node NP dominates\nall the nodes in the tree (Det, Nom, Noun, a, flight). We can say further that it\nimmediatelydominatesthenodesDetandNom.\nThe formal language defined by a CFG is the set of strings that are derivable\nstartsymbol from the designated start symbol. Each grammar must have one designated start\n1 Whentalkingabouttheseruleswecanpronouncetherightarrow→as“goesto”,andsowemight\nreadthefirstruleaboveas“NPgoestoDetNominal”. 18.2 • CONTEXT-FREEGRAMMARS 5\nGrammarRules Examples\nS → NPVP I+wantamorningflight\nNP → Pronoun I\n| Proper-Noun LosAngeles\n| DetNominal a+flight\nNominal → NominalNoun morning+flight\n| Noun flights\nVP → Verb do\n| VerbNP want+aflight\n| VerbNPPP leave+Boston+inthemorning\n| VerbPP leaving+onThursday\nPP → PrepositionNP from+LosAngeles\nFigure18.3 ThegrammarforL ,withexamplephrasesforeachrule.\n0\nS\nNP VP\nPro Verb NP\nI prefer Det Nom\na Nom Noun\nNoun flight\nmorning\nFigure18.4 Theparsetreefor“Ipreferamorningflight”accordingtogrammarL .\n0\nI),andarandomexpansionofVP(let’ssay,toVerbNP),andsoonuntilwegenerate\nthestringIpreferamorningflight. Figure18.4showsaparsetreethatrepresentsa\ncompletederivationofIpreferamorningflight.\nWecanalsorepresentaparse treeinamorecompactformatcalledbracketed\nbracketed notation;hereisthebracketedrepresentationoftheparsetreeofFig.18.4:\nnotation\n(18.1) [S[NP[ProI]][VP[Vprefer][NP[Deta][Nom[Nmorning][Nom[Nflight]]]]]]\nACFGlikethatofL definesaformallanguage. Sentences(stringsofwords)\n0\nthatcanbederivedbyagrammarareintheformallanguagedefinedbythatgram-\ngrammatical mar, and are called grammatical sentences. Sentences that cannot be derived by\na given formal grammar are not in the language defined by that grammar and are\nungrammatical referredtoasungrammatical. Thishardlinebetween“in”and“out”characterizes\nall formal languages but is only a very simplified model of how natural languages\nreallywork. Thisisbecausedeterminingwhetheragivensentenceispartofagiven\nnaturallanguage(say,English)oftendependsonthecontext. Inlinguistics,theuse\ngenerative offormallanguagestomodelnaturallanguagesiscalledgenerativegrammarsince\ngrammar\nthelanguageisdefinedbythesetofpossiblesentences“generated”bythegrammar.\n(Note that this is a different sense of the word ‘generate’ than when we talk about 18.2 • CONTEXT-FREEGRAMMARS 5\nGrammarRules Examples\nS → NPVP I+wantamorningflight\nNP → Pronoun I\n| Proper-Noun LosAngeles\n| DetNominal a+flight\nNominal → NominalNoun morning+flight\n| Noun flights\nVP → Verb do\n| VerbNP want+aflight\n| VerbNPPP leave+Boston+inthemorning\n| VerbPP leaving+onThursday\nPP → PrepositionNP from+LosAngeles\nFigure18.3 ThegrammarforL ,withexamplephrasesforeachrule.\n0\nS\nNP VP\nPro Verb NP\nI prefer Det Nom\na Nom Noun\nNoun flight\nmorning\nFigure18.4 Theparsetreefor“Ipreferamorningflight”accordingtogrammarL .\n0\nI),andarandomexpansionofVP(let’ssay,toVerbNP),andsoonuntilwegenerate\nthestringIpreferamorningflight. Figure18.4showsaparsetreethatrepresentsa\ncompletederivationofIpreferamorningflight.\nWecanalsorepresentaparse treeinamorecompactformatcalledbracketed\nbracketed notation;hereisthebracketedrepresentationoftheparsetreeofFig.18.4:\nnotation\n(18.1) [S[NP[ProI]][VP[Vprefer][NP[Deta][Nom[Nmorning][Nom[Nflight]]]]]]\nACFGlikethatofL definesaformallanguage. Sentences(stringsofwords)\n0\nthatcanbederivedbyagrammarareintheformallanguagedefinedbythatgram-\ngrammatical mar, and are called grammatical sentences. Sentences that cannot be derived by\na given formal grammar are not in the language defined by that grammar and are\nungrammatical referredtoasungrammatical. Thishardlinebetween“in”and“out”characterizes\nall formal languages but is only a very simplified model of how natural languages\nreallywork. Thisisbecausedeterminingwhetheragivensentenceispartofagiven\nnaturallanguage(say,English)oftendependsonthecontext. Inlinguistics,theuse\ngenerative offormallanguagestomodelnaturallanguagesiscalledgenerativegrammarsince\ngrammar\nthelanguageisdefinedbythesetofpossiblesentences“generated”bythegrammar.\n(Note that this is a different sense of the word ‘generate’ than when we talk about 6 CHAPTER18 • CONTEXT-FREEGRAMMARSANDCONSTITUENCYPARSING\nlanguagemodelsgeneratingtext.)\n18.2.1 FormalDefinitionofContext-FreeGrammar\nWe conclude this section with a quick, formal description of a context-free gram-\nmar and the language it generates. A context-free grammar G is defined by four\nparameters: N,Σ,R,S(technicallyitisa“4-tuple”).\nN asetofnon-terminalsymbols(orvariables)\nΣ asetofterminalsymbols(disjointfromN)\nR asetofrulesorproductions,eachoftheformA→β ,\nwhereAisanon-terminal,\nβ isastringofsymbolsfromtheinfinitesetofstrings(Σ∪N)∗\nS adesignatedstartsymbolandamemberofN\nFor the remainder of the book we adhere to the following conventions when dis-\ncussing the formal properties of context-free grammars (as opposed to explaining\nparticularfactsaboutEnglishorotherlanguages).\nCapitalletterslikeA,B,andS Non-terminals\nS Thestartsymbol\nLower-caseGreekletterslikeα,β,andγ Stringsdrawnfrom(Σ∪N)∗\nLower-caseRomanletterslikeu,v,andw Stringsofterminals\nAlanguageisdefinedthroughtheconceptofderivation. Onestringderivesan-\notheroneifitcanberewrittenasthesecondonebysomeseriesofruleapplications.\nMoreformally,followingHopcroftandUllman(1979),\nifA→β isaproductionofRandα andγ areanystringsintheset\ndirectlyderives (Σ∪N)∗,thenwesaythatαAγ directlyderivesαβγ,orαAγ ⇒αβγ.\nDerivationisthenageneralizationofdirectderivation:\nLetα ,α ,...,α bestringsin(Σ∪N)∗,m≥1,suchthat\n1 2 m\nα ⇒α ,α ⇒α ,...,α ⇒α\n1 2 2 3 m−1 m\n∗\nderives Wesaythatα 1derivesα m,orα 1⇒α m.\nWecanthenformallydefinethelanguageL generatedbyagrammarGasthe\nG\nsetofstringscomposedofterminalsymbolsthatcanbederivedfromthedesignated\nstartsymbolS.\nL ={w|wisinΣ∗andS⇒∗ w}\nG\nThe problem of mapping from a string of words to its parse tree is called syn-\nsyntactic tacticparsing,aswe’llseeinSection18.6.\nparsing\n18.3 Treebanks\ntreebank Acorpusinwhicheverysentenceisannotatedwithaparsetreeiscalledatreebank. 18.3 • TREEBANKS 7\nTreebanksplayanimportantroleinparsingaswellasinlinguisticinvestigationsof\nsyntacticphenomena.\nTreebanksaregenerallymadebyrunningaparserovereachsentenceandthen\nhaving the resulting parse hand-corrected by human linguists. Figure 18.5 shows\nPennTreebank sentences from the Penn Treebank project, which includes various treebanks in\nEnglish,Arabic,andChinese.ThePennTreebankpart-of-speechtagsetwasdefined\ninChapter17,butwe’llseeminorformattingdifferencesacrosstreebanks. Theuse\nofLISP-styleparenthesizednotationfortreesisextremelycommonandresembles\nthebracketednotationwesawearlierin(18.1). Forthosewhoarenotfamiliarwith\nitweshowastandardnode-and-linetreerepresentationinFig.18.6.\n((S\n(NP-SBJ (DT That) ((S\n(JJ cold) (, ,) (NP-SBJ The/DT flight/NN )\n(JJ empty) (NN sky) ) (VP should/MD\n(VP (VBD was) (VP arrive/VB\n(ADJP-PRD (JJ full) (PP-TMP at/IN\n(PP (IN of) (NP eleven/CD a.m/RB ))\n(NP (NN fire) (NP-TMP tomorrow/NN )))))\n(CC and)\n(NN light) ))))\n(. .) ))\n(a) (b)\nFigure18.5 ParsesfromtheLDCTreebank3for(a)Brownand(b)ATISsentences.\nS\nNP-SBJ VP .\nDT JJ , JJ NN VBD ADJP-PRD .\nThat cold , empty sky was JJ PP\nfull IN NP\nof NN CC NN\nfire and light\nFigure18.6 ThetreecorrespondingtotheBrowncorpussentenceinthepreviousfigure.\nThesentencesinatreebankimplicitlyconstituteagrammarofthelanguage.For\nexample,fromtheparsedsentencesinFig.18.5wecanextracttheCFGrulesshown\nin Fig. 18.7 (with rule suffixes (-SBJ) stripped for simplicity). The grammar used\ntoparsethePennTreebankisveryflat,resultinginverymanyrules. Forexample, 10 CHAPTER18 • CONTEXT-FREEGRAMMARSANDCONSTITUENCYPARSING\nS S\nNP VP NP VP\nPronoun Verb NP Pronoun VP PP\nI shot Det Nominal I Verb NP inmypajamas\nan Nominal PP shot Det Nominal\nNoun inmypajamas an Noun\nelephant elephant\nFigure18.9 Twoparsetreesforanambiguoussentence. Theparseontheleftcorrespondstothehumorous\nreading in which the elephant is in the pajamas, the parse on the right corresponds to the reading in which\nCaptainSpauldingdidtheshootinginhispajamas.\nCrackers is ambiguous because the phrase in my pajamas can be part of the NP\nheaded by elephant or a part of the verb phrase headed by shot. Figure 18.9 illus-\ntratesthesetwoanalysesofMarx’slineusingrulesfromL .\n1\nStructuralambiguity,appropriatelyenough,comesinmanyforms.Twocommon\nkinds of ambiguity are attachment ambiguity and coordination ambiguity. A\nattachment sentencehasanattachmentambiguityifaparticularconstituentcanbeattachedto\nambiguity\nthe parse tree at more than one place. The Groucho Marx sentence is an example\nPP-attachment ofPP-attachmentambiguity: theprepositionphrasecanbeattachedeitheraspart\nambiguity\nof the NP or as part of the VP. Various kinds of adverbial phrases are also subject\ntothiskindofambiguity. Forinstance,inthefollowingexamplethegerundive-VP\nflyingtoPariscanbepartofagerundivesentencewhosesubjectistheEiffelTower\noritcanbeanadjunctmodifyingtheVPheadedbysaw:\n(18.2) WesawtheEiffelTowerflyingtoParis.\ncoordination Incoordinationambiguityphrasescanbeconjoinedbyaconjunctionlikeand.\nambiguity\nFor example, the phrase old men and women can be bracketed as [old [men and\nwomen]], referring to old men and old women, or as [old men] and [women], in\nwhichcaseitisonlythemenwhoareold. Theseambiguitiescombineincomplex\nwaysinrealsentences,likethefollowingnewssentencefromtheBrowncorpus:\n(18.3) PresidentKennedytodaypushedasideotherWhiteHousebusinessto\ndevoteallhistimeandattentiontoworkingontheBerlincrisisaddresshe\nwilldelivertomorrownighttotheAmericanpeopleovernationwide\ntelevisionandradio.\nThissentencehasanumberofambiguities,althoughsincetheyaresemantically\nunreasonable,itrequiresacarefulreadingtoseethem.Thelastnounphrasecouldbe\nparsed [nationwide [television and radio]] or [[nationwide television] and radio].\nThedirectobjectofpushedasideshouldbeotherWhiteHousebusinessbutcould\nalso be the bizarre phrase [other White House business to devote all his time and\nattentiontoworking](i.e.,astructurelikeKennedyaffirmed[hisintentiontopropose\nanewbudgettoaddressthedeficit]).ThenthephraseontheBerlincrisisaddresshe 24 CHAPTER18 • CONTEXT-FREEGRAMMARSANDCONSTITUENCYPARSING\nExercises\n18.1 Implementthealgorithmtoconvertarbitrarycontext-freegrammarstoCNF.\nApplyyourprogramtotheL grammar.\n1\n18.2 ImplementtheCKYalgorithmandtestitwithyourconvertedL grammar.\n1\n18.3 RewritetheCKYalgorithmgiveninFig.18.12onpage14sothatitcanaccept\ngrammarsthatcontainunitproductions.\n18.4 Discusshowtoaugmentaparsertodealwithinputthatmaybeincorrect,for\nexample,containingspellingerrorsormistakesarisingfromautomaticspeech\nrecognition.\n18.5 Implement the PARSEVAL metrics described in Section 18.8. Next, use a\nparserandatreebank, compareyourmetricsagainstastandardimplementa-\ntion. Analyzetheerrorsinyourapproach. 24 CHAPTER18 • CONTEXT-FREEGRAMMARSANDCONSTITUENCYPARSING\nExercises\n18.1 Implementthealgorithmtoconvertarbitrarycontext-freegrammarstoCNF.\nApplyyourprogramtotheL grammar.\n1\n18.2 ImplementtheCKYalgorithmandtestitwithyourconvertedL grammar.\n1\n18.3 RewritetheCKYalgorithmgiveninFig.18.12onpage14sothatitcanaccept\ngrammarsthatcontainunitproductions.\n18.4 Discusshowtoaugmentaparsertodealwithinputthatmaybeincorrect,for\nexample,containingspellingerrorsormistakesarisingfromautomaticspeech\nrecognition.\n18.5 Implement the PARSEVAL metrics described in Section 18.8. Next, use a\nparserandatreebank, compareyourmetricsagainstastandardimplementa-\ntion. Analyzetheerrorsinyourapproach."
    },
    "3": {
        "chapter": "21",
        "sections": "21.1, 21.2",
        "topic": "Semantic Roles and Diathesis Alternations",
        "original_category": "L",
        "original_text": "2 CHAPTER21 • SEMANTICROLELABELING\n21.1 Semantic Roles\nConsider the meanings of the arguments Sasha, Pat, the window, and the door in\nthesetwosentences.\n(21.1) Sashabrokethewindow.\n(21.2) Patopenedthedoor.\nThe subjects Sasha and Pat, what we might call the breaker of the window-\nbreaking event and the opener of the door-opening event, have something in com-\nmon. They are both volitional actors, often animate, and they have direct causal\nresponsibilityfortheirevents.\nthematicroles Thematicrolesareawaytocapturethissemanticcommonalitybetweenbreak-\nagents ers and openers. We say that the subjects of both these verbs are agents. Thus,\nAGENTisthethematicrolethatrepresentsanabstractideasuchasvolitionalcausa-\ntion.Similarly,thedirectobjectsofboththeseverbs,theBrokenThingandOpenedThing,\narebothprototypicallyinanimateobjectsthatareaffectedinsomewaybytheaction.\ntheme Thesemanticrolefortheseparticipantsistheme.\nThematicRole Definition\nAGENT Thevolitionalcauserofanevent\nEXPERIENCER Theexperiencerofanevent\nFORCE Thenon-volitionalcauseroftheevent\nTHEME Theparticipantmostdirectlyaffectedbyanevent\nRESULT Theendproductofanevent\nCONTENT Thepropositionorcontentofapropositionalevent\nINSTRUMENT Aninstrumentusedinanevent\nBENEFICIARY Thebeneficiaryofanevent\nSOURCE Theoriginoftheobjectofatransferevent\nGOAL Thedestinationofanobjectofatransferevent\nFigure21.1 Somecommonlyusedthematicroleswiththeirdefinitions.\nAlthoughthematicrolesareoneoftheoldestlinguisticmodels,aswesawabove,\ntheir modern formulation is due to Fillmore (1968) and Gruber (1965). Although\nthere is no universally agreed-upon set of roles, Figs. 21.1 and 21.2 list some the-\nmaticrolesthathavebeenusedinvariouscomputationalpapers,togetherwithrough\ndefinitionsandexamples.Mostthematicrolesetshaveaboutadozenroles,butwe’ll\nseesetswithsmallernumbersofroleswithevenmoreabstractmeanings, andsets\nwithverylargenumbersofrolesthatarespecifictosituations. We’llusethegeneral\nsemanticroles termsemanticrolesforallsetsofroles,whethersmallorlarge.\n21.2 Diathesis Alternations\nThe main reason computational systems use semantic roles is to act as a shallow\nmeaning representation that can let us make simple inferences that aren’t possible\nfrom the pure surface string of words, or even from the parse tree. To extend the\nearlier examples, if a document says that Company A acquired Company B, we’d\nliketoknowthatthisanswersthequeryWasCompanyBacquired? despitethefact\nthat the two sentences have very different surface syntax. Similarly, this shallow\nsemanticsmightactasausefulintermediatelanguageinmachinetranslation. 2 CHAPTER21 • SEMANTICROLELABELING\n21.1 Semantic Roles\nConsider the meanings of the arguments Sasha, Pat, the window, and the door in\nthesetwosentences.\n(21.1) Sashabrokethewindow.\n(21.2) Patopenedthedoor.\nThe subjects Sasha and Pat, what we might call the breaker of the window-\nbreaking event and the opener of the door-opening event, have something in com-\nmon. They are both volitional actors, often animate, and they have direct causal\nresponsibilityfortheirevents.\nthematicroles Thematicrolesareawaytocapturethissemanticcommonalitybetweenbreak-\nagents ers and openers. We say that the subjects of both these verbs are agents. Thus,\nAGENTisthethematicrolethatrepresentsanabstractideasuchasvolitionalcausa-\ntion.Similarly,thedirectobjectsofboththeseverbs,theBrokenThingandOpenedThing,\narebothprototypicallyinanimateobjectsthatareaffectedinsomewaybytheaction.\ntheme Thesemanticrolefortheseparticipantsistheme.\nThematicRole Definition\nAGENT Thevolitionalcauserofanevent\nEXPERIENCER Theexperiencerofanevent\nFORCE Thenon-volitionalcauseroftheevent\nTHEME Theparticipantmostdirectlyaffectedbyanevent\nRESULT Theendproductofanevent\nCONTENT Thepropositionorcontentofapropositionalevent\nINSTRUMENT Aninstrumentusedinanevent\nBENEFICIARY Thebeneficiaryofanevent\nSOURCE Theoriginoftheobjectofatransferevent\nGOAL Thedestinationofanobjectofatransferevent\nFigure21.1 Somecommonlyusedthematicroleswiththeirdefinitions.\nAlthoughthematicrolesareoneoftheoldestlinguisticmodels,aswesawabove,\ntheir modern formulation is due to Fillmore (1968) and Gruber (1965). Although\nthere is no universally agreed-upon set of roles, Figs. 21.1 and 21.2 list some the-\nmaticrolesthathavebeenusedinvariouscomputationalpapers,togetherwithrough\ndefinitionsandexamples.Mostthematicrolesetshaveaboutadozenroles,butwe’ll\nseesetswithsmallernumbersofroleswithevenmoreabstractmeanings, andsets\nwithverylargenumbersofrolesthatarespecifictosituations. We’llusethegeneral\nsemanticroles termsemanticrolesforallsetsofroles,whethersmallorlarge.\n21.2 Diathesis Alternations\nThe main reason computational systems use semantic roles is to act as a shallow\nmeaning representation that can let us make simple inferences that aren’t possible\nfrom the pure surface string of words, or even from the parse tree. To extend the\nearlier examples, if a document says that Company A acquired Company B, we’d\nliketoknowthatthisanswersthequeryWasCompanyBacquired? despitethefact\nthat the two sentences have very different surface syntax. Similarly, this shallow\nsemanticsmightactasausefulintermediatelanguageinmachinetranslation. 21.2 • DIATHESISALTERNATIONS 3\nThematicRole Example\nAGENT Thewaiterspilledthesoup.\nEXPERIENCER Johnhasaheadache.\nFORCE Thewindblowsdebrisfromthemallintoouryards.\nTHEME OnlyafterBenjaminFranklinbroketheice...\nRESULT Thecitybuiltaregulation-sizebaseballdiamond...\nCONTENT Monaasked“YoumetMaryAnnatasupermarket?”\nINSTRUMENT Hepoachedcatfish,stunningthemwithashockingdevice...\nBENEFICIARY WheneverAnnCallahanmakeshotelreservationsforherboss...\nSOURCE IflewinfromBoston.\nGOAL IdrovetoPortland.\nFigure21.2 Someprototypicalexamplesofvariousthematicroles.\nSemantic roles thus help generalize over different surface realizations of pred-\nicate arguments. For example, while the AGENT is often realized as the subject of\nthesentence,inothercasesthe THEME canbethesubject. Considerthesepossible\nrealizationsofthethematicargumentsoftheverbbreak:\n(21.3) John brokethewindow.\nAGENT THEME\n(21.4) John brokethewindowwitharock.\nAGENT THEME INSTRUMENT\n(21.5) Therock brokethewindow.\nINSTRUMENT THEME\n(21.6) Thewindowbroke.\nTHEME\n(21.7) ThewindowwasbrokenbyJohn.\nTHEME AGENT\nTheseexamplessuggestthatbreakhas(atleast)thepossibleargumentsAGENT,\nTHEME, and INSTRUMENT. The set of thematic role arguments taken by a verb is\nthematicgrid often called the thematic grid, θ-grid, or case frame. We can see that there are\ncaseframe (amongothers)thefollowingpossibilitiesfortherealizationoftheseargumentsof\nbreak:\nAGENT/Subject, THEME/Object\nAGENT/Subject, THEME/Object, INSTRUMENT/PP\nwith\nINSTRUMENT/Subject, THEME/Object\nTHEME/Subject\nItturnsoutthatmanyverbsallowtheirthematicrolestoberealizedinvarious\nsyntacticpositions. Forexample,verbslikegivecanrealizethe THEME and GOAL\nargumentsintwodifferentways:\n(21.8) a. Doris gavethebooktoCary.\nAGENT THEME GOAL\nb. Doris gaveCary thebook.\nAGENT GOALTHEME\nThesemultipleargumentstructurerealizations(thefactthatbreakcantakeAGENT,\nINSTRUMENT, or THEME as subject, and give can realize its THEME and GOAL in\nverb eitherorder)arecalledverbalternationsordiathesisalternations. Thealternation\nalternation\ndative weshowedaboveforgive,thedativealternation,seemstooccurwithparticularse-\nalternation\nmanticclassesofverbs,including“verbsoffuturehaving”(advance,allocate,offer,"
    },
    "4": {
        "chapter": "G",
        "sections": "G.1, G.2",
        "topic": "Word Senses and Relations Between Senses",
        "original_category": "L",
        "original_text": "2 APPENDIXG • WORDSENSESANDWORDNET\nanalyticdirection.\nG.1 Word Senses\nwordsense Asense(orwordsense)isadiscreterepresentationofoneaspectofthemeaningof\naword. Looselyfollowinglexicographictradition, werepresenteachsensewitha\nsuperscript: bank1 andbank2,mouse1 andmouse2. Incontext,it’seasytoseethe\ndifferentmeanings:\nmouse1: .... amousecontrollingacomputersystemin1968.\nmouse2: .... aquietanimallikeamouse\nbank1: ...abankcanholdtheinvestmentsinacustodialaccount...\nbank2: ...asagricultureburgeonsontheeastbank,theriver...\nG.1.1 DefiningWordSenses\nHowcanwedefinethemeaningofawordsense? WeintroducedinChapter6the\nstandardcomputationalapproachofrepresentingawordasanembedding,apoint\nin semantic space. The intuition of embedding models like word2vec or GloVe is\nthatthemeaningofawordcanbedefinedbyitsco-occurrences,thecountsofwords\nthatoftenoccurnearby. Butthatdoesn’ttellushowtodefinethemeaningofaword\nsense. As we saw in Chapter 11, contextual embeddings like BERT go further by\nofferinganembeddingthatrepresentsthemeaningofawordinitstextualcontext,\nandwe’llseethatcontextualembeddingslieattheheartofmodernalgorithmsfor\nwordsensedisambiguation.\nBut first, we need to consider the alternative ways that dictionaries and the-\nsaurusesofferfordefiningsenses. Oneisbasedonthefactthatdictionariesorthe-\ngloss saurusesgivetextualdefinitionsforeachsensecalledglosses. Herearetheglosses\nfortwosensesofbank:\n1. financial institution that accepts deposits and channels\nthe money into lending activities\n2. sloping land (especially the slope beside a body of water)\nGlossesarenotaformalmeaningrepresentation;theyarejustwrittenforpeople.\nConsiderthefollowingfragmentsfromthedefinitionsofright,left,red,andblood\nfromtheAmericanHeritageDictionary(Morris,1985).\nright adj. locatednearertherighthandesp. beingontherightwhen\nfacingthesamedirectionastheobserver.\nleft adj. locatednearertothissideofthebodythantheright.\nred n. thecolorofbloodoraruby.\nblood n. theredliquidthatcirculatesintheheart,arteriesandveinsof\nanimals.\nNotethecircularityinthesedefinitions. Thedefinitionofrightmakestwodirect\nreferences to itself, and the entry for left contains an implicit self-reference in the\nphrasethissideofthebody,whichpresumablymeanstheleftside. Theentriesfor\nredandbloodreferenceeachotherintheirdefinitions. Forhumans,suchentriesare\nusefulsincetheuserofthedictionaryhassufficientgraspoftheseotherterms. G.1 • WORDSENSES 3\nYet despite their circularity and lack of formal representation, glosses can still\nbeusefulforcomputationalmodelingofsenses.Thisisbecauseaglossisjustasen-\ntence,andfromsentenceswecancomputesentenceembeddingsthattellussome-\nthing about the meaning of the sense. Dictionaries often give example sentences\nalongwithglosses,andthesecanagainbeusedtohelpbuildasenserepresentation.\nThesecondwaythatthesaurusesofferfordefiningasenseis—likethedictionary\ndefinitions—definingasensethroughitsrelationshipwithothersenses. Forexam-\nple,theabovedefinitionsmakeitclearthatrightandleftaresimilarkindsoflemmas\nthatstandinsomekindofalternation, oropposition, tooneanother. Similarly, we\ncangleanthatredisacolorandthatbloodisaliquid. Senserelationsofthissort\n(IS-A,orantonymy)areexplicitlylistedinon-linedatabaseslikeWordNet. Given\na sufficiently large database of such relations, many applications are quite capable\nof performing sophisticated semantic tasks about word senses (even if they do not\nreallyknowtheirrightfromtheirleft).\nG.1.2 Howmanysensesdowordshave?\nDictionariesandthesaurusesgivediscretelistsofsenses. Bycontrast,embeddings\n(whetherstaticorcontextual)offeracontinuoushigh-dimensionalmodelofmeaning\nthatdoesn’tdivideupintodiscretesenses.\nThereforecreatingathesaurusdependsoncriteriafordecidingwhenthediffer-\ning uses of a word should be represented with discrete senses. We might consider\ntwosensesdiscreteiftheyhaveindependenttruthconditions,differentsyntacticbe-\nhavior,andindependentsenserelations,oriftheyexhibitantagonisticmeanings.\nConsiderthefollowingusesoftheverbservefromtheWSJcorpus:\n(G.1) Theyrarelyserveredmeat,preferringtoprepareseafood.\n(G.2) HeservedasU.S.ambassadortoNorwayin1976and1977.\n(G.3) Hemighthaveservedhistime,comeoutandledanupstandinglife.\nThe serve of serving red meat and that of serving time clearly have different truth\nconditions and presuppositions; the serve of serve as ambassador has the distinct\nsubcategorizationstructureserveasNP.Theseheuristicssuggestthattheseareprob-\nably three distinct senses of serve. One practical technique for determining if two\nsenses are distinct is to conjoin two uses of a word in a single sentence; this kind\nzeugma of conjunction of antagonistic readings is called zeugma. Consider the following\nexamples:\n(G.4) Whichofthoseflightsservebreakfast?\n(G.5) DoesAirFranceservePhiladelphia?\n(G.6) ?DoesAirFranceservebreakfastandPhiladelphia?\nWeuse(?) tomarkthoseexamplesthataresemanticallyill-formed. Theoddnessof\ntheinventedthirdexample(acaseofzeugma)indicatesthereisnosensiblewayto\nmakeasinglesenseofserveworkforbothbreakfastandPhiladelphia. Wecanuse\nthisasevidencethatservehastwodifferentsensesinthiscase.\nDictionariestendtousemanyfine-grainedsensessoastocapturesubtlemeaning\ndifferences, a reasonable approach given that the traditional role of dictionaries is\naiding word learners. For computational purposes, we often don’t need these fine\ndistinctions,soweoftengrouporclusterthesenses; wehavealreadydonethisfor\nsome of the examples in this chapter. Indeed, clustering examples into senses, or\nsensesintobroader-grainedcategories,isanimportantcomputationaltaskthatwe’ll\ndiscussinSectionG.7. G.1 • WORDSENSES 3\nYet despite their circularity and lack of formal representation, glosses can still\nbeusefulforcomputationalmodelingofsenses.Thisisbecauseaglossisjustasen-\ntence,andfromsentenceswecancomputesentenceembeddingsthattellussome-\nthing about the meaning of the sense. Dictionaries often give example sentences\nalongwithglosses,andthesecanagainbeusedtohelpbuildasenserepresentation.\nThesecondwaythatthesaurusesofferfordefiningasenseis—likethedictionary\ndefinitions—definingasensethroughitsrelationshipwithothersenses. Forexam-\nple,theabovedefinitionsmakeitclearthatrightandleftaresimilarkindsoflemmas\nthatstandinsomekindofalternation, oropposition, tooneanother. Similarly, we\ncangleanthatredisacolorandthatbloodisaliquid. Senserelationsofthissort\n(IS-A,orantonymy)areexplicitlylistedinon-linedatabaseslikeWordNet. Given\na sufficiently large database of such relations, many applications are quite capable\nof performing sophisticated semantic tasks about word senses (even if they do not\nreallyknowtheirrightfromtheirleft).\nG.1.2 Howmanysensesdowordshave?\nDictionariesandthesaurusesgivediscretelistsofsenses. Bycontrast,embeddings\n(whetherstaticorcontextual)offeracontinuoushigh-dimensionalmodelofmeaning\nthatdoesn’tdivideupintodiscretesenses.\nThereforecreatingathesaurusdependsoncriteriafordecidingwhenthediffer-\ning uses of a word should be represented with discrete senses. We might consider\ntwosensesdiscreteiftheyhaveindependenttruthconditions,differentsyntacticbe-\nhavior,andindependentsenserelations,oriftheyexhibitantagonisticmeanings.\nConsiderthefollowingusesoftheverbservefromtheWSJcorpus:\n(G.1) Theyrarelyserveredmeat,preferringtoprepareseafood.\n(G.2) HeservedasU.S.ambassadortoNorwayin1976and1977.\n(G.3) Hemighthaveservedhistime,comeoutandledanupstandinglife.\nThe serve of serving red meat and that of serving time clearly have different truth\nconditions and presuppositions; the serve of serve as ambassador has the distinct\nsubcategorizationstructureserveasNP.Theseheuristicssuggestthattheseareprob-\nably three distinct senses of serve. One practical technique for determining if two\nsenses are distinct is to conjoin two uses of a word in a single sentence; this kind\nzeugma of conjunction of antagonistic readings is called zeugma. Consider the following\nexamples:\n(G.4) Whichofthoseflightsservebreakfast?\n(G.5) DoesAirFranceservePhiladelphia?\n(G.6) ?DoesAirFranceservebreakfastandPhiladelphia?\nWeuse(?) tomarkthoseexamplesthataresemanticallyill-formed. Theoddnessof\ntheinventedthirdexample(acaseofzeugma)indicatesthereisnosensiblewayto\nmakeasinglesenseofserveworkforbothbreakfastandPhiladelphia. Wecanuse\nthisasevidencethatservehastwodifferentsensesinthiscase.\nDictionariestendtousemanyfine-grainedsensessoastocapturesubtlemeaning\ndifferences, a reasonable approach given that the traditional role of dictionaries is\naiding word learners. For computational purposes, we often don’t need these fine\ndistinctions,soweoftengrouporclusterthesenses; wehavealreadydonethisfor\nsome of the examples in this chapter. Indeed, clustering examples into senses, or\nsensesintobroader-grainedcategories,isanimportantcomputationaltaskthatwe’ll\ndiscussinSectionG.7. 4 APPENDIXG • WORDSENSESANDWORDNET\nG.2 Relations Between Senses\nThissectionexplorestherelationsbetweenwordsenses,especiallythosethathave\nreceivedsignificantcomputationalinvestigationlikesynonymy,antonymy,andhy-\npernymy.\nSynonymy\nWe introduced in Chapter 6 the idea that when two senses of two different words\nsynonym (lemmas) are identical, or nearly identical, we say the two senses are synonyms.\nSynonymsincludesuchpairsas\ncouch/sofa vomit/throwup filbert/hazelnut car/automobile\nAnd we mentioned that in practice, the word synonym is commonly used to\ndescribe a relationship of approximate or rough synonymy. But furthermore, syn-\nonymyisactuallyarelationshipbetweensensesratherthanwords. Consideringthe\nwords big and large. These mayseemto besynonyms inthefollowing sentences,\nsincewecouldswapbigandlargeineithersentenceandretainthesamemeaning:\n(G.7) Howbigisthatplane?\n(G.8) WouldIbeflyingonalargeorsmallplane?\nButnotethefollowingsentenceinwhichwecannotsubstitutelargeforbig:\n(G.9) MissNelson,forinstance,becameakindofbigsistertoBenjamin.\n(G.10) ?MissNelson,forinstance,becameakindoflargesistertoBenjamin.\nThisisbecausethewordbighasasensethatmeansbeingolderorgrownup,while\nlargelacksthissense. Thus, wesaythatsomesensesofbigandlargeare(nearly)\nsynonymouswhileotheronesarenot.\nAntonymy\nantonym Whereas synonyms are words with identical or similar meanings, antonyms are\nwordswithanoppositemeaning,like:\nlong/short big/little fast/slow cold/hot dark/light\nrise/fall up/down in/out\nTwosensescanbeantonymsiftheydefineabinaryoppositionorareatopposite\nendsofsomescale. Thisisthecaseforlong/short,fast/slow,orbig/little,whichare\nreversives atoppositeendsofthelengthorsizescale. Anothergroupofantonyms,reversives,\ndescribechangeormovementinoppositedirections,suchasrise/fallorup/down.\nAntonymsthusdiffercompletelywithrespecttooneaspectoftheirmeaning—\ntheir position on a scale or their direction—but are otherwise very similar, sharing\nalmostallotheraspectsof meaning. Thus, automaticallydistinguishingsynonyms\nfromantonymscanbedifficult.\nTaxonomicRelations\nAnother way word senses can be related is taxonomically. A word (or sense) is a\nhyponym hyponymofanotherwordorsenseifthefirstismorespecific,denotingasubclass\noftheother. Forexample,carisahyponymofvehicle,dogisahyponymofanimal,\nhypernym andmangoisahyponymoffruit. Conversely,wesaythatvehicleisahypernymof\ncar,andanimalisahypernymofdog.Itisunfortunatethatthetwowords(hypernym G.2 • RELATIONSBETWEENSENSES 5\nandhyponym)areverysimilarandhenceeasilyconfused;forthisreason,theword\nsuperordinate superordinateisoftenusedinsteadofhypernym.\nSuperordinate vehicle fruit furniture mammal\nSubordinate car mango chair dog\nWe can define hypernymy more formally by saying that the class denoted by\nthe superordinate extensionally includes the class denoted by the hyponym. Thus,\ntheclassofanimalsincludesasmembersalldogs,andtheclassofmovingactions\nincludes all walking actions. Hypernymy can also be defined in terms of entail-\nment. Under this definition, a sense A is a hyponym of a sense B if everything\nthatisAisalsoB,andhencebeinganAentailsbeingaB,or∀x A(x)⇒B(x). Hy-\nponymy/hypernymyisusuallyatransitiverelation;ifAisahyponymofBandBisa\nhyponymofC,thenAisahyponymofC.Anothernameforthehypernym/hyponym\nIS-A structureistheIS-Ahierarchy,inwhichwesayAIS-AB,orBsubsumesA.\nHypernymy is useful for tasks like textual entailment or question answering;\nknowingthatleukemiaisatypeofcancer,forexample,wouldcertainlybeusefulin\nansweringquestionsaboutleukemia.\nMeronymy\npart-whole Anothercommonrelationismeronymy,thepart-wholerelation. Alegispartofa\nchair;awheelispartofacar. Wesaythatwheelisameronymofcar,andcarisa\nholonymofwheel.\nStructuredPolysemy\nThe senses of a word can also be related semantically, in which case we call the\nstructured relationshipbetweenthemstructuredpolysemy. Considerthissensebank:\npolysemy\n(G.11) ThebankisonthecornerofNassauandWitherspoon.\nThis sense, perhaps bank4, means something like “the building belonging to\na financial institution”. These two kinds of senses (an organization and the build-\ning associated with an organization ) occur together for many other words as well\n(school,university,hospital,etc.). Thus,thereisasystematicrelationshipbetween\nsensesthatwemightrepresentas\nBUILDING↔ORGANIZATION\nmetonymy Thisparticularsubtypeofpolysemyrelationiscalledmetonymy. Metonymyis\ntheuseofoneaspectofaconceptorentitytorefertootheraspectsoftheentityor\ntotheentityitself. WeareperformingmetonymywhenweusethephrasetheWhite\nHouse to refer to the administration whose office is in the White House. Other\ncommonexamplesofmetonymyincludetherelationbetweenthefollowingpairings\nofsenses:\nAUTHOR ↔ WORKSOFAUTHOR\n(JaneAustenwroteEmma) (IreallyloveJaneAusten)\nFRUITTREE ↔ FRUIT\n(Plumshavebeautifulblossoms) (Iateapreservedplumyesterday) 6 APPENDIXG • WORDSENSESANDWORDNET\nG.3 WordNet: A Database of Lexical Relations\nThe most commonly used resource for sense relations in English and many other\nWordNet languages is the WordNet lexical database (Fellbaum, 1998). English WordNet\nconsists of three separate databases, one each for nouns and verbs and a third for\nadjectivesandadverbs;closedclasswordsarenotincluded. Eachdatabasecontains\nasetoflemmas,eachoneannotatedwithasetofsenses. TheWordNet3.0release\nhas117,798nouns, 11,529verbs, 22,479adjectives, and4,481adverbs. Theaver-\nage noun has 1.23 senses, and the average verb has 2.16 senses. WordNet can be\naccessedontheWebordownloadedlocally. FigureG.1showsthelemmaentryfor\nthenounbass.\nThenoun“bass”has8sensesinWordNet.\n1.bass1-(thelowestpartofthemusicalrange)\n2.bass2,basspart1-(thelowestpartinpolyphonicmusic)\n3.bass3,basso1-(anadultmalesingerwiththelowestvoice)\n4.seabass1,bass4-(theleanfleshofasaltwaterfishofthefamilySerranidae)\n5.freshwaterbass1,bass5-(anyofvariousNorthAmericanfreshwaterfishwith\nleanflesh(especiallyofthegenusMicropterus))\n6.bass6,bassvoice1,basso2-(thelowestadultmalesingingvoice)\n7.bass7-(thememberwiththelowestrangeofafamilyofmusicalinstruments)\n8.bass8-(nontechnicalnameforanyofnumerousediblemarineand\nfreshwaterspiny-finnedfishes)\nFigureG.1 AportionoftheWordNet3.0entryforthenounbass.\ngloss Note that there are eight senses, each of which has a gloss (a dictionary-style\ndefinition), a list of synonyms for the sense, and sometimes also usage examples.\nWordNetdoesn’trepresentpronunciation, sodoesn’tdistinguishthepronunciation\n[baes]inbass4,bass5,andbass8fromtheothersensespronounced[beys].\nsynset Thesetofnear-synonymsforaWordNetsenseiscalledasynset(forsynonym\nset); synsets are an important primitive in WordNet. The entry for bass includes\nsynsets like {bass1, deep6}, or {bass6, bass voice1, basso2}. We can think of a\nsynset as representing a concept of the type we discussed in Appendix F. Thus,\ninsteadofrepresentingconceptsinlogicalterms,WordNetrepresentsthemaslists\nof the word senses that can be used to express the concept. Here’s another synset\nexample:\n{chump1, fool2, gull1, mark9, patsy1, fall guy1,\nsucker1, soft touch1, mug2}\nTheglossofthissynsetdescribesitas:\nGloss: apersonwhoisgullibleandeasytotakeadvantageof.\nGlossesarepropertiesofasynset,sothateachsenseincludedinthesynsethasthe\nsame gloss and can express this concept. Because they share glosses, synsets like\nthis one are the fundamental unit associated with WordNet entries, and hence it is\nsynsets,notwordforms,lemmas,orindividualsenses,thatparticipateinmostofthe\nlexicalsenserelationsinWordNet.\nWordNet also labels each synset with a lexicographic category drawn from a\nsemantic field for example the 26 categories for nouns shown in Fig. G.2, as well\nas15forverbs(plus2foradjectivesand1foradverbs). Thesecategoriesareoften 18 APPENDIXG • WORDSENSESANDWORDNET\nresourcesincludeAmsler’s1981useoftheMerriamWebsterdictionaryandLong-\nman’sDictionaryofContemporaryEnglish(BoguraevandBriscoe,1989).\nSupervised approaches to disambiguation began with the use of decision trees\nbyBlack(1988). InadditiontotheIMSandcontextual-embeddingbasedmethods\nforsupervisedWSD,recentsupervisedalgorithmsincludesencoder-decodermodels\n(Raganatoetal.,2017a).\nThe need for large amounts of annotated text in supervised methods led early\nontoinvestigationsintotheuseofbootstrappingmethods(Hearst1991,Yarowsky\n1995). For example the semi-supervised algorithm of Diab and Resnik (2002) is\nbased on aligned parallel corpora in two languages. For example, the fact that the\nFrench word catastrophe might be translated as English disaster in one instance\nand tragedy in another instance can be used to disambiguate the senses of the two\nEnglishwords(i.e.,tochoosesensesofdisasterandtragedythataresimilar).\nThe earliest use of clustering in the study of word senses was by Sparck Jones\n(1986);PedersenandBruce(1997),Schu¨tze(1997),andSchu¨tze(1998)applieddis-\ncoarsesenses tributionalmethods. Clusteringwordsensesintocoarsesenseshasalsobeenused\nto address the problem of dictionary senses being too fine-grained (Section G.5.3)\n(Dolan 1994, Chen and Chang 1998, Mihalcea and Moldovan 2001, Agirre and\ndeLacalle2003,Palmeretal.2004,Navigli2006,Snowetal.2007,Pilehvaretal.\n2013). Corporawithclusteredwordsensesfortrainingsupervisedclusteringalgo-\nOntoNotes rithmsincludePalmeretal.(2006)andOntoNotes(Hovyetal.,2006).\nSeePustejovsky(1995), PustejovskyandBoguraev(1996),Martin(1986), and\nCopestakeandBriscoe(1995), interalia, forcomputationalapproachestotherep-\ngenerative resentation of polysemy. Pustejovsky’s theory of the generative lexicon, and in\nlexicon\nqualia particularhistheoryofthequaliastructureofwords,isawayofaccountingforthe\nstructure\ndynamicsystematicpolysemyofwordsincontext.\nHistoricaloverviewsofWSDincludeAgirreandEdmonds(2006)andNavigli\n(2009).\nExercises\nG.1 Collect a small corpus of example sentences of varying lengths from any\nnewspaper or magazine. Using WordNet or any standard dictionary, deter-\nminehowmanysensesthereareforeachoftheopen-classwordsineachsen-\ntence. Howmanydistinctcombinationsofsensesarethereforeachsentence?\nHowdoesthisnumberseemtovarywithsentencelength?\nG.2 UsingWordNetorastandardreferencedictionary, tageachopen-classword\ninyourcorpuswithitscorrecttag. Waschoosingthecorrectsensealwaysa\nstraightforwardtask? Reportonanydifficultiesyouencountered.\nG.3 Using your favorite dictionary, simulate the original Lesk word overlap dis-\nambiguationalgorithmdescribedonpage13onthephraseTimeflieslikean\narrow. Assume that the words are to be disambiguated one at a time, from\nleft to right, and that the results from earlier decisions are used later in the\nprocess.\nG.4 Build an implementation of your solution to the previous exercise. Using\nWordNet, implement the original Lesk word overlap disambiguation algo-\nrithmdescribedonpage13onthephraseTimeflieslikeanarrow. 18 APPENDIXG • WORDSENSESANDWORDNET\nresourcesincludeAmsler’s1981useoftheMerriamWebsterdictionaryandLong-\nman’sDictionaryofContemporaryEnglish(BoguraevandBriscoe,1989).\nSupervised approaches to disambiguation began with the use of decision trees\nbyBlack(1988). InadditiontotheIMSandcontextual-embeddingbasedmethods\nforsupervisedWSD,recentsupervisedalgorithmsincludesencoder-decodermodels\n(Raganatoetal.,2017a).\nThe need for large amounts of annotated text in supervised methods led early\nontoinvestigationsintotheuseofbootstrappingmethods(Hearst1991,Yarowsky\n1995). For example the semi-supervised algorithm of Diab and Resnik (2002) is\nbased on aligned parallel corpora in two languages. For example, the fact that the\nFrench word catastrophe might be translated as English disaster in one instance\nand tragedy in another instance can be used to disambiguate the senses of the two\nEnglishwords(i.e.,tochoosesensesofdisasterandtragedythataresimilar).\nThe earliest use of clustering in the study of word senses was by Sparck Jones\n(1986);PedersenandBruce(1997),Schu¨tze(1997),andSchu¨tze(1998)applieddis-\ncoarsesenses tributionalmethods. Clusteringwordsensesintocoarsesenseshasalsobeenused\nto address the problem of dictionary senses being too fine-grained (Section G.5.3)\n(Dolan 1994, Chen and Chang 1998, Mihalcea and Moldovan 2001, Agirre and\ndeLacalle2003,Palmeretal.2004,Navigli2006,Snowetal.2007,Pilehvaretal.\n2013). Corporawithclusteredwordsensesfortrainingsupervisedclusteringalgo-\nOntoNotes rithmsincludePalmeretal.(2006)andOntoNotes(Hovyetal.,2006).\nSeePustejovsky(1995), PustejovskyandBoguraev(1996),Martin(1986), and\nCopestakeandBriscoe(1995), interalia, forcomputationalapproachestotherep-\ngenerative resentation of polysemy. Pustejovsky’s theory of the generative lexicon, and in\nlexicon\nqualia particularhistheoryofthequaliastructureofwords,isawayofaccountingforthe\nstructure\ndynamicsystematicpolysemyofwordsincontext.\nHistoricaloverviewsofWSDincludeAgirreandEdmonds(2006)andNavigli\n(2009).\nExercises\nG.1 Collect a small corpus of example sentences of varying lengths from any\nnewspaper or magazine. Using WordNet or any standard dictionary, deter-\nminehowmanysensesthereareforeachoftheopen-classwordsineachsen-\ntence. Howmanydistinctcombinationsofsensesarethereforeachsentence?\nHowdoesthisnumberseemtovarywithsentencelength?\nG.2 UsingWordNetorastandardreferencedictionary, tageachopen-classword\ninyourcorpuswithitscorrecttag. Waschoosingthecorrectsensealwaysa\nstraightforwardtask? Reportonanydifficultiesyouencountered.\nG.3 Using your favorite dictionary, simulate the original Lesk word overlap dis-\nambiguationalgorithmdescribedonpage13onthephraseTimeflieslikean\narrow. Assume that the words are to be disambiguated one at a time, from\nleft to right, and that the results from earlier decisions are used later in the\nprocess.\nG.4 Build an implementation of your solution to the previous exercise. Using\nWordNet, implement the original Lesk word overlap disambiguation algo-\nrithmdescribedonpage13onthephraseTimeflieslikeanarrow."
    },
    "5": {
        "chapter": "H",
        "sections": "H.1, H.2",
        "topic": "Phonetics",
        "original_category": "L",
        "original_text": "Speech and Language Processing. Daniel Jurafsky & James H. Martin. Copyright © 2024. All\nrights reserved. Draft of August 20, 2024.\nCHAPTER\nH Phonetics\nThecharactersthatmakeupthetextswe’vebeendiscussinginthisbookaren’tjust\nrandomsymbols. Theyarealsoanamazingscientificinvention: atheoreticalmodel\noftheelementsthatmakeuphumanspeech.\nThe earliest writing systems we know of (Sumerian, Chinese, Mayan) were\nmainly logographic: one symbol representing a whole word. But from the ear-\nliest stages we can find, some symbols were also used to represent the sounds\nthat made up words. The cuneiform sign to the right pro-\nnounced ba and meaning “ration” in Sumerian could also\nfunctionpurelyasthesound/ba/. TheearliestChinesechar-\nacters we have, carved into bones for divination, similarly\ncontain phonetic elements. Purely sound-based writing systems, whether syllabic\n(likeJapanesehiragana),alphabetic(liketheRomanalphabet),orconsonantal(like\nSemitic writing systems), trace back to these early logo-syllabic systems, often as\ntwoculturescametogether. Thus,theArabic,Aramaic,Hebrew,Greek,andRoman\nsystemsallderivefromaWestSemiticscriptthatispresumedtohavebeenmodified\nbyWesternSemiticmercenariesfromacursiveformofEgyptianhieroglyphs. The\nJapanesesyllabariesweremodifiedfromacursiveformofChinesephoneticcharac-\nters,whichthemselveswereusedinChinesetophoneticallyrepresenttheSanskrit\nintheBuddhistscripturesthatcametoChinaintheTangdynasty.\nThisimplicitideathatthespokenwordiscomposedofsmallerunitsofspeech\nunderliesalgorithmsforbothspeechrecognition(transcribingwaveformsintotext)\nandtext-to-speech(convertingtextintowaveforms). Inthischapterwegiveacom-\nphonetics putational perspective on phonetics, the study of the speech sounds used in the\nlanguagesoftheworld, howtheyareproducedinthehumanvocaltract, howthey\narerealizedacoustically,andhowtheycanbedigitizedandprocessed.\nH.1 Speech Sounds and Phonetic Transcription\nA letter like ‘p’ or ‘a’ is already a useful model of the sounds of human speech,\nand indeed we’ll see in Chapter 16 how to map between letters and waveforms.\nNonetheless,itishelpfultorepresentsoundsslightlymoreabstractly. We’llrepre-\nphone sent the pronunciation of a word as a string of phones, which are speech sounds,\neachrepresentedwithsymbolsadaptedfromtheRomanalphabet.\nThe standard phonetic representation for transcribing the world’s languages is\nIPA theInternationalPhoneticAlphabet(IPA),anevolvingstandardfirstdevelopedin\n1888,Butinthischapterwe’llinsteadrepresentphoneswiththeARPAbet(Shoup,\n1980),asimplephoneticalphabet(Fig.H.1)thatconvenientlyusesASCIIsymbols\ntorepresentanAmerican-EnglishsubsetoftheIPA.\nMany of the IPA and ARPAbet symbols are equivalent to familiar Roman let-\nters. So,forexample,theARPAbetphone[p]representstheconsonantsoundatthe 2 APPENDIXH • PHONETICS\nARPAbet IPA ARPAbet ARPAbet IPA ARPAbet\nSymbol Symbol Word Transcription Symbol Symbol Word Transcription\n[p] [p] parsley [paarsliy] [iy] [i] lily [lihliy]\n[t] [t] tea [tiy] [ih] [I] lily [lihliy]\n[k] [k] cook [kuhk] [ey] [eI] daisy [deyziy]\n[b] [b] bay [bey] [eh] [E] pen [pehn]\n[d] [d] dill [dihl] [ae] [æ] aster [aestaxr]\n[g] [g] garlic [gaarlixk] [aa] [A] poppy [paapiy]\n[m] [m] mint [mihnt] [ao] [O] orchid [aorkixd]\n[n] [n] nutmeg [nahtmehg] [uh] [U] wood [wuhd]\n[ng] [N] baking [beykixng] [ow] [oU] lotus [lowdxaxs]\n[f] [f] flour [flawaxr] [uw] [u] tulip [tuwlixp]\n[v] [v] clove [klowv] [ah] [2] butter [bahdxaxr]\n[th] [T] thick [thihk] [er] [Ç] bird [berd]\n[dh] [D] those [dhowz] [ay] [aI] iris [ayrixs]\n[s] [s] soup [suwp] [aw] [aU] flower [flawaxr]\n[z] [z] eggs [ehgz] [oy] [oI] soil [soyl]\n[sh] [S] squash [skwaash] [ax] [@] pita [piytax]\n[zh] [Z] ambrosia [aembrowzhax]\n[ch] [tS] cherry [chehriy]\n[jh] [dZ] jar [jhaar]\n[l] [l] licorice [lihkaxrixsh]\n[w] [w] kiwi [kiywiy]\n[r] [r] rice [rays]\n[y] [j] yellow [yehlow]\n[h] [h] honey [hahniy]\nFigureH.1 ARPAbetandIPAsymbolsforEnglishconsonants(left)andvowels(right).\nbeginningofplatypus,puma,andplantain,themiddleofleopard,ortheendofan-\ntelope. Ingeneral,however,themappingbetweenthelettersofEnglishorthography\nandphonesisrelativelyopaque; asinglelettercanrepresentverydifferentsounds\nindifferentcontexts. TheEnglishletterccorrespondstophone[k]incougar[kuw\ngaxr],butphone[s]incell[sehl]. Besidesappearingascandk,thephone[k]can\nappearaspartofx(fox[faaks]),asck(jackal[jhaekel])andascc(raccoon[rae\nkuwn]). Manyotherlanguages,forexample,Spanish,aremuchmoretransparent\nintheirsound-orthographymappingthanEnglish.\nH.2 Articulatory Phonetics\narticulatory Articulatoryphoneticsisthestudyofhowthesephonesareproducedasthevarious\nphonetics\norgansinthemouth,throat,andnosemodifytheairflowfromthelungs.\nTheVocalOrgans\nFigureH.2showstheorgansofspeech. Soundisproducedbytherapidmovement\nofair. Humansproducemostsoundsinspokenlanguagesbyexpellingairfromthe\nlungs through the windpipe (technically, the trachea) and then out the mouth or\nnose. Asitpassesthroughthetrachea,theairpassesthroughthelarynx,commonly\nknown as the Adam’s apple or voice box. The larynx contains two small folds of H.2 • ARTICULATORYPHONETICS 3\nFigureH.2 The vocal organs, shown in side view. (Figure from OpenStax University\nPhysics,CCBY4.0)\nmuscle,thevocalfolds(oftenreferredtonon-technicallyasthevocalcords),which\ncan be moved together or apart. The space between these two folds is called the\nglottis glottis. Ifthefoldsareclosetogether(butnottightlyclosed),theywillvibrateasair\npassesthroughthem;iftheyarefarapart,theywon’tvibrate. Soundsmadewiththe\nvoicedsound vocalfoldstogetherandvibratingarecalledvoiced;soundsmadewithoutthisvocal\nunvoicedsound cordvibrationarecalledunvoicedorvoiceless. Voicedsoundsinclude[b],[d],[g],\n[v],[z],andalltheEnglishvowels,amongothers. Unvoicedsoundsinclude[p],[t],\n[k],[f],[s],andothers.\nTheareaabovethetracheaiscalledthevocaltract;itconsistsoftheoraltract\nandthenasaltract. Aftertheairleavesthetrachea,itcanexitthebodythroughthe\nmouthorthenose. Mostsoundsaremadebyairpassingthroughthemouth. Sounds\nnasal made by air passing through the nose are called nasal sounds; nasal sounds (like\nEnglish[m],[n],and[ng])useboththeoralandnasaltractsasresonatingcavities.\nconsonant Phonesaredividedintotwomainclasses: consonantsandvowels. Bothkinds\nvowel ofsoundsareformedbythemotionofairthroughthemouth,throatornose. Con-\nsonantsaremadebyrestrictionorblockingoftheairflowinsomeway,andcanbe\nvoicedorunvoiced. Vowelshavelessobstruction,areusuallyvoiced,andaregen-\nerallylouderandlonger-lastingthanconsonants. Thetechnicaluseofthesetermsis\nmuchlikethecommonusage;[p],[b],[t],[d],[k],[g],[f],[v],[s],[z],[r],[l],etc.,\nare consonants; [aa], [ae], [ao], [ih], [aw], [ow], [uw], etc., are vowels. Semivow-\nels (such as [y] and [w]) have some of the properties of both; they are voiced like\nvowels,buttheyareshortandlesssyllabiclikeconsonants. H.2 • ARTICULATORYPHONETICS 5\nhasvoicedstopslike[b],[d],and[g]aswellasunvoicedstopslike[p],[t],and[k].\nStopsarealsocalledplosives.\nnasal Thenasalsounds[n],[m],and[ng]aremadebyloweringthevelumandallow-\ningairtopassintothenasalcavity.\nfricatives In fricatives, airflow is constricted but not cut off completely. The turbulent\nairflowthatresultsfromtheconstrictionproducesacharacteristic“hissing”sound.\nThe English labiodental fricatives [f] and [v] are produced by pressing the lower\nlip against the upper teeth, allowing a restricted airflow between the upper teeth.\nThedentalfricatives[th]and[dh]allowairtoflowaroundthetonguebetweenthe\nteeth. The alveolar fricatives [s] and [z] are produced with the tongue against the\nalveolarridge,forcingairovertheedgeoftheteeth. Inthepalato-alveolarfricatives\n[sh] and [zh], the tongue is at the back of the alveolar ridge, forcing air through a\ngrooveformedinthetongue. Thehigher-pitchedfricatives(inEnglish[s],[z],[sh]\nsibilants and[zh])arecalledsibilants. Stopsthatarefollowedimmediatelybyfricativesare\ncalledaffricates;theseincludeEnglish[ch](chicken)and[jh](giraffe).\napproximant Inapproximants,thetwoarticulatorsareclosetogetherbutnotcloseenoughto\ncauseturbulentairflow. InEnglish[y](yellow),thetonguemovesclosetotheroof\nofthemouthbutnotcloseenoughtocausetheturbulencethatwouldcharacterizea\nfricative. InEnglish[w](wood), thebackofthetonguecomesclosetothevelum.\nAmerican [r] can be formed in at least two ways; with just the tip of the tongue\nextendedandclosetothepalateorwiththewholetonguebunchedupnearthepalate.\n[l]isformedwiththetipofthetongueupagainstthealveolarridgeortheteeth,with\none or both sides of the tongue lowered to allow air to flow over it. [l] is called a\nlateralsoundbecauseofthedropinthesidesofthetongue.\ntap Ataporflap[dx]isaquickmotionofthetongueagainstthealveolarridge.The\nconsonantinthemiddleofthewordlotus([lowdxaxs])isatapinmostdialectsof\nAmericanEnglish;speakersofmanyU.K.dialectswouldusea[t]instead.\nVowels\nLikeconsonants, vowelscanbecharacterizedby thepositionofthearticulatorsas\nthey are made. The three most relevant parameters for vowels are what is called\nvowel height, which correlates roughly with the height of the highest part of the\ntongue, vowelfrontnessorbackness, indicatingwhetherthishighpointistoward\nthe front or back of the oral tract and whether the shape of the lips is rounded or\nnot. FigureH.4showsthepositionofthetonguefordifferentvowels.\ntongue palate\nclosed\nvelum\nbeet [iy] bat [ae] boot [uw]\nFigureH.4 TonguepositionsforEnglishhighfront[iy],lowfront[ae]andhighback[uw].\nIn the vowel [iy], for example, the highest point of the tongue is toward the\nfront of the mouth. In the vowel [uw], by contrast, the high-point of the tongue is\nlocatedtowardthebackofthemouth. Vowelsinwhichthetongueisraisedtoward\nFrontvowel the front are called front vowels; those in which the tongue is raised toward the H.3 • PROSODY 7\nσ σ σ\nOnset Rime Onset Rime Rime\nh Nucleus Coda g r Nucleus Coda Nucleus Coda\nae m iy n eh g z\nFigureH.6 Syllablestructureofham,green,eggs.σ=syllable.\nH.3 Prosody\nprosody Prosody is the study of the intonational and rhythmic aspects of language, and in\nparticular the use of F0, energy, and duration to convey pragmatic, affective, or\nconversation-interactional meanings.1 We’ll introduce these acoustic quantities in\ndetail in the next section when we turn to acoustic phonetics, but briefly we can\nthink of energy as the acoustic quality that we perceive as loudness, and F0 as the\nfrequency of the sound that is produced, the acoustic quality that we hear as the\npitch of an utterance. Prosody can be used to mark discourse structure, like the\ndifferencebetweenstatementsandquestions,orthewaythataconversationisstruc-\ntured. Prosodyisusedtomarkthesaliencyofaparticularwordorphrase. Prosody\nis heavily used for paralinguistic functions like conveying affective meanings like\nhappiness, surprise, or anger. And prosody plays an important role in managing\nturn-takinginconversation.\nH.3.1 ProsodicProminence: Accent,StressandSchwa\nprominence InanaturalutteranceofAmericanEnglish,somewordssoundmoreprominentthan\nothers, and certain syllables in these words are also more prominent than others.\nWhatwemeanbyprominenceisthatthesewordsorsyllablesareperceptuallymore\nsalient to the listener. Speakers make a word or syllable more salient in English\nbysayingitlouder, sayingitslower(soithasalongerduration), orbyvaryingF0\nduringtheword,makingithigherormorevariable.\npitchaccent Accent Werepresentprominenceviaalinguisticmarkercalledpitchaccent.Words\norsyllablesthatareprominentaresaidtobear(beassociatedwith)apitchaccent.\nThusthisutterancemightbepronouncedbyaccentingtheunderlinedwords:\n(H.1) I’malittlesurprisedtohearitcharacterizedashappy.\nLexicalStress Thesyllablesthatbearpitchaccentarecalledaccentedsyllables.\nNoteverysyllableofawordcanbeaccented: pitchaccenthastoberealizedonthe\nlexicalstress syllablethathaslexicalstress. Lexicalstressisapropertyoftheword’spronuncia-\ntionindictionaries;thesyllablethathaslexicalstressistheonethatwillbelouder\norlongerifthewordisaccented. Forexample,thewordsurprisedisstressedonits\nsecondsyllable,notitsfirst. (TrystressingtheothersyllablebysayingSURprised;\nhopefully that sounds wrong to you). Thus, if the word surprised receives a pitch\naccentinasentence,itisthesecondsyllablethatwillbestronger. Thefollowingex-\n1 Thewordisusedinadifferentbutrelatedwayinpoetry,tomeanthestudyofversemetricalstructure. 8 APPENDIXH • PHONETICS\nampleshowsunderlinedaccentedwordswiththestressedsyllablebearingtheaccent\n(thelouder,longersyllable)inboldface:\n(H.2) I’malittlesurprisedtohearitcharacterizedashappy.\nStress is marked in dictionaries. The CMU dictionary (CMU, 1993), for ex-\nample, marks vowels with 0 (unstressed) or 1 (stressed) as in entries for counter:\n[K AW1 N T ER0], or table: [T EY1 B AH0 L]. Difference in lexical stress can\naffectwordmeaning;thenouncontentispronounced[KAA1NTEH0NT],while\ntheadjectiveispronounced[KAA0NTEH1NT].\nReducedVowelsandSchwa Unstressedvowelscanbeweakenedevenfurtherto\nreducedvowel reducedvowels,themostcommonofwhichisschwa([ax]),asinthesecondvowel\nschwa of parakeet: [p ae r ax k iy t]. In a reduced vowel the articulatory gesture isn’t as\ncompleteasforafullvowel. Notallunstressedvowelsarereduced;anyvowel,and\ndiphthongsinparticular, canretainitsfullqualityeveninunstressedposition. For\nexample,thevowel[iy]canappearinstressedpositionasinthewordeat[iyt]orin\nunstressedpositionasinthewordcarry[kaeriy].\nprominence Insummary,thereisacontinuumofprosodicprominence,forwhichitisoften\nusefultorepresentlevelslikeaccented,stressed,fullvowel,andreducedvowel.\nH.3.2 ProsodicStructure\nSpokensentenceshaveprosodicstructure: somewordsseemtogroupnaturallyto-\ngether, while some words seem to have a noticeable break or disjuncture between\nprosodic them. Prosodic structure is often described in terms of prosodic phrasing, mean-\nphrasing\ning that an utterance has a prosodic phrase structure in a similar way to it having\na syntactic phrase structure. For example, the sentence I wanted to go to London,\nintonation but could only get tickets for France seems to have two main intonation phrases,\nphrase\ntheirboundaryoccurringatthecomma.Furthermore,inthefirstphrase,thereseems\nto be another set of lesser prosodic phrase boundaries (often called intermediate\nintermediate phrases) that split up the words as I wanted | to go | to London. These kinds of\nphrase\nintonation phrases are often correlated with syntactic structure constituents (Price\netal.1991,BennettandElfner2019).\nAutomatically predicting prosodic boundaries can be important for tasks like\nTTS.Modernapproachesusesequencemodelsthattakeeitherrawtextortextan-\nnotatedwithfeatureslikeparsetreesasinput,andmakeabreak/no-breakdecision\nateachwordboundary. Theycanbetrainedondatalabeledforprosodicstructure\nliketheBostonUniversityRadioNewsCorpus(Ostendorfetal.,1995).\nH.3.3 Tune\nTwo utterances with the same prominence and phrasing patterns can still differ\ntune prosodically by having different tunes. The tune of an utterance is the rise and\nfallofits F0overtime. A veryobviousexampleoftune isthedifferencebetween\nstatementsandyes-noquestionsinEnglish. Thesamewordscanbesaidwithafinal\nquestionrise F0risetoindicateayes-noquestion(calledaquestionrise):\nYou know what I mean ?\nfinalfall orafinaldropinF0(calledafinalfall)toindicateadeclarativeintonation: 22 APPENDIXH • PHONETICS\nspectrograph (Koenig et al., 1946), theoretical insights like the working out of the\nsource-filtertheoryandotherissuesinthemappingbetweenarticulationandacous-\ntics((Fant,1960),Stevensetal.1953,StevensandHouse1955,HeinzandStevens\n1961, Stevens and House 1961) the F1xF2 space of vowel formants (Peterson and\nBarney, 1952), the understanding of the phonetic nature of stress and the use of\nduration and intensity as cues (Fry, 1955), and a basic understanding of issues in\nphoneperception(MillerandNicely1955,Libermanetal.1952). Lehiste(1967)is\nacollectionofclassicpapersonacousticphonetics. Manyoftheseminalpapersof\nGunnarFanthavebeencollectedinFant(2004).\nExcellent textbooks on acoustic phonetics include Johnson (2003) and Lade-\nfoged (1996). Coleman (2005) includes an introduction to computational process-\ning of acoustics and speech from a linguistic perspective. Stevens (1998) lays out\nan influential theory of speech sound production. There are a number of software\npackagesforacousticphoneticanalysis.ProbablythemostwidelyusedoneisPraat\n(BoersmaandWeenink,2005).\nExercises\nH.1 FindthemistakesintheARPAbettranscriptionsofthefollowingwords:\na. “three”[dhri] d. “study”[stuhdi] g. “slight”[sliyt]\nb. “sing”[sihng] e. “though”[thow]\nc. “eyes”[ays] f. “planning”[pplaanihng]\nH.2 Ira Gershwin’s lyric for Let’s Call the Whole Thing Off talks about two pro-\nnunciations(each)ofthewords“tomato”,“potato”,and“either”. Transcribe\nintotheARPAbetbothpronunciationsofeachofthesethreewords.\nH.3 TranscribethefollowingwordsintheARPAbet:\n1. dark\n2. suit\n3. greasy\n4. wash\n5. water\nH.4 Takeawavefileofyourchoice. Someexamplesareonthetextbookwebsite.\nDownloadthePraatsoftware,anduseittotranscribethewavefilesattheword\nlevel and into ARPAbet phones, using Praat to help you play pieces of each\nwavefileandtolookatthewavefileandthespectrogram.\nH.5 RecordyourselfsayingfiveoftheEnglishvowels: [aa],[eh],[ae],[iy],[uw].\nFindF1andF2foreachofyourvowels. 22 APPENDIXH • PHONETICS\nspectrograph (Koenig et al., 1946), theoretical insights like the working out of the\nsource-filtertheoryandotherissuesinthemappingbetweenarticulationandacous-\ntics((Fant,1960),Stevensetal.1953,StevensandHouse1955,HeinzandStevens\n1961, Stevens and House 1961) the F1xF2 space of vowel formants (Peterson and\nBarney, 1952), the understanding of the phonetic nature of stress and the use of\nduration and intensity as cues (Fry, 1955), and a basic understanding of issues in\nphoneperception(MillerandNicely1955,Libermanetal.1952). Lehiste(1967)is\nacollectionofclassicpapersonacousticphonetics. Manyoftheseminalpapersof\nGunnarFanthavebeencollectedinFant(2004).\nExcellent textbooks on acoustic phonetics include Johnson (2003) and Lade-\nfoged (1996). Coleman (2005) includes an introduction to computational process-\ning of acoustics and speech from a linguistic perspective. Stevens (1998) lays out\nan influential theory of speech sound production. There are a number of software\npackagesforacousticphoneticanalysis.ProbablythemostwidelyusedoneisPraat\n(BoersmaandWeenink,2005).\nExercises\nH.1 FindthemistakesintheARPAbettranscriptionsofthefollowingwords:\na. “three”[dhri] d. “study”[stuhdi] g. “slight”[sliyt]\nb. “sing”[sihng] e. “though”[thow]\nc. “eyes”[ays] f. “planning”[pplaanihng]\nH.2 Ira Gershwin’s lyric for Let’s Call the Whole Thing Off talks about two pro-\nnunciations(each)ofthewords“tomato”,“potato”,and“either”. Transcribe\nintotheARPAbetbothpronunciationsofeachofthesethreewords.\nH.3 TranscribethefollowingwordsintheARPAbet:\n1. dark\n2. suit\n3. greasy\n4. wash\n5. water\nH.4 Takeawavefileofyourchoice. Someexamplesareonthetextbookwebsite.\nDownloadthePraatsoftware,anduseittotranscribethewavefilesattheword\nlevel and into ARPAbet phones, using Praat to help you play pieces of each\nwavefileandtolookatthewavefileandthespectrogram.\nH.5 RecordyourselfsayingfiveoftheEnglishvowels: [aa],[eh],[ae],[iy],[uw].\nFindF1andF2foreachofyourvowels."
    },
    "6": {
        "chapter": "7",
        "sections": "7.3",
        "topic": "Feed-Forward Neural Networks (FFNNs)",
        "original_category": "CS",
        "original_text": "2 CHAPTER7 • NEURALNETWORKS\n7.1 Units\nThebuildingblockofaneuralnetworkisasinglecomputationalunit. Aunittakes\na set of real valued numbers as input, performs some computation on them, and\nproducesanoutput.\nAtitsheart,aneuralunitistakingaweightedsumofitsinputs,withoneaddi-\nbiasterm tional term in the sum called a bias term. Given a set of inputs x 1...x n, a unit has\nasetofcorrespondingweightsw ...w andabiasb, sotheweightedsumzcanbe\n1 n\nrepresentedas:\n(cid:88)\nz=b+ wx (7.1)\ni i\ni\nOftenit’smoreconvenienttoexpressthisweightedsumusingvectornotation;recall\nvector from linear algebra that a vector is, at heart, just a list or array of numbers. Thus\nwe’lltalkaboutzintermsofaweightvectorw,ascalarbiasb,andaninputvector\nx,andwe’llreplacethesumwiththeconvenientdotproduct:\nz=w·x+b (7.2)\nAsdefinedinEq.7.2,zisjustarealvaluednumber.\nFinally, instead of using z, a linear function of x, as the output, neural units\napply a non-linear function f to z. We will refer to the output of this function as\nactivation the activation value for the unit, a. Since we are just modeling a single unit, the\nactivationforthenodeisinfactthefinaloutputofthenetwork,whichwe’llgenerally\ncally. Sothevalueyisdefinedas:\ny=a= f(z)\nWe’lldiscussthreepopularnon-linearfunctions f below(thesigmoid,thetanh,and\ntherectifiedlinearunitorReLU)butit’spedagogicallyconvenienttostartwiththe\nsigmoid sigmoidfunctionsincewesawitinChapter5:\n1\ny=σ(z)= (7.3)\n1+e−z\nThesigmoid(showninFig.7.1)hasanumberofadvantages;itmapstheoutput\ninto the range (0,1), which is useful in squashing outliers toward 0 or 1. And it’s\ndifferentiable,whichaswesawinSection??willbehandyforlearning.\nFigure7.1 The sigmoid function takes a real value and maps it to the range (0,1). It is\nnearlylineararound0butoutliervaluesgetsquashedtoward0or1.\nSubstitutingEq.7.2intoEq.7.3givesustheoutputofaneuralunit:\n1\ny=σ(w·x+b)= (7.4)\n1+exp(−(w·x+b)) 7.3 • FEEDFORWARDNEURALNETWORKS 7\nx 2 h 2\n1 1\n0 0\nx\n1 h\n0 1 0 1 2 1\na) The original x space b) The new (linearly separable) h space\nFigure7.7 The hidden layer forming a new representation of the input. (b) shows the\nrepresentationofthehiddenlayer,h,comparedtotheoriginalinputrepresentationxin(a).\nNotice that the input point [0, 1] has been collapsed with the input point [1, 0], making it\npossibletolinearlyseparatethepositiveandnegativecasesofXOR.AfterGoodfellowetal.\n(2016).\n7.3 Feedforward Neural Networks\nLet’snowwalkthroughaslightlymoreformalpresentationofthesimplestkindof\nfeedforward neuralnetwork,thefeedforwardnetwork. Afeedforwardnetworkisamultilayer\nnetwork\nnetworkinwhichtheunitsareconnectedwithnocycles; theoutputsfromunitsin\neach layer are passed to units in the next higher layer, and no outputs are passed\nback to lower layers. (In Chapter 8 we’ll introduce networks with cycles, called\nrecurrentneuralnetworks.)\nForhistoricalreasonsmultilayernetworks,especiallyfeedforwardnetworks,are\nmulti-layer sometimescalledmulti-layerperceptrons(orMLPs);thisisatechnicalmisnomer,\nperceptrons\nMLP since the units in modern multilayer networks aren’t perceptrons (perceptrons are\npurely linear, but modern networks are made up of units with non-linearities like\nsigmoids),butatsomepointthenamestuck.\nSimple feedforward networks have three kinds of nodes: input units, hidden\nunits,andoutputunits.\nFig.7.8showsapicture.Theinputlayerxisavectorofsimplescalarvaluesjust\naswesawinFig.7.2.\nhiddenlayer Thecoreoftheneuralnetworkisthehiddenlayerhformedofhiddenunitsh i,\neachofwhichisaneuralunitasdescribedinSection7.1,takingaweightedsumof\nitsinputsandthenapplyinganon-linearity. Inthestandardarchitecture,eachlayer\nfully-connected isfully-connected, meaningthateachunitineachlayertakesasinputtheoutputs\nfromalltheunitsinthepreviouslayer,andthereisalinkbetweeneverypairofunits\nfromtwoadjacentlayers. Thuseachhiddenunitsumsoveralltheinputunits.\nRecallthatasinglehiddenunithasasparametersaweightvectorandabias. We\nrepresenttheparametersfortheentirehiddenlayerbycombiningtheweightvector\nandbiasforeachunitiintoasingleweightmatrixWandasinglebiasvectorbfor\nthewholelayer(seeFig.7.8). EachelementW oftheweightmatrixWrepresents\nji\ntheweightoftheconnectionfromtheithinputunitx tothe jthhiddenunith .\ni j\nTheadvantageofusingasinglematrixW fortheweightsoftheentirelayeris\nthatnowthehiddenlayercomputationforafeedforwardnetworkcanbedonevery\nefficiently with simple matrix operations. In fact, the computation only has three 8 CHAPTER7 • NEURALNETWORKS\nx\n1\nx\n2\nx\nn\n0\n…\n…\nb\n+1\n…\nW U\ny\n1\nh\n1\nh\n2 y\n2\nh\n3\nh\nn\n1\ny\nn\n2\ninput layer hidden layer output layer\nFigure7.8 Asimple2-layerfeedforwardnetwork,withonehiddenlayer,oneoutputlayer,\nandoneinputlayer(theinputlayerisusuallynotcountedwhenenumeratinglayers).\nsteps: multiplyingtheweightmatrixbytheinputvectorx,addingthebiasvectorb,\nandapplyingtheactivationfunctiong(suchasthesigmoid,tanh,orReLUactivation\nfunctiondefinedabove).\nTheoutputofthehiddenlayer,thevectorh,isthusthefollowing(forthisexam-\nplewe’llusethesigmoidfunctionσ asouractivationfunction):\nh=σ(Wx+b) (7.8)\nNotice that we’re applying the σ function here to a vector, while in Eq. 7.3 it was\napplied to a scalar. We’re thus allowing σ(·), and indeed any activation function\ng(·),toapplytoavectorelement-wise,sog[z ,z ,z ]=[g(z ),g(z ),g(z )].\n1 2 3 1 2 3\nLet’sintroducesomeconstantstorepresentthedimensionalitiesofthesevectors\nand matrices. We’ll refer to the input layer as layer 0 of the network, and have n\n0\nrepresent the number of inputs, so x is a vector of real numbers of dimension n ,\n0\normoreformallyx∈Rn0, acolumnvectorofdimensionality[n 0,1]. Let’scallthe\nhiddenlayerlayer1andtheoutputlayerlayer2. Thehiddenlayerhasdimensional-\nityn 1,soh∈Rn1 andalsob∈Rn1 (sinceeachhiddenunitcantakeadifferentbias\nvalue). AndtheweightmatrixWhasdimensionalityW∈Rn1×n0,i.e. [n 1,n 0].\nTakeamomenttoconvinceyourselfthatthematrixmultiplicationinEq.7.8will\ncomputethevalueofeachh\nasσ(cid:0)(cid:80)n0\nW x +b\n(cid:1)\n.\nj i=1 ji i j\nAswesawinSection7.2,theresultingvalueh(forhiddenbutalsoforhypoth-\nesis) forms a representation of the input. The role of the output layer is to take\nthis new representation h and compute a final output. This output could be a real-\nvalued number, but in many cases the goal of the network is to make some sort of\nclassificationdecision,andsowewillfocusonthecaseofclassification.\nIfwearedoingabinarytasklikesentimentclassification,wemighthaveasin-\ngleoutputnode,anditsscalarvalueyistheprobabilityofpositiveversusnegative\nsentiment. If we are doing multinomial classification, such as assigning a part-of-\nspeechtag,wemighthaveoneoutputnodeforeachpotentialpart-of-speech,whose\noutputvalueistheprobabilityofthatpart-of-speech,andthevaluesofalltheoutput\nnodesmustsumtoone. Theoutputlayeristhusavectory thatgivesaprobability\ndistributionacrosstheoutputnodes.\nLet’sseehowthishappens. Likethehiddenlayer,theoutputlayerhasaweight\nmatrix(let’scallitU),butsomemodelsdon’tincludeabiasvectorbintheoutput 7.3 • FEEDFORWARDNEURALNETWORKS 9\nlayer, sowe’llsimplifybyeliminatingthebiasvectorinthisexample. Theweight\nmatrixismultipliedbyitsinputvector(h)toproducetheintermediateoutputz:\nz=Uh\nThere are n\n2\noutput nodes, so z∈Rn2, weight matrix U has dimensionality U∈\nRn2×n1,andelementU\nij\nistheweightfromunit jinthehiddenlayertounitiinthe\noutputlayer.\nHowever,zcan’tbetheoutputoftheclassifier,sinceit’savectorofreal-valued\nnumbers,whilewhatweneedforclassificationisavectorofprobabilities. Thereis\nnormalizing a convenient function for normalizing a vector of real values, by which we mean\nconvertingittoavectorthatencodesaprobabilitydistribution(allthenumberslie\nsoftmax between 0 and 1 and sum to 1): the softmax function that we saw on page ?? of\nChapter 5. More generally for any vector z of dimensionality d, the softmax is\ndefinedas:\nexp(z)\ni\nsoftmax(z i) =\n(cid:80)d\nexp(z )\n1≤i≤d (7.9)\nj=1 j\nThusforexamplegivenavector\nz=[0.6,1.1,−1.5,1.2,3.2,−1.1], (7.10)\nthesoftmaxfunctionwillnormalizeittoaprobabilitydistribution(shownrounded):\nsoftmax(z)=[0.055,0.090,0.0067,0.10,0.74,0.010] (7.11)\nYou may recall that we used softmax to create a probability distribution from a\nvectorofreal-valuednumbers(computedfromsummingweightstimesfeatures)in\nthemultinomialversionoflogisticregressioninChapter5.\nThat means we can think of a neural network classifier with one hidden layer\nasbuildingavectorhwhichisahiddenlayerrepresentationoftheinput,andthen\nrunning standard multinomial logistic regression on the features that the network\ndevelopsinh. Bycontrast,inChapter5thefeaturesweremainlydesignedbyhand\nvia feature templates. So a neural network is like multinomial logistic regression,\nbut(a)withmanylayers,sinceadeepneuralnetworkislikelayerafterlayeroflo-\ngisticregressionclassifiers;(b)withthoseintermediatelayershavingmanypossible\nactivation functions (tanh, ReLU, sigmoid) instead of just sigmoid (although we’ll\ncontinuetouseσ forconveniencetomeananyactivationfunction);(c)ratherthan\nformingthefeaturesbyfeaturetemplates,thepriorlayersofthenetworkinducethe\nfeaturerepresentationsthemselves.\nHerearethefinalequationsforafeedforwardnetworkwithasinglehiddenlayer,\nwhichtakesaninputvectorx,outputsaprobabilitydistributiony,andisparameter-\nizedbyweightmatricesWandUandabiasvectorb:\nh = σ(Wx+b)\nz = Uh\ny = softmax(z) (7.12)\nAnd just to remember the shapes of all our variables, x∈Rn0, h∈Rn1, b∈Rn1,\nW∈Rn1×n0,U∈Rn2×n1,andtheoutputvectory∈Rn2.We’llcallthisnetworka2-\nlayernetwork(wetraditionallydon’tcounttheinputlayerwhennumberinglayers,\nbutdocounttheoutputlayer).Sobythisterminologylogisticregressionisa1-layer\nnetwork. 10 CHAPTER7 • NEURALNETWORKS\n7.3.1 Moredetailsonfeedforwardnetworks\nLet’s now set up some notation to make it easier to talk about deeper networks of\ndepth more than 2. We’ll use superscripts in square brackets to mean layer num-\nbers, starting at 0 for the input layer. So W[1] will mean the weight matrix for the\n(first)hiddenlayer,andb[1] willmeanthebiasvectorforthe(first)hiddenlayer. n\nj\nwillmeanthenumberof unitsatlayer j. We’ll useg(·)tostandfortheactivation\nfunction, which will tend to be ReLU or tanh for intermediate layers and softmax\nforoutputlayers. We’llusea[i] tomeantheoutputfromlayeri,andz[i] tomeanthe\ncombinationofpreviouslayeroutput, weightsandbiasesW[i]a[i−1]+b[i]. The0th\nlayerisforinputs,sowe’llrefertotheinputsxmoregenerallyasa[0].\nThuswecanre-representour2-layernetfromEq.7.12asfollows:\nz[1] = W[1]a[0]+b[1]\na[1] = g[1](z[1])\nz[2] = W[2]a[1]+b[2]\na[2] = g[2](z[2])\nyˆ = a[2] (7.13)\nNotethatwiththisnotation,theequationsforthecomputationdoneateachlayerare\nthesame. Thealgorithmforcomputingtheforwardstepinann-layerfeedforward\nnetwork,giventheinputvectora[0]isthussimply:\nforiin1,...,n\nz[i] = W[i]a[i−1] + b[i]\na[i] = g[i](z[i])\nyˆ = a[n]\nIt’softenusefultohaveanameforthefinalsetofactivationsrightbeforethefinal\nsoftmax. So however many layers we have, we’ll generally call the unnormalized\nvaluesinthefinalvectorz[n],thevectorofscoresrightbeforethefinalsoftmax,the\nlogits logits(see(??).\nThe need for non-linear activation functions One of the reasons we use non-\nlinearactivationfunctionsforeachlayerinaneuralnetworkisthatifwedidnot,the\nresultingnetworkisexactlyequivalenttoasingle-layernetwork. Let’sseewhythis\nistrue. Imaginethefirsttwolayersofsuchanetworkofpurelylinearlayers:\nz[1] = W[1]x+b[1]\nz[2] = W[2]z[1]+b[2]\nWecanrewritethefunctionthatthenetworkiscomputingas:\nz[2] = W[2]z[1]+b[2]\n= W[2](W[1]x+b[1])+b[2]\n= W[2]W[1]x+W[2]b[1]+b[2]\n= W(cid:48)x+b(cid:48) (7.14)\nThisgeneralizestoanynumberoflayers.Sowithoutnon-linearactivationfunctions,\na multilayer network is just a notational variant of a single layer network with a\ndifferent set of weights, and we lose all the representational power of multilayer\nnetworks."
    },
    "7": {
        "chapter": "7a",
        "sections": "7.5",
        "topic": " Training FFNNs (Loss Function and Gradient)",
        "original_category": "CS",
        "original_text": "7.1 • UNITS 3\nFig.7.2showsafinalschematicofabasicneuralunit. Inthisexampletheunit\ntakes3inputvaluesx ,x ,andx ,andcomputesaweightedsum,multiplyingeach\n1 2 3\nvaluebyaweight(w ,w ,andw ,respectively),addsthemtoabiastermb,andthen\n1 2 3\npassestheresultingsumthroughasigmoidfunctiontoresultinanumberbetween0\nand1.\nx\n1 w 1\nx w 2 ∑ z σ a y\n2\nw\n3\nx b\n3\n+1\nFigure7.2 Aneuralunit,taking3inputsx ,x ,andx (andabiasbthatwerepresentasa\n1 2 3\nweightforaninputclampedat+1)andproducinganoutputy. Weincludesomeconvenient\nintermediatevariables: theoutputofthesummation,z,andtheoutputofthesigmoid,a. In\nthiscasetheoutputoftheunityisthesameasa,butindeepernetworkswe’llreserveyto\nmeanthefinaloutputoftheentirenetwork,leavingaastheactivationofanindividualnode.\nLet’swalkthroughanexamplejusttogetanintuition. Let’ssupposewehavea\nunitwiththefollowingweightvectorandbias:\nw = [0.2,0.3,0.9]\nb = 0.5\nWhatwouldthisunitdowiththefollowinginputvector:\nx = [0.5,0.6,0.1]\nTheresultingoutputywouldbe:\n1 1 1\ny=σ(w·x+b)= = = =.70\n1+e−(w·x+b) 1+e−(.5∗.2+.6∗.3+.1∗.9+.5) 1+e−0.87\nInpractice,thesigmoidisnotcommonlyusedasanactivationfunction. Afunction\ntanh thatisverysimilarbutalmostalwaysbetteristhetanhfunctionshowninFig.7.3a;\ntanhisavariantofthesigmoidthatrangesfrom-1to+1:\nez−e−z\ny=tanh(z)= (7.5)\nez+e−z\nThesimplestactivationfunction, andperhapsthemostcommonlyused, istherec-\nReLU tified linear unit, also called the ReLU, shown in Fig. 7.3b. It’s just the same as z\nwhenzispositive,and0otherwise:\ny=ReLU(z)=max(z,0) (7.6)\nTheseactivationfunctionshavedifferentpropertiesthatmakethemusefulfordiffer-\nentlanguageapplicationsornetworkarchitectures. Forexample,thetanhfunction\nhasthenicepropertiesofbeingsmoothlydifferentiableandmappingoutliervalues\ntowardthemean. Therectifierfunction,ontheotherhand,hasnicepropertiesthat 4 CHAPTER7 • NEURALNETWORKS\n(a) (b)\nFigure7.3 ThetanhandReLUactivationfunctions.\nresultfromitbeingveryclosetolinear. Inthesigmoidortanhfunctions,veryhigh\nsaturated valuesofzresultinvaluesofythataresaturated,i.e.,extremelycloseto1,andhave\nderivativesverycloseto0. Zeroderivativescauseproblemsforlearning,becauseas\nwe’ll see in Section 7.5, we’ll train networks by propagating an error signal back-\nwards, multiplying gradients (partial derivatives) from each layer of the network;\ngradientsthatarealmost0causetheerrorsignaltogetsmallerandsmalleruntilitis\nvanishing toosmalltobeusedfortraining,aproblemcalledthevanishinggradientproblem.\ngradient\nRectifiersdon’thavethisproblem,sincethederivativeofReLUforhighvaluesofz\nis1ratherthanverycloseto0.\n7.2 The XOR problem\nEarlyinthehistoryofneuralnetworksitwasrealizedthatthepowerofneuralnet-\nworks, as with the real neurons that inspired them, comes from combining these\nunitsintolargernetworks.\nOneofthemostcleverdemonstrationsoftheneedformulti-layernetworkswas\nthe proof by Minsky and Papert (1969) that a single neural unit cannot compute\nsomeverysimplefunctionsofitsinput. Considerthetaskofcomputingelementary\nlogical functions of two inputs, like AND, OR, and XOR. As a reminder, here are\nthetruthtablesforthosefunctions:\nAND OR XOR\nx1 x2 y x1 x2 y x1 x2 y\n0 0 0 0 0 0 0 0 0\n0 1 0 0 1 1 0 1 1\n1 0 0 1 0 1 1 0 1\n1 1 1 1 1 1 1 1 0\nperceptron Thisexamplewasfirstshownfortheperceptron,whichisaverysimpleneural\nunitthathasabinaryoutputanddoesnothaveanon-linearactivationfunction. The\noutputyofaperceptronis0or1,andiscomputedasfollows(usingthesameweight\nw,inputx,andbiasbasinEq.7.2):\n(cid:26)\n0, ifw·x+b≤0\ny= (7.7)\n1, ifw·x+b>0 14 CHAPTER7 • NEURALNETWORKS\ndistributionforanentiretestset:\n(cid:124)\nH = σ(XW +b)\n(cid:124)\nZ = HU\nYˆ = softmax(Z) (7.22)\nThe idea of using word2vec or GloVe embeddings as our input representation—\nandmoregenerallytheideaofrelyingonanotheralgorithmtohavealreadylearned\npretraining an embedding representation for our input words—is called pretraining. Using\npretrainedembeddingrepresentations,whethersimplestaticwordembeddingslike\nword2vec or the much more powerful contextual embeddings we’ll introduce in\nChapter 11, is one of the central ideas of deep learning. (It’s also possible, how-\never,totrainthewordembeddingsaspartofanNLPtask; we’lltalkabouthowto\ndothisinSection7.7inthecontextoftheneurallanguagemodelingtask.)\n7.5 Training Neural Nets\nAfeedforwardneuralnetisaninstanceofsupervisedmachinelearninginwhichwe\nknow the correct output y for each observation x. What the system produces, via\nEq.7.13,isyˆ,thesystem’sestimateofthetruey. Thegoalofthetrainingprocedure\nis to learn parameters W[i] and b[i] for each layer i that make yˆ for each training\nobservationascloseaspossibletothetruey.\nIngeneral,wedoallthisbydrawingonthemethodsweintroducedinChapter5\nforlogisticregression,sothereadershouldbecomfortablewiththatchapterbefore\nproceeding.\nFirst, we’ll need a loss function that models the distance between the system\noutputandthegoldoutput,andit’scommontousethelossfunctionusedforlogistic\nregression,thecross-entropyloss.\nSecond, to find the parameters that minimize this loss function, we’ll use the\ngradientdescentoptimizationalgorithmintroducedinChapter5.\nThird, gradientdescentrequiresknowingthegradientofthelossfunction, the\nvector that contains the partial derivative of the loss function with respect to each\nof the parameters. In logistic regression, for each observation we could directly\ncomputethederivativeofthelossfunctionwithrespecttoanindividualworb. But\nforneuralnetworks,withmillionsofparametersinmanylayers,it’smuchharderto\nsee how to compute the partial derivative of some weight in layer 1 when the loss\nisattachedtosomemuchlaterlayer. Howdowepartialoutthelossoverallthose\nintermediatelayers? Theansweristhealgorithmcallederrorbackpropagationor\nbackwarddifferentiation.\n7.5.1 Lossfunction\ncross-entropy Thecross-entropylossthatisusedinneuralnetworksisthesameonewesawfor\nloss\nlogistic regression. If the neural network is being used as a binary classifier, with\nthe sigmoid at the final layer, the loss function is the same logistic regression loss\nwesawinEq.??:\nL CE(yˆ,y)=−logp(y|x) = −[ylogyˆ+(1−y)log(1−yˆ)] (7.23)\nIfweareusingthenetworktoclassifyinto3ormoreclasses,thelossfunctionis\nexactlythesameasthelossformultinomialregressionthatwesawinChapter5on 7.5 • TRAININGNEURALNETS 15\npage??. Let’sbrieflysummarizetheexplanationhereforconvenience. First,when\nwehavemorethan2classeswe’llneedtorepresentbothy andyˆ asvectors. Let’s\nassume we’re doing hard classification, where only one class is the correct one.\nThe true label y is then a vector with K elements, each corresponding to a class,\nwithy =1ifthecorrectclassisc,withallotherelementsofybeing0. Recallthat\nc\navectorlikethis,withonevalueequalto1andtherest0,iscalledaone-hotvector.\nAndourclassifierwillproduceanestimatevectorwithK elementsyˆ,eachelement\nyˆ ofwhichrepresentstheestimatedprobability p(y =1|x).\nk k\nThelossfunctionforasingleexamplexisthenegativesumofthelogsoftheK\noutputclasses,eachweightedbytheirprobabilityy :\nk\nK\n(cid:88)\nL CE(yˆ,y)=− y klogyˆ k (7.24)\nk=1\nWecansimplifythisequationfurther;let’sfirstrewritetheequationusingthefunc-\ntion 1{} which evaluates to 1 if the condition in the brackets is true and to 0 oth-\nerwise. ThismakesitmoreobviousthatthetermsinthesuminEq.7.24willbe0\nexceptforthetermcorrespondingtothetrueclassforwhichy =1:\nk\nK\n(cid:88)\nL (yˆ,y) = − 1{y =1}logyˆ\nCE k k\nk=1\nInotherwords,thecross-entropylossissimplythenegativelogoftheoutputproba-\nbilitycorrespondingtothecorrectclass,andwethereforealsocallthisthenegative\nnegativelog loglikelihoodloss:\nlikelihoodloss\nL CE(yˆ,y) = −logyˆ c (wherecisthecorrectclass) (7.25)\nPlugginginthesoftmaxformulafromEq.7.9,andwithK thenumberofclasses:\nexp(z )\nc\nL CE(yˆ,y) = −log\n(cid:80)K\nexp(z )\n(wherecisthecorrectclass) (7.26)\nj=1 j\n7.5.2 ComputingtheGradient\nHow do we compute the gradient of this loss function? Computing the gradient\nrequires the partial derivative of the loss function with respect to each parameter.\nFor a network with one weight layer and sigmoid output (which is what logistic\nregressionis),wecouldsimplyusethederivativeofthelossthatweusedforlogistic\nregressioninEq.7.27(andderivedinSection??):\n∂L (yˆ,y)\nCE\n= (yˆ−y)x\nj\n∂w\nj\n= (σ(w·x+b)−y)x j (7.27)\nOrforanetworkwithoneweightlayerandsoftmaxoutput(=multinomiallogistic\nregression),wecouldusethederivativeofthesoftmaxlossfromEq.??,shownfor\naparticularweightw andinputx\nk i\n∂L (yˆ,y)\nCE = −(y −yˆ )x\nk k i\n∂w\nk,i\n= −(y −p(y =1|x))x\nk k i\n(cid:32) (cid:33)\nexp(w ·x+b )\n= − y k− (cid:80)K expk\n(w\n·x+k\nb )\nx i (7.28)\nj=1 j j 16 CHAPTER7 • NEURALNETWORKS\nButthesederivativesonlygivecorrectupdatesforoneweightlayer:thelastone!\nFordeepnetworks,computingthegradientsforeachweightismuchmorecomplex,\nsincewearecomputingthederivativewithrespecttoweightparametersthatappear\nall the way back in the very early layers of the network, even though the loss is\ncomputedonlyattheveryendofthenetwork.\nThesolutiontocomputingthisgradientisanalgorithmcallederrorbackprop-\nerrorback- agation or backprop (Rumelhart et al., 1986). While backprop was invented spe-\npropagation\nciallyforneuralnetworks, itturnsouttobethesameasamoregeneralprocedure\ncalled backward differentiation, which depends on the notion of computation\ngraphs. Let’sseehowthatworksinthenextsubsection.\n7.5.3 ComputationGraphs\nAcomputationgraphisarepresentationoftheprocessofcomputingamathematical\nexpression,inwhichthecomputationisbrokendownintoseparateoperations,each\nofwhichismodeledasanodeinagraph.\nConsidercomputingthefunctionL(a,b,c)=c(a+2b). Ifwemakeeachofthe\ncomponentadditionandmultiplicationoperationsexplicit,andaddnames(dande)\nfortheintermediateoutputs,theresultingseriesofcomputationsis:\nd = 2∗b\ne = a+d\nL = c∗e\nWe can now represent this as a graph, with nodes for each operation, and di-\nrected edges showing the outputs from each operation as the inputs to the next, as\nin Fig. 7.12. The simplest use of computation graphs is to compute the value of\nthefunctionwithsomegiveninputs. Inthefigure,we’veassumedtheinputsa=3,\nb=1,c=−2,andwe’veshowntheresultoftheforwardpasstocomputethere-\nsultL(3,1,−2)=−10. Intheforwardpassofacomputationgraph,weapplyeach\noperation left to right, passing the outputs of each computation as the input to the\nnextnode.\nforward pass\na=3\na\ne=5\ne=a+d\nd=2\nb=1\nb d = 2b L=ce L=-10\nc=-2\nc\nFigure7.12 ComputationgraphforthefunctionL(a,b,c)=c(a+2b),withvaluesforinput\nnodesa=3,b=1,c=−2,showingtheforwardpasscomputationofL.\n7.5.4 Backwarddifferentiationoncomputationgraphs\nThe importance of the computation graph comes from the backward pass, which\nis used to compute the derivatives that we’ll need for the weight update. In this\nexampleourgoalistocomputethederivativeoftheoutputfunctionLwithrespect 7.5 • TRAININGNEURALNETS 17\nto eachof the input variables, i.e., ∂L, ∂L, and ∂L. The derivative ∂L tells ushow\n∂a ∂b ∂c ∂a\nmuchasmallchangeinaaffectsL.\nchainrule Backwards differentiation makes use of the chain rule in calculus, so let’s re-\nmind ourselves of that. Suppose we are computing the derivative of a composite\nfunction f(x)=u(v(x)). Thederivativeof f(x)isthederivativeofu(x)withrespect\ntov(x)timesthederivativeofv(x)withrespecttox:\ndf du dv\n= · (7.29)\ndx dv dx\nThechainruleextendstomorethantwofunctions. Ifcomputingthederivativeofa\ncompositefunction f(x)=u(v(w(x))),thederivativeof f(x)is:\ndf du dv dw\n= · · (7.30)\ndx dv dw dx\nTheintuitionofbackwarddifferentiationistopassgradientsbackfromthefinal\nnodetoallthenodesinthegraph.Fig.7.13showspartofthebackwardcomputation\natonenodee. Eachnodetakesanupstreamgradientthatispassedinfromitsparent\nnodetotheright,andforeachofitsinputscomputesalocalgradient(thegradient\nofitsoutputwithrespecttoitsinput),andusesthechainruletomultiplythesetwo\ntocomputeadownstreamgradienttobepassedontothenextearliernode.\nd e\nd e L\n∂L ∂L ∂e ∂e ∂L\n=\n∂d ∂e ∂d ∂d ∂e\ndownstream local upstream\ngradient gradient gradient\nFigure7.13 Eachnode(likeehere)takesanupstreamgradient,multipliesitbythelocal\ngradient(thegradientofitsoutputwithrespecttoitsinput),andusesthechainruletocompute\na downstream gradient to be passed on to a prior node. A node may have multiple local\ngradientsifithasmultipleinputs.\nLet’s now compute the 3 derivatives we need. Since in the computation graph\nL=ce,wecandirectlycomputethederivative ∂L:\n∂c\n∂L\n=e (7.31)\n∂c\nFortheothertwo,we’llneedtousethechainrule:\n∂L ∂L∂e\n=\n∂a ∂e∂a\n∂L ∂L∂e∂d\n= (7.32)\n∂b ∂e∂d ∂b\nEq.7.32andEq.7.31thusrequirefiveintermediatederivatives: ∂L, ∂L, ∂e, ∂e,and\n∂e ∂c ∂a ∂d\n∂d,whichareasfollows(makinguseofthefactthatthederivativeofasumisthe\n∂b 7.5 • TRAININGNEURALNETS 19\nForthebackwardpasswe’llalsoneedtocomputethelossL. Thelossfunction\nforbinarysigmoidoutputfromEq.7.23is\nL CE(yˆ,y) = −[ylogyˆ+(1−y)log(1−yˆ)] (7.34)\nOuroutputyˆ=a[2],sowecanrephrasethisas\n(cid:104) (cid:105)\nL CE(a[2],y) = − yloga[2]+(1−y)log(1−a[2]) (7.35)\n[1]\nw\n11 *\n[1]\nw\n12 z[1] = a[1] =\n* 1 1\n+ ReLU\nx\n1\n*\n[1]\nb [2]\nx\n2\n1 w 1[ 12] z + = a[2] = σ L (a[2] ,y)\n*\n*\n[1] [2]\nw [1] [1] w\n21 * z 2 = a 2 = 12\n+ ReLU\n[1]\nw [2]\n22 b\n1\n[1]\nb\n2\nFigure7.15 Samplecomputationgraphforasimple2-layerneuralnet(=1hiddenlayer)withtwoinputunits\nand2hiddenunits. We’veadjustedthenotationabittoavoidlongequationsinthenodesbyjustmentioning\n[1]\nthefunctionthatisbeingcomputed,andtheresultingvariablename.Thusthe*totherightofnodew means\n11\nthatw[1] istobemultipliedbyx ,andthenodez[1]=+meansthatthevalueofz[1] iscomputedbysumming\n11 1\n[1]\nthethreenodesthatfeedintoit(thetwoproducts,andthebiastermb ).\ni\nThe weights that need updating (those for which we need to know the partial\nderivativeofthelossfunction)areshowninteal. Inordertodothebackwardpass,\nwe’llneedtoknowthederivativesofallthefunctionsinthegraph. Wealreadysaw\ninSection??thederivativeofthesigmoidσ:\ndσ(z)\n=σ(z)(1−σ(z)) (7.36)\ndz\nWe’ll also need the derivatives of each of the other activation functions. The\nderivativeoftanhis:\ndtanh(z)\n=1−tanh2(z) (7.37)\ndz\nThederivativeoftheReLUis2\n(cid:26)\ndReLU(z) 0 for z<0\n= (7.38)\ndz 1 for z≥0\n2 Thederivativeisactuallyundefinedatthepointz=0,butbyconventionwetreatitas1. 20 CHAPTER7 • NEURALNETWORKS\nWe’llgivethestartofthecomputation,computingthederivativeofthelossfunction\nLwithrespecttoz,or ∂L (andleavingtherestofthecomputationasanexercisefor\n∂z\nthereader). Bythechainrule:\n∂L ∂L ∂a[2]\n= (7.39)\n∂z ∂a[2] ∂z\nSolet’sfirstcompute ∂L ,takingthederivativeofEq.7.35,repeatedhere:\n∂a[2]\n(cid:104) (cid:105)\nL (a[2],y) = − yloga[2]+(1−y)log(1−a[2])\nCE\n(cid:32)(cid:32) (cid:33) (cid:33)\n∂L ∂log(a[2]) ∂log(1−a[2])\n= − y +(1−y)\n∂a[2] ∂a[2] ∂a[2]\n(cid:18)(cid:18) (cid:19) (cid:19)\n1 1\n= − y +(1−y) (−1)\na[2] 1−a[2]\n(cid:18) (cid:19)\ny y−1\n= − + (7.40)\na[2] 1−a[2]\nNext,bythederivativeofthesigmoid:\n∂a[2]\n=a[2](1−a[2])\n∂z\nFinally,wecanusethechainrule:\n∂L ∂L ∂a[2]\n=\n∂z ∂a[2] ∂z\n(cid:18) (cid:19)\ny y−1\n= − + a[2](1−a[2])\na[2] 1−a[2]\n= a[2]−y (7.41)\nContinuingthebackwardcomputationofthegradients(nextbypassingthegra-\n[2]\ndientsoverb andthetwoproductnodes,andsoon,backtoallthetealnodes),is\n1\nleftasanexerciseforthereader.\n7.5.5 Moredetailsonlearning\nOptimizationinneuralnetworksisanon-convexoptimizationproblem,morecom-\nplexthanforlogisticregression,andforthatandotherreasonstherearemanybest\npracticesforsuccessfullearning.\nForlogisticregressionwecaninitializegradientdescentwithalltheweightsand\nbiaseshavingthevalue0. Inneuralnetworks,bycontrast,weneedtoinitializethe\nweightswithsmallrandomnumbers. It’salsohelpfultonormalizetheinputvalues\ntohave0meanandunitvariance.\nVariousformsofregularizationareusedtopreventoverfitting. Oneofthemost\ndropout important is dropout: randomly dropping some units and their connections from\nthe network during training (Hinton et al. 2012, Srivastava et al. 2014). At each\niterationoftraining(wheneverweupdateparameters,i.e. eachmini-batchifweare\nusing mini-batch gradient descent), we repeatedly choose a probability p and for\neach unit we replace its output with zero with probability p (and renormalize the\nrestoftheoutputsfromthatlayer)."
    },
    "8": {
        "chapter": "7b",
        "sections": "7.5",
        "topic": "Backward Pass",
        "original_category": "CS",
        "original_text": "7.1 • UNITS 3\nFig.7.2showsafinalschematicofabasicneuralunit. Inthisexampletheunit\ntakes3inputvaluesx ,x ,andx ,andcomputesaweightedsum,multiplyingeach\n1 2 3\nvaluebyaweight(w ,w ,andw ,respectively),addsthemtoabiastermb,andthen\n1 2 3\npassestheresultingsumthroughasigmoidfunctiontoresultinanumberbetween0\nand1.\nx\n1 w 1\nx w 2 ∑ z σ a y\n2\nw\n3\nx b\n3\n+1\nFigure7.2 Aneuralunit,taking3inputsx ,x ,andx (andabiasbthatwerepresentasa\n1 2 3\nweightforaninputclampedat+1)andproducinganoutputy. Weincludesomeconvenient\nintermediatevariables: theoutputofthesummation,z,andtheoutputofthesigmoid,a. In\nthiscasetheoutputoftheunityisthesameasa,butindeepernetworkswe’llreserveyto\nmeanthefinaloutputoftheentirenetwork,leavingaastheactivationofanindividualnode.\nLet’swalkthroughanexamplejusttogetanintuition. Let’ssupposewehavea\nunitwiththefollowingweightvectorandbias:\nw = [0.2,0.3,0.9]\nb = 0.5\nWhatwouldthisunitdowiththefollowinginputvector:\nx = [0.5,0.6,0.1]\nTheresultingoutputywouldbe:\n1 1 1\ny=σ(w·x+b)= = = =.70\n1+e−(w·x+b) 1+e−(.5∗.2+.6∗.3+.1∗.9+.5) 1+e−0.87\nInpractice,thesigmoidisnotcommonlyusedasanactivationfunction. Afunction\ntanh thatisverysimilarbutalmostalwaysbetteristhetanhfunctionshowninFig.7.3a;\ntanhisavariantofthesigmoidthatrangesfrom-1to+1:\nez−e−z\ny=tanh(z)= (7.5)\nez+e−z\nThesimplestactivationfunction, andperhapsthemostcommonlyused, istherec-\nReLU tified linear unit, also called the ReLU, shown in Fig. 7.3b. It’s just the same as z\nwhenzispositive,and0otherwise:\ny=ReLU(z)=max(z,0) (7.6)\nTheseactivationfunctionshavedifferentpropertiesthatmakethemusefulfordiffer-\nentlanguageapplicationsornetworkarchitectures. Forexample,thetanhfunction\nhasthenicepropertiesofbeingsmoothlydifferentiableandmappingoutliervalues\ntowardthemean. Therectifierfunction,ontheotherhand,hasnicepropertiesthat 4 CHAPTER7 • NEURALNETWORKS\n(a) (b)\nFigure7.3 ThetanhandReLUactivationfunctions.\nresultfromitbeingveryclosetolinear. Inthesigmoidortanhfunctions,veryhigh\nsaturated valuesofzresultinvaluesofythataresaturated,i.e.,extremelycloseto1,andhave\nderivativesverycloseto0. Zeroderivativescauseproblemsforlearning,becauseas\nwe’ll see in Section 7.5, we’ll train networks by propagating an error signal back-\nwards, multiplying gradients (partial derivatives) from each layer of the network;\ngradientsthatarealmost0causetheerrorsignaltogetsmallerandsmalleruntilitis\nvanishing toosmalltobeusedfortraining,aproblemcalledthevanishinggradientproblem.\ngradient\nRectifiersdon’thavethisproblem,sincethederivativeofReLUforhighvaluesofz\nis1ratherthanverycloseto0.\n7.2 The XOR problem\nEarlyinthehistoryofneuralnetworksitwasrealizedthatthepowerofneuralnet-\nworks, as with the real neurons that inspired them, comes from combining these\nunitsintolargernetworks.\nOneofthemostcleverdemonstrationsoftheneedformulti-layernetworkswas\nthe proof by Minsky and Papert (1969) that a single neural unit cannot compute\nsomeverysimplefunctionsofitsinput. Considerthetaskofcomputingelementary\nlogical functions of two inputs, like AND, OR, and XOR. As a reminder, here are\nthetruthtablesforthosefunctions:\nAND OR XOR\nx1 x2 y x1 x2 y x1 x2 y\n0 0 0 0 0 0 0 0 0\n0 1 0 0 1 1 0 1 1\n1 0 0 1 0 1 1 0 1\n1 1 1 1 1 1 1 1 0\nperceptron Thisexamplewasfirstshownfortheperceptron,whichisaverysimpleneural\nunitthathasabinaryoutputanddoesnothaveanon-linearactivationfunction. The\noutputyofaperceptronis0or1,andiscomputedasfollows(usingthesameweight\nw,inputx,andbiasbasinEq.7.2):\n(cid:26)\n0, ifw·x+b≤0\ny= (7.7)\n1, ifw·x+b>0 14 CHAPTER7 • NEURALNETWORKS\ndistributionforanentiretestset:\n(cid:124)\nH = σ(XW +b)\n(cid:124)\nZ = HU\nYˆ = softmax(Z) (7.22)\nThe idea of using word2vec or GloVe embeddings as our input representation—\nandmoregenerallytheideaofrelyingonanotheralgorithmtohavealreadylearned\npretraining an embedding representation for our input words—is called pretraining. Using\npretrainedembeddingrepresentations,whethersimplestaticwordembeddingslike\nword2vec or the much more powerful contextual embeddings we’ll introduce in\nChapter 11, is one of the central ideas of deep learning. (It’s also possible, how-\never,totrainthewordembeddingsaspartofanNLPtask; we’lltalkabouthowto\ndothisinSection7.7inthecontextoftheneurallanguagemodelingtask.)\n7.5 Training Neural Nets\nAfeedforwardneuralnetisaninstanceofsupervisedmachinelearninginwhichwe\nknow the correct output y for each observation x. What the system produces, via\nEq.7.13,isyˆ,thesystem’sestimateofthetruey. Thegoalofthetrainingprocedure\nis to learn parameters W[i] and b[i] for each layer i that make yˆ for each training\nobservationascloseaspossibletothetruey.\nIngeneral,wedoallthisbydrawingonthemethodsweintroducedinChapter5\nforlogisticregression,sothereadershouldbecomfortablewiththatchapterbefore\nproceeding.\nFirst, we’ll need a loss function that models the distance between the system\noutputandthegoldoutput,andit’scommontousethelossfunctionusedforlogistic\nregression,thecross-entropyloss.\nSecond, to find the parameters that minimize this loss function, we’ll use the\ngradientdescentoptimizationalgorithmintroducedinChapter5.\nThird, gradientdescentrequiresknowingthegradientofthelossfunction, the\nvector that contains the partial derivative of the loss function with respect to each\nof the parameters. In logistic regression, for each observation we could directly\ncomputethederivativeofthelossfunctionwithrespecttoanindividualworb. But\nforneuralnetworks,withmillionsofparametersinmanylayers,it’smuchharderto\nsee how to compute the partial derivative of some weight in layer 1 when the loss\nisattachedtosomemuchlaterlayer. Howdowepartialoutthelossoverallthose\nintermediatelayers? Theansweristhealgorithmcallederrorbackpropagationor\nbackwarddifferentiation.\n7.5.1 Lossfunction\ncross-entropy Thecross-entropylossthatisusedinneuralnetworksisthesameonewesawfor\nloss\nlogistic regression. If the neural network is being used as a binary classifier, with\nthe sigmoid at the final layer, the loss function is the same logistic regression loss\nwesawinEq.??:\nL CE(yˆ,y)=−logp(y|x) = −[ylogyˆ+(1−y)log(1−yˆ)] (7.23)\nIfweareusingthenetworktoclassifyinto3ormoreclasses,thelossfunctionis\nexactlythesameasthelossformultinomialregressionthatwesawinChapter5on 7.5 • TRAININGNEURALNETS 15\npage??. Let’sbrieflysummarizetheexplanationhereforconvenience. First,when\nwehavemorethan2classeswe’llneedtorepresentbothy andyˆ asvectors. Let’s\nassume we’re doing hard classification, where only one class is the correct one.\nThe true label y is then a vector with K elements, each corresponding to a class,\nwithy =1ifthecorrectclassisc,withallotherelementsofybeing0. Recallthat\nc\navectorlikethis,withonevalueequalto1andtherest0,iscalledaone-hotvector.\nAndourclassifierwillproduceanestimatevectorwithK elementsyˆ,eachelement\nyˆ ofwhichrepresentstheestimatedprobability p(y =1|x).\nk k\nThelossfunctionforasingleexamplexisthenegativesumofthelogsoftheK\noutputclasses,eachweightedbytheirprobabilityy :\nk\nK\n(cid:88)\nL CE(yˆ,y)=− y klogyˆ k (7.24)\nk=1\nWecansimplifythisequationfurther;let’sfirstrewritetheequationusingthefunc-\ntion 1{} which evaluates to 1 if the condition in the brackets is true and to 0 oth-\nerwise. ThismakesitmoreobviousthatthetermsinthesuminEq.7.24willbe0\nexceptforthetermcorrespondingtothetrueclassforwhichy =1:\nk\nK\n(cid:88)\nL (yˆ,y) = − 1{y =1}logyˆ\nCE k k\nk=1\nInotherwords,thecross-entropylossissimplythenegativelogoftheoutputproba-\nbilitycorrespondingtothecorrectclass,andwethereforealsocallthisthenegative\nnegativelog loglikelihoodloss:\nlikelihoodloss\nL CE(yˆ,y) = −logyˆ c (wherecisthecorrectclass) (7.25)\nPlugginginthesoftmaxformulafromEq.7.9,andwithK thenumberofclasses:\nexp(z )\nc\nL CE(yˆ,y) = −log\n(cid:80)K\nexp(z )\n(wherecisthecorrectclass) (7.26)\nj=1 j\n7.5.2 ComputingtheGradient\nHow do we compute the gradient of this loss function? Computing the gradient\nrequires the partial derivative of the loss function with respect to each parameter.\nFor a network with one weight layer and sigmoid output (which is what logistic\nregressionis),wecouldsimplyusethederivativeofthelossthatweusedforlogistic\nregressioninEq.7.27(andderivedinSection??):\n∂L (yˆ,y)\nCE\n= (yˆ−y)x\nj\n∂w\nj\n= (σ(w·x+b)−y)x j (7.27)\nOrforanetworkwithoneweightlayerandsoftmaxoutput(=multinomiallogistic\nregression),wecouldusethederivativeofthesoftmaxlossfromEq.??,shownfor\naparticularweightw andinputx\nk i\n∂L (yˆ,y)\nCE = −(y −yˆ )x\nk k i\n∂w\nk,i\n= −(y −p(y =1|x))x\nk k i\n(cid:32) (cid:33)\nexp(w ·x+b )\n= − y k− (cid:80)K expk\n(w\n·x+k\nb )\nx i (7.28)\nj=1 j j 16 CHAPTER7 • NEURALNETWORKS\nButthesederivativesonlygivecorrectupdatesforoneweightlayer:thelastone!\nFordeepnetworks,computingthegradientsforeachweightismuchmorecomplex,\nsincewearecomputingthederivativewithrespecttoweightparametersthatappear\nall the way back in the very early layers of the network, even though the loss is\ncomputedonlyattheveryendofthenetwork.\nThesolutiontocomputingthisgradientisanalgorithmcallederrorbackprop-\nerrorback- agation or backprop (Rumelhart et al., 1986). While backprop was invented spe-\npropagation\nciallyforneuralnetworks, itturnsouttobethesameasamoregeneralprocedure\ncalled backward differentiation, which depends on the notion of computation\ngraphs. Let’sseehowthatworksinthenextsubsection.\n7.5.3 ComputationGraphs\nAcomputationgraphisarepresentationoftheprocessofcomputingamathematical\nexpression,inwhichthecomputationisbrokendownintoseparateoperations,each\nofwhichismodeledasanodeinagraph.\nConsidercomputingthefunctionL(a,b,c)=c(a+2b). Ifwemakeeachofthe\ncomponentadditionandmultiplicationoperationsexplicit,andaddnames(dande)\nfortheintermediateoutputs,theresultingseriesofcomputationsis:\nd = 2∗b\ne = a+d\nL = c∗e\nWe can now represent this as a graph, with nodes for each operation, and di-\nrected edges showing the outputs from each operation as the inputs to the next, as\nin Fig. 7.12. The simplest use of computation graphs is to compute the value of\nthefunctionwithsomegiveninputs. Inthefigure,we’veassumedtheinputsa=3,\nb=1,c=−2,andwe’veshowntheresultoftheforwardpasstocomputethere-\nsultL(3,1,−2)=−10. Intheforwardpassofacomputationgraph,weapplyeach\noperation left to right, passing the outputs of each computation as the input to the\nnextnode.\nforward pass\na=3\na\ne=5\ne=a+d\nd=2\nb=1\nb d = 2b L=ce L=-10\nc=-2\nc\nFigure7.12 ComputationgraphforthefunctionL(a,b,c)=c(a+2b),withvaluesforinput\nnodesa=3,b=1,c=−2,showingtheforwardpasscomputationofL.\n7.5.4 Backwarddifferentiationoncomputationgraphs\nThe importance of the computation graph comes from the backward pass, which\nis used to compute the derivatives that we’ll need for the weight update. In this\nexampleourgoalistocomputethederivativeoftheoutputfunctionLwithrespect 7.5 • TRAININGNEURALNETS 17\nto eachof the input variables, i.e., ∂L, ∂L, and ∂L. The derivative ∂L tells ushow\n∂a ∂b ∂c ∂a\nmuchasmallchangeinaaffectsL.\nchainrule Backwards differentiation makes use of the chain rule in calculus, so let’s re-\nmind ourselves of that. Suppose we are computing the derivative of a composite\nfunction f(x)=u(v(x)). Thederivativeof f(x)isthederivativeofu(x)withrespect\ntov(x)timesthederivativeofv(x)withrespecttox:\ndf du dv\n= · (7.29)\ndx dv dx\nThechainruleextendstomorethantwofunctions. Ifcomputingthederivativeofa\ncompositefunction f(x)=u(v(w(x))),thederivativeof f(x)is:\ndf du dv dw\n= · · (7.30)\ndx dv dw dx\nTheintuitionofbackwarddifferentiationistopassgradientsbackfromthefinal\nnodetoallthenodesinthegraph.Fig.7.13showspartofthebackwardcomputation\natonenodee. Eachnodetakesanupstreamgradientthatispassedinfromitsparent\nnodetotheright,andforeachofitsinputscomputesalocalgradient(thegradient\nofitsoutputwithrespecttoitsinput),andusesthechainruletomultiplythesetwo\ntocomputeadownstreamgradienttobepassedontothenextearliernode.\nd e\nd e L\n∂L ∂L ∂e ∂e ∂L\n=\n∂d ∂e ∂d ∂d ∂e\ndownstream local upstream\ngradient gradient gradient\nFigure7.13 Eachnode(likeehere)takesanupstreamgradient,multipliesitbythelocal\ngradient(thegradientofitsoutputwithrespecttoitsinput),andusesthechainruletocompute\na downstream gradient to be passed on to a prior node. A node may have multiple local\ngradientsifithasmultipleinputs.\nLet’s now compute the 3 derivatives we need. Since in the computation graph\nL=ce,wecandirectlycomputethederivative ∂L:\n∂c\n∂L\n=e (7.31)\n∂c\nFortheothertwo,we’llneedtousethechainrule:\n∂L ∂L∂e\n=\n∂a ∂e∂a\n∂L ∂L∂e∂d\n= (7.32)\n∂b ∂e∂d ∂b\nEq.7.32andEq.7.31thusrequirefiveintermediatederivatives: ∂L, ∂L, ∂e, ∂e,and\n∂e ∂c ∂a ∂d\n∂d,whichareasfollows(makinguseofthefactthatthederivativeofasumisthe\n∂b 7.5 • TRAININGNEURALNETS 19\nForthebackwardpasswe’llalsoneedtocomputethelossL. Thelossfunction\nforbinarysigmoidoutputfromEq.7.23is\nL CE(yˆ,y) = −[ylogyˆ+(1−y)log(1−yˆ)] (7.34)\nOuroutputyˆ=a[2],sowecanrephrasethisas\n(cid:104) (cid:105)\nL CE(a[2],y) = − yloga[2]+(1−y)log(1−a[2]) (7.35)\n[1]\nw\n11 *\n[1]\nw\n12 z[1] = a[1] =\n* 1 1\n+ ReLU\nx\n1\n*\n[1]\nb [2]\nx\n2\n1 w 1[ 12] z + = a[2] = σ L (a[2] ,y)\n*\n*\n[1] [2]\nw [1] [1] w\n21 * z 2 = a 2 = 12\n+ ReLU\n[1]\nw [2]\n22 b\n1\n[1]\nb\n2\nFigure7.15 Samplecomputationgraphforasimple2-layerneuralnet(=1hiddenlayer)withtwoinputunits\nand2hiddenunits. We’veadjustedthenotationabittoavoidlongequationsinthenodesbyjustmentioning\n[1]\nthefunctionthatisbeingcomputed,andtheresultingvariablename.Thusthe*totherightofnodew means\n11\nthatw[1] istobemultipliedbyx ,andthenodez[1]=+meansthatthevalueofz[1] iscomputedbysumming\n11 1\n[1]\nthethreenodesthatfeedintoit(thetwoproducts,andthebiastermb ).\ni\nThe weights that need updating (those for which we need to know the partial\nderivativeofthelossfunction)areshowninteal. Inordertodothebackwardpass,\nwe’llneedtoknowthederivativesofallthefunctionsinthegraph. Wealreadysaw\ninSection??thederivativeofthesigmoidσ:\ndσ(z)\n=σ(z)(1−σ(z)) (7.36)\ndz\nWe’ll also need the derivatives of each of the other activation functions. The\nderivativeoftanhis:\ndtanh(z)\n=1−tanh2(z) (7.37)\ndz\nThederivativeoftheReLUis2\n(cid:26)\ndReLU(z) 0 for z<0\n= (7.38)\ndz 1 for z≥0\n2 Thederivativeisactuallyundefinedatthepointz=0,butbyconventionwetreatitas1. 20 CHAPTER7 • NEURALNETWORKS\nWe’llgivethestartofthecomputation,computingthederivativeofthelossfunction\nLwithrespecttoz,or ∂L (andleavingtherestofthecomputationasanexercisefor\n∂z\nthereader). Bythechainrule:\n∂L ∂L ∂a[2]\n= (7.39)\n∂z ∂a[2] ∂z\nSolet’sfirstcompute ∂L ,takingthederivativeofEq.7.35,repeatedhere:\n∂a[2]\n(cid:104) (cid:105)\nL (a[2],y) = − yloga[2]+(1−y)log(1−a[2])\nCE\n(cid:32)(cid:32) (cid:33) (cid:33)\n∂L ∂log(a[2]) ∂log(1−a[2])\n= − y +(1−y)\n∂a[2] ∂a[2] ∂a[2]\n(cid:18)(cid:18) (cid:19) (cid:19)\n1 1\n= − y +(1−y) (−1)\na[2] 1−a[2]\n(cid:18) (cid:19)\ny y−1\n= − + (7.40)\na[2] 1−a[2]\nNext,bythederivativeofthesigmoid:\n∂a[2]\n=a[2](1−a[2])\n∂z\nFinally,wecanusethechainrule:\n∂L ∂L ∂a[2]\n=\n∂z ∂a[2] ∂z\n(cid:18) (cid:19)\ny y−1\n= − + a[2](1−a[2])\na[2] 1−a[2]\n= a[2]−y (7.41)\nContinuingthebackwardcomputationofthegradients(nextbypassingthegra-\n[2]\ndientsoverb andthetwoproductnodes,andsoon,backtoallthetealnodes),is\n1\nleftasanexerciseforthereader.\n7.5.5 Moredetailsonlearning\nOptimizationinneuralnetworksisanon-convexoptimizationproblem,morecom-\nplexthanforlogisticregression,andforthatandotherreasonstherearemanybest\npracticesforsuccessfullearning.\nForlogisticregressionwecaninitializegradientdescentwithalltheweightsand\nbiaseshavingthevalue0. Inneuralnetworks,bycontrast,weneedtoinitializethe\nweightswithsmallrandomnumbers. It’salsohelpfultonormalizetheinputvalues\ntohave0meanandunitvariance.\nVariousformsofregularizationareusedtopreventoverfitting. Oneofthemost\ndropout important is dropout: randomly dropping some units and their connections from\nthe network during training (Hinton et al. 2012, Srivastava et al. 2014). At each\niterationoftraining(wheneverweupdateparameters,i.e. eachmini-batchifweare\nusing mini-batch gradient descent), we repeatedly choose a probability p and for\neach unit we replace its output with zero with probability p (and renormalize the\nrestoftheoutputsfromthatlayer)."
    },
    "9": {
        "chapter": "8",
        "sections": "8.1",
        "topic": "Recurrent Neural Networks (RNNs)",
        "original_category": "CS",
        "original_text": "Speech and Language Processing. Daniel Jurafsky & James H. Martin. Copyright © 2024. All\nrights reserved. Draft of August 20, 2024.\nCHAPTER\n8 RNNs and LSTMs\nTimewillexplain.\nJaneAusten,Persuasion\nLanguageisaninherentlytemporalphenomenon. Spokenlanguageisasequenceof\nacousticeventsovertime,andwecomprehendandproducebothspokenandwritten\nlanguageasasequentialinputstream. Thetemporalnatureoflanguageisreflected\ninthemetaphorsweuse;wetalkoftheflowofconversations,newsfeeds,andtwitter\nstreams,allofwhichemphasizethatlanguageisasequencethatunfoldsintime.\nThis temporal nature is reflected in some language processing algorithms. For\nexample,theViterbialgorithmweintroducedforHMMpart-of-speechtaggingpro-\nceedsthroughtheinputawordatatime,carryingforwardinformationgleanedalong\ntheway. Butothermachinelearningapproaches,likethosewe’vestudiedforsenti-\nmentanalysisorothertextclassificationtasksdon’thavethistemporalnature–they\nassumesimultaneousaccesstoallaspectsoftheirinput.\nThe feedforward networks of Chapter 7 also assumed simultaneous access, al-\nthoughtheyalsohadasimplemodelfortime. Recallthatweappliedfeedforward\nnetworks to language modeling by having them look only at a fixed-size window\nofwords,andthenslidingthiswindowovertheinput,makingindependentpredic-\ntionsalongtheway. Thissliding-windowapproachisalsousedinthetransformer\narchitecturewewillintroduceinChapter9.\nThis chapter introduces a deep learning architecture that offers an alternative\nwayofrepresentingtime: recurrentneuralnetworks(RNNs),andtheirvariantslike\nLSTMs. RNNshaveamechanismthatdealsdirectlywiththesequentialnatureof\nlanguage,allowingthemtohandlethetemporalnatureoflanguagewithouttheuseof\narbitraryfixed-sizedwindows. Therecurrentnetworkoffersanewwaytorepresent\nthe prior context, in its recurrent connections, allowing the model’s decision to\ndependoninformationfromhundredsofwordsinthepast. We’llseehowtoapply\nthe model to the task of language modeling, to sequence modeling tasks like part-\nof-speechtagging,andtotextclassificationtaskslikesentimentanalysis.\n8.1 Recurrent Neural Networks\nA recurrent neural network (RNN) is any network that contains a cycle within its\nnetworkconnections, meaningthatthevalueofsomeunitisdirectly, orindirectly,\ndependent on its own earlier outputs as an input. While powerful, such networks\naredifficulttoreasonaboutandtotrain. However,withinthegeneralclassofrecur-\nrent networks there are constrained architectures that have proven to be extremely\neffectivewhenappliedtolanguage. Inthissection,weconsideraclassofrecurrent\nElman networksreferredtoasElmanNetworks(Elman,1990)orsimplerecurrentnet-\nNetworks 2 CHAPTER8 • RNNSANDLSTMS\nworks. Thesenetworksareusefulintheirownrightandserveasthebasisformore\ncomplexapproachesliketheLongShort-TermMemory(LSTM)networksdiscussed\nlaterinthischapter. InthischapterwhenweusethetermRNNwe’llbereferringto\nthesesimplermoreconstrainednetworks(althoughyouwilloftenseethetermRNN\ntomeananynetwithrecurrentpropertiesincludingLSTMs).\nx t h t y t\nFigure8.1 SimplerecurrentneuralnetworkafterElman(1990).Thehiddenlayerincludes\narecurrentconnectionaspartofitsinput. Thatis, theactivationvalueofthehiddenlayer\ndepends on the current input as well as the activation value of the hidden layer from the\nprevioustimestep.\nFig. 8.1 illustrates the structure of an RNN. As with ordinary feedforward net-\nworks, an input vector representing the current input, x, is multiplied by a weight\nt\nmatrixandthenpassedthroughanon-linearactivationfunctiontocomputetheval-\nues for a layer of hidden units. This hidden layer is then used to calculate a cor-\nrespondingoutput, y. Inadeparturefromourearlierwindow-basedapproach, se-\nt\nquences are processed by presenting one item at a time to the network. We’ll use\nsubscriptstorepresenttime,thusx willmeantheinputvectorxattimet. Thekey\nt\ndifferencefromafeedforwardnetworkliesintherecurrentlinkshowninthefigure\nwiththedashedline. Thislinkaugmentstheinputtothecomputationatthehidden\nlayerwiththevalueofthehiddenlayerfromtheprecedingpointintime.\nThe hidden layer from the previous time step provides a form of memory, or\ncontext, that encodes earlier processing and informs the decisions to be made at\nlater points in time. Critically, this approach does not impose a fixed-length limit\nonthispriorcontext;thecontextembodiedintheprevioushiddenlayercaninclude\ninformationextendingbacktothebeginningofthesequence.\nAddingthistemporaldimensionmakesRNNsappeartobemorecomplexthan\nnon-recurrent architectures. But in reality, they’re not all that different. Given an\ninputvectorandthevaluesforthehiddenlayerfromtheprevioustimestep, we’re\nstill performing the standard feedforward calculation introduced in Chapter 7. To\nsee this, consider Fig. 8.2 which clarifies the nature of the recurrence and how it\nfactorsintothecomputationatthehiddenlayer. Themostsignificantchangeliesin\nthenewsetofweights,U,thatconnectthehiddenlayerfromtheprevioustimestep\ntothecurrenthiddenlayer. Theseweightsdeterminehowthenetworkmakesuseof\npastcontextincalculatingtheoutputforthecurrentinput.Aswiththeotherweights\ninthenetwork,theseconnectionsaretrainedviabackpropagation.\n8.1.1 InferenceinRNNs\nForward inference (mapping a sequence of inputs to a sequence of outputs) in an\nRNNisnearlyidenticaltowhatwe’vealreadyseenwithfeedforwardnetworks. To\ncompute an output y for an input x, we need the activation value for the hidden\nt t\nlayerh. Tocalculatethis,wemultiplytheinputx withtheweightmatrixW,and\nt t\nthe hidden layer from the previous time step h with the weight matrix U. We\nt 1\nadd these values together and pass them through−a suitable activation function, g,\nto arrive at the activation value for the current hidden layer, h. Once we have the\nt 8.1 • RECURRENTNEURALNETWORKS 3\ny\nt\nV\nh\nt\n+\nU W\nh t-1 x t\nFigure8.2 Simplerecurrentneuralnetworkillustratedasafeedforwardnetwork.Thehid-\ndenlayerh\nt 1\nfromthepriortimestepismultipliedbyweightmatrixUandthenaddedto\nthefeedforw−ardcomponentfromthecurrenttimestep.\nvaluesforthehiddenlayer,weproceedwiththeusualcomputationtogeneratethe\noutputvector.\nh t = g(Uh t 1+Wx t) (8.1)\n−\ny t = f(Vh t) (8.2)\nLet’s refer to the input, hidden and output layer dimensions as d , d , and d\nin h out\nrespectively.Giventhis,ourthreeparametermatricesare:W Rdh×din,U Rdh×dh,\nandV Rdout ×dh. ∈ ∈\n∈\nWe compute y via a softmax computation that gives a probability distribution\nt\noverthepossibleoutputclasses.\ny t = softmax(Vh t) (8.3)\nThe fact that the computation at timet requires the value of the hidden layer from\ntimet 1mandatesanincrementalinferencealgorithmthatproceedsfromthestart\n−\nofthesequencetotheendasillustratedinFig.8.3. Thesequentialnatureofsimple\nrecurrentnetworkscanalsobeseenbyunrollingthenetworkintimeasisshownin\nFig. 8.4. In this figure, the various layers of units are copied for each time step to\nillustratethattheywillhavedifferingvaluesovertime. However,thevariousweight\nmatricesaresharedacrosstime.\nfunctionFORWARDRNN(x,network)returnsoutputsequencey\nh 0\n0\n←\nfori 1toLENGTH(x) do\n←\nh i g(Uh i 1 + Wx i)\ny i← f(Vh i−)\n←\nreturny\nFigure8.3 Forwardinferenceinasimplerecurrentnetwork.ThematricesU,VandWare\nsharedacrosstime,whilenewvaluesforhandyarecalculatedwitheachtimestep.\n8.1.2 Training\nAs with feedforward networks, we’ll use a training set, a loss function, and back-\npropagation to obtain the gradients needed to adjust the weights in these recurrent"
    },
    "10": {
        "chapter": "8a",
        "sections": "8.5",
        "topic": "Long Short-Term Memory (LSTMs)",
        "original_category": "CS",
        "original_text": "6 CHAPTER8 • RNNSANDLSTMS\n^\na) y t b)\n^\ny\nt\nU\nV\nh\nt\nh t-1 U h t-1 U h t\nW\nW W W\ne e e e e e\nt-2 t-1 t t-2 t-1 t\nFigure8.5 Simplified sketch of two LM architectures moving through a text, showing a\nschematiccontextofthreetokens:(a)afeedforwardneurallanguagemodelwhichhasafixed\ncontextinputtotheweightmatrixW,(b)anRNNlanguagemodel,inwhichthehiddenstate\nh t 1summarizesthepriorcontext.\n−\nis,attimet:\ne t = Ex t (8.4)\nh t = g(Uh t 1+We t) (8.5)\n−\nˆy t = softmax(Vh t) (8.6)\nWhenwedolanguagemodelingwithRNNs(andwe’llseethisagaininChapter9\nwith transformers), it’s convenient to make the assumption that the embedding di-\nmension d and the hidden dimension d are the same. So we’ll just call both of\ne h\nthesethemodeldimensiond. SotheembeddingmatrixEisofshape[d V ],and\n×| |\nx isaone-hotvectorofshape[V 1]. Theproducte isthusofshape[d 1]. W\nt t\n| |× ×\nand U are of shape [d d], so h is also of shape [d 1]. V is of shape [V d],\nt\n× × | |×\nso the result of Vh is a vector of shape [V 1]. This vector can be thought of as\n| |×\nasetofscoresoverthevocabularygiventheevidenceprovidedinh. Passingthese\nscoresthroughthesoftmaxnormalizesthescoresintoaprobabilitydistribution.The\nprobabilitythataparticularwordkinthevocabularyisthenextwordisrepresented\nbyˆy[k],thekthcomponentofˆy:\nt t\nP(w t+1=kw 1,...,w t) = ˆy t[k] (8.7)\n|\nTheprobabilityofanentiresequenceisjusttheproductoftheprobabilitiesofeach\niteminthesequence,wherewe’lluseˆy[w]tomeantheprobabilityofthetrueword\ni i\nw attimestepi.\ni\nn\nP(w 1:n) = P(w i w 1:i 1) (8.8)\n| −\ni=1\n(cid:89)\nn\n= ˆy i[w i] (8.9)\ni=1\n(cid:89)\n8.2.2 TraininganRNNlanguagemodel\nself-supervision To train an RNN as a language model, we use the same self-supervision (or self-\ntraining) algorithm we saw in Section ??: we take a corpus of text as training 8.5 • THELSTM 13\nateachpointintime. Hereweuseeitherthesemicolon”;”ortheequivalentsymbol\ntomeanvectorconcatenation:\n⊕\nh = [hf ; hb]\nt t t\n= hf hb (8.18)\nt t\n⊕\nFig. 8.11 illustrates such a bidirectional network that concatenates the outputs of\nthe forward and backward pass. Other simple ways to combine the forward and\nbackward contexts include element-wise addition or multiplication. The output at\neachstepintimethuscapturesinformationtotheleftandtotherightofthecurrent\ninput. Insequencelabelingapplications,theseconcatenatedoutputscanserveasthe\nbasisforalocallabelingdecision.\ny y y y\n1 2 3 n\nconcatenated\noutputs\nRNN 2\nRNN 1\nx 1 x 2 x 3 x n\nFigure8.11 AbidirectionalRNN.Separatemodelsaretrainedintheforwardandbackward\ndirections, with the output of each model at each time point concatenated to represent the\nbidirectionalstateatthattimepoint.\nBidirectionalRNNshavealsoproventobequiteeffectiveforsequenceclassifi-\ncation. RecallfromFig.8.8thatforsequenceclassificationweusedthefinalhidden\nstate of the RNN as the input to a subsequent feedforward classifier. A difficulty\nwith this approach is that the final state naturally reflects more information about\nthe end of the sentence than its beginning. Bidirectional RNNs provide a simple\nsolutiontothisproblem;asshowninFig.8.12,wesimplycombinethefinalhidden\nstates from the forward and backward passes (for example by concatenation) and\nusethatasinputforfollow-onprocessing.\n8.5 The LSTM\nInpractice,itisquitedifficulttotrainRNNsfortasksthatrequireanetworktomake\nuseofinformationdistantfromthecurrentpointofprocessing. Despitehavingac-\ncesstotheentireprecedingsequence,theinformationencodedinhiddenstatestends\nto be fairly local, more relevant to the most recent parts of the input sequence and\nrecentdecisions. Yetdistantinformationiscriticaltomanylanguageapplications.\nConsiderthefollowingexampleinthecontextoflanguagemodeling. 8.5 • THELSTM 15\ncision making. The key to solving both problems is to learn how to manage this\ncontextratherthanhard-codingastrategyintothearchitecture. LSTMsaccomplish\nthis by first adding an explicit context layer to the architecture (in addition to the\nusual recurrent hidden layer), and through the use of specialized neural units that\nmake use of gates to control the flow of information into and out of the units that\ncomprisethenetworklayers. Thesegatesareimplementedthroughtheuseofaddi-\ntionalweightsthatoperatesequentiallyontheinput,andprevioushiddenlayer,and\npreviouscontextlayers.\nThegatesinanLSTMshareacommondesignpattern;eachconsistsofafeed-\nforward layer, followed by a sigmoid activation function, followed by a pointwise\nmultiplicationwiththelayerbeinggated.Thechoiceofthesigmoidastheactivation\nfunctionarisesfromitstendencytopushitsoutputstoeither0or1. Combiningthis\nwithapointwisemultiplicationhasaneffectsimilartothatofabinarymask. Values\ninthelayerbeinggatedthatalignwithvaluesnear1inthemaskarepassedthrough\nnearlyunchanged;valuescorrespondingtolowervaluesareessentiallyerased.\nforgetgate The first gate we’ll consider is the forget gate. The purpose of this gate is\nto delete information from the context that is no longer needed. The forget gate\ncomputes a weighted sum of the previous state’s hidden layer and the current in-\nput and passes that through a sigmoid. This mask is then multiplied element-wise\nby the context vector to remove the information from context that is no longer re-\nquired. Element-wisemultiplicationoftwovectors(representedbytheoperator ,\n(cid:12)\nandsometimescalledtheHadamardproduct)isthevectorofthesamedimension\nasthetwoinputvectors,whereeachelementiistheproductofelementiinthetwo\ninputvectors:\nf t = σ(U fh t 1+W fx t) (8.20)\n−\nk t = c t 1 f t (8.21)\n− (cid:12)\nThenexttaskistocomputetheactualinformationweneedtoextractfromtheprevi-\noushiddenstateandcurrentinputs—thesamebasiccomputationwe’vebeenusing\nforallourrecurrentnetworks.\ng t = tanh(U gh t 1+W gx t) (8.22)\n−\naddgate Next,wegeneratethemaskfortheaddgatetoselecttheinformationtoaddtothe\ncurrentcontext.\ni t = σ(U ih t 1+W ix t) (8.23)\n−\nj t = g t i t (8.24)\n(cid:12)\nNext,weaddthistothemodifiedcontextvectortogetournewcontextvector.\nc t =j t+k t (8.25)\noutputgate Thefinalgatewe’lluseistheoutputgatewhichisusedtodecidewhatinforma-\ntionisrequiredforthecurrenthiddenstate(asopposedtowhatinformationneeds\ntobepreservedforfuturedecisions).\no t = σ(U oh t 1+W ox t) (8.26)\n−\nh t = o t tanh(c t) (8.27)\n(cid:12)\nFig. 8.13 illustrates the complete computation for a single LSTM unit. Given the 16 CHAPTER8 • RNNSANDLSTMS\n+\nc t-1\nct c t\nh t-1 ht-1\ntanh\nht h t\nx t xt\n+ σ\ntanh\nσ\nσ\n+\n+\n+\nf\ng\ni\no\n⦿\n⦿ ⦿\nct-1\nLSTM\nFigure8.13 AsingleLSTMunitdisplayedasacomputationgraph. Theinputstoeachunitconsistsofthe\ncurrentinput,x,theprevioushiddenstate,h t 1,andthepreviouscontext,c t 1. Theoutputsareanewhidden\nstate,ht andanupdatedcontext,ct. − −\nappropriate weights for the various gates, an LSTM accepts as input the context\nlayer, and hidden layer from the previous time step, along with the current input\nvector. Itthengeneratesupdatedcontextandhiddenvectorsasoutput.\nItisthehiddenstate,h,thatprovidestheoutputfortheLSTMateachtimestep.\nt\nThisoutputcanbeusedastheinputtosubsequentlayersinastackedRNN,oratthe\nfinallayerofanetworkh canbeusedtoprovidethefinaloutputoftheLSTM.\nt\n8.5.1 GatedUnits,LayersandNetworks\nTheneuralunitsusedinLSTMsareobviouslymuchmorecomplexthanthoseused\ninbasicfeedforwardnetworks. Fortunately,thiscomplexityisencapsulatedwithin\nthebasicprocessingunits, allowingustomaintainmodularityandtoeasilyexper-\niment with different architectures. To see this, consider Fig. 8.14 which illustrates\ntheinputsandoutputsassociatedwitheachkindofunit.\nAtthefarleft,(a)isthebasicfeedforwardunitwhereasinglesetofweightsand\nasingleactivationfunctiondetermineitsoutput,andwhenarrangedinalayerthere\nare no connections among the units in the layer. Next, (b) represents the unit in a\nsimplerecurrentnetwork. Nowtherearetwoinputsandanadditionalsetofweights\ntogowithit. However,thereisstillasingleactivationfunctionandoutput.\nThe increased complexity of the LSTM units is encapsulated within the unit\nitself.TheonlyadditionalexternalcomplexityfortheLSTMoverthebasicrecurrent\nunit(b)isthepresenceoftheadditionalcontextvectorasaninputandoutput.\nThismodularityiskeytothepowerandwidespreadapplicabilityofLSTMunits.\nLSTMunits(orothervarieties,likeGRUs)canbesubstitutedintoanyofthenetwork\narchitectures described in Section 8.4. And, as with simple RNNs, multi-layered\nnetworksmakinguseofgatedunitscanbeunrolledintodeepfeedforwardnetworks\nandtrainedintheusualfashionwithbackpropagation.Inpractice,therefore,LSTMs\nratherthanRNNshavebecomethestandardunitforanymodernsystemthatmakes\nuseofrecurrentnetworks."
    },
    "11": {
        "chapter": "4",
        "sections": "4.1, 4.2",
        "topic": "Naive Bayes Classifiers",
        "original_category": "CL",
        "original_text": "2 CHAPTER4 • NAIVEBAYES,TEXTCLASSIFICATION,ANDSENTIMENT\nFinally, one of the oldest tasks in text classification is assigning a library sub-\nject category or topic label to a text. Deciding whether a research paper concerns\nepidemiologyorinstead,perhaps,embryology,isanimportantcomponentofinfor-\nmationretrieval.Varioussetsofsubjectcategoriesexist,suchastheMeSH(Medical\nSubjectHeadings) thesaurus. Infact,aswewillsee,subjectcategoryclassification\nisthetaskforwhichthenaiveBayesalgorithmwasinventedin1961Maron(1961).\nClassification is essential for tasks below the level of the document as well.\nWe’vealreadyseenperioddisambiguation(decidingifaperiodistheendofasen-\ntence or part of a word), and word tokenization (deciding if a character should be\na word boundary). Even language modeling can be viewed as classification: each\nwordcanbethoughtofasaclass,andsopredictingthenextwordisclassifyingthe\ncontext-so-farintoaclassforeachnextword. Apart-of-speechtagger(Chapter17)\nclassifieseachoccurrenceofawordinasentenceas,e.g.,anounoraverb.\nThe goal of classification is to take a single observation, extract some useful\nfeatures, and thereby classify the observation into one of a set of discrete classes.\nOne method for classifying text is to use rules handwritten by humans. Handwrit-\ntenrule-basedclassifierscanbecomponentsofstate-of-the-artsystemsinlanguage\nprocessing. Butrulescanbefragile,assituationsordatachangeovertime,andfor\nsometaskshumansaren’tnecessarilygoodatcomingupwiththerules.\nThe most common way of doing text classification in language processing is\nsupervised\nmachine insteadviasupervisedmachinelearning,thesubjectofthischapter. Insupervised\nlearning\nlearning,wehaveadatasetofinputobservations,eachassociatedwithsomecorrect\noutput (a ‘supervision signal’). The goal of the algorithm is to learn how to map\nfromanewobservationtoacorrectoutput.\nFormally, the task of supervised classification is to take an input x and a fixed\nset of output classes Y ={y ,y ,...,y } and return a predicted class y∈Y. For\n1 2 M\ntext classification, we’ll sometimes talk about c (for “class”) instead of y as our\noutput variable, and d (for “document”) instead of x as our input variable. In the\nsupervisedsituationwehaveatrainingsetofNdocumentsthathaveeachbeenhand-\nlabeledwithaclass: {(d ,c ),....,(d ,c )}. Ourgoalistolearnaclassifierthatis\n1 1 N N\ncapable of mapping from a new document d to its correct class c∈C, whereC is\nsomesetofusefuldocumentclasses.Aprobabilisticclassifieradditionallywilltell\nus the probability of the observation being in the class. This full distribution over\nthe classes can be useful information for downstream decisions; avoiding making\ndiscretedecisionsearlyoncanbeusefulwhencombiningsystems.\nMany kinds of machine learning algorithms are used to build classifiers. This\nchapter introduces naive Bayes; the following one introduces logistic regression.\nTheseexemplifytwowaysofdoingclassification. Generativeclassifierslikenaive\nBayes build a model of how a class could generate some input data. Given an ob-\nservation, theyreturntheclassmostlikelytohavegeneratedtheobservation. Dis-\ncriminativeclassifierslikelogisticregressioninsteadlearnwhatfeaturesfromthe\ninputaremostusefultodiscriminatebetweenthedifferentpossibleclasses. While\ndiscriminative systems are often more accurate and hence more commonly used,\ngenerativeclassifiersstillhavearole.\n4.1 Naive Bayes Classifiers\nnaiveBayes In this section we introduce the multinomial naive Bayes classifier, so called be-\nclassifier\ncause it is a Bayesian classifier that makes a simplifying (naive) assumption about 4.1 • NAIVEBAYESCLASSIFIERS 3\nhowthefeaturesinteract.\nTheintuitionoftheclassifierisshowninFig.4.1. Werepresentatextdocument\nbagofwords as if it were a bag of words, that is, an unordered set of words with their position\nignored,keepingonlytheirfrequencyinthedocument. Intheexampleinthefigure,\ninsteadofrepresentingthewordorderinallthephraseslike“Ilovethismovie”and\n“I would recommend it”, we simply note that the word I occurred 5 times in the\nentireexcerpt,thewordit6times,thewordslove,recommend,andmovieonce,and\nsoon.\nit 6\nI 5\nI love this movie! It's sweet, the 4\nbut with satirical humor. The fairy always lovetoit to 3\ndialogue is great and the it whimsical it I and 3\nadventure scenes are fun... and seen are anyone seen 2\nfriend\nIt manages to be whimsical happy dialogue yet 1\nand romantic while laughing adventure recommend would 1\nat the conventions of the whosweet of msa ot vir ieical it whimsical 1\nfairy tale genre. I would it I butto romantic I times 1\nrecommend it to just about several yet sweet 1\nhumor\nanyone. I've seen it several again it the satirical 1\ntimes, and I'm always happy th toe sces ne ee sn I would adventure 1\nto see it again whenever I the themanages genre 1\nfun times\nhave a friend who hasn't I and and fairy 1\nabout\nseen it yet! whenever while humor 1\nhave\nconventions have 1\nwith great 1\n… …\nFigure4.1 IntuitionofthemultinomialnaiveBayesclassifierappliedtoamoviereview. Thepositionofthe\nwordsisignored(thebag-of-wordsassumption)andwemakeuseofthefrequencyofeachword.\nNaiveBayesisaprobabilisticclassifier, meaningthatforadocumentd, outof\nallclassesc∈C theclassifierreturnstheclasscˆwhichhasthemaximumposterior\nˆ probabilitygiventhedocument. InEq.4.1weusethehatnotationˆ tomean“our\nargmax estimateofthecorrectclass”,andweuseargmaxtomeananoperationthatselects\nthe argument (in this case the class c) that maximizes a function (in this case the\nprobabilityP(c|d).\ncˆ = argmaxP(c|d) (4.1)\nc∈C\nBayesian This idea of Bayesian inference has been known since the work of Bayes (1763),\ninference\nand was first applied to text classification by Mosteller and Wallace (1964). The\nintuition of Bayesian classification is to use Bayes’ rule to transform Eq. 4.1 into\nother probabilities that have some useful properties. Bayes’ rule is presented in\nEq. 4.2; it gives us a way to break down any conditional probability P(x|y) into\nthreeotherprobabilities:\nP(y|x)P(x)\nP(x|y)= (4.2)\nP(y) 4.1 • NAIVEBAYESCLASSIFIERS 3\nhowthefeaturesinteract.\nTheintuitionoftheclassifierisshowninFig.4.1. Werepresentatextdocument\nbagofwords as if it were a bag of words, that is, an unordered set of words with their position\nignored,keepingonlytheirfrequencyinthedocument. Intheexampleinthefigure,\ninsteadofrepresentingthewordorderinallthephraseslike“Ilovethismovie”and\n“I would recommend it”, we simply note that the word I occurred 5 times in the\nentireexcerpt,thewordit6times,thewordslove,recommend,andmovieonce,and\nsoon.\nit 6\nI 5\nI love this movie! It's sweet, the 4\nbut with satirical humor. The fairy always lovetoit to 3\ndialogue is great and the it whimsical it I and 3\nadventure scenes are fun... and seen are anyone seen 2\nfriend\nIt manages to be whimsical happy dialogue yet 1\nand romantic while laughing adventure recommend would 1\nat the conventions of the whosweet of msa ot vir ieical it whimsical 1\nfairy tale genre. I would it I butto romantic I times 1\nrecommend it to just about several yet sweet 1\nhumor\nanyone. I've seen it several again it the satirical 1\ntimes, and I'm always happy th toe sces ne ee sn I would adventure 1\nto see it again whenever I the themanages genre 1\nfun times\nhave a friend who hasn't I and and fairy 1\nabout\nseen it yet! whenever while humor 1\nhave\nconventions have 1\nwith great 1\n… …\nFigure4.1 IntuitionofthemultinomialnaiveBayesclassifierappliedtoamoviereview. Thepositionofthe\nwordsisignored(thebag-of-wordsassumption)andwemakeuseofthefrequencyofeachword.\nNaiveBayesisaprobabilisticclassifier, meaningthatforadocumentd, outof\nallclassesc∈C theclassifierreturnstheclasscˆwhichhasthemaximumposterior\nˆ probabilitygiventhedocument. InEq.4.1weusethehatnotationˆ tomean“our\nargmax estimateofthecorrectclass”,andweuseargmaxtomeananoperationthatselects\nthe argument (in this case the class c) that maximizes a function (in this case the\nprobabilityP(c|d).\ncˆ = argmaxP(c|d) (4.1)\nc∈C\nBayesian This idea of Bayesian inference has been known since the work of Bayes (1763),\ninference\nand was first applied to text classification by Mosteller and Wallace (1964). The\nintuition of Bayesian classification is to use Bayes’ rule to transform Eq. 4.1 into\nother probabilities that have some useful properties. Bayes’ rule is presented in\nEq. 4.2; it gives us a way to break down any conditional probability P(x|y) into\nthreeotherprobabilities:\nP(y|x)P(x)\nP(x|y)= (4.2)\nP(y) 4.2 • TRAININGTHENAIVEBAYESCLASSIFIER 5\nToapplythenaiveBayesclassifiertotext,wewilluseeachwordinthedocuments\nasafeature,assuggestedabove,andweconsidereachofthewordsinthedocument\nbywalkinganindexthrougheverywordpositioninthedocument:\npositions ← allwordpositionsintestdocument\n(cid:89)\nc NB = argmaxP(c) P(w i|c) (4.9)\nc∈C\ni∈positions\nNaiveBayescalculations, likecalculationsforlanguagemodeling, aredoneinlog\nspace, to avoid underflow and increase speed. Thus Eq. 4.9 is generally instead\nexpressed1as\n(cid:88)\nc NB = argmaxlogP(c)+ logP(w i|c) (4.10)\nc∈C\ni∈positions\nByconsideringfeaturesinlogspace,Eq.4.10computesthepredictedclassasalin-\nearfunctionofinputfeatures. Classifiersthatusealinearcombinationoftheinputs\nto make a classification decision —like naive Bayes and also logistic regression—\nlinear arecalledlinearclassifiers.\nclassifiers\n4.2 Training the Naive Bayes Classifier\nHowcanwelearntheprobabilitiesP(c)andP(f|c)? Let’sfirstconsiderthemaxi-\ni\nmumlikelihoodestimate. We’llsimplyusethefrequenciesinthedata. Fortheclass\npriorP(c)weaskwhatpercentageofthedocumentsinourtrainingsetareineach\nclass c. Let N be the number of documents in our training data with class c and\nc\nN bethetotalnumberofdocuments. Then:\ndoc\nN\nPˆ(c)= c (4.11)\nN\ndoc\nTolearntheprobabilityP(f|c),we’llassumeafeatureisjusttheexistenceofaword\ni\nin the document’s bag of words, and so we’ll want P(w|c), which we compute as\ni\nthefractionoftimesthewordw appearsamongallwordsinalldocumentsoftopic\ni\nc. Wefirstconcatenatealldocumentswithcategorycintoonebig“categoryc”text.\nThenweusethefrequencyofw inthisconcatenateddocumenttogiveamaximum\ni\nlikelihoodestimateoftheprobability:\ncount(w,c)\nPˆ(w i|c) = (cid:80) i (4.12)\ncount(w,c)\nw∈V\nHerethevocabularyVconsistsoftheunionofallthewordtypesinallclasses,not\njustthewordsinoneclassc.\nThere is a problem, however, with maximum likelihood training. Imagine we\naretryingtoestimatethelikelihoodoftheword“fantastic”givenclasspositive,but\nsupposetherearenotrainingdocumentsthatbothcontaintheword“fantastic”and\nare classified as positive. Perhaps the word “fantastic” happens to occur (sarcasti-\ncally?) intheclass negative. Insuchacasetheprobabilityforthisfeaturewillbe\nzero:\ncount(“fantastic”,positive)\nPˆ(“fantastic”|positive) = (cid:80) =0 (4.13)\ncount(w,positive)\nw∈V\n1 Inpracticethroughoutthisbook,we’lluselogtomeannaturallog(ln)whenthebaseisnotspecified. EXERCISES 21\n2014,Droretal.2017).\nFeatureselectionisamethodofremovingfeaturesthatareunlikelytogeneralize\nwell.Featuresaregenerallyrankedbyhowinformativetheyareabouttheclassifica-\ninformation tiondecision. Averycommonmetric,informationgain,tellsushowmanybitsof\ngain\ninformationthepresenceofthewordgivesusforguessingtheclass. Otherfeature\nselection metrics include χ2, pointwise mutual information, and GINI index; see\nYangandPedersen(1997)foracomparisonandGuyonandElisseeff(2003)foran\nintroductiontofeatureselection.\nExercises\n4.1 Assume the following likelihoods for each word being part of a positive or\nnegativemoviereview,andequalpriorprobabilitiesforeachclass.\npos neg\nI 0.09 0.16\nalways 0.07 0.06\nlike 0.29 0.06\nforeign 0.04 0.15\nfilms 0.08 0.11\nWhat class will Naive bayes assign to the sentence “I always like foreign\nfilms.”?\n4.2 Given the following short movie reviews, each labeled with a genre, either\ncomedyoraction:\n1. fun,couple,love,love comedy\n2. fast,furious,shoot action\n3. couple,fly,fast,fun,fun comedy\n4. furious,shoot,shoot,fun action\n5. fly,fast,shoot,love action\nandanewdocumentD:\nfast,couple,shoot,fly\ncomputethemostlikelyclassforD.AssumeanaiveBayesclassifieranduse\nadd-1smoothingforthelikelihoods.\n4.3 Traintwomodels, multinomialnaiveBayesandbinarizednaiveBayes, both\nwith add-1 smoothing, on the following document counts for key sentiment\nwords,withpositiveornegativeclassassignedasnoted.\ndoc “good” “poor” “great” (class)\nd1. 3 0 3 pos\nd2. 0 1 2 pos\nd3. 1 3 0 neg\nd4. 1 5 2 neg\nd5. 0 2 0 neg\nUsebothnaiveBayesmodelstoassignaclass(posorneg)tothissentence:\nAgood,goodplotandgreatcharacters,butpooracting.\nRecallfrompage6thatwithnaiveBayestextclassification,wesimplyignore\n(throwout)anywordthatneveroccurredinthetrainingdocument. (Wedon’t\nthrowoutwordsthatappearinsomeclassesbutnotothers; that’swhatadd-\nonesmoothingisfor.) Dothetwomodelsagreeordisagree? EXERCISES 21\n2014,Droretal.2017).\nFeatureselectionisamethodofremovingfeaturesthatareunlikelytogeneralize\nwell.Featuresaregenerallyrankedbyhowinformativetheyareabouttheclassifica-\ninformation tiondecision. Averycommonmetric,informationgain,tellsushowmanybitsof\ngain\ninformationthepresenceofthewordgivesusforguessingtheclass. Otherfeature\nselection metrics include χ2, pointwise mutual information, and GINI index; see\nYangandPedersen(1997)foracomparisonandGuyonandElisseeff(2003)foran\nintroductiontofeatureselection.\nExercises\n4.1 Assume the following likelihoods for each word being part of a positive or\nnegativemoviereview,andequalpriorprobabilitiesforeachclass.\npos neg\nI 0.09 0.16\nalways 0.07 0.06\nlike 0.29 0.06\nforeign 0.04 0.15\nfilms 0.08 0.11\nWhat class will Naive bayes assign to the sentence “I always like foreign\nfilms.”?\n4.2 Given the following short movie reviews, each labeled with a genre, either\ncomedyoraction:\n1. fun,couple,love,love comedy\n2. fast,furious,shoot action\n3. couple,fly,fast,fun,fun comedy\n4. furious,shoot,shoot,fun action\n5. fly,fast,shoot,love action\nandanewdocumentD:\nfast,couple,shoot,fly\ncomputethemostlikelyclassforD.AssumeanaiveBayesclassifieranduse\nadd-1smoothingforthelikelihoods.\n4.3 Traintwomodels, multinomialnaiveBayesandbinarizednaiveBayes, both\nwith add-1 smoothing, on the following document counts for key sentiment\nwords,withpositiveornegativeclassassignedasnoted.\ndoc “good” “poor” “great” (class)\nd1. 3 0 3 pos\nd2. 0 1 2 pos\nd3. 1 3 0 neg\nd4. 1 5 2 neg\nd5. 0 2 0 neg\nUsebothnaiveBayesmodelstoassignaclass(posorneg)tothissentence:\nAgood,goodplotandgreatcharacters,butpooracting.\nRecallfrompage6thatwithnaiveBayestextclassification,wesimplyignore\n(throwout)anywordthatneveroccurredinthetrainingdocument. (Wedon’t\nthrowoutwordsthatappearinsomeclassesbutnotothers; that’swhatadd-\nonesmoothingisfor.) Dothetwomodelsagreeordisagree? 22 Chapter4 • NaiveBayes,TextClassification,andSentiment\nAggarwal,C.C.andC.Zhai.2012. Asurveyoftextclassi- Heckerman,D.,E.Horvitz,M.Sahami,andS.T.Dumais.\nficationalgorithms. InC.C.AggarwalandC.Zhai,eds, 1998.Abayesianapproachtofilteringjunke-mail.AAAI-\nMiningtextdata,163–222.Springer. 98WorkshoponLearningforTextCategorization.\nBayes,T.1763. AnEssayTowardSolvingaProbleminthe Hu,M.andB.Liu.2004.Miningandsummarizingcustomer\nDoctrineofChances,volume53.ReprintedinFacsimiles reviews.KDD.\nofTwoPapersbyBayes,HafnerPublishing,1963. Hutchinson, B., V. Prabhakaran, E. Denton, K. Webster,\nBerg-Kirkpatrick, T., D.Burkett, andD.Klein.2012. An Y. Zhong, and S. Denuyl. 2020. Social biases in NLP\nempiricalinvestigationofstatisticalsignificanceinNLP. modelsasbarriersforpersonswithdisabilities.ACL.\nEMNLP. Jaech,A.,G.Mulcaire,S.Hathi,M.Ostendorf,andN.A.\nBisani,M.andH.Ney.2004. Bootstrapestimatesforconfi- Smith.2016.Hierarchicalcharacter-wordmodelsforlan-\ndenceintervalsinASRperformanceevaluation.ICASSP. guageidentification. ACLWorkshoponNLPforSocial\nMedia.\nBishop,C.M.2006.Patternrecognitionandmachinelearn-\ning.Springer. Jauhiainen, T., M. Lui, M. Zampieri, T. Baldwin, and\nK.Linde´n.2019. Automaticlanguageidentificationin\nBlodgett,S.L.,S.Barocas,H.Daume´III,andH.Wallach.\ntexts:Asurvey.JAIR,65(1):675–682.\n2020. Language(technology)ispower:Acriticalsurvey\nof“bias”inNLP.ACL. Jurgens,D.,Y.Tsvetkov,andD.Jurafsky.2017. Incorpo-\nratingdialectalvariabilityforsociallyequitablelanguage\nBlodgett,S.L.,L.Green,andB.O’Connor.2016. Demo-\nidentification.ACL.\ngraphicdialectalvariationinsocialmedia: Acasestudy\nofAfrican-AmericanEnglish.EMNLP. Kiritchenko, S. and S. M. Mohammad. 2018. Examining\ngenderandracebiasintwohundredsentimentanalysis\nBorges,J.L.1964.Theanalyticallanguageofjohnwilkins.\nsystems.*SEM.\nIn Other inquisitions 1937–1952. University of Texas\nPress.Trans.RuthL.C.Simms. Liu,B.andL.Zhang.2012.Asurveyofopinionminingand\nsentimentanalysis. InC.C.AggarwalandC.Zhai,eds,\nCaliskan,A.,J.J.Bryson,andA.Narayanan.2017.Seman-\nMiningtextdata,415–464.Springer.\nticsderivedautomaticallyfromlanguagecorporacontain\nhuman-likebiases.Science,356(6334):183–186. Lui,M.andT.Baldwin.2011. Cross-domainfeatureselec-\ntionforlanguageidentification.IJCNLP.\nChinchor,N.,L.Hirschman,andD.L.Lewis.1993. Eval-\nuatingMessageUnderstandingsystems: Ananalysisof Lui,M.andT.Baldwin.2012.langid.py:Anoff-the-shelf\nthethirdMessageUnderstandingConference. Computa- languageidentificationtool.ACL.\ntionalLinguistics,19(3):409–449. Manning,C.D.,P.Raghavan,andH.Schu¨tze.2008. Intro-\nCrawford, K. 2017. The trouble with bias. Keynote at ductiontoInformationRetrieval.Cambridge.\nNeurIPS. Maron,M.E.1961. Automaticindexing: anexperimental\nDavidson,T.,D.Bhattacharya,andI.Weber.2019. Racial inquiry.JournaloftheACM,8(3):404–417.\nbias in hate speech and abusive language detection McCallum,A.andK.Nigam.1998. Acomparisonofevent\ndatasets.ThirdWorkshoponAbusiveLanguageOnline. modelsfornaivebayestextclassification.AAAI/ICML-98\nDiasOliva,T.,D.Antonialli,andA.Gomes.2021.Fighting WorkshoponLearningforTextCategorization.\nhatespeech,silencingdragqueens?artificialintelligence Metsis, V., I. Androutsopoulos, and G. Paliouras. 2006.\nin content moderation and risks to lgbtq voices online. Spam filtering with naive bayes-which naive bayes?\nSexuality&Culture,25:700–732. CEAS.\nDixon,L.,J.Li,J.Sorensen,N.Thain,andL.Vasserman. Minsky,M.1961. Stepstowardartificialintelligence. Pro-\n2018. Measuringandmitigatingunintendedbiasintext ceedingsoftheIRE,49(1):8–30.\nclassification.2018AAAI/ACMConferenceonAI,Ethics,\nMitchell,M.,S.Wu,A.Zaldivar,P.Barnes,L.Vasserman,\nandSociety.\nB.Hutchinson,E.Spitzer,I.D.Raji,andT.Gebru.2019.\nDror,R.,G.Baumer,M.Bogomolov,andR.Reichart.2017. Modelcardsformodelreporting.ACMFAccT.\nReplicability analysis for natural language processing:\nMosteller,F.andD.L.Wallace.1963. Inferenceinanau-\nTestingsignificancewithmultipledatasets.TACL,5:471–\nthorshipproblem:Acomparativestudyofdiscrimination\n–486.\nmethodsappliedtotheauthorshipofthedisputedfeder-\nDror, R., L. Peled-Cohen, S. Shlomov, and R. Reichart. alistpapers.JournaloftheAmericanStatisticalAssocia-\n2020. Statistical Significance Testing for Natural Lan- tion,58(302):275–309.\nguage Processing, volume 45 of Synthesis Lectures on\nMosteller,F.andD.L.Wallace.1964. InferenceandDis-\nHumanLanguageTechnologies.Morgan&Claypool.\nputedAuthorship:TheFederalist.Springer-Verlag.1984\nEfron,B.andR.J.Tibshirani.1993. Anintroductiontothe 2ndedition:AppliedBayesianandClassicalInference.\nbootstrap.CRCpress.\nMurphy,K.P.2012. Machinelearning:Aprobabilisticper-\nGillick,L.andS.J.Cox.1989.Somestatisticalissuesinthe spective.MITPress.\ncomparisonofspeechrecognitionalgorithms.ICASSP.\nNoreen,E.W.1989.ComputerIntensiveMethodsforTesting\nGuyon,I.andA.Elisseeff.2003.Anintroductiontovariable Hypothesis.Wiley.\nandfeatureselection.JMLR,3:1157–1182.\nPang,B.andL.Lee.2008. Opinionminingandsentiment\nHastie,T.,R.J.Tibshirani,andJ.H.Friedman.2001. The analysis.Foundationsandtrendsininformationretrieval,\nElementsofStatisticalLearning.Springer. 2(1-2):1–135."
    },
    "12": {
        "chapter": "6",
        "sections": "6.2",
        "topic": "Vector Semantics",
        "original_category": "CL",
        "original_text": "6.2 • VECTORSEMANTICS 5\nValence Arousal Dominance\ncourageous 8.05 5.5 7.38\nmusic 7.67 5.57 6.5\nheartbreak 2.45 5.65 3.58\ncub 6.71 3.95 4.24\nOsgood et al. (1957) noticed that in using these 3 numbers to represent the\nmeaning of a word, the model was representing each word as a point in a three-\ndimensional space, a vector whose three dimensions corresponded to the word’s\nratingonthethreescales. Thisrevolutionaryideathatwordmeaningcouldberep-\nresented as a point in space (e.g., that part of the meaning of heartbreak can be\nrepresentedasthepoint[2.45,5.65,3.58])wasthefirstexpressionofthevectorse-\nmanticsmodelsthatweintroducenext.\n6.2 Vector Semantics\nvector Vector semantics is the standard way to represent word meaning in NLP, helping\nsemantics\nusmodelmanyoftheaspectsofwordmeaningwesawintheprevioussection. The\nroots of the model lie in the 1950s when two big ideas converged: Osgood’s 1957\nidea mentioned above to use a point in three-dimensional space to represent the\nconnotationofaword,andtheproposalbylinguistslikeJoos(1950),Harris(1954),\nand Firth (1957) to define the meaning of a word by its distribution in language\nuse, meaning its neighboring words or grammatical environments. Their idea was\nthattwowordsthatoccurinverysimilardistributions(whoseneighboringwordsare\nsimilar)havesimilarmeanings.\nForexample,supposeyoudidn’tknowthemeaningofthewordongchoi(are-\ncentborrowingfromCantonese)butyouseeitinthefollowingcontexts:\n(6.1) Ongchoiisdelicioussauteedwithgarlic.\n(6.2) Ongchoiissuperboverrice.\n(6.3) ...ongchoileaveswithsaltysauces...\nAndsupposethatyouhadseenmanyofthesecontextwordsinothercontexts:\n(6.4) ...spinachsauteedwithgarlicoverrice...\n(6.5) ...chardstemsandleavesaredelicious...\n(6.6) ...collardgreensandothersaltyleafygreens\nThe fact that ongchoi occurs with words like rice and garlic and delicious and\nsalty,asdowordslikespinach,chard,andcollardgreensmightsuggestthatongchoi\nis a leafy green similar to these other leafy greens.1 We can do the same thing\ncomputationallybyjustcountingwordsinthecontextofongchoi.\nTheideaofvectorsemanticsistorepresentawordasapointinamultidimen-\nsional semantic space that is derived (in ways we’ll see) from the distributions of\nembeddings word neighbors. Vectors for representing words are called embeddings (although\nthe term is sometimes more strictly applied only to dense vectors like word2vec\n(Section 6.8), rather than sparse tf-idf or PPMI vectors (Section 6.3-Section 6.6)).\nTheword“embedding”derivesfromitsmathematicalsenseasamappingfromone\nspace or structure to another, although the meaning has shifted; see the end of the\nchapter.\n1 It’sinfactIpomoeaaquatica,arelativeofmorningglorysometimescalledwaterspinachinEnglish. 6.2 • VECTORSEMANTICS 5\nValence Arousal Dominance\ncourageous 8.05 5.5 7.38\nmusic 7.67 5.57 6.5\nheartbreak 2.45 5.65 3.58\ncub 6.71 3.95 4.24\nOsgood et al. (1957) noticed that in using these 3 numbers to represent the\nmeaning of a word, the model was representing each word as a point in a three-\ndimensional space, a vector whose three dimensions corresponded to the word’s\nratingonthethreescales. Thisrevolutionaryideathatwordmeaningcouldberep-\nresented as a point in space (e.g., that part of the meaning of heartbreak can be\nrepresentedasthepoint[2.45,5.65,3.58])wasthefirstexpressionofthevectorse-\nmanticsmodelsthatweintroducenext.\n6.2 Vector Semantics\nvector Vector semantics is the standard way to represent word meaning in NLP, helping\nsemantics\nusmodelmanyoftheaspectsofwordmeaningwesawintheprevioussection. The\nroots of the model lie in the 1950s when two big ideas converged: Osgood’s 1957\nidea mentioned above to use a point in three-dimensional space to represent the\nconnotationofaword,andtheproposalbylinguistslikeJoos(1950),Harris(1954),\nand Firth (1957) to define the meaning of a word by its distribution in language\nuse, meaning its neighboring words or grammatical environments. Their idea was\nthattwowordsthatoccurinverysimilardistributions(whoseneighboringwordsare\nsimilar)havesimilarmeanings.\nForexample,supposeyoudidn’tknowthemeaningofthewordongchoi(are-\ncentborrowingfromCantonese)butyouseeitinthefollowingcontexts:\n(6.1) Ongchoiisdelicioussauteedwithgarlic.\n(6.2) Ongchoiissuperboverrice.\n(6.3) ...ongchoileaveswithsaltysauces...\nAndsupposethatyouhadseenmanyofthesecontextwordsinothercontexts:\n(6.4) ...spinachsauteedwithgarlicoverrice...\n(6.5) ...chardstemsandleavesaredelicious...\n(6.6) ...collardgreensandothersaltyleafygreens\nThe fact that ongchoi occurs with words like rice and garlic and delicious and\nsalty,asdowordslikespinach,chard,andcollardgreensmightsuggestthatongchoi\nis a leafy green similar to these other leafy greens.1 We can do the same thing\ncomputationallybyjustcountingwordsinthecontextofongchoi.\nTheideaofvectorsemanticsistorepresentawordasapointinamultidimen-\nsional semantic space that is derived (in ways we’ll see) from the distributions of\nembeddings word neighbors. Vectors for representing words are called embeddings (although\nthe term is sometimes more strictly applied only to dense vectors like word2vec\n(Section 6.8), rather than sparse tf-idf or PPMI vectors (Section 6.3-Section 6.6)).\nTheword“embedding”derivesfromitsmathematicalsenseasamappingfromone\nspace or structure to another, although the meaning has shifted; see the end of the\nchapter.\n1 It’sinfactIpomoeaaquatica,arelativeofmorningglorysometimescalledwaterspinachinEnglish. 6.3 • WORDSANDVECTORS 7\naparticularword(definedbytherow)occursinaparticulardocument(definedby\nthecolumn). Thusfoolappeared58timesinTwelfthNight.\nAsYouLikeIt TwelfthNight JuliusCaesar HenryV\nbattle 1 0 7 13\ngood 114 80 62 89\nfool 36 58 1 4\nwit 20 15 2 3\nFigure6.2 Theterm-documentmatrixforfourwordsinfourShakespeareplays.Eachcell\ncontainsthenumberoftimesthe(row)wordoccursinthe(column)document.\nThe term-document matrix of Fig. 6.2 was first defined as part of the vector\nvectorspace space model of information retrieval (Salton, 1971). In this model, a document is\nmodel\nrepresentedasacountvector,acolumninFig.6.3.\nvector Toreviewsomebasiclinearalgebra, avectoris, atheart, justalistorarrayof\nnumbers. SoAsYouLikeItisrepresentedasthelist[1,114,36,20](thefirstcolumn\nvector in Fig.6.3) and Julius Caesar is representedas the list [7,62,1,2](the third\nvectorspace column vector). A vector space is a collection of vectors, and is characterized by\ndimension its dimension. Vectors in a 3-dimensional vector space have an element for each\ndimensionofthespace. Wewilllooselyrefertoavectorina4-dimensionalspace\nasa4-dimensionalvector,withoneelementalongeachdimension. Intheexample\ninFig.6.3,we’vechosentomakethedocumentvectorsofdimension4,justsothey\nfit on the page; in real term-document matrices, the document vectors would have\ndimensionality|V|,thevocabularysize.\nTheorderingofthenumbersinavectorspaceindicatesthedifferentdimensions\nonwhichdocumentsvary. Thefirstdimensionforboththesevectorscorrespondsto\nthe number of times the word battle occurs, and we can compare each dimension,\nnotingforexamplethatthevectorsforAsYouLikeItandTwelfthNighthavesimilar\nvalues(1and0,respectively)forthefirstdimension.\nAsYouLikeIt TwelfthNight JuliusCaesar HenryV\nbattle 1 0 7 13\ngood 114 80 62 89\nfool 36 58 1 4\nwit 20 15 2 3\nFigure6.3 Theterm-documentmatrixforfourwordsinfourShakespeareplays. Thered\nboxesshowthateachdocumentisrepresentedasacolumnvectoroflengthfour.\nWecanthinkofthevectorforadocumentasapointin|V|-dimensionalspace;\nthusthedocumentsinFig.6.3arepointsin4-dimensionalspace.Since4-dimensional\nspacesarehardtovisualize,Fig.6.4showsavisualizationintwodimensions;we’ve\narbitrarilychosenthedimensionscorrespondingtothewordsbattleandfool.\nTerm-document matrices were originally defined as a means of finding similar\ndocumentsforthetaskofdocumentinformationretrieval. Twodocumentsthatare\nsimilar will tend to have similar words, and if two documents have similar words\ntheir column vectors will tend to be similar. The vectors for the comedies As You\nLikeIt[1,114,36,20]andTwelfthNight[0,80,58,15]lookalotmorelikeeachother\n(more fools and wit than battles) than they look like Julius Caesar [7,62,1,2] or\nHenry V [13,89,4,3]. This is clear with the raw numbers; in the first dimension\n(battle)thecomedieshavelownumbersandtheothershavehighnumbers,andwe\ncan see it visually in Fig. 6.4; we’ll see very shortly how to quantify this intuition\nmoreformally. 6.3 • WORDSANDVECTORS 7\naparticularword(definedbytherow)occursinaparticulardocument(definedby\nthecolumn). Thusfoolappeared58timesinTwelfthNight.\nAsYouLikeIt TwelfthNight JuliusCaesar HenryV\nbattle 1 0 7 13\ngood 114 80 62 89\nfool 36 58 1 4\nwit 20 15 2 3\nFigure6.2 Theterm-documentmatrixforfourwordsinfourShakespeareplays.Eachcell\ncontainsthenumberoftimesthe(row)wordoccursinthe(column)document.\nThe term-document matrix of Fig. 6.2 was first defined as part of the vector\nvectorspace space model of information retrieval (Salton, 1971). In this model, a document is\nmodel\nrepresentedasacountvector,acolumninFig.6.3.\nvector Toreviewsomebasiclinearalgebra, avectoris, atheart, justalistorarrayof\nnumbers. SoAsYouLikeItisrepresentedasthelist[1,114,36,20](thefirstcolumn\nvector in Fig.6.3) and Julius Caesar is representedas the list [7,62,1,2](the third\nvectorspace column vector). A vector space is a collection of vectors, and is characterized by\ndimension its dimension. Vectors in a 3-dimensional vector space have an element for each\ndimensionofthespace. Wewilllooselyrefertoavectorina4-dimensionalspace\nasa4-dimensionalvector,withoneelementalongeachdimension. Intheexample\ninFig.6.3,we’vechosentomakethedocumentvectorsofdimension4,justsothey\nfit on the page; in real term-document matrices, the document vectors would have\ndimensionality|V|,thevocabularysize.\nTheorderingofthenumbersinavectorspaceindicatesthedifferentdimensions\nonwhichdocumentsvary. Thefirstdimensionforboththesevectorscorrespondsto\nthe number of times the word battle occurs, and we can compare each dimension,\nnotingforexamplethatthevectorsforAsYouLikeItandTwelfthNighthavesimilar\nvalues(1and0,respectively)forthefirstdimension.\nAsYouLikeIt TwelfthNight JuliusCaesar HenryV\nbattle 1 0 7 13\ngood 114 80 62 89\nfool 36 58 1 4\nwit 20 15 2 3\nFigure6.3 Theterm-documentmatrixforfourwordsinfourShakespeareplays. Thered\nboxesshowthateachdocumentisrepresentedasacolumnvectoroflengthfour.\nWecanthinkofthevectorforadocumentasapointin|V|-dimensionalspace;\nthusthedocumentsinFig.6.3arepointsin4-dimensionalspace.Since4-dimensional\nspacesarehardtovisualize,Fig.6.4showsavisualizationintwodimensions;we’ve\narbitrarilychosenthedimensionscorrespondingtothewordsbattleandfool.\nTerm-document matrices were originally defined as a means of finding similar\ndocumentsforthetaskofdocumentinformationretrieval. Twodocumentsthatare\nsimilar will tend to have similar words, and if two documents have similar words\ntheir column vectors will tend to be similar. The vectors for the comedies As You\nLikeIt[1,114,36,20]andTwelfthNight[0,80,58,15]lookalotmorelikeeachother\n(more fools and wit than battles) than they look like Julius Caesar [7,62,1,2] or\nHenry V [13,89,4,3]. This is clear with the raw numbers; in the first dimension\n(battle)thecomedieshavelownumbersandtheothershavehighnumbers,andwe\ncan see it visually in Fig. 6.4; we’ll see very shortly how to quantify this intuition\nmoreformally. 10 CHAPTER6 • VECTORSEMANTICSANDEMBEDDINGS\ninthetrainingcorpus; keepingwordsafteraboutthemostfrequent50,000orsois\ngenerallynothelpful). Sincemostofthesenumbersarezerothesearesparsevector\nrepresentations;thereareefficientalgorithmsforstoringandcomputingwithsparse\nmatrices.\nNowthatwehavesomeintuitions,let’smoveontoexaminethedetailsofcom-\nputingwordsimilarity. Afterwardswe’lldiscussmethodsforweightingcells.\n6.4 Cosine for measuring similarity\nTo measure similarity between two target words v and w, we need a metric that\ntakestwovectors(ofthesamedimensionality,eitherbothwithwordsasdimensions,\nhenceoflength|V|,orbothwithdocumentsasdimensions,oflength|D|)andgives\nameasureoftheirsimilarity.Byfarthemostcommonsimilaritymetricisthecosine\noftheanglebetweenthevectors.\nThecosine—likemostmeasuresforvectorsimilarityusedinNLP—isbasedon\ndotproduct thedotproductoperatorfromlinearalgebra,alsocalledtheinnerproduct:\ninnerproduct\nN\n(cid:88)\ndotproduct(v,w)=v·w= v iw i=v 1w 1+v 2w 2+...+v Nw N (6.7)\ni=1\nThedotproductactsasasimilaritymetricbecauseitwilltendtobehighjustwhen\nthetwovectorshavelargevaluesinthesamedimensions. Alternatively,vectorsthat\nhavezerosindifferentdimensions—orthogonalvectors—willhaveadotproductof\n0,representingtheirstrongdissimilarity.\nThis raw dot product, however, has a problem as a similarity metric: it favors\nvectorlength longvectors. Thevectorlengthisdefinedas\n(cid:118)\n(cid:117) N\n(cid:117)(cid:88)\n|v| = (cid:116) v2 (6.8)\ni\ni=1\nThedotproductishigherifavectorislonger,withhighervaluesineachdimension.\nMore frequent words have longer vectors, since they tend to co-occur with more\nwordsandhavehigherco-occurrencevalueswitheachofthem.Therawdotproduct\nthuswillbehigherforfrequentwords. Butthisisaproblem;we’dlikeasimilarity\nmetricthattellsushowsimilartwowordsareregardlessoftheirfrequency.\nWe modify the dot product to normalize for the vector length by dividing the\ndotproductbythelengthsofeachofthetwovectors. Thisnormalizeddotproduct\nturnsouttobethesameasthecosineoftheanglebetweenthetwovectors,following\nfromthedefinitionofthedotproductbetweentwovectorsaandb:\na·b = |a||b|cosθ\na·b\n= cosθ (6.9)\n|a||b|\ncosine Thecosinesimilaritymetricbetweentwovectorsvandwthuscanbecomputedas: 6.5 • TF-IDF:WEIGHINGTERMSINTHEVECTOR 13\nWeemphasizediscriminativewordslikeRomeoviatheinversedocumentfre-\nidf quencyoridftermweight(SparckJones,1972). Theidfisdefinedusingthefrac-\ntion N/df, where N is the total number of documents in the collection, and df is\nt t\nthenumberofdocumentsinwhichtermt occurs. Thefewerdocumentsinwhicha\ntermoccurs,thehigherthisweight. Thelowestweightof1isassignedtotermsthat\noccurinallthedocuments. It’susuallyclearwhatcountsasadocument: inShake-\nspearewewoulduseaplay; whenprocessingacollectionofencyclopediaarticles\nlikeWikipedia,thedocumentisaWikipediapage;inprocessingnewspaperarticles,\nthedocumentisasinglearticle. Occasionallyyourcorpusmightnothaveappropri-\natedocumentdivisionsandyoumightneedtobreakupthecorpusintodocuments\nyourselfforthepurposesofcomputingidf.\nBecause of the large number of documents in many collections, this measure\ntoo is usually squashed with a log function. The resulting definition for inverse\ndocumentfrequency(idf)isthus\n(cid:18) (cid:19)\nN\nidf t = log 10 df (6.13)\nt\nHere are some idf values for some words in the Shakespeare corpus, (along with\nthedocumentfrequencydfvaluesonwhichtheyarebased)rangingfromextremely\ninformativewordswhichoccurinonlyoneplaylikeRomeo,tothosethatoccurina\nfewlikesaladorFalstaff,tothosewhichareverycommonlikefoolorsocommon\nastobecompletelynon-discriminativesincetheyoccurinall37playslikegoodor\nsweet.3\nWord df idf\nRomeo 1 1.57\nsalad 2 1.27\nFalstaff 4 0.967\nforest 12 0.489\nbattle 21 0.246\nwit 34 0.037\nfool 36 0.012\ngood 37 0\nsweet 37 0\ntf-idf The tf-idf weighted value w t,d for word t in document d thus combines term\nfrequencytf (definedeitherbyEq.6.11orbyEq.6.12)withidffromEq.6.13:\nt,d\nw t,d =tf t,d×idf t (6.14)\nFig.6.9appliestf-idfweightingtotheShakespeareterm-documentmatrixinFig.6.2,\nusing the tf equation Eq. 6.12. Note that the tf-idf values for the dimension corre-\nspondingtothewordgoodhavenowallbecome0;sincethiswordappearsinevery\ndocument,thetf-idfweightingleadsittobeignored.Similarly,thewordfool,which\nappearsin36outofthe37plays,hasamuchlowerweight.\nThe tf-idf weighting is the way for weighting co-occurrence matrices in infor-\nmation retrieval, but also plays a role in many other aspects of natural language\nprocessing. It’salsoagreatbaseline,thesimplethingtotryfirst. We’lllookatother\nweightingslikePPMI(PositivePointwiseMutualInformation)inSection6.6.\n3 SweetwasoneofShakespeare’sfavoriteadjectives, afactprobablyrelatedtotheincreaseduseof\nsugarinEuropeanrecipesaroundtheturnofthe16thcentury(Jurafsky,2014,p.175). 14 CHAPTER6 • VECTORSEMANTICSANDEMBEDDINGS\nAsYouLikeIt TwelfthNight JuliusCaesar HenryV\nbattle 0.246 0 0.454 0.520\ngood 0 0 0 0\nfool 0.030 0.033 0.0012 0.0019\nwit 0.085 0.081 0.048 0.054\nFigure6.9 Aportionofthetf-idfweightedterm-documentmatrixforfourwordsinShake-\nspeare plays, showing a selection of 4 plays, using counts from Fig. 6.2. For example the\n0.085valueforwitinAsYouLikeItistheproductoftf=1+log (20)=2.301andidf=.037.\n10\nNotethattheidfweightinghaseliminatedtheimportanceoftheubiquitouswordgoodand\nvastlyreducedtheimpactofthealmost-ubiquitouswordfool.\n6.6 Pointwise Mutual Information (PMI)\nAnalternativeweightingfunctiontotf-idf,PPMI(positivepointwisemutualinfor-\nmation),isusedforterm-term-matrices,whenthevectordimensionscorrespondto\nwordsratherthandocuments.PPMIdrawsontheintuitionthatthebestwaytoweigh\ntheassociationbetweentwowordsistoaskhowmuchmorethetwowordsco-occur\ninourcorpusthanwewouldhaveaprioriexpectedthemtoappearbychance.\npointwise\nmutual\nPointwisemutualinformation(Fano,1961)4isoneofthemostimportantcon-\ninformation\nceptsinNLP.Itisameasureofhowoftentwoeventsxandyoccur,comparedwith\nwhatwewouldexpectiftheywereindependent:\nP(x,y)\nI(x,y)=log (6.16)\n2P(x)P(y)\nThepointwisemutualinformationbetweenatargetwordwandacontextword\nc(ChurchandHanks1989,ChurchandHanks1990)isthendefinedas:\nP(w,c)\nPMI(w,c)=log (6.17)\n2P(w)P(c)\nThe numerator tells us how often we observed the two words together (assuming\nwe compute probability by using the MLE). The denominator tells us how often\nwewouldexpectthetwowordstoco-occurassumingtheyeachoccurredindepen-\ndently; recall that the probability of two independent events both occurring is just\nthe product of the probabilities of the two events. Thus, the ratio gives us an esti-\nmateofhowmuchmorethetwowordsco-occurthanweexpectbychance. PMIis\nausefultoolwheneverweneedtofindwordsthatarestronglyassociated.\nPMI values range from negative to positive infinity. But negative PMI values\n(which imply things are co-occurring less often than we would expect by chance)\ntend to be unreliable unless our corpora are enormous. To distinguish whether\ntwowordswhoseindividualprobabilityiseach10−6 occurtogetherlessoftenthan\nchance, we would need to be certain that the probability of the two occurring to-\ngetherissignificantlylessthan10−12,andthiskindofgranularitywouldrequirean\nenormous corpus. Furthermore it’s not clear whether it’s even possible to evaluate\nsuch scores of ‘unrelatedness’ with human judgments. For this reason it is more\n4 PMIisbasedonthemutualinformationbetweentworandomvariablesXandY,definedas:\n(cid:88)(cid:88) P(x,y)\nI(X,Y)= P(x,y)log (6.15)\n2P(x)P(y)\nx y\nInaconfusionofterminology,Fanousedthephrasemutualinformationtorefertowhatwenowcall\npointwisemutualinformationandthephraseexpectationofthemutualinformationforwhatwenowcall\nmutualinformation"
    },
    "13": {
        "chapter": "6a",
        "sections": "6.4",
        "topic": "Cosine Similarity",
        "original_category": "CL",
        "original_text": "6.2 • VECTORSEMANTICS 5\nValence Arousal Dominance\ncourageous 8.05 5.5 7.38\nmusic 7.67 5.57 6.5\nheartbreak 2.45 5.65 3.58\ncub 6.71 3.95 4.24\nOsgood et al. (1957) noticed that in using these 3 numbers to represent the\nmeaning of a word, the model was representing each word as a point in a three-\ndimensional space, a vector whose three dimensions corresponded to the word’s\nratingonthethreescales. Thisrevolutionaryideathatwordmeaningcouldberep-\nresented as a point in space (e.g., that part of the meaning of heartbreak can be\nrepresentedasthepoint[2.45,5.65,3.58])wasthefirstexpressionofthevectorse-\nmanticsmodelsthatweintroducenext.\n6.2 Vector Semantics\nvector Vector semantics is the standard way to represent word meaning in NLP, helping\nsemantics\nusmodelmanyoftheaspectsofwordmeaningwesawintheprevioussection. The\nroots of the model lie in the 1950s when two big ideas converged: Osgood’s 1957\nidea mentioned above to use a point in three-dimensional space to represent the\nconnotationofaword,andtheproposalbylinguistslikeJoos(1950),Harris(1954),\nand Firth (1957) to define the meaning of a word by its distribution in language\nuse, meaning its neighboring words or grammatical environments. Their idea was\nthattwowordsthatoccurinverysimilardistributions(whoseneighboringwordsare\nsimilar)havesimilarmeanings.\nForexample,supposeyoudidn’tknowthemeaningofthewordongchoi(are-\ncentborrowingfromCantonese)butyouseeitinthefollowingcontexts:\n(6.1) Ongchoiisdelicioussauteedwithgarlic.\n(6.2) Ongchoiissuperboverrice.\n(6.3) ...ongchoileaveswithsaltysauces...\nAndsupposethatyouhadseenmanyofthesecontextwordsinothercontexts:\n(6.4) ...spinachsauteedwithgarlicoverrice...\n(6.5) ...chardstemsandleavesaredelicious...\n(6.6) ...collardgreensandothersaltyleafygreens\nThe fact that ongchoi occurs with words like rice and garlic and delicious and\nsalty,asdowordslikespinach,chard,andcollardgreensmightsuggestthatongchoi\nis a leafy green similar to these other leafy greens.1 We can do the same thing\ncomputationallybyjustcountingwordsinthecontextofongchoi.\nTheideaofvectorsemanticsistorepresentawordasapointinamultidimen-\nsional semantic space that is derived (in ways we’ll see) from the distributions of\nembeddings word neighbors. Vectors for representing words are called embeddings (although\nthe term is sometimes more strictly applied only to dense vectors like word2vec\n(Section 6.8), rather than sparse tf-idf or PPMI vectors (Section 6.3-Section 6.6)).\nTheword“embedding”derivesfromitsmathematicalsenseasamappingfromone\nspace or structure to another, although the meaning has shifted; see the end of the\nchapter.\n1 It’sinfactIpomoeaaquatica,arelativeofmorningglorysometimescalledwaterspinachinEnglish. 6.3 • WORDSANDVECTORS 7\naparticularword(definedbytherow)occursinaparticulardocument(definedby\nthecolumn). Thusfoolappeared58timesinTwelfthNight.\nAsYouLikeIt TwelfthNight JuliusCaesar HenryV\nbattle 1 0 7 13\ngood 114 80 62 89\nfool 36 58 1 4\nwit 20 15 2 3\nFigure6.2 Theterm-documentmatrixforfourwordsinfourShakespeareplays.Eachcell\ncontainsthenumberoftimesthe(row)wordoccursinthe(column)document.\nThe term-document matrix of Fig. 6.2 was first defined as part of the vector\nvectorspace space model of information retrieval (Salton, 1971). In this model, a document is\nmodel\nrepresentedasacountvector,acolumninFig.6.3.\nvector Toreviewsomebasiclinearalgebra, avectoris, atheart, justalistorarrayof\nnumbers. SoAsYouLikeItisrepresentedasthelist[1,114,36,20](thefirstcolumn\nvector in Fig.6.3) and Julius Caesar is representedas the list [7,62,1,2](the third\nvectorspace column vector). A vector space is a collection of vectors, and is characterized by\ndimension its dimension. Vectors in a 3-dimensional vector space have an element for each\ndimensionofthespace. Wewilllooselyrefertoavectorina4-dimensionalspace\nasa4-dimensionalvector,withoneelementalongeachdimension. Intheexample\ninFig.6.3,we’vechosentomakethedocumentvectorsofdimension4,justsothey\nfit on the page; in real term-document matrices, the document vectors would have\ndimensionality|V|,thevocabularysize.\nTheorderingofthenumbersinavectorspaceindicatesthedifferentdimensions\nonwhichdocumentsvary. Thefirstdimensionforboththesevectorscorrespondsto\nthe number of times the word battle occurs, and we can compare each dimension,\nnotingforexamplethatthevectorsforAsYouLikeItandTwelfthNighthavesimilar\nvalues(1and0,respectively)forthefirstdimension.\nAsYouLikeIt TwelfthNight JuliusCaesar HenryV\nbattle 1 0 7 13\ngood 114 80 62 89\nfool 36 58 1 4\nwit 20 15 2 3\nFigure6.3 Theterm-documentmatrixforfourwordsinfourShakespeareplays. Thered\nboxesshowthateachdocumentisrepresentedasacolumnvectoroflengthfour.\nWecanthinkofthevectorforadocumentasapointin|V|-dimensionalspace;\nthusthedocumentsinFig.6.3arepointsin4-dimensionalspace.Since4-dimensional\nspacesarehardtovisualize,Fig.6.4showsavisualizationintwodimensions;we’ve\narbitrarilychosenthedimensionscorrespondingtothewordsbattleandfool.\nTerm-document matrices were originally defined as a means of finding similar\ndocumentsforthetaskofdocumentinformationretrieval. Twodocumentsthatare\nsimilar will tend to have similar words, and if two documents have similar words\ntheir column vectors will tend to be similar. The vectors for the comedies As You\nLikeIt[1,114,36,20]andTwelfthNight[0,80,58,15]lookalotmorelikeeachother\n(more fools and wit than battles) than they look like Julius Caesar [7,62,1,2] or\nHenry V [13,89,4,3]. This is clear with the raw numbers; in the first dimension\n(battle)thecomedieshavelownumbersandtheothershavehighnumbers,andwe\ncan see it visually in Fig. 6.4; we’ll see very shortly how to quantify this intuition\nmoreformally. 10 CHAPTER6 • VECTORSEMANTICSANDEMBEDDINGS\ninthetrainingcorpus; keepingwordsafteraboutthemostfrequent50,000orsois\ngenerallynothelpful). Sincemostofthesenumbersarezerothesearesparsevector\nrepresentations;thereareefficientalgorithmsforstoringandcomputingwithsparse\nmatrices.\nNowthatwehavesomeintuitions,let’smoveontoexaminethedetailsofcom-\nputingwordsimilarity. Afterwardswe’lldiscussmethodsforweightingcells.\n6.4 Cosine for measuring similarity\nTo measure similarity between two target words v and w, we need a metric that\ntakestwovectors(ofthesamedimensionality,eitherbothwithwordsasdimensions,\nhenceoflength|V|,orbothwithdocumentsasdimensions,oflength|D|)andgives\nameasureoftheirsimilarity.Byfarthemostcommonsimilaritymetricisthecosine\noftheanglebetweenthevectors.\nThecosine—likemostmeasuresforvectorsimilarityusedinNLP—isbasedon\ndotproduct thedotproductoperatorfromlinearalgebra,alsocalledtheinnerproduct:\ninnerproduct\nN\n(cid:88)\ndotproduct(v,w)=v·w= v iw i=v 1w 1+v 2w 2+...+v Nw N (6.7)\ni=1\nThedotproductactsasasimilaritymetricbecauseitwilltendtobehighjustwhen\nthetwovectorshavelargevaluesinthesamedimensions. Alternatively,vectorsthat\nhavezerosindifferentdimensions—orthogonalvectors—willhaveadotproductof\n0,representingtheirstrongdissimilarity.\nThis raw dot product, however, has a problem as a similarity metric: it favors\nvectorlength longvectors. Thevectorlengthisdefinedas\n(cid:118)\n(cid:117) N\n(cid:117)(cid:88)\n|v| = (cid:116) v2 (6.8)\ni\ni=1\nThedotproductishigherifavectorislonger,withhighervaluesineachdimension.\nMore frequent words have longer vectors, since they tend to co-occur with more\nwordsandhavehigherco-occurrencevalueswitheachofthem.Therawdotproduct\nthuswillbehigherforfrequentwords. Butthisisaproblem;we’dlikeasimilarity\nmetricthattellsushowsimilartwowordsareregardlessoftheirfrequency.\nWe modify the dot product to normalize for the vector length by dividing the\ndotproductbythelengthsofeachofthetwovectors. Thisnormalizeddotproduct\nturnsouttobethesameasthecosineoftheanglebetweenthetwovectors,following\nfromthedefinitionofthedotproductbetweentwovectorsaandb:\na·b = |a||b|cosθ\na·b\n= cosθ (6.9)\n|a||b|\ncosine Thecosinesimilaritymetricbetweentwovectorsvandwthuscanbecomputedas:"
    },
    "14": {
        "chapter": "13",
        "sections": "13.2",
        "topic": "Machine Translation",
        "original_category": "CL",
        "original_text": "2 CHAPTER13 • MACHINETRANSLATION\ninmachinetranslation,thewordsofthetargetlanguagedon’tnecessarilyagreewith\nthewordsofthesourcelanguageinnumberororder. Considertranslatingthefol-\nlowingmade-upEnglishsentenceintoJapanese.\n(13.1) English: Hewrotealettertoafriend\nJapanese: tomodachinitegami-okaita\nfriend toletter wrote\nNote thatthe elementsof thesentences arein verydifferent placesin thedifferent\nlanguages. InEnglish,theverbisinthemiddleofthesentence,whileinJapanese,\ntheverbkaitacomesattheend. TheJapanesesentencedoesn’trequirethepronoun\nhe,whileEnglishdoes.\nSuchdifferencesbetweenlanguagescanbequitecomplex. Inthefollowingac-\ntualsentencefromtheUnitedNations,noticethemanychangesbetweentheChinese\nsentence (we’ve given in red a word-by-word gloss of the Chinese characters) and\nitsEnglishequivalentproducedbyhumantranslators.\n(13.2) 大会/GeneralAssembly在/on1982年/198212月/December10日/10通过\n了/adopted第37号/37th决议/resolution，核准了/approved第二\n次/second探索/exploration及/and和平peaceful利用/using外层空\n间/outerspace会议/conference的/of各项/various建议/suggestions。\nOn10December1982,theGeneralAssemblyadoptedresolution37in\nwhichitendorsedtherecommendationsoftheSecondUnitedNations\nConferenceontheExplorationandPeacefulUsesofOuterSpace.\nNote the many ways the English and Chinese differ. For example the order-\ningdiffers inmajorways; the Chineseorderof thenounphraseis “peacefulusing\nouterspaceconferenceofsuggestions”whiletheEnglishhas“suggestionsofthe...\nconference on peaceful use of outer space”). And the order differs in minor ways\n(the date is ordered differently). English requires the in many places that Chinese\ndoesn’t, and adds some details (like “in which” and “it”) that aren’t necessary in\nChinese. Chinese doesn’t grammatically mark plurality on nouns (unlike English,\nwhichhasthe“-s”in“recommendations”),andsotheChinesemustusethemodi-\nfier各项/varioustomakeitclearthatthereisnotjustonerecommendation. English\ncapitalizessomewordsbutnotothers. Encoder-decodernetworksareverysuccess-\nfulathandlingthesesortsofcomplicatedcasesofsequencemappings.\nWe’ll begin in the next section by considering the linguistic background about\nhowlanguagesvary,andtheimplicationsthisvariancehasforthetaskofMT.Then\nwe’llsketchoutthestandardalgorithm,givedetailsaboutthingslikeinputtokeniza-\ntion and creating training corpora of parallel sentences, give some more low-level\ndetailsabouttheencoder-decodernetwork,andfinallydiscusshowMTisevaluated,\nintroducingthesimplechrFmetric.\n13.1 Language Divergences and Typology\nThere are about 7,000 languages in the world. Some aspects of human language\nuniversal seemtobeuniversal,holdingtrueforeveryoneoftheselanguages,orarestatistical\nuniversals,holdingtrueformostoftheselanguages. Manyuniversalsarisefromthe\nfunctionalroleoflanguageasacommunicativesystembyhumans. Everylanguage,\nfor example, seems to have words for referring to people, for talking about eating\nand drinking, for being polite or not. There are also structural linguistic univer-\nsals;forexample,everylanguageseemstohavenounsandverbs(Chapter17),has 4 CHAPTER13 • MACHINETRANSLATION\ntoafriend,inwhichtheprepositiontoisfollowedbyitsargumentafriend. Arabic,\nwithaVSOorder,alsohastheverbbeforetheobjectandprepositions. Bycontrast,\nintheJapaneseexamplethatfollows,eachoftheseorderingsisreversed;theverbis\nprecededbyitsarguments,andthepostpositionfollowsitsargument.\n(13.3) English: Hewrotealettertoafriend\nJapanese: tomodachinitegami-okaita\nfriend toletter wrote\nArabic: katabtrisa¯lali s˙adq\nwrote letter tofriend\nOtherkindsoforderingpreferencesvaryidiosyncraticallyfromlanguagetolan-\nguage. InsomeSVOlanguages(likeEnglishandMandarin)adjectivestendtoap-\npearbeforenouns,whileinotherslanguageslikeSpanishandModernHebrew,ad-\njectivesappearafterthenoun:\n(13.4) Spanish bruja verde English green witch\n(a) (b)\nFigure13.2 Examples of other word order differences: (a) In German, adverbs occur in\ninitialpositionthatinEnglisharemorenaturallater,andtensedverbsoccurinsecondposi-\ntion. (b)InMandarin,prepositionphrasesexpressinggoalsoftenoccurpre-verbally,unlike\ninEnglish.\nFig. 13.2 shows examples of other word order differences. All of these word\norder differences between languages can cause problems for translation, requiring\nthesystemtodohugestructuralreorderingsasitgeneratestheoutput.\n13.1.2 LexicalDivergences\nOfcoursewealsoneedtotranslatetheindividualwordsfromonelanguagetoan-\nother. Foranytranslation,theappropriatewordcanvarydependingonthecontext.\nTheEnglishsource-languagewordbass,forexample,canappearinSpanishasthe\nfishlubinaorthemusicalinstrumentbajo. Germanusestwodistinctwordsforwhat\ninEnglishwouldbecalledawall: Wandforwallsinsideabuilding,andMauerfor\nwalls outside a building. Where English uses the word brother for any male sib-\nling, Chinese and many other languages have distinct words for older brother and\nyounger brother (Mandarin gege and didi, respectively). In all these cases, trans-\nlating bass, wall, or brother from English would require a kind of specialization,\ndisambiguating the different uses of a word. For this reason the fields of MT and\nWordSenseDisambiguation(AppendixG)arecloselylinked.\nSometimes one language places more grammatical constraints on word choice\nthananother. WesawabovethatEnglishmarksnounsforwhethertheyaresingular\norplural. Mandarindoesn’t. OrFrenchandSpanish, forexample, markgrammat-\nicalgenderonadjectives, soanEnglishtranslationintoFrenchrequiresspecifying\nadjectivegender.\nThewaythatlanguagesdifferinlexicallydividingupconceptualspacemaybe\nmorecomplexthanthisone-to-manytranslationproblem,leadingtomany-to-many 6 CHAPTER13 • MACHINETRANSLATION\nfusion atively clean boundaries, to fusion languages like Russian, in which a single affix\nmay conflate multiple morphemes, like -om in the word stolom (table-SG-INSTR-\nDECL1), which fuses the distinct morphological categories instrumental, singular,\nandfirstdeclension.\nTranslatingbetweenlanguageswithrichmorphologyrequiresdealingwithstruc-\nturebelowthewordlevel,andforthisreasonmodernsystemsgenerallyusesubword\nmodelslikethewordpieceorBPEmodelsofSection13.2.1.\n13.1.4 Referentialdensity\nFinally,languagesvaryalongatypologicaldimensionrelatedtothethingstheytend\ntoomit.Somelanguages,likeEnglish,requirethatweuseanexplicitpronounwhen\ntalkingaboutareferentthatisgiveninthediscourse. Inotherlanguages,however,\nwecansometimesomitpronounsaltogether,asthefollowingexamplefromSpanish\nshows1:\n(13.6) [Eljefe]idioconunlibro. 0/iMostro´ suhallazgoaundescifradorambulante.\n[Theboss]cameuponabook. [He]showedhisfindtoawanderingdecoder.\npro-drop Languagesthatcanomitpronounsarecalledpro-droplanguages. Evenamong\nthe pro-drop languages, there are marked differences in frequencies of omission.\nJapaneseandChinese,forexample,tendtoomitfarmorethandoesSpanish. This\ndimensionofvariationacrosslanguagesiscalledthedimensionofreferentialden-\nreferential sity. Wesaythatlanguagesthattendtousemorepronounsaremorereferentially\ndensity\ndensethanthosethatusemorezeros.Referentiallysparselanguages,likeChineseor\nJapanese,thatrequirethehearertodomoreinferentialworktorecoverantecedents\ncoldlanguage arealsocalledcoldlanguages. Languagesthataremoreexplicitandmakeiteasier\nhotlanguage forthehearerarecalledhotlanguages. Thetermshotandcoldareborrowedfrom\nMarshallMcLuhan’s1964distinctionbetweenhotmedialikemovies,whichfillin\nmanydetailsfortheviewer,versuscoldmedialikecomics,whichrequirethereader\ntodomoreinferentialworktofillouttherepresentation(Bickel,2003).\nTranslatingfromlanguageswithextensivepro-drop,likeChineseorJapanese,to\nnon-pro-droplanguageslikeEnglishcanbedifficultsincethemodelmustsomehow\nidentifyeachzeroandrecoverwhoorwhatisbeingtalkedaboutinordertoinsert\ntheproperpronoun.\n13.2 Machine Translation using Encoder-Decoder\nThestandardarchitectureforMTistheencoder-decodertransformerorsequence-\nto-sequence model, an architecture we saw for RNNs in Chapter 8. We’ll see the\ndetailsofhowtoapplythisarchitecturetotransformersinSection13.3,butfirstlet’s\ntalkabouttheoveralltask.\nMostmachinetranslationtasksmakethesimplificationthatwecantranslateeach\nsentenceindependently,sowe’lljustconsiderindividualsentencesfornow. Given\na sentence in a source language, the MT task is then to generate a corresponding\nsentence in a target language. For example, an MT system is given an English\nsentencelike\nThegreenwitcharrived\n1 Hereweusethe0/-notation;we’llintroducethisanddiscussthisissuefurtherinChapter23 13.2 • MACHINETRANSLATIONUSINGENCODER-DECODER 7\nandmusttranslateitintotheSpanishsentence:\nLlego´ labrujaverde\nMT uses supervised machine learning: at training time the system is given a\nlarge set of parallel sentences (each sentence in a source language matched with\na sentence in the target language), and learns to map source sentences into target\nsentences. Inpractice, ratherthanusingwords(asintheexampleabove), wesplit\nthesentencesintoasequenceofsubwordtokens(tokenscanbewords,orsubwords,\norindividualcharacters). Thesystemsarethentrainedtomaximizetheprobability\nof the sequence of tokens in the target language y 1,...,y m given the sequence of\ntokensinthesourcelanguagex 1,...,x n:\nP(y 1,...,y m x 1,...,x n) (13.7)\n|\nRatherthanusetheinputtokensdirectly,theencoder-decoderarchitecturecon-\nsists of two components, an encoder and a decoder. The encoder takes the input\nwordsx=[x 1,...,x n]andproducesanintermediatecontexth.Atdecodingtime,the\nsystemtakeshand,wordbyword,generatestheoutputy:\nh = encoder(x) (13.8)\ny t+1 = decoder(h,y 1,...,y t)) t [1,...,m] (13.9)\n∀ ∈\nInthenexttwosectionswe’lltalkaboutsubwordtokenization,andthenhowtoget\nparallel corpora for training, and then we’ll introduce the details of the encoder-\ndecoderarchitecture.\n13.2.1 Tokenization\nMachine translation systems use a vocabulary that is fixed in advance, and rather\nthan using space-separated words, this vocabulary is generated with subword to-\nkenization algorithms, like the BPE algorithm sketched in Chapter 2. A shared\nvocabularyisusedforthesourceandtargetlanguages,whichmakesiteasytocopy\ntokens(likenames)fromsourcetotarget. Usingsubwordtokenizationwithtokens\nsharedbetweenlanguagesmakesitnaturaltotranslatebetweenlanguageslikeEn-\nglishorHindithatusespacestoseparatewords,andlanguageslikeChineseorThai\nthatdon’t.\nWebuildthevocabularybyrunningasubwordtokenizationalgorithmonacor-\npusthatcontainsbothsourceandtargetlanguagedata.\nRatherthanthesimpleBPEalgorithmfromFig.??, modernsystemsoftenuse\nmorepowerfultokenizationalgorithms. Somesystems(likeBERT)useavariantof\nwordpiece BPEcalledthewordpiecealgorithm, whichinsteadofchoosingthemostfrequent\nsetoftokenstomerge,choosesmergesbasedonwhichonemostincreasesthelan-\nguagemodelprobabilityofthetokenization. Wordpiecesuseaspecialsymbolatthe\nbeginningofeachtoken;here’saresultingtokenizationfromtheGoogleMTsystem\n(Wuetal.,2016):\nwords: Jetmakersfeudoverseatwidthwithbigordersatstake\nwordpieces: Jet makers feud over seat width with big orders at stake\nThewordpiecealgorithmisgivenatrainingcorpusandadesiredvocabularysize\nV,andproceedsasfollows:\n1. Initializethewordpiecelexiconwithcharacters(forexampleasubsetofUni-\ncodecharacters,collapsingalltheremainingcharacterstoaspecialunknown\ncharactertoken). 8 CHAPTER13 • MACHINETRANSLATION\n2. RepeatuntilthereareVwordpieces:\n(a) Trainann-gramlanguagemodelonthetrainingcorpus,usingthecurrent\nsetofwordpieces.\n(b) Considerthesetofpossiblenewwordpiecesmadebyconcatenatingtwo\nwordpiecesfromthecurrentlexicon.Choosetheonenewwordpiecethat\nmostincreasesthelanguagemodelprobabilityofthetrainingcorpus.\nRecall that with BPE we had to specify the number of merges to perform; in\nwordpiece, by contrast, we specify the total vocabulary, which is a more intuitive\nparameter. Avocabularyof8Kto32Kwordpiecesiscommonlyused.\nAn even more commonly used tokenization algorithm is (somewhat ambigu-\nunigram ously)calledtheunigramalgorithm(Kudo,2018)orsometimestheSentencePiece\nSentencePiece algorithm, and is used in systems like ALBERT (Lan et al., 2020) and T5 (Raf-\nfel et al., 2020). (Because unigram is the default tokenization algorithm used in\nalibrarycalledSentencePiecethataddsausefulwrapperaroundtokenizationalgo-\nrithms(KudoandRichardson,2018),authorsoftensaytheyareusingSentencePiece\ntokenizationbutreallymeantheyareusingtheunigramalgorithm).\nInunigramtokenization,insteadofbuildingupavocabularybymergingtokens,\nwe start with a huge vocabulary of every individual unicode character plus all fre-\nquent sequences of characters (including all space-separated words, for languages\nwithspaces),anditerativelyremovesometokenstogettoadesiredfinalvocabulary\nsize. The algorithm is complex (involving suffix-trees for efficiently storing many\ntokens,andtheEMalgorithmforiterativelyassigningprobabilitiestotokens),sowe\ndon’tgiveithere,butseeKudo(2018)andKudoandRichardson(2018). Roughly\nspeaking the algorithm proceeds iteratively by estimating the probability of each\ntoken, tokenizing the input data using various tokenizations, then removing a per-\ncentageoftokensthatdon’toccurinhigh-probabilitytokenization,andtheniterates\nuntilthevocabularyhasbeenreduceddowntothedesirednumberoftokens.\nWhydoesunigramtokenizationworkbetterthanBPE?BPEtendstocreatelots\nofverysmallnon-meaningfultokens(becauseBPEcanonlycreatelargerwordsor\nmorphemes by merging characters one at a time), and it also tends to merge very\ncommon tokens, like the suffix ed, onto their neighbors. We can see from these\nexamples from Bostrom and Durrett (2020) that unigram tends to produce tokens\nthataremoresemanticallymeaningful:\nOriginal: corrupted Original: Completely preposterous suggestions\nBPE: cor rupted BPE: Comple t ely prep ost erous suggest ions\nUnigram: corrupt ed Unigram: Complete ly pre post er ous suggestion s\n13.2.2 CreatingtheTrainingdata\nparallelcorpus Machine translation models are trained on a parallel corpus, sometimes called a\nbitext, a text that appears in two (or more) languages. Large numbers of paral-\nEuroparl lel corpora are available. Some are governmental; the Europarl corpus (Koehn,\n2005),extractedfromtheproceedingsoftheEuropeanParliament,containsbetween\n400,000and2millionsentenceseachfrom21Europeanlanguages. TheUnitedNa-\ntionsParallelCorpuscontainsontheorderof10millionsentencesinthesixofficial\nlanguagesoftheUnitedNations(Arabic,Chinese,English,French,Russian,Span-\nish)Ziemskietal.(2016). Otherparallelcorporahavebeenmadefrommovieand\nTVsubtitles,liketheOpenSubtitlescorpus(LisonandTiedemann,2016),orfrom\ngeneralwebtext,liketheParaCrawlcorpusof223millionsentencepairsbetween\n23EUlanguagesandEnglishextractedfromtheCommonCrawlBan˜o´netal.(2020). 13.2 • MACHINETRANSLATIONUSINGENCODER-DECODER 9\nSentencealignment\nStandardtrainingcorporaforMTcomeasalignedpairsofsentences. Whencreat-\ningnewcorpora,forexampleforunderresourcedlanguagesornewdomains,these\nsentencealignmentsmustbecreated.Fig.13.4givesasamplehypotheticalsentence\nalignment.\nE1: “Good morning,\" said the little prince. F1: -Bonjour, dit le petit prince.\nE2: “Good morning,\" said the merchant. F2: -Bonjour, dit le marchand de pilules perfectionnées qui\napaisent la soif.\nE3: This was a merchant who sold pills that had\nF3: On en avale une par semaine et l'on n'éprouve plus le\nbeen perfected to quench thirst.\nbesoin de boire.\nE4: You just swallow one pill a week and you F4: -C’est une grosse économie de temps, dit le marchand.\nwon’t feel the need for anything to drink.\nE5: “They save a huge amount of time,\" said the merchant. F5: Les experts ont fait des calculs.\nE6: “Fifty−three minutes a week.\" F6: On épargne cinquante-trois minutes par semaine.\nE7: “If I had fifty−three minutes to spend?\" said the F7: “Moi, se dit le petit prince, si j'avais cinquante-trois minutes\nlittle prince to himself. à dépenser, je marcherais tout doucement vers une fontaine...\"\nE8: “I would take a stroll to a spring of fresh water”\nFigure13.4 A sample alignment between sentences in English and French, with sentences extracted from\nAntoinedeSaint-Exupery’sLePetitPrinceandahypotheticaltranslation. Sentencealignmenttakessentences\ne 1,...,en,and f 1,...,fn andfindsminimalsetsofsentencesthataretranslationsofeachother,includingsingle\nsentencemappingslike(e ,f ),(e ,f ),(e ,f ),(e ,f )aswellas2-1alignments(e /e ,f ),(e /e ,f ),andnull\n1 1 4 3 5 4 6 6 2 3 2 7 8 7\nalignments(f ).\n5\nGiventwodocumentsthataretranslationsofeachother,wegenerallyneedtwo\nstepstoproducesentencealignments:\n• acostfunctionthattakesaspanofsourcesentencesandaspanoftargetsen-\ntencesandreturnsascoremeasuringhowlikelythesespansaretobetransla-\ntions.\n• an alignment algorithm that takes these scores to find a good alignment be-\ntweenthedocuments.\nTo score the similarity of sentences across languages, we need to make use of\namultilingualembeddingspace,inwhichsentencesfromdifferentlanguagesare\nin the same embedding space (Artetxe and Schwenk, 2019). Given such a space,\ncosinesimilarityofsuchembeddingsprovidesanaturalscoringfunction(Schwenk,\n2018). ThompsonandKoehn(2019)givethefollowingcostfunctionbetweentwo\nsentencesorspansx,yfromthesourceandtargetdocumentsrespectively:\n(1 cos(x,y))nSents(x)nSents(y)\nc(x,y)= − (13.10)\nS s=11 −cos(x,y s)+ S s=11 −cos(x s,y)\nwherenSents()givesthe(cid:80)numberofsentences(cid:80)(thisbiasesthemetrictowardmany\nalignments of single sentences instead of aligning very large spans). The denom-\ninator helps to normalize the similarities, and so x 1,...,x S,y 1,...,y S, are randomly\nselectedsentencessampledfromtherespectivedocuments.\nUsually dynamic programming is used as the alignment algorithm (Gale and\nChurch, 1993), in a simple extension of the minimum edit distance algorithm we\nintroducedinChapter2.\nFinally,it’shelpfultodosomecorpuscleanupbyremovingnoisysentencepairs.\nThis can involve handwritten rules to remove low-precision pairs (for example re-\nmoving sentences that are too long, too short, have different URLs, or even pairs"
    },
    "15": {
        "chapter": "16",
        "sections": "16.2",
        "topic": "Automatic Speech Recognition (ASR)",
        "original_category": "CL",
        "original_text": "4 CHAPTER16 • AUTOMATICSPEECHRECOGNITIONANDTEXT-TO-SPEECH\ncorpus contains recordings of twenty different dinner parties in real homes, each\nwith four participants, and in three locations (kitchen, dining area, living room),\nrecorded both with distant room microphones and with body-worn mikes. The\nHKUST HKUST Mandarin Telephone Speech corpus has 1206 ten-minute telephone con-\nversationsbetweenspeakersofMandarinacrossChina,includingtranscriptsofthe\nconversations,whicharebetweeneitherfriendsorstrangers(Liuetal.,2006). The\nAISHELL-1 AISHELL-1corpuscontains170hoursofMandarinreadspeechofsentencestaken\nfrom various domains, read by different speakers mainly from northern China (Bu\netal.,2017).\nFigure16.1showstheroughpercentageofincorrectwords(theworderrorrate,\norWER,definedonpage16)fromstate-of-the-artsystemsonsomeofthesetasks.\nNote that the error rate on read speech (like the LibriSpeech audiobook corpus) is\naround2%;thisisasolvedtask,althoughthesenumberscomefromsystemsthatre-\nquireenormouscomputationalresources. Bycontrast,theerrorratefortranscribing\nconversationsbetweenhumansismuchhigher;5.8to11%fortheSwitchboardand\nCALLHOME corpora. The error rate is higher yet again for speakers of varieties\nlikeAfricanAmericanVernacularEnglish,andyetagainfordifficultconversational\ntasksliketranscriptionof4-speakerdinnerpartyspeech,whichcanhaveerrorrates\nas high as 81.3%. Character error rates (CER) are also much lower for read Man-\ndarinspeechthanfornaturalconversation.\nEnglishTasks WER%\nLibriSpeechaudiobooks960hourclean 1.4\nLibriSpeechaudiobooks960hourother 2.6\nSwitchboardtelephoneconversationsbetweenstrangers 5.8\nCALLHOMEtelephoneconversationsbetweenfamily 11.0\nSociolinguisticinterviews,CORAAL(AAL) 27.0\nCHiMe5dinnerpartieswithbody-wornmicrophones 47.9\nCHiMe5dinnerpartieswithdistantmicrophones 81.3\nChinese(Mandarin)Tasks CER%\nAISHELL-1Mandarinreadspeechcorpus 6.7\nHKUSTMandarinChinesetelephoneconversations 23.5\nFigure16.1 RoughWordErrorRates(WER=%ofwordsmisrecognized)reportedaround\n2020forASRonvariousAmericanEnglishrecognitiontasks,andcharactererrorrates(CER)\nfortwoChineserecognitiontasks.\n16.2 Feature Extraction for ASR: Log Mel Spectrum\nThefirststepinASRistotransformtheinputwaveformintoasequenceofacoustic\nfeaturevector feature vectors, each vector representing the information in a small time window\nof the signal. Let’s see how to convert a raw wavefile to the most commonly used\nfeatures,sequencesoflogmelspectrumvectors.Aspeechsignalprocessingcourse\nisrecommendedformoredetails.\n16.2.1 SamplingandQuantization\nThe input to a speech recognizer is a complex series of changes in air pressure.\nThese changes in air pressure obviously originate with the speaker and are caused 16.2 • FEATUREEXTRACTIONFORASR:LOGMELSPECTRUM 5\nbythespecificwaythatairpassesthroughtheglottisandouttheoralornasalcav-\nities. We represent sound waves by plotting the change in air pressure over time.\nOnemetaphorwhichsometimeshelpsinunderstandingthesegraphsisthatofaver-\ntical plate blocking the air pressure waves (perhaps in a microphone in front of a\nspeaker’smouth,ortheeardruminahearer’sear). Thegraphmeasurestheamount\nof compression or rarefaction (uncompression) of the air molecules at this plate.\nFigure16.2showsashortsegmentofawaveformtakenfromtheSwitchboardcorpus\noftelephonespeechofthevowel[iy]fromsomeonesaying“shejusthadababy”.\n0.02283\n0\n–0.01697\n0 0.03875\nTime (s)\nFigure16.2 Awaveformofaninstanceofthevowel[iy](thelastvowelintheword“baby”). They-axis\nshowsthelevelofairpressureaboveandbelownormalatmosphericpressure. Thex-axisshowstime. Notice\nthatthewaverepeatsregularly.\nThe first step in digitizing a sound wave like Fig. 16.2 is to convert the analog\nrepresentations(firstairpressureandthenanalogelectricsignalsinamicrophone)\nsampling intoadigitalsignal.Thisanalog-to-digitalconversionhastwosteps:samplingand\nquantization. Tosampleasignal,wemeasureitsamplitudeataparticulartime;the\nsamplingrateisthenumberofsamplestakenpersecond. Toaccuratelymeasurea\nwave,wemusthaveatleasttwosamplesineachcycle: onemeasuringthepositive\npart of the wave and one measuring the negative part. More than two samples per\ncycleincreasestheamplitudeaccuracy,butfewerthantwosamplescausesthefre-\nquencyofthewavetobecompletelymissed. Thus, themaximumfrequencywave\nthat can be measured is one whose frequency is half the sample rate (since every\ncycle needs two samples). This maximum frequency for a given sampling rate is\nNyquist calledtheNyquistfrequency. Mostinformationinhumanspeechisinfrequencies\nfrequency\nbelow 10,000 Hz; thus, a 20,000 Hz sampling rate would be necessary for com-\npleteaccuracy. Buttelephonespeechisfilteredbytheswitchingnetwork,andonly\nfrequencies less than 4,000 Hz are transmitted by telephones. Thus, an 8,000 Hz\nsampling rate is sufficient for telephone-bandwidth speech like the Switchboard\ncorpus,while16,000Hzsamplingisoftenusedformicrophonespeech.\nAlthoughusinghighersamplingratesproduceshigherASRaccuracy,wecan’t\ncombine different sampling rates for training and testing ASR systems. Thus if\nwearetestingonatelephonecorpuslikeSwitchboard(8KHzsampling),wemust\ndownsample our training corpus to 8 KHz. Similarly, if we are training on mul-\ntiple corpora and one of them includes telephone speech, we downsample all the\nwidebandcorporato8Khz.\nAmplitudemeasurementsarestoredasintegers,either8bit(valuesfrom-128–\n127)or16bit(valuesfrom-32768–32767).Thisprocessofrepresentingreal-valued\nquantization numbersasintegersiscalledquantization; allvaluesthatareclosertogetherthan\ntheminimumgranularity(thequantumsize)arerepresentedidentically. Wereferto\neachsampleattimeindexninthedigitized,quantizedwaveformasx[n].\nOnce data is quantized, it is stored in various formats. One parameter of these\nformats is the sample rate and sample size discussed above; telephone speech is\noften sampled at 8 kHz and stored as 8-bit samples, and microphone data is often\nsampledat16kHzandstoredas16-bitsamples.Anotherparameteristhenumberof 6 CHAPTER16 • AUTOMATICSPEECHRECOGNITIONANDTEXT-TO-SPEECH\nchannel channels.Forstereodataorfortwo-partyconversations,wecanstorebothchannels\ninthesamefileorwecanstoretheminseparatefiles.Afinalparameterisindividual\nsamplestorage—linearlyorcompressed.Onecommoncompressionformatusedfor\ntelephone speech is µ-law (often written u-law but still pronounced mu-law). The\nintuition of log compression algorithms like µ-law is that human hearing is more\nsensitive at small intensities than large ones; the log represents small values with\nmorefaithfulnessattheexpenseofmoreerroronlargevalues.Thelinear(unlogged)\nPCM valuesaregenerallyreferredtoaslinearPCMvalues(PCMstandsforpulsecode\nmodulation,butnevermindthat).Here’stheequationforcompressingalinearPCM\nsamplevaluexto8-bitµ-law,(whereµ=255for8bits):\nsgn(x)log(1+µ x)\nF(x) = | | 1 x 1 (16.1)\nlog(1+µ) − ≤ ≤\nThereareanumberofstandardfileformatsforstoringtheresultingdigitizedwave-\nfile,suchasMicrosoft’s.wavandApple’sAIFFallofwhichhavespecialheaders;\nsimpleheaderless“raw”filesarealsoused. Forexample,the.wavformatisasub-\nset of Microsoft’s RIFF format for multimedia files; RIFF is a general format that\ncanrepresentaseriesofnestedchunksofdataandcontrolinformation. Figure16.3\nshowsasimple.wavfilewithasingledatachunktogetherwithitsformatchunk.\nFigure16.3 Microsoftwavefileheaderformat,assumingsimplefilewithonechunk. Fol-\nlowingthis44-byteheaderwouldbethedatachunk.\n16.2.2 Windowing\nFrom the digitized, quantized representation of the waveform, we need to extract\nspectral features from a small window of speech that characterizes part of a par-\nticular phoneme. Inside this small window, we can roughly think of the signal as\nstationary stationary (that is, its statistical properties are constant within this region). (By\nnon-stationary contrast, in general, speech is a non-stationary signal, meaning that its statistical\npropertiesarenotconstantovertime). Weextractthisroughlystationaryportionof\nspeechbyusingawindowwhichisnon-zeroinsidearegionandzeroelsewhere,run-\nningthiswindowacrossthespeechsignalandmultiplyingitbytheinputwaveform\ntoproduceawindowedwaveform.\nframe The speech extracted from each window is called a frame. The windowing is\ncharacterized by three parameters: the window size or frame size of the window\nstride (its width in milliseconds), the frame stride, (also called shift or offset) between\nsuccessivewindows,andtheshapeofthewindow.\nTo extract the signal we multiply the value of the signal at time n, s[n] by the\nvalueofthewindowattimen,w[n]:\ny[n]=w[n]s[n] (16.2)\nrectangular The window shape sketched in Fig. 16.4 is rectangular; you can see the ex-\ntractedwindowedsignallooksjustliketheoriginalsignal. Therectangularwindow, 16.2 • FEATUREEXTRACTIONFORASR:LOGMELSPECTRUM 7\nWindow\n25 ms\nShift\nWindow\n10\nms 25 ms\nShift\nWindow\n10\nms 25 ms\nFigure16.4 Windowing,showinga25msrectangularwindowwitha10msstride.\nhowever,abruptlycutsoffthesignalatitsboundaries,whichcreatesproblemswhen\nwedoFourieranalysis. Forthisreason,foracousticfeaturecreationwemorecom-\nHamming monly use the Hamming window, which shrinks the values of the signal toward\nzero at the window boundaries, avoiding discontinuities. Figure 16.5 shows both;\ntheequationsareasfollows(assumingawindowthatisLframeslong):\n(cid:26)\n1 0 n L 1\nrectangular w[n] = ≤ ≤ − (16.3)\n0 otherwise\n(cid:26) 0.54 0.46cos(2πn) 0 n L 1\nHamming w[n] = − L ≤ ≤ − (16.4)\n0 otherwise\n0.4999\n0\n–0.5\n0 0.0475896\nTime (s)\nRectangular window Hamming window\n0.4999 0.4999\n0 0\n–0.5 –0.4826\n0.00455938 0.0256563 0.00455938 0.0256563\nTime (s) Time (s)\nFigure16.5 WindowingasinewavewiththerectangularorHammingwindows.\n16.2.3 DiscreteFourierTransform\nThenextstepistoextractspectralinformationforourwindowedsignal;weneedto\nknow how much energy the signal contains at different frequency bands. The tool 8 CHAPTER16 • AUTOMATICSPEECHRECOGNITIONANDTEXT-TO-SPEECH\nfor extracting spectral information for discrete frequency bands for a discrete-time\nDiscrete\nFourier (sampled)signalisthediscreteFouriertransformorDFT.\ntransf Do Frm T TheinputtotheDFTisawindowedsignalx[n]...x[m],andtheoutput,foreach\nof N discrete frequency bands, is a complex number X[k] representing the magni-\ntude and phase of that frequency component in the original signal. If we plot the\nmagnitude against the frequency, we can visualize the spectrum (see Appendix H\nfor more on spectra). For example, Fig. 16.6 shows a 25 ms Hamming-windowed\nportion of a signal and its spectrum as computed by a DFT (with some additional\nsmoothing).\n0.04414\n0\n–0.04121\n0.0141752 0.039295 0 8000\nTime (s) Frequency (Hz)\n)zH/Bd(\nlevel\nerusserp\ndnuoS\n20\n0\n–20\n(a) (b)\nFigure16.6 (a) A 25 ms Hamming-windowed portion of a signal from the vowel [iy]\nand (b)itsspectrumcomputedbyaDFT.\nWe do not introduce the mathematical details of the DFT here, except to note\nEuler’sformula thatFourieranalysisreliesonEuler’sformula,with jastheimaginaryunit:\nejθ =cosθ+jsinθ (16.5)\nAsabriefreminderforthosestudentswhohavealreadystudiedsignalprocessing,\ntheDFTisdefinedasfollows:\nN 1\nX[k]=(cid:88)− x[n]e−j2 Nπkn\n(16.6)\nn=0\nfastFourier AcommonlyusedalgorithmforcomputingtheDFTisthefastFouriertransform\ntransform\nFFT orFFT.ThisimplementationoftheDFTisveryefficientbutonlyworksforvalues\nofN thatarepowersof2.\n16.2.4 MelFilterBankandLog\nThe results of the FFT tell us the energy at each frequency band. Human hearing,\nhowever,isnotequallysensitiveatallfrequencybands;itislesssensitiveathigher\nfrequencies. This bias toward low frequencies helps human recognition, since in-\nformationinlowfrequencies(likeformants)iscrucialfordistinguishingvowelsor\nnasals, whileinformationinhighfrequencies(likestopburstsorfricativenoise)is\nless crucial for successful recognition. Modeling this human perceptual property\nimprovesspeechrecognitionperformanceinthesameway.\nWeimplementthisintuitionbycollectingenergies,notequallyateachfrequency\nmel band, butaccordingtothemelscale, anauditoryfrequencyscale. Amel(Stevens\netal.1937,StevensandVolkmann1940)isaunitofpitch. Pairsofsoundsthatare\nperceptuallyequidistantinpitchareseparatedbyanequalnumberofmels. Themel"
    } 
}