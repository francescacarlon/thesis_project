{
    "1": {
        "chapter": "17",
        "section": "17.1",
        "chapter_title": "Title of Sections 17.1",
        "original_category": "",
        "original_text": "2 CHAPTER17 • SEQUENCELABELINGFORPARTSOFSPEECHANDNAMEDENTITIES\n17.1 (Mostly) English Word Classes\nUntilnowwehavebeenusingpart-of-speechtermslikenounandverbratherfreely.\nIn this section we give more complete definitions. While word classes do have\nsemantictendencies—adjectives, forexample, oftendescribepropertiesandnouns\npeople—partsofspeecharedefinedinsteadbasedontheirgrammaticalrelationship\nwithneighboringwordsorthemorphologicalpropertiesabouttheiraffixes.\nTag Description Example\nssalCnepO\nADJ Adjective:nounmodifiersdescribingproperties red,young,awesome\nADV Adverb:verbmodifiersoftime,place,manner very,slowly,home,yesterday\nNOUN wordsforpersons,places,things,etc. algorithm,cat,mango,beauty\nVERB wordsforactionsandprocesses draw,provide,go\nPROPN Propernoun:nameofaperson,organization,place,etc.. Regina,IBM,Colorado\nINTJ Interjection:exclamation,greeting,yes/noresponse,etc. oh,um,yes,hello\nsdroWssalCdesolC\nADP Adposition (Preposition/Postposition): marks a noun’s in,on,by,under\nspacial,temporal,orotherrelation\nAUX Auxiliary:helpingverbmarkingtense,aspect,mood,etc., can,may,should,are\nCCONJ CoordinatingConjunction:joinstwophrases/clauses and,or,but\nDET Determiner:marksnounphraseproperties a,an,the,this\nNUM Numeral one,two,2026,11:00,hundred\nPART Particle: afunctionwordthatmustbeassociatedwithan- ’s,not,(infinitive)to\notherword\nPRON Pronoun:ashorthandforreferringtoanentityorevent she,who,I,others\nSCONJ Subordinating Conjunction: joins a main clause with a whether,because\nsubordinateclausesuchasasententialcomplement\nrehtO\nPUNCT Punctuation , ,()\n˙\nSYM Symbolslike$oremoji $,%\nX Other asdf,qwfg\nFigure17.1 The17partsofspeechintheUniversalDependenciestagset(deMarneffeetal.,2021).Features\ncanbeaddedtomakefiner-graineddistinctions(withpropertieslikenumber,case,definiteness,andsoon).\nclosedclass Parts of speech fall into two broad categories: closed class and open class.\nopenclass Closed classes are those with relatively fixed membership, such as prepositions—\nnewprepositionsarerarelycoined. Bycontrast,nounsandverbsareopenclasses—\nnewnounsandverbslikeiPhoneortofaxarecontinuallybeingcreatedorborrowed.\nfunctionword Closedclasswordsaregenerallyfunctionwordslikeof,it,and,oryou,whichtend\ntobeveryshort,occurfrequently,andoftenhavestructuringusesingrammar.\nFourmajoropenclassesoccurinthelanguagesoftheworld: nouns(including\npropernouns),verbs,adjectives,andadverbs,aswellasthesmalleropenclassof\ninterjections. Englishhasallfive,althoughnoteverylanguagedoes.\nnoun Nounsarewordsforpeople,places,orthings,butincludeothersaswell. Com-\ncommonnoun monnounsincludeconcretetermslikecatandmango,abstractionslikealgorithm\nandbeauty,andverb-liketermslikepacingasinHispacingtoandfrobecamequite\nannoying. Nouns in English can occur with determiners (a goat, this bandwidth)\ntakepossessives(IBM’sannualrevenue),andmayoccurintheplural(goats,abaci).\ncountnoun Many languages, including English, divide common nouns into count nouns and\nmassnoun mass nouns. Count nouns can occur in the singular and plural (goat/goats, rela-\ntionship/relationships) and can be counted (one goat, two goats). Mass nouns are\nusedwhensomethingisconceptualizedasahomogeneousgroup.Sosnow,salt,and\npropernoun communismarenotcounted(i.e.,*twosnowsor*twocommunisms).Propernouns,\nlikeRegina,Colorado,andIBM,arenamesofspecificpersonsorentities. 17.1 • (MOSTLY)ENGLISHWORDCLASSES 3\nverb Verbs refer to actions and processes, including main verbs like draw, provide,\nandgo.Englishverbshaveinflections(non-third-person-singular(eat),third-person-\nsingular (eats), progressive (eating), past participle (eaten)). While many scholars\nbelievethatallhumanlanguageshavethecategoriesofnounandverb,othershave\narguedthatsomelanguages,suchasRiauIndonesianandTongan,don’tevenmake\nthisdistinction(Broschart1997;Evans2000;Gil2000).\nadjective Adjectives often describe properties or qualities of nouns, like color (white,\nblack), age (old, young), and value (good, bad), but there are languages without\nadjectives. In Korean, for example, the words corresponding to English adjectives\nact as a subclass of verbs, so what is in English an adjective “beautiful” acts in\nKoreanlikeaverbmeaning“tobebeautiful”.\nadverb Adverbsareahodge-podge.Alltheitalicizedwordsinthisexampleareadverbs:\nActually,Iranhomeextremelyquicklyyesterday\nAdverbs generally modify something (often verbs, hence the name “adverb”, but\nlocative also other adverbs and entire verb phrases). Directional adverbs or locative ad-\ndegree verbs(home,here,downhill)specifythedirectionorlocationofsomeaction;degree\nadverbs(extremely,very,somewhat)specifytheextentofsomeaction,process,or\nmanner property;manneradverbs(slowly,slinkily,delicately)describethemannerofsome\ntemporal actionorprocess;andtemporaladverbsdescribethetimethatsomeactionorevent\ntookplace(yesterday,Monday).\ninterjection Interjections(oh,hey,alas,uh,um)areasmalleropenclassthatalsoincludes\ngreetings(hello,goodbye)andquestionresponses(yes,no,uh-huh).\npreposition Englishadpositionsoccurbeforenouns,hencearecalledprepositions.Theycan\nindicatespatialortemporalrelations,whetherliteral(onit,beforethen,bythehouse)\normetaphorical(ontime,withgusto,besideherself),andrelationslikemarkingthe\nagentinHamletwaswrittenbyShakespeare.\nparticle Aparticleresemblesaprepositionoranadverbandisusedincombinationwith\na verb. Particles often have extended meanings that aren’t quite the same as the\nprepositionstheyresemble, asintheparticleoverinsheturnedthepaperover. A\nphrasalverb verb and a particle acting as a single unit is called a phrasal verb. The meaning\nof phrasal verbs is often non-compositional—not predictable from the individual\nmeanings of the verb and the particle. Thus, turn down means ‘reject’, rule out\n‘eliminate’,andgoon‘continue’.\ndeterminer Determinerslikethisandthat(thischapter,thatpage)canmarkthestartofan\narticle Englishnounphrase. Articleslikea,an,andthe,areatypeofdeterminerthatmark\ndiscourse properties of the noun and are quite frequent; the is the most common\nwordinwrittenEnglish,withaandanrightbehind.\nconjunction Conjunctions join two phrases, clauses, or sentences. Coordinating conjunc-\ntionslikeand,or,andbutjointwoelementsofequalstatus. Subordinatingconjunc-\ntions are used when one of the elements has some embedded status. For example,\nthesubordinatingconjunctionthatin“Ithoughtthatyoumightlikesomemilk”links\nthemainclauseIthoughtwiththesubordinateclauseyoumightlikesomemilk.This\nclause is called subordinate because this entire clause is the “content” of the main\nverbthought. Subordinatingconjunctionslikethatwhichlinkaverbtoitsargument\ncomplementizer inthiswayarealsocalledcomplementizers.\npronoun Pronouns act as a shorthand for referring to an entity or event. Personal pro-\nnounsrefertopersonsorentities(you,she,I,it,me,etc.). Possessivepronounsare\nformsofpersonalpronounsthatindicateeitheractualpossessionormoreoftenjust\nanabstractrelationbetweenthepersonandsomeobject(my,your,his,her,its,one’s,\nwh our,their). Wh-pronouns(what,who,whom,whoever)areusedincertainquestion 4 CHAPTER17 • SEQUENCELABELINGFORPARTSOFSPEECHANDNAMEDENTITIES\nforms,oractascomplementizers(Frida,whomarriedDiego...).\nauxiliary Auxiliaryverbsmarksemanticfeaturesofamainverbsuchasitstense,whether\nit is completed (aspect), whether it is negated (polarity), and whether an action is\nnecessary, possible, suggested, or desired (mood). English auxiliaries include the\ncopula copulaverbbe,thetwoverbsdoandhave,forms,aswellasmodalverbsusedto\nmodal markthemoodassociatedwiththeeventdepictedbythemainverb: canindicates\nabilityorpossibility,maypermissionorpossibility,mustnecessity.\nAnEnglish-specifictagset,thePennTreebanktagset(Marcusetal.,1993),shown\nin Fig. 17.2, has been used to label many syntactically annotated corpora like the\nPennTreebankcorpora,soitisworthknowingabout.\nTag Description Example Tag Description Example Tag Description Example\nCC coord.conj. and,but,or NNP propernoun,sing. IBM TO infinitiveto to\nCD cardinalnumber one,two NNPS propernoun,plu. Carolinas UH interjection ah,oops\nDT determiner a,the NNS noun,plural llamas VB verbbase eat\nEX existential‘there’ there PDT predeterminer all,both VBD verbpasttense ate\nFW foreignword meaculpa POS possessiveending ’s VBG verbgerund eating\nIN preposition/ of,in,by PRP personalpronoun I,you,he VBN verb past partici- eaten\nsubordin-conj ple\nJJ adjective yellow PRP$ possess.pronoun your VBP verbnon-3sg-pr eat\nJJR comparativeadj bigger RB adverb quickly VBZ verb3sgpres eats\nJJS superlativeadj wildest RBR comparativeadv faster WDT wh-determ. which,that\nLS listitemmarker 1,2,One RBS superlatv.adv fastest WP wh-pronoun what,who\nMD modal can,should RP particle up,off WP$ wh-possess. whose\nNN singormassnoun llama SYM symbol +,%,& WRB wh-adverb how,where\nFigure17.2 PennTreebankcore36part-of-speechtags.\nBelowweshowsomeexampleswitheachwordtaggedaccordingtoboththeUD\n(in blue) and Penn (in red) tagsets. Notice that the Penn tagset distinguishes tense\nandparticiplesonverbs,andhasaspecialtagfortheexistentialthereconstructionin\nEnglish. NotethatsinceLondonJournalofMedicineisapropernoun,bothtagsets\nmarkitscomponentnounsasPROPN/NNP,includingjournalandmedicine,which\nmightotherwisebelabeledascommonnouns(NOUN/NN).\n(17.1) There/PRON/EXare/VERB/VBP70/NUM/CDchildren/NOUN/NNS\nthere/ADV/RB./PUNC/.\n(17.2) Preliminary/ADJ/JJfindings/NOUN/NNSwere/AUX/VBD\nreported/VERB/VBNin/ADP/INtoday/NOUN/NN’s/PART/POS\nLondon/PROPN/NNPJournal/PROPN/NNPof/ADP/INMedicine/PROPN/NNP\n17.2 Part-of-Speech Tagging\npart-of-speech Part-of-speechtaggingistheprocessofassigningapart-of-speechtoeachwordin\ntagging\na text. The input is a sequence x ,x ,...,x of (tokenized) words and a tagset, and\n1 2 n\ntheoutputisasequencey ,y ,...,y oftags,eachoutputy correspondingexactlyto\n1 2 n i\noneinputx,asshownintheintuitioninFig.17.3.\ni\nambiguous Taggingisadisambiguationtask;wordsareambiguous—havemorethanone\npossible part-of-speech—and the goal is to find the correct tag for the situation.\nFor example, book can be a verb (book that flight) or a noun (hand me that book).\nThat can be a determiner (Does that flight serve dinner) or a complementizer (I 24 CHAPTER17 • SEQUENCELABELINGFORPARTSOFSPEECHANDNAMEDENTITIES\nLog-linear models for POS tagging were introduced by Ratnaparkhi (1996),\nwhointroducedasystemcalledMXPOSTwhichimplementedamaximumentropy\nMarkov model (MEMM), a slightly simpler version of a CRF. Around the same\ntime,sequencelabelerswereappliedtothetaskofnamedentitytagging,firstwith\nHMMs (Bikel et al., 1997) and MEMMs (McCallum et al., 2000), and then once\nCRFs were developed (Lafferty et al. 2001), they were also applied to NER (Mc-\nCallumandLi,2003). Awideexplorationoffeaturesfollowed(Zhouetal.,2005).\nNeural approaches to NER mainly follow from the pioneering results of Collobert\netal.(2011),whoappliedaCRFontopofaconvolutionalnet. BiLSTMswithword\nand character-based embeddings as input followed shortly and became a standard\nneural algorithm for NER (Huang et al. 2015, Ma and Hovy 2016, Lample et al.\n2016)followedbythemorerecentuseofTransformersandBERT.\nTheideaofusinglettersuffixesforunknownwordsisquiteold;theearlyKlein\nand Simmons (1963) system checked all final letter suffixes of lengths 1-5. The\nunknownwordfeaturesdescribedonpage17comemainlyfromRatnaparkhi(1996),\nwithaugmentationsfromToutanovaetal.(2003)andManning(2011).\nStateoftheartPOStaggersuseneuralalgorithms,eitherbidirectionalRNNsor\nTransformerslikeBERT;seeChapter8toChapter11. HMM(Brants2000;Thede\nandHarper1999)andCRFtaggeraccuraciesarelikelyjustatadlower.\nManning(2011)investigatestheremaining2.7%oferrorsinahigh-performing\ntagger(Toutanovaetal.,2003). Hesuggeststhatathirdorhalfoftheseremaining\nerrorsareduetoerrorsorinconsistenciesinthetrainingdata,athirdmightbesolv-\nablewithricherlinguisticmodels, andfortheremainderthetaskisunderspecified\norunclear.\nSupervised tagging relies heavily on in-domain training data hand-labeled by\nexperts. Waystorelaxthisassumptionincludeunsupervisedalgorithmsforcluster-\ningwordsintopart-of-speech-likeclasses,summarizedinChristodoulopoulosetal.\n(2010),andwaystocombinelabeledandunlabeleddata,forexamplebyco-training\n(Clarketal.2003;Søgaard2010).\nSee Householder (1995) for historical notes on parts of speech, and Sampson\n(1987)andGarsideetal.(1997)ontheprovenanceoftheBrownandothertagsets.\nExercises\n17.1 Findonetaggingerrorineachofthefollowingsentencesthataretaggedwith\nthePennTreebanktagset:\n1. I/PRPneed/VBPa/DTflight/NNfrom/INAtlanta/NN\n2. Does/VBZthis/DTflight/NNserve/VBdinner/NNS\n3. I/PRPhave/VBa/DTfriend/NNliving/VBGin/INDenver/NNP\n4. Can/VBPyou/PRPlist/VBthe/DTnonstop/JJafternoon/NNflights/NNS\n17.2 Use the Penn Treebank tagset to tag each word in the following sentences\nfrom Damon Runyon’s short stories. You may ignore punctuation. Some of\nthesearequitedifficult;doyourbest.\n1. Itisanicenight.\n2. ThiscrapgameisoveragarageinFifty-secondStreet...\n3. ...Nobodyevertakesthenewspapersshesells...\n4. He is a tall, skinny guy with a long, sad, mean-looking kisser, and a\nmournfulvoice."
    },
    "2": {
        "chapter": "18",
        "section": "18.1 & 18.2",
        "chapter_title": "Title of Sections 18.1, 18.2",
        "original_category": "",
        "original_text": "2 CHAPTER18 • CONTEXT-FREEGRAMMARSANDCONSTITUENCYPARSING\n18.1 Constituency\nSyntactic constituency is the idea that groups of words can behave as single units,\norconstituents. Partofdevelopingagrammarinvolvesbuildinganinventoryofthe\nconstituents in the language. How do words group together in English? Consider\nnounphrase thenounphrase,asequenceofwordssurroundingatleastonenoun.Herearesome\nexamplesofnounphrases(thankstoDamonRunyon):\nHarrytheHorse ahigh-classspotsuchasMindy’s\ntheBroadwaycoppers thereasonhecomesintotheHotBox\nthey threepartiesfromBrooklyn\nWhatevidencedowehavethatthesewordsgrouptogether(or“formconstituents”)?\nOnepieceofevidenceisthattheycanallappearinsimilarsyntacticenvironments,\nforexample,beforeaverb.\nthreepartiesfromBrooklynarrive...\nahigh-classspotsuchasMindy’sattracts...\ntheBroadwaycopperslove...\ntheysit\nButwhilethewholenounphrasecanoccurbeforeaverb,thisisnottrueofeach\noftheindividualwordsthatmakeupanounphrase.Thefollowingarenotgrammat-\nicalsentencesofEnglish(recallthatweuseanasterisk(*)tomarkfragmentsthat\narenotgrammaticalEnglishsentences):\n*fromarrive... *asattracts...\n*theis... *spotsat...\nThus, to correctly describe facts about the ordering of these words in English, we\nmustbeabletosaythingslike“NounPhrasescanoccurbeforeverbs”. Let’snow\nseehowtodothisinamoreformalway!\n18.2 Context-Free Grammars\nA widely used formal system for modeling constituent structure in natural lan-\nCFG guageisthecontext-freegrammar,orCFG.Context-freegrammarsarealsocalled\nphrase-structuregrammars,andtheformalismisequivalenttoBackus-Naurform,\norBNF.Theideaofbasingagrammaronconstituentstructuredatesbacktothepsy-\nchologistWilhelmWundt(1900)butwasnotformalizeduntilChomsky(1956)and,\nindependently,Backus(1959).\nrules Acontext-freegrammarconsistsofasetofrulesorproductions,eachofwhich\nexpresses the ways that symbols of the language can be grouped and ordered to-\nlexicon gether,andalexiconofwordsandsymbols.Forexample,thefollowingproductions\nNP express that an NP (or noun phrase) can be composed of either a ProperNoun or\nadeterminer(Det)followedbyaNominal; aNominalinturncanconsistofoneor 2 CHAPTER18 • CONTEXT-FREEGRAMMARSANDCONSTITUENCYPARSING\n18.1 Constituency\nSyntactic constituency is the idea that groups of words can behave as single units,\norconstituents. Partofdevelopingagrammarinvolvesbuildinganinventoryofthe\nconstituents in the language. How do words group together in English? Consider\nnounphrase thenounphrase,asequenceofwordssurroundingatleastonenoun.Herearesome\nexamplesofnounphrases(thankstoDamonRunyon):\nHarrytheHorse ahigh-classspotsuchasMindy’s\ntheBroadwaycoppers thereasonhecomesintotheHotBox\nthey threepartiesfromBrooklyn\nWhatevidencedowehavethatthesewordsgrouptogether(or“formconstituents”)?\nOnepieceofevidenceisthattheycanallappearinsimilarsyntacticenvironments,\nforexample,beforeaverb.\nthreepartiesfromBrooklynarrive...\nahigh-classspotsuchasMindy’sattracts...\ntheBroadwaycopperslove...\ntheysit\nButwhilethewholenounphrasecanoccurbeforeaverb,thisisnottrueofeach\noftheindividualwordsthatmakeupanounphrase.Thefollowingarenotgrammat-\nicalsentencesofEnglish(recallthatweuseanasterisk(*)tomarkfragmentsthat\narenotgrammaticalEnglishsentences):\n*fromarrive... *asattracts...\n*theis... *spotsat...\nThus, to correctly describe facts about the ordering of these words in English, we\nmustbeabletosaythingslike“NounPhrasescanoccurbeforeverbs”. Let’snow\nseehowtodothisinamoreformalway!\n18.2 Context-Free Grammars\nA widely used formal system for modeling constituent structure in natural lan-\nCFG guageisthecontext-freegrammar,orCFG.Context-freegrammarsarealsocalled\nphrase-structuregrammars,andtheformalismisequivalenttoBackus-Naurform,\norBNF.Theideaofbasingagrammaronconstituentstructuredatesbacktothepsy-\nchologistWilhelmWundt(1900)butwasnotformalizeduntilChomsky(1956)and,\nindependently,Backus(1959).\nrules Acontext-freegrammarconsistsofasetofrulesorproductions,eachofwhich\nexpresses the ways that symbols of the language can be grouped and ordered to-\nlexicon gether,andalexiconofwordsandsymbols.Forexample,thefollowingproductions\nNP express that an NP (or noun phrase) can be composed of either a ProperNoun or\nadeterminer(Det)followedbyaNominal; aNominalinturncanconsistofoneor 18.2 • CONTEXT-FREEGRAMMARS 3\nmoreNouns.1\nNP → DetNominal\nNP → ProperNoun\nNominal → Noun | NominalNoun\nContext-freerulescanbehierarchicallyembedded,sowecancombinetheprevious\nruleswithothers,likethefollowing,thatexpressfactsaboutthelexicon:\nDet → a\nDet → the\nNoun → flight\nThe symbols that are used in a CFG are divided into two classes. The symbols\nterminal that correspond to words in the language (“the”, “nightclub”) are called terminal\nsymbols; thelexiconisthesetofrulesthatintroducetheseterminalsymbols. The\nnon-terminal symbolsthatexpressabstractionsovertheseterminalsarecallednon-terminals. In\neachcontext-freerule,theitemtotherightofthearrow(→)isanorderedlistofone\normoreterminalsandnon-terminals;totheleftofthearrowisasinglenon-terminal\nsymbolexpressingsomeclusterorgeneralization.Thenon-terminalassociatedwith\neachwordinthelexiconisitslexicalcategory,orpartofspeech.\nA CFG can be thought of in two ways: as a device for generating sentences\nand as a device for assigning a structure to a given sentence. Viewing a CFG as a\ngenerator,wecanreadthe→arrowas“rewritethesymbolontheleftwiththestring\nofsymbolsontheright”.\nSostartingfromthesymbol: NP\nwecanuseourfirstruletorewriteNPas: DetNominal\nandthenrewriteNominalas: Noun\nandfinallyrewritetheseparts-of-speechas: aflight\nWesaythestringaflightcanbederivedfromthenon-terminalNP.Thus,aCFG\ncanbeusedtogenerateasetofstrings. Thissequenceofruleexpansionsiscalleda\nderivation derivationofthestringofwords. Itiscommontorepresentaderivationbyaparse\nparsetree tree (commonlyshowninvertedwiththerootatthetop).Figure18.1showsthetree\nrepresentationofthisderivation.\nNP\nDet Nom\na Noun\nflight\nFigure18.1 Aparsetreefor“aflight”.\ndominates In the parse tree shown in Fig. 18.1, we can say that the node NP dominates\nall the nodes in the tree (Det, Nom, Noun, a, flight). We can say further that it\nimmediatelydominatesthenodesDetandNom.\nThe formal language defined by a CFG is the set of strings that are derivable\nstartsymbol from the designated start symbol. Each grammar must have one designated start\n1 Whentalkingabouttheseruleswecanpronouncetherightarrow→as“goesto”,andsowemight\nreadthefirstruleaboveas“NPgoestoDetNominal”. 18.2 • CONTEXT-FREEGRAMMARS 3\nmoreNouns.1\nNP → DetNominal\nNP → ProperNoun\nNominal → Noun | NominalNoun\nContext-freerulescanbehierarchicallyembedded,sowecancombinetheprevious\nruleswithothers,likethefollowing,thatexpressfactsaboutthelexicon:\nDet → a\nDet → the\nNoun → flight\nThe symbols that are used in a CFG are divided into two classes. The symbols\nterminal that correspond to words in the language (“the”, “nightclub”) are called terminal\nsymbols; thelexiconisthesetofrulesthatintroducetheseterminalsymbols. The\nnon-terminal symbolsthatexpressabstractionsovertheseterminalsarecallednon-terminals. In\neachcontext-freerule,theitemtotherightofthearrow(→)isanorderedlistofone\normoreterminalsandnon-terminals;totheleftofthearrowisasinglenon-terminal\nsymbolexpressingsomeclusterorgeneralization.Thenon-terminalassociatedwith\neachwordinthelexiconisitslexicalcategory,orpartofspeech.\nA CFG can be thought of in two ways: as a device for generating sentences\nand as a device for assigning a structure to a given sentence. Viewing a CFG as a\ngenerator,wecanreadthe→arrowas“rewritethesymbolontheleftwiththestring\nofsymbolsontheright”.\nSostartingfromthesymbol: NP\nwecanuseourfirstruletorewriteNPas: DetNominal\nandthenrewriteNominalas: Noun\nandfinallyrewritetheseparts-of-speechas: aflight\nWesaythestringaflightcanbederivedfromthenon-terminalNP.Thus,aCFG\ncanbeusedtogenerateasetofstrings. Thissequenceofruleexpansionsiscalleda\nderivation derivationofthestringofwords. Itiscommontorepresentaderivationbyaparse\nparsetree tree (commonlyshowninvertedwiththerootatthetop).Figure18.1showsthetree\nrepresentationofthisderivation.\nNP\nDet Nom\na Noun\nflight\nFigure18.1 Aparsetreefor“aflight”.\ndominates In the parse tree shown in Fig. 18.1, we can say that the node NP dominates\nall the nodes in the tree (Det, Nom, Noun, a, flight). We can say further that it\nimmediatelydominatesthenodesDetandNom.\nThe formal language defined by a CFG is the set of strings that are derivable\nstartsymbol from the designated start symbol. Each grammar must have one designated start\n1 Whentalkingabouttheseruleswecanpronouncetherightarrow→as“goesto”,andsowemight\nreadthefirstruleaboveas“NPgoestoDetNominal”. 18.2 • CONTEXT-FREEGRAMMARS 5\nGrammarRules Examples\nS → NPVP I+wantamorningflight\nNP → Pronoun I\n| Proper-Noun LosAngeles\n| DetNominal a+flight\nNominal → NominalNoun morning+flight\n| Noun flights\nVP → Verb do\n| VerbNP want+aflight\n| VerbNPPP leave+Boston+inthemorning\n| VerbPP leaving+onThursday\nPP → PrepositionNP from+LosAngeles\nFigure18.3 ThegrammarforL ,withexamplephrasesforeachrule.\n0\nS\nNP VP\nPro Verb NP\nI prefer Det Nom\na Nom Noun\nNoun flight\nmorning\nFigure18.4 Theparsetreefor“Ipreferamorningflight”accordingtogrammarL .\n0\nI),andarandomexpansionofVP(let’ssay,toVerbNP),andsoonuntilwegenerate\nthestringIpreferamorningflight. Figure18.4showsaparsetreethatrepresentsa\ncompletederivationofIpreferamorningflight.\nWecanalsorepresentaparse treeinamorecompactformatcalledbracketed\nbracketed notation;hereisthebracketedrepresentationoftheparsetreeofFig.18.4:\nnotation\n(18.1) [S[NP[ProI]][VP[Vprefer][NP[Deta][Nom[Nmorning][Nom[Nflight]]]]]]\nACFGlikethatofL definesaformallanguage. Sentences(stringsofwords)\n0\nthatcanbederivedbyagrammarareintheformallanguagedefinedbythatgram-\ngrammatical mar, and are called grammatical sentences. Sentences that cannot be derived by\na given formal grammar are not in the language defined by that grammar and are\nungrammatical referredtoasungrammatical. Thishardlinebetween“in”and“out”characterizes\nall formal languages but is only a very simplified model of how natural languages\nreallywork. Thisisbecausedeterminingwhetheragivensentenceispartofagiven\nnaturallanguage(say,English)oftendependsonthecontext. Inlinguistics,theuse\ngenerative offormallanguagestomodelnaturallanguagesiscalledgenerativegrammarsince\ngrammar\nthelanguageisdefinedbythesetofpossiblesentences“generated”bythegrammar.\n(Note that this is a different sense of the word ‘generate’ than when we talk about 18.2 • CONTEXT-FREEGRAMMARS 5\nGrammarRules Examples\nS → NPVP I+wantamorningflight\nNP → Pronoun I\n| Proper-Noun LosAngeles\n| DetNominal a+flight\nNominal → NominalNoun morning+flight\n| Noun flights\nVP → Verb do\n| VerbNP want+aflight\n| VerbNPPP leave+Boston+inthemorning\n| VerbPP leaving+onThursday\nPP → PrepositionNP from+LosAngeles\nFigure18.3 ThegrammarforL ,withexamplephrasesforeachrule.\n0\nS\nNP VP\nPro Verb NP\nI prefer Det Nom\na Nom Noun\nNoun flight\nmorning\nFigure18.4 Theparsetreefor“Ipreferamorningflight”accordingtogrammarL .\n0\nI),andarandomexpansionofVP(let’ssay,toVerbNP),andsoonuntilwegenerate\nthestringIpreferamorningflight. Figure18.4showsaparsetreethatrepresentsa\ncompletederivationofIpreferamorningflight.\nWecanalsorepresentaparse treeinamorecompactformatcalledbracketed\nbracketed notation;hereisthebracketedrepresentationoftheparsetreeofFig.18.4:\nnotation\n(18.1) [S[NP[ProI]][VP[Vprefer][NP[Deta][Nom[Nmorning][Nom[Nflight]]]]]]\nACFGlikethatofL definesaformallanguage. Sentences(stringsofwords)\n0\nthatcanbederivedbyagrammarareintheformallanguagedefinedbythatgram-\ngrammatical mar, and are called grammatical sentences. Sentences that cannot be derived by\na given formal grammar are not in the language defined by that grammar and are\nungrammatical referredtoasungrammatical. Thishardlinebetween“in”and“out”characterizes\nall formal languages but is only a very simplified model of how natural languages\nreallywork. Thisisbecausedeterminingwhetheragivensentenceispartofagiven\nnaturallanguage(say,English)oftendependsonthecontext. Inlinguistics,theuse\ngenerative offormallanguagestomodelnaturallanguagesiscalledgenerativegrammarsince\ngrammar\nthelanguageisdefinedbythesetofpossiblesentences“generated”bythegrammar.\n(Note that this is a different sense of the word ‘generate’ than when we talk about 6 CHAPTER18 • CONTEXT-FREEGRAMMARSANDCONSTITUENCYPARSING\nlanguagemodelsgeneratingtext.)\n18.2.1 FormalDefinitionofContext-FreeGrammar\nWe conclude this section with a quick, formal description of a context-free gram-\nmar and the language it generates. A context-free grammar G is defined by four\nparameters: N,Σ,R,S(technicallyitisa“4-tuple”).\nN asetofnon-terminalsymbols(orvariables)\nΣ asetofterminalsymbols(disjointfromN)\nR asetofrulesorproductions,eachoftheformA→β ,\nwhereAisanon-terminal,\nβ isastringofsymbolsfromtheinfinitesetofstrings(Σ∪N)∗\nS adesignatedstartsymbolandamemberofN\nFor the remainder of the book we adhere to the following conventions when dis-\ncussing the formal properties of context-free grammars (as opposed to explaining\nparticularfactsaboutEnglishorotherlanguages).\nCapitalletterslikeA,B,andS Non-terminals\nS Thestartsymbol\nLower-caseGreekletterslikeα,β,andγ Stringsdrawnfrom(Σ∪N)∗\nLower-caseRomanletterslikeu,v,andw Stringsofterminals\nAlanguageisdefinedthroughtheconceptofderivation. Onestringderivesan-\notheroneifitcanberewrittenasthesecondonebysomeseriesofruleapplications.\nMoreformally,followingHopcroftandUllman(1979),\nifA→β isaproductionofRandα andγ areanystringsintheset\ndirectlyderives (Σ∪N)∗,thenwesaythatαAγ directlyderivesαβγ,orαAγ ⇒αβγ.\nDerivationisthenageneralizationofdirectderivation:\nLetα ,α ,...,α bestringsin(Σ∪N)∗,m≥1,suchthat\n1 2 m\nα ⇒α ,α ⇒α ,...,α ⇒α\n1 2 2 3 m−1 m\n∗\nderives Wesaythatα 1derivesα m,orα 1⇒α m.\nWecanthenformallydefinethelanguageL generatedbyagrammarGasthe\nG\nsetofstringscomposedofterminalsymbolsthatcanbederivedfromthedesignated\nstartsymbolS.\nL ={w|wisinΣ∗andS⇒∗ w}\nG\nThe problem of mapping from a string of words to its parse tree is called syn-\nsyntactic tacticparsing,aswe’llseeinSection18.6.\nparsing\n18.3 Treebanks\ntreebank Acorpusinwhicheverysentenceisannotatedwithaparsetreeiscalledatreebank. 18.3 • TREEBANKS 7\nTreebanksplayanimportantroleinparsingaswellasinlinguisticinvestigationsof\nsyntacticphenomena.\nTreebanksaregenerallymadebyrunningaparserovereachsentenceandthen\nhaving the resulting parse hand-corrected by human linguists. Figure 18.5 shows\nPennTreebank sentences from the Penn Treebank project, which includes various treebanks in\nEnglish,Arabic,andChinese.ThePennTreebankpart-of-speechtagsetwasdefined\ninChapter17,butwe’llseeminorformattingdifferencesacrosstreebanks. Theuse\nofLISP-styleparenthesizednotationfortreesisextremelycommonandresembles\nthebracketednotationwesawearlierin(18.1). Forthosewhoarenotfamiliarwith\nitweshowastandardnode-and-linetreerepresentationinFig.18.6.\n((S\n(NP-SBJ (DT That) ((S\n(JJ cold) (, ,) (NP-SBJ The/DT flight/NN )\n(JJ empty) (NN sky) ) (VP should/MD\n(VP (VBD was) (VP arrive/VB\n(ADJP-PRD (JJ full) (PP-TMP at/IN\n(PP (IN of) (NP eleven/CD a.m/RB ))\n(NP (NN fire) (NP-TMP tomorrow/NN )))))\n(CC and)\n(NN light) ))))\n(. .) ))\n(a) (b)\nFigure18.5 ParsesfromtheLDCTreebank3for(a)Brownand(b)ATISsentences.\nS\nNP-SBJ VP .\nDT JJ , JJ NN VBD ADJP-PRD .\nThat cold , empty sky was JJ PP\nfull IN NP\nof NN CC NN\nfire and light\nFigure18.6 ThetreecorrespondingtotheBrowncorpussentenceinthepreviousfigure.\nThesentencesinatreebankimplicitlyconstituteagrammarofthelanguage.For\nexample,fromtheparsedsentencesinFig.18.5wecanextracttheCFGrulesshown\nin Fig. 18.7 (with rule suffixes (-SBJ) stripped for simplicity). The grammar used\ntoparsethePennTreebankisveryflat,resultinginverymanyrules. Forexample, 10 CHAPTER18 • CONTEXT-FREEGRAMMARSANDCONSTITUENCYPARSING\nS S\nNP VP NP VP\nPronoun Verb NP Pronoun VP PP\nI shot Det Nominal I Verb NP inmypajamas\nan Nominal PP shot Det Nominal\nNoun inmypajamas an Noun\nelephant elephant\nFigure18.9 Twoparsetreesforanambiguoussentence. Theparseontheleftcorrespondstothehumorous\nreading in which the elephant is in the pajamas, the parse on the right corresponds to the reading in which\nCaptainSpauldingdidtheshootinginhispajamas.\nCrackers is ambiguous because the phrase in my pajamas can be part of the NP\nheaded by elephant or a part of the verb phrase headed by shot. Figure 18.9 illus-\ntratesthesetwoanalysesofMarx’slineusingrulesfromL .\n1\nStructuralambiguity,appropriatelyenough,comesinmanyforms.Twocommon\nkinds of ambiguity are attachment ambiguity and coordination ambiguity. A\nattachment sentencehasanattachmentambiguityifaparticularconstituentcanbeattachedto\nambiguity\nthe parse tree at more than one place. The Groucho Marx sentence is an example\nPP-attachment ofPP-attachmentambiguity: theprepositionphrasecanbeattachedeitheraspart\nambiguity\nof the NP or as part of the VP. Various kinds of adverbial phrases are also subject\ntothiskindofambiguity. Forinstance,inthefollowingexamplethegerundive-VP\nflyingtoPariscanbepartofagerundivesentencewhosesubjectistheEiffelTower\noritcanbeanadjunctmodifyingtheVPheadedbysaw:\n(18.2) WesawtheEiffelTowerflyingtoParis.\ncoordination Incoordinationambiguityphrasescanbeconjoinedbyaconjunctionlikeand.\nambiguity\nFor example, the phrase old men and women can be bracketed as [old [men and\nwomen]], referring to old men and old women, or as [old men] and [women], in\nwhichcaseitisonlythemenwhoareold. Theseambiguitiescombineincomplex\nwaysinrealsentences,likethefollowingnewssentencefromtheBrowncorpus:\n(18.3) PresidentKennedytodaypushedasideotherWhiteHousebusinessto\ndevoteallhistimeandattentiontoworkingontheBerlincrisisaddresshe\nwilldelivertomorrownighttotheAmericanpeopleovernationwide\ntelevisionandradio.\nThissentencehasanumberofambiguities,althoughsincetheyaresemantically\nunreasonable,itrequiresacarefulreadingtoseethem.Thelastnounphrasecouldbe\nparsed [nationwide [television and radio]] or [[nationwide television] and radio].\nThedirectobjectofpushedasideshouldbeotherWhiteHousebusinessbutcould\nalso be the bizarre phrase [other White House business to devote all his time and\nattentiontoworking](i.e.,astructurelikeKennedyaffirmed[hisintentiontopropose\nanewbudgettoaddressthedeficit]).ThenthephraseontheBerlincrisisaddresshe 24 CHAPTER18 • CONTEXT-FREEGRAMMARSANDCONSTITUENCYPARSING\nExercises\n18.1 Implementthealgorithmtoconvertarbitrarycontext-freegrammarstoCNF.\nApplyyourprogramtotheL grammar.\n1\n18.2 ImplementtheCKYalgorithmandtestitwithyourconvertedL grammar.\n1\n18.3 RewritetheCKYalgorithmgiveninFig.18.12onpage14sothatitcanaccept\ngrammarsthatcontainunitproductions.\n18.4 Discusshowtoaugmentaparsertodealwithinputthatmaybeincorrect,for\nexample,containingspellingerrorsormistakesarisingfromautomaticspeech\nrecognition.\n18.5 Implement the PARSEVAL metrics described in Section 18.8. Next, use a\nparserandatreebank, compareyourmetricsagainstastandardimplementa-\ntion. Analyzetheerrorsinyourapproach. 24 CHAPTER18 • CONTEXT-FREEGRAMMARSANDCONSTITUENCYPARSING\nExercises\n18.1 Implementthealgorithmtoconvertarbitrarycontext-freegrammarstoCNF.\nApplyyourprogramtotheL grammar.\n1\n18.2 ImplementtheCKYalgorithmandtestitwithyourconvertedL grammar.\n1\n18.3 RewritetheCKYalgorithmgiveninFig.18.12onpage14sothatitcanaccept\ngrammarsthatcontainunitproductions.\n18.4 Discusshowtoaugmentaparsertodealwithinputthatmaybeincorrect,for\nexample,containingspellingerrorsormistakesarisingfromautomaticspeech\nrecognition.\n18.5 Implement the PARSEVAL metrics described in Section 18.8. Next, use a\nparserandatreebank, compareyourmetricsagainstastandardimplementa-\ntion. Analyzetheerrorsinyourapproach."
    },
    "3": {
        "chapter": "21",
        "section": "21.1 & 21.2",
        "chapter_title": "Title of Sections 21.1, 21.2",
        "original_category": "",
        "original_text": "2 CHAPTER21 • SEMANTICROLELABELING\n21.1 Semantic Roles\nConsider the meanings of the arguments Sasha, Pat, the window, and the door in\nthesetwosentences.\n(21.1) Sashabrokethewindow.\n(21.2) Patopenedthedoor.\nThe subjects Sasha and Pat, what we might call the breaker of the window-\nbreaking event and the opener of the door-opening event, have something in com-\nmon. They are both volitional actors, often animate, and they have direct causal\nresponsibilityfortheirevents.\nthematicroles Thematicrolesareawaytocapturethissemanticcommonalitybetweenbreak-\nagents ers and openers. We say that the subjects of both these verbs are agents. Thus,\nAGENTisthethematicrolethatrepresentsanabstractideasuchasvolitionalcausa-\ntion.Similarly,thedirectobjectsofboththeseverbs,theBrokenThingandOpenedThing,\narebothprototypicallyinanimateobjectsthatareaffectedinsomewaybytheaction.\ntheme Thesemanticrolefortheseparticipantsistheme.\nThematicRole Definition\nAGENT Thevolitionalcauserofanevent\nEXPERIENCER Theexperiencerofanevent\nFORCE Thenon-volitionalcauseroftheevent\nTHEME Theparticipantmostdirectlyaffectedbyanevent\nRESULT Theendproductofanevent\nCONTENT Thepropositionorcontentofapropositionalevent\nINSTRUMENT Aninstrumentusedinanevent\nBENEFICIARY Thebeneficiaryofanevent\nSOURCE Theoriginoftheobjectofatransferevent\nGOAL Thedestinationofanobjectofatransferevent\nFigure21.1 Somecommonlyusedthematicroleswiththeirdefinitions.\nAlthoughthematicrolesareoneoftheoldestlinguisticmodels,aswesawabove,\ntheir modern formulation is due to Fillmore (1968) and Gruber (1965). Although\nthere is no universally agreed-upon set of roles, Figs. 21.1 and 21.2 list some the-\nmaticrolesthathavebeenusedinvariouscomputationalpapers,togetherwithrough\ndefinitionsandexamples.Mostthematicrolesetshaveaboutadozenroles,butwe’ll\nseesetswithsmallernumbersofroleswithevenmoreabstractmeanings, andsets\nwithverylargenumbersofrolesthatarespecifictosituations. We’llusethegeneral\nsemanticroles termsemanticrolesforallsetsofroles,whethersmallorlarge.\n21.2 Diathesis Alternations\nThe main reason computational systems use semantic roles is to act as a shallow\nmeaning representation that can let us make simple inferences that aren’t possible\nfrom the pure surface string of words, or even from the parse tree. To extend the\nearlier examples, if a document says that Company A acquired Company B, we’d\nliketoknowthatthisanswersthequeryWasCompanyBacquired? despitethefact\nthat the two sentences have very different surface syntax. Similarly, this shallow\nsemanticsmightactasausefulintermediatelanguageinmachinetranslation. 2 CHAPTER21 • SEMANTICROLELABELING\n21.1 Semantic Roles\nConsider the meanings of the arguments Sasha, Pat, the window, and the door in\nthesetwosentences.\n(21.1) Sashabrokethewindow.\n(21.2) Patopenedthedoor.\nThe subjects Sasha and Pat, what we might call the breaker of the window-\nbreaking event and the opener of the door-opening event, have something in com-\nmon. They are both volitional actors, often animate, and they have direct causal\nresponsibilityfortheirevents.\nthematicroles Thematicrolesareawaytocapturethissemanticcommonalitybetweenbreak-\nagents ers and openers. We say that the subjects of both these verbs are agents. Thus,\nAGENTisthethematicrolethatrepresentsanabstractideasuchasvolitionalcausa-\ntion.Similarly,thedirectobjectsofboththeseverbs,theBrokenThingandOpenedThing,\narebothprototypicallyinanimateobjectsthatareaffectedinsomewaybytheaction.\ntheme Thesemanticrolefortheseparticipantsistheme.\nThematicRole Definition\nAGENT Thevolitionalcauserofanevent\nEXPERIENCER Theexperiencerofanevent\nFORCE Thenon-volitionalcauseroftheevent\nTHEME Theparticipantmostdirectlyaffectedbyanevent\nRESULT Theendproductofanevent\nCONTENT Thepropositionorcontentofapropositionalevent\nINSTRUMENT Aninstrumentusedinanevent\nBENEFICIARY Thebeneficiaryofanevent\nSOURCE Theoriginoftheobjectofatransferevent\nGOAL Thedestinationofanobjectofatransferevent\nFigure21.1 Somecommonlyusedthematicroleswiththeirdefinitions.\nAlthoughthematicrolesareoneoftheoldestlinguisticmodels,aswesawabove,\ntheir modern formulation is due to Fillmore (1968) and Gruber (1965). Although\nthere is no universally agreed-upon set of roles, Figs. 21.1 and 21.2 list some the-\nmaticrolesthathavebeenusedinvariouscomputationalpapers,togetherwithrough\ndefinitionsandexamples.Mostthematicrolesetshaveaboutadozenroles,butwe’ll\nseesetswithsmallernumbersofroleswithevenmoreabstractmeanings, andsets\nwithverylargenumbersofrolesthatarespecifictosituations. We’llusethegeneral\nsemanticroles termsemanticrolesforallsetsofroles,whethersmallorlarge.\n21.2 Diathesis Alternations\nThe main reason computational systems use semantic roles is to act as a shallow\nmeaning representation that can let us make simple inferences that aren’t possible\nfrom the pure surface string of words, or even from the parse tree. To extend the\nearlier examples, if a document says that Company A acquired Company B, we’d\nliketoknowthatthisanswersthequeryWasCompanyBacquired? despitethefact\nthat the two sentences have very different surface syntax. Similarly, this shallow\nsemanticsmightactasausefulintermediatelanguageinmachinetranslation. 21.2 • DIATHESISALTERNATIONS 3\nThematicRole Example\nAGENT Thewaiterspilledthesoup.\nEXPERIENCER Johnhasaheadache.\nFORCE Thewindblowsdebrisfromthemallintoouryards.\nTHEME OnlyafterBenjaminFranklinbroketheice...\nRESULT Thecitybuiltaregulation-sizebaseballdiamond...\nCONTENT Monaasked“YoumetMaryAnnatasupermarket?”\nINSTRUMENT Hepoachedcatfish,stunningthemwithashockingdevice...\nBENEFICIARY WheneverAnnCallahanmakeshotelreservationsforherboss...\nSOURCE IflewinfromBoston.\nGOAL IdrovetoPortland.\nFigure21.2 Someprototypicalexamplesofvariousthematicroles.\nSemantic roles thus help generalize over different surface realizations of pred-\nicate arguments. For example, while the AGENT is often realized as the subject of\nthesentence,inothercasesthe THEME canbethesubject. Considerthesepossible\nrealizationsofthethematicargumentsoftheverbbreak:\n(21.3) John brokethewindow.\nAGENT THEME\n(21.4) John brokethewindowwitharock.\nAGENT THEME INSTRUMENT\n(21.5) Therock brokethewindow.\nINSTRUMENT THEME\n(21.6) Thewindowbroke.\nTHEME\n(21.7) ThewindowwasbrokenbyJohn.\nTHEME AGENT\nTheseexamplessuggestthatbreakhas(atleast)thepossibleargumentsAGENT,\nTHEME, and INSTRUMENT. The set of thematic role arguments taken by a verb is\nthematicgrid often called the thematic grid, θ-grid, or case frame. We can see that there are\ncaseframe (amongothers)thefollowingpossibilitiesfortherealizationoftheseargumentsof\nbreak:\nAGENT/Subject, THEME/Object\nAGENT/Subject, THEME/Object, INSTRUMENT/PP\nwith\nINSTRUMENT/Subject, THEME/Object\nTHEME/Subject\nItturnsoutthatmanyverbsallowtheirthematicrolestoberealizedinvarious\nsyntacticpositions. Forexample,verbslikegivecanrealizethe THEME and GOAL\nargumentsintwodifferentways:\n(21.8) a. Doris gavethebooktoCary.\nAGENT THEME GOAL\nb. Doris gaveCary thebook.\nAGENT GOALTHEME\nThesemultipleargumentstructurerealizations(thefactthatbreakcantakeAGENT,\nINSTRUMENT, or THEME as subject, and give can realize its THEME and GOAL in\nverb eitherorder)arecalledverbalternationsordiathesisalternations. Thealternation\nalternation\ndative weshowedaboveforgive,thedativealternation,seemstooccurwithparticularse-\nalternation\nmanticclassesofverbs,including“verbsoffuturehaving”(advance,allocate,offer,"
    },
    "4": {
        "chapter": "G",
        "section": "G.1 & G.2",
        "chapter_title": "Title of Sections G.1, G.2",
        "original_category": "",
        "original_text": "2 APPENDIXG • WORDSENSESANDWORDNET\nanalyticdirection.\nG.1 Word Senses\nwordsense Asense(orwordsense)isadiscreterepresentationofoneaspectofthemeaningof\naword. Looselyfollowinglexicographictradition, werepresenteachsensewitha\nsuperscript: bank1 andbank2,mouse1 andmouse2. Incontext,it’seasytoseethe\ndifferentmeanings:\nmouse1: .... amousecontrollingacomputersystemin1968.\nmouse2: .... aquietanimallikeamouse\nbank1: ...abankcanholdtheinvestmentsinacustodialaccount...\nbank2: ...asagricultureburgeonsontheeastbank,theriver...\nG.1.1 DefiningWordSenses\nHowcanwedefinethemeaningofawordsense? WeintroducedinChapter6the\nstandardcomputationalapproachofrepresentingawordasanembedding,apoint\nin semantic space. The intuition of embedding models like word2vec or GloVe is\nthatthemeaningofawordcanbedefinedbyitsco-occurrences,thecountsofwords\nthatoftenoccurnearby. Butthatdoesn’ttellushowtodefinethemeaningofaword\nsense. As we saw in Chapter 11, contextual embeddings like BERT go further by\nofferinganembeddingthatrepresentsthemeaningofawordinitstextualcontext,\nandwe’llseethatcontextualembeddingslieattheheartofmodernalgorithmsfor\nwordsensedisambiguation.\nBut first, we need to consider the alternative ways that dictionaries and the-\nsaurusesofferfordefiningsenses. Oneisbasedonthefactthatdictionariesorthe-\ngloss saurusesgivetextualdefinitionsforeachsensecalledglosses. Herearetheglosses\nfortwosensesofbank:\n1. financial institution that accepts deposits and channels\nthe money into lending activities\n2. sloping land (especially the slope beside a body of water)\nGlossesarenotaformalmeaningrepresentation;theyarejustwrittenforpeople.\nConsiderthefollowingfragmentsfromthedefinitionsofright,left,red,andblood\nfromtheAmericanHeritageDictionary(Morris,1985).\nright adj. locatednearertherighthandesp. beingontherightwhen\nfacingthesamedirectionastheobserver.\nleft adj. locatednearertothissideofthebodythantheright.\nred n. thecolorofbloodoraruby.\nblood n. theredliquidthatcirculatesintheheart,arteriesandveinsof\nanimals.\nNotethecircularityinthesedefinitions. Thedefinitionofrightmakestwodirect\nreferences to itself, and the entry for left contains an implicit self-reference in the\nphrasethissideofthebody,whichpresumablymeanstheleftside. Theentriesfor\nredandbloodreferenceeachotherintheirdefinitions. Forhumans,suchentriesare\nusefulsincetheuserofthedictionaryhassufficientgraspoftheseotherterms. G.1 • WORDSENSES 3\nYet despite their circularity and lack of formal representation, glosses can still\nbeusefulforcomputationalmodelingofsenses.Thisisbecauseaglossisjustasen-\ntence,andfromsentenceswecancomputesentenceembeddingsthattellussome-\nthing about the meaning of the sense. Dictionaries often give example sentences\nalongwithglosses,andthesecanagainbeusedtohelpbuildasenserepresentation.\nThesecondwaythatthesaurusesofferfordefiningasenseis—likethedictionary\ndefinitions—definingasensethroughitsrelationshipwithothersenses. Forexam-\nple,theabovedefinitionsmakeitclearthatrightandleftaresimilarkindsoflemmas\nthatstandinsomekindofalternation, oropposition, tooneanother. Similarly, we\ncangleanthatredisacolorandthatbloodisaliquid. Senserelationsofthissort\n(IS-A,orantonymy)areexplicitlylistedinon-linedatabaseslikeWordNet. Given\na sufficiently large database of such relations, many applications are quite capable\nof performing sophisticated semantic tasks about word senses (even if they do not\nreallyknowtheirrightfromtheirleft).\nG.1.2 Howmanysensesdowordshave?\nDictionariesandthesaurusesgivediscretelistsofsenses. Bycontrast,embeddings\n(whetherstaticorcontextual)offeracontinuoushigh-dimensionalmodelofmeaning\nthatdoesn’tdivideupintodiscretesenses.\nThereforecreatingathesaurusdependsoncriteriafordecidingwhenthediffer-\ning uses of a word should be represented with discrete senses. We might consider\ntwosensesdiscreteiftheyhaveindependenttruthconditions,differentsyntacticbe-\nhavior,andindependentsenserelations,oriftheyexhibitantagonisticmeanings.\nConsiderthefollowingusesoftheverbservefromtheWSJcorpus:\n(G.1) Theyrarelyserveredmeat,preferringtoprepareseafood.\n(G.2) HeservedasU.S.ambassadortoNorwayin1976and1977.\n(G.3) Hemighthaveservedhistime,comeoutandledanupstandinglife.\nThe serve of serving red meat and that of serving time clearly have different truth\nconditions and presuppositions; the serve of serve as ambassador has the distinct\nsubcategorizationstructureserveasNP.Theseheuristicssuggestthattheseareprob-\nably three distinct senses of serve. One practical technique for determining if two\nsenses are distinct is to conjoin two uses of a word in a single sentence; this kind\nzeugma of conjunction of antagonistic readings is called zeugma. Consider the following\nexamples:\n(G.4) Whichofthoseflightsservebreakfast?\n(G.5) DoesAirFranceservePhiladelphia?\n(G.6) ?DoesAirFranceservebreakfastandPhiladelphia?\nWeuse(?) tomarkthoseexamplesthataresemanticallyill-formed. Theoddnessof\ntheinventedthirdexample(acaseofzeugma)indicatesthereisnosensiblewayto\nmakeasinglesenseofserveworkforbothbreakfastandPhiladelphia. Wecanuse\nthisasevidencethatservehastwodifferentsensesinthiscase.\nDictionariestendtousemanyfine-grainedsensessoastocapturesubtlemeaning\ndifferences, a reasonable approach given that the traditional role of dictionaries is\naiding word learners. For computational purposes, we often don’t need these fine\ndistinctions,soweoftengrouporclusterthesenses; wehavealreadydonethisfor\nsome of the examples in this chapter. Indeed, clustering examples into senses, or\nsensesintobroader-grainedcategories,isanimportantcomputationaltaskthatwe’ll\ndiscussinSectionG.7. G.1 • WORDSENSES 3\nYet despite their circularity and lack of formal representation, glosses can still\nbeusefulforcomputationalmodelingofsenses.Thisisbecauseaglossisjustasen-\ntence,andfromsentenceswecancomputesentenceembeddingsthattellussome-\nthing about the meaning of the sense. Dictionaries often give example sentences\nalongwithglosses,andthesecanagainbeusedtohelpbuildasenserepresentation.\nThesecondwaythatthesaurusesofferfordefiningasenseis—likethedictionary\ndefinitions—definingasensethroughitsrelationshipwithothersenses. Forexam-\nple,theabovedefinitionsmakeitclearthatrightandleftaresimilarkindsoflemmas\nthatstandinsomekindofalternation, oropposition, tooneanother. Similarly, we\ncangleanthatredisacolorandthatbloodisaliquid. Senserelationsofthissort\n(IS-A,orantonymy)areexplicitlylistedinon-linedatabaseslikeWordNet. Given\na sufficiently large database of such relations, many applications are quite capable\nof performing sophisticated semantic tasks about word senses (even if they do not\nreallyknowtheirrightfromtheirleft).\nG.1.2 Howmanysensesdowordshave?\nDictionariesandthesaurusesgivediscretelistsofsenses. Bycontrast,embeddings\n(whetherstaticorcontextual)offeracontinuoushigh-dimensionalmodelofmeaning\nthatdoesn’tdivideupintodiscretesenses.\nThereforecreatingathesaurusdependsoncriteriafordecidingwhenthediffer-\ning uses of a word should be represented with discrete senses. We might consider\ntwosensesdiscreteiftheyhaveindependenttruthconditions,differentsyntacticbe-\nhavior,andindependentsenserelations,oriftheyexhibitantagonisticmeanings.\nConsiderthefollowingusesoftheverbservefromtheWSJcorpus:\n(G.1) Theyrarelyserveredmeat,preferringtoprepareseafood.\n(G.2) HeservedasU.S.ambassadortoNorwayin1976and1977.\n(G.3) Hemighthaveservedhistime,comeoutandledanupstandinglife.\nThe serve of serving red meat and that of serving time clearly have different truth\nconditions and presuppositions; the serve of serve as ambassador has the distinct\nsubcategorizationstructureserveasNP.Theseheuristicssuggestthattheseareprob-\nably three distinct senses of serve. One practical technique for determining if two\nsenses are distinct is to conjoin two uses of a word in a single sentence; this kind\nzeugma of conjunction of antagonistic readings is called zeugma. Consider the following\nexamples:\n(G.4) Whichofthoseflightsservebreakfast?\n(G.5) DoesAirFranceservePhiladelphia?\n(G.6) ?DoesAirFranceservebreakfastandPhiladelphia?\nWeuse(?) tomarkthoseexamplesthataresemanticallyill-formed. Theoddnessof\ntheinventedthirdexample(acaseofzeugma)indicatesthereisnosensiblewayto\nmakeasinglesenseofserveworkforbothbreakfastandPhiladelphia. Wecanuse\nthisasevidencethatservehastwodifferentsensesinthiscase.\nDictionariestendtousemanyfine-grainedsensessoastocapturesubtlemeaning\ndifferences, a reasonable approach given that the traditional role of dictionaries is\naiding word learners. For computational purposes, we often don’t need these fine\ndistinctions,soweoftengrouporclusterthesenses; wehavealreadydonethisfor\nsome of the examples in this chapter. Indeed, clustering examples into senses, or\nsensesintobroader-grainedcategories,isanimportantcomputationaltaskthatwe’ll\ndiscussinSectionG.7. 4 APPENDIXG • WORDSENSESANDWORDNET\nG.2 Relations Between Senses\nThissectionexplorestherelationsbetweenwordsenses,especiallythosethathave\nreceivedsignificantcomputationalinvestigationlikesynonymy,antonymy,andhy-\npernymy.\nSynonymy\nWe introduced in Chapter 6 the idea that when two senses of two different words\nsynonym (lemmas) are identical, or nearly identical, we say the two senses are synonyms.\nSynonymsincludesuchpairsas\ncouch/sofa vomit/throwup filbert/hazelnut car/automobile\nAnd we mentioned that in practice, the word synonym is commonly used to\ndescribe a relationship of approximate or rough synonymy. But furthermore, syn-\nonymyisactuallyarelationshipbetweensensesratherthanwords. Consideringthe\nwords big and large. These mayseemto besynonyms inthefollowing sentences,\nsincewecouldswapbigandlargeineithersentenceandretainthesamemeaning:\n(G.7) Howbigisthatplane?\n(G.8) WouldIbeflyingonalargeorsmallplane?\nButnotethefollowingsentenceinwhichwecannotsubstitutelargeforbig:\n(G.9) MissNelson,forinstance,becameakindofbigsistertoBenjamin.\n(G.10) ?MissNelson,forinstance,becameakindoflargesistertoBenjamin.\nThisisbecausethewordbighasasensethatmeansbeingolderorgrownup,while\nlargelacksthissense. Thus, wesaythatsomesensesofbigandlargeare(nearly)\nsynonymouswhileotheronesarenot.\nAntonymy\nantonym Whereas synonyms are words with identical or similar meanings, antonyms are\nwordswithanoppositemeaning,like:\nlong/short big/little fast/slow cold/hot dark/light\nrise/fall up/down in/out\nTwosensescanbeantonymsiftheydefineabinaryoppositionorareatopposite\nendsofsomescale. Thisisthecaseforlong/short,fast/slow,orbig/little,whichare\nreversives atoppositeendsofthelengthorsizescale. Anothergroupofantonyms,reversives,\ndescribechangeormovementinoppositedirections,suchasrise/fallorup/down.\nAntonymsthusdiffercompletelywithrespecttooneaspectoftheirmeaning—\ntheir position on a scale or their direction—but are otherwise very similar, sharing\nalmostallotheraspectsof meaning. Thus, automaticallydistinguishingsynonyms\nfromantonymscanbedifficult.\nTaxonomicRelations\nAnother way word senses can be related is taxonomically. A word (or sense) is a\nhyponym hyponymofanotherwordorsenseifthefirstismorespecific,denotingasubclass\noftheother. Forexample,carisahyponymofvehicle,dogisahyponymofanimal,\nhypernym andmangoisahyponymoffruit. Conversely,wesaythatvehicleisahypernymof\ncar,andanimalisahypernymofdog.Itisunfortunatethatthetwowords(hypernym G.2 • RELATIONSBETWEENSENSES 5\nandhyponym)areverysimilarandhenceeasilyconfused;forthisreason,theword\nsuperordinate superordinateisoftenusedinsteadofhypernym.\nSuperordinate vehicle fruit furniture mammal\nSubordinate car mango chair dog\nWe can define hypernymy more formally by saying that the class denoted by\nthe superordinate extensionally includes the class denoted by the hyponym. Thus,\ntheclassofanimalsincludesasmembersalldogs,andtheclassofmovingactions\nincludes all walking actions. Hypernymy can also be defined in terms of entail-\nment. Under this definition, a sense A is a hyponym of a sense B if everything\nthatisAisalsoB,andhencebeinganAentailsbeingaB,or∀x A(x)⇒B(x). Hy-\nponymy/hypernymyisusuallyatransitiverelation;ifAisahyponymofBandBisa\nhyponymofC,thenAisahyponymofC.Anothernameforthehypernym/hyponym\nIS-A structureistheIS-Ahierarchy,inwhichwesayAIS-AB,orBsubsumesA.\nHypernymy is useful for tasks like textual entailment or question answering;\nknowingthatleukemiaisatypeofcancer,forexample,wouldcertainlybeusefulin\nansweringquestionsaboutleukemia.\nMeronymy\npart-whole Anothercommonrelationismeronymy,thepart-wholerelation. Alegispartofa\nchair;awheelispartofacar. Wesaythatwheelisameronymofcar,andcarisa\nholonymofwheel.\nStructuredPolysemy\nThe senses of a word can also be related semantically, in which case we call the\nstructured relationshipbetweenthemstructuredpolysemy. Considerthissensebank:\npolysemy\n(G.11) ThebankisonthecornerofNassauandWitherspoon.\nThis sense, perhaps bank4, means something like “the building belonging to\na financial institution”. These two kinds of senses (an organization and the build-\ning associated with an organization ) occur together for many other words as well\n(school,university,hospital,etc.). Thus,thereisasystematicrelationshipbetween\nsensesthatwemightrepresentas\nBUILDING↔ORGANIZATION\nmetonymy Thisparticularsubtypeofpolysemyrelationiscalledmetonymy. Metonymyis\ntheuseofoneaspectofaconceptorentitytorefertootheraspectsoftheentityor\ntotheentityitself. WeareperformingmetonymywhenweusethephrasetheWhite\nHouse to refer to the administration whose office is in the White House. Other\ncommonexamplesofmetonymyincludetherelationbetweenthefollowingpairings\nofsenses:\nAUTHOR ↔ WORKSOFAUTHOR\n(JaneAustenwroteEmma) (IreallyloveJaneAusten)\nFRUITTREE ↔ FRUIT\n(Plumshavebeautifulblossoms) (Iateapreservedplumyesterday) 6 APPENDIXG • WORDSENSESANDWORDNET\nG.3 WordNet: A Database of Lexical Relations\nThe most commonly used resource for sense relations in English and many other\nWordNet languages is the WordNet lexical database (Fellbaum, 1998). English WordNet\nconsists of three separate databases, one each for nouns and verbs and a third for\nadjectivesandadverbs;closedclasswordsarenotincluded. Eachdatabasecontains\nasetoflemmas,eachoneannotatedwithasetofsenses. TheWordNet3.0release\nhas117,798nouns, 11,529verbs, 22,479adjectives, and4,481adverbs. Theaver-\nage noun has 1.23 senses, and the average verb has 2.16 senses. WordNet can be\naccessedontheWebordownloadedlocally. FigureG.1showsthelemmaentryfor\nthenounbass.\nThenoun“bass”has8sensesinWordNet.\n1.bass1-(thelowestpartofthemusicalrange)\n2.bass2,basspart1-(thelowestpartinpolyphonicmusic)\n3.bass3,basso1-(anadultmalesingerwiththelowestvoice)\n4.seabass1,bass4-(theleanfleshofasaltwaterfishofthefamilySerranidae)\n5.freshwaterbass1,bass5-(anyofvariousNorthAmericanfreshwaterfishwith\nleanflesh(especiallyofthegenusMicropterus))\n6.bass6,bassvoice1,basso2-(thelowestadultmalesingingvoice)\n7.bass7-(thememberwiththelowestrangeofafamilyofmusicalinstruments)\n8.bass8-(nontechnicalnameforanyofnumerousediblemarineand\nfreshwaterspiny-finnedfishes)\nFigureG.1 AportionoftheWordNet3.0entryforthenounbass.\ngloss Note that there are eight senses, each of which has a gloss (a dictionary-style\ndefinition), a list of synonyms for the sense, and sometimes also usage examples.\nWordNetdoesn’trepresentpronunciation, sodoesn’tdistinguishthepronunciation\n[baes]inbass4,bass5,andbass8fromtheothersensespronounced[beys].\nsynset Thesetofnear-synonymsforaWordNetsenseiscalledasynset(forsynonym\nset); synsets are an important primitive in WordNet. The entry for bass includes\nsynsets like {bass1, deep6}, or {bass6, bass voice1, basso2}. We can think of a\nsynset as representing a concept of the type we discussed in Appendix F. Thus,\ninsteadofrepresentingconceptsinlogicalterms,WordNetrepresentsthemaslists\nof the word senses that can be used to express the concept. Here’s another synset\nexample:\n{chump1, fool2, gull1, mark9, patsy1, fall guy1,\nsucker1, soft touch1, mug2}\nTheglossofthissynsetdescribesitas:\nGloss: apersonwhoisgullibleandeasytotakeadvantageof.\nGlossesarepropertiesofasynset,sothateachsenseincludedinthesynsethasthe\nsame gloss and can express this concept. Because they share glosses, synsets like\nthis one are the fundamental unit associated with WordNet entries, and hence it is\nsynsets,notwordforms,lemmas,orindividualsenses,thatparticipateinmostofthe\nlexicalsenserelationsinWordNet.\nWordNet also labels each synset with a lexicographic category drawn from a\nsemantic field for example the 26 categories for nouns shown in Fig. G.2, as well\nas15forverbs(plus2foradjectivesand1foradverbs). Thesecategoriesareoften 18 APPENDIXG • WORDSENSESANDWORDNET\nresourcesincludeAmsler’s1981useoftheMerriamWebsterdictionaryandLong-\nman’sDictionaryofContemporaryEnglish(BoguraevandBriscoe,1989).\nSupervised approaches to disambiguation began with the use of decision trees\nbyBlack(1988). InadditiontotheIMSandcontextual-embeddingbasedmethods\nforsupervisedWSD,recentsupervisedalgorithmsincludesencoder-decodermodels\n(Raganatoetal.,2017a).\nThe need for large amounts of annotated text in supervised methods led early\nontoinvestigationsintotheuseofbootstrappingmethods(Hearst1991,Yarowsky\n1995). For example the semi-supervised algorithm of Diab and Resnik (2002) is\nbased on aligned parallel corpora in two languages. For example, the fact that the\nFrench word catastrophe might be translated as English disaster in one instance\nand tragedy in another instance can be used to disambiguate the senses of the two\nEnglishwords(i.e.,tochoosesensesofdisasterandtragedythataresimilar).\nThe earliest use of clustering in the study of word senses was by Sparck Jones\n(1986);PedersenandBruce(1997),Schu¨tze(1997),andSchu¨tze(1998)applieddis-\ncoarsesenses tributionalmethods. Clusteringwordsensesintocoarsesenseshasalsobeenused\nto address the problem of dictionary senses being too fine-grained (Section G.5.3)\n(Dolan 1994, Chen and Chang 1998, Mihalcea and Moldovan 2001, Agirre and\ndeLacalle2003,Palmeretal.2004,Navigli2006,Snowetal.2007,Pilehvaretal.\n2013). Corporawithclusteredwordsensesfortrainingsupervisedclusteringalgo-\nOntoNotes rithmsincludePalmeretal.(2006)andOntoNotes(Hovyetal.,2006).\nSeePustejovsky(1995), PustejovskyandBoguraev(1996),Martin(1986), and\nCopestakeandBriscoe(1995), interalia, forcomputationalapproachestotherep-\ngenerative resentation of polysemy. Pustejovsky’s theory of the generative lexicon, and in\nlexicon\nqualia particularhistheoryofthequaliastructureofwords,isawayofaccountingforthe\nstructure\ndynamicsystematicpolysemyofwordsincontext.\nHistoricaloverviewsofWSDincludeAgirreandEdmonds(2006)andNavigli\n(2009).\nExercises\nG.1 Collect a small corpus of example sentences of varying lengths from any\nnewspaper or magazine. Using WordNet or any standard dictionary, deter-\nminehowmanysensesthereareforeachoftheopen-classwordsineachsen-\ntence. Howmanydistinctcombinationsofsensesarethereforeachsentence?\nHowdoesthisnumberseemtovarywithsentencelength?\nG.2 UsingWordNetorastandardreferencedictionary, tageachopen-classword\ninyourcorpuswithitscorrecttag. Waschoosingthecorrectsensealwaysa\nstraightforwardtask? Reportonanydifficultiesyouencountered.\nG.3 Using your favorite dictionary, simulate the original Lesk word overlap dis-\nambiguationalgorithmdescribedonpage13onthephraseTimeflieslikean\narrow. Assume that the words are to be disambiguated one at a time, from\nleft to right, and that the results from earlier decisions are used later in the\nprocess.\nG.4 Build an implementation of your solution to the previous exercise. Using\nWordNet, implement the original Lesk word overlap disambiguation algo-\nrithmdescribedonpage13onthephraseTimeflieslikeanarrow. 18 APPENDIXG • WORDSENSESANDWORDNET\nresourcesincludeAmsler’s1981useoftheMerriamWebsterdictionaryandLong-\nman’sDictionaryofContemporaryEnglish(BoguraevandBriscoe,1989).\nSupervised approaches to disambiguation began with the use of decision trees\nbyBlack(1988). InadditiontotheIMSandcontextual-embeddingbasedmethods\nforsupervisedWSD,recentsupervisedalgorithmsincludesencoder-decodermodels\n(Raganatoetal.,2017a).\nThe need for large amounts of annotated text in supervised methods led early\nontoinvestigationsintotheuseofbootstrappingmethods(Hearst1991,Yarowsky\n1995). For example the semi-supervised algorithm of Diab and Resnik (2002) is\nbased on aligned parallel corpora in two languages. For example, the fact that the\nFrench word catastrophe might be translated as English disaster in one instance\nand tragedy in another instance can be used to disambiguate the senses of the two\nEnglishwords(i.e.,tochoosesensesofdisasterandtragedythataresimilar).\nThe earliest use of clustering in the study of word senses was by Sparck Jones\n(1986);PedersenandBruce(1997),Schu¨tze(1997),andSchu¨tze(1998)applieddis-\ncoarsesenses tributionalmethods. Clusteringwordsensesintocoarsesenseshasalsobeenused\nto address the problem of dictionary senses being too fine-grained (Section G.5.3)\n(Dolan 1994, Chen and Chang 1998, Mihalcea and Moldovan 2001, Agirre and\ndeLacalle2003,Palmeretal.2004,Navigli2006,Snowetal.2007,Pilehvaretal.\n2013). Corporawithclusteredwordsensesfortrainingsupervisedclusteringalgo-\nOntoNotes rithmsincludePalmeretal.(2006)andOntoNotes(Hovyetal.,2006).\nSeePustejovsky(1995), PustejovskyandBoguraev(1996),Martin(1986), and\nCopestakeandBriscoe(1995), interalia, forcomputationalapproachestotherep-\ngenerative resentation of polysemy. Pustejovsky’s theory of the generative lexicon, and in\nlexicon\nqualia particularhistheoryofthequaliastructureofwords,isawayofaccountingforthe\nstructure\ndynamicsystematicpolysemyofwordsincontext.\nHistoricaloverviewsofWSDincludeAgirreandEdmonds(2006)andNavigli\n(2009).\nExercises\nG.1 Collect a small corpus of example sentences of varying lengths from any\nnewspaper or magazine. Using WordNet or any standard dictionary, deter-\nminehowmanysensesthereareforeachoftheopen-classwordsineachsen-\ntence. Howmanydistinctcombinationsofsensesarethereforeachsentence?\nHowdoesthisnumberseemtovarywithsentencelength?\nG.2 UsingWordNetorastandardreferencedictionary, tageachopen-classword\ninyourcorpuswithitscorrecttag. Waschoosingthecorrectsensealwaysa\nstraightforwardtask? Reportonanydifficultiesyouencountered.\nG.3 Using your favorite dictionary, simulate the original Lesk word overlap dis-\nambiguationalgorithmdescribedonpage13onthephraseTimeflieslikean\narrow. Assume that the words are to be disambiguated one at a time, from\nleft to right, and that the results from earlier decisions are used later in the\nprocess.\nG.4 Build an implementation of your solution to the previous exercise. Using\nWordNet, implement the original Lesk word overlap disambiguation algo-\nrithmdescribedonpage13onthephraseTimeflieslikeanarrow."
    },
    "5": {
        "chapter": "H",
        "section": "H.1 & H.2",
        "chapter_title": "Title of Sections H.1, H.2",
        "original_category": "",
        "original_text": "Speech and Language Processing. Daniel Jurafsky & James H. Martin. Copyright © 2024. All\nrights reserved. Draft of August 20, 2024.\nCHAPTER\nH Phonetics\nThecharactersthatmakeupthetextswe’vebeendiscussinginthisbookaren’tjust\nrandomsymbols. Theyarealsoanamazingscientificinvention: atheoreticalmodel\noftheelementsthatmakeuphumanspeech.\nThe earliest writing systems we know of (Sumerian, Chinese, Mayan) were\nmainly logographic: one symbol representing a whole word. But from the ear-\nliest stages we can find, some symbols were also used to represent the sounds\nthat made up words. The cuneiform sign to the right pro-\nnounced ba and meaning “ration” in Sumerian could also\nfunctionpurelyasthesound/ba/. TheearliestChinesechar-\nacters we have, carved into bones for divination, similarly\ncontain phonetic elements. Purely sound-based writing systems, whether syllabic\n(likeJapanesehiragana),alphabetic(liketheRomanalphabet),orconsonantal(like\nSemitic writing systems), trace back to these early logo-syllabic systems, often as\ntwoculturescametogether. Thus,theArabic,Aramaic,Hebrew,Greek,andRoman\nsystemsallderivefromaWestSemiticscriptthatispresumedtohavebeenmodified\nbyWesternSemiticmercenariesfromacursiveformofEgyptianhieroglyphs. The\nJapanesesyllabariesweremodifiedfromacursiveformofChinesephoneticcharac-\nters,whichthemselveswereusedinChinesetophoneticallyrepresenttheSanskrit\nintheBuddhistscripturesthatcametoChinaintheTangdynasty.\nThisimplicitideathatthespokenwordiscomposedofsmallerunitsofspeech\nunderliesalgorithmsforbothspeechrecognition(transcribingwaveformsintotext)\nandtext-to-speech(convertingtextintowaveforms). Inthischapterwegiveacom-\nphonetics putational perspective on phonetics, the study of the speech sounds used in the\nlanguagesoftheworld, howtheyareproducedinthehumanvocaltract, howthey\narerealizedacoustically,andhowtheycanbedigitizedandprocessed.\nH.1 Speech Sounds and Phonetic Transcription\nA letter like ‘p’ or ‘a’ is already a useful model of the sounds of human speech,\nand indeed we’ll see in Chapter 16 how to map between letters and waveforms.\nNonetheless,itishelpfultorepresentsoundsslightlymoreabstractly. We’llrepre-\nphone sent the pronunciation of a word as a string of phones, which are speech sounds,\neachrepresentedwithsymbolsadaptedfromtheRomanalphabet.\nThe standard phonetic representation for transcribing the world’s languages is\nIPA theInternationalPhoneticAlphabet(IPA),anevolvingstandardfirstdevelopedin\n1888,Butinthischapterwe’llinsteadrepresentphoneswiththeARPAbet(Shoup,\n1980),asimplephoneticalphabet(Fig.H.1)thatconvenientlyusesASCIIsymbols\ntorepresentanAmerican-EnglishsubsetoftheIPA.\nMany of the IPA and ARPAbet symbols are equivalent to familiar Roman let-\nters. So,forexample,theARPAbetphone[p]representstheconsonantsoundatthe 2 APPENDIXH • PHONETICS\nARPAbet IPA ARPAbet ARPAbet IPA ARPAbet\nSymbol Symbol Word Transcription Symbol Symbol Word Transcription\n[p] [p] parsley [paarsliy] [iy] [i] lily [lihliy]\n[t] [t] tea [tiy] [ih] [I] lily [lihliy]\n[k] [k] cook [kuhk] [ey] [eI] daisy [deyziy]\n[b] [b] bay [bey] [eh] [E] pen [pehn]\n[d] [d] dill [dihl] [ae] [æ] aster [aestaxr]\n[g] [g] garlic [gaarlixk] [aa] [A] poppy [paapiy]\n[m] [m] mint [mihnt] [ao] [O] orchid [aorkixd]\n[n] [n] nutmeg [nahtmehg] [uh] [U] wood [wuhd]\n[ng] [N] baking [beykixng] [ow] [oU] lotus [lowdxaxs]\n[f] [f] flour [flawaxr] [uw] [u] tulip [tuwlixp]\n[v] [v] clove [klowv] [ah] [2] butter [bahdxaxr]\n[th] [T] thick [thihk] [er] [Ç] bird [berd]\n[dh] [D] those [dhowz] [ay] [aI] iris [ayrixs]\n[s] [s] soup [suwp] [aw] [aU] flower [flawaxr]\n[z] [z] eggs [ehgz] [oy] [oI] soil [soyl]\n[sh] [S] squash [skwaash] [ax] [@] pita [piytax]\n[zh] [Z] ambrosia [aembrowzhax]\n[ch] [tS] cherry [chehriy]\n[jh] [dZ] jar [jhaar]\n[l] [l] licorice [lihkaxrixsh]\n[w] [w] kiwi [kiywiy]\n[r] [r] rice [rays]\n[y] [j] yellow [yehlow]\n[h] [h] honey [hahniy]\nFigureH.1 ARPAbetandIPAsymbolsforEnglishconsonants(left)andvowels(right).\nbeginningofplatypus,puma,andplantain,themiddleofleopard,ortheendofan-\ntelope. Ingeneral,however,themappingbetweenthelettersofEnglishorthography\nandphonesisrelativelyopaque; asinglelettercanrepresentverydifferentsounds\nindifferentcontexts. TheEnglishletterccorrespondstophone[k]incougar[kuw\ngaxr],butphone[s]incell[sehl]. Besidesappearingascandk,thephone[k]can\nappearaspartofx(fox[faaks]),asck(jackal[jhaekel])andascc(raccoon[rae\nkuwn]). Manyotherlanguages,forexample,Spanish,aremuchmoretransparent\nintheirsound-orthographymappingthanEnglish.\nH.2 Articulatory Phonetics\narticulatory Articulatoryphoneticsisthestudyofhowthesephonesareproducedasthevarious\nphonetics\norgansinthemouth,throat,andnosemodifytheairflowfromthelungs.\nTheVocalOrgans\nFigureH.2showstheorgansofspeech. Soundisproducedbytherapidmovement\nofair. Humansproducemostsoundsinspokenlanguagesbyexpellingairfromthe\nlungs through the windpipe (technically, the trachea) and then out the mouth or\nnose. Asitpassesthroughthetrachea,theairpassesthroughthelarynx,commonly\nknown as the Adam’s apple or voice box. The larynx contains two small folds of H.2 • ARTICULATORYPHONETICS 3\nFigureH.2 The vocal organs, shown in side view. (Figure from OpenStax University\nPhysics,CCBY4.0)\nmuscle,thevocalfolds(oftenreferredtonon-technicallyasthevocalcords),which\ncan be moved together or apart. The space between these two folds is called the\nglottis glottis. Ifthefoldsareclosetogether(butnottightlyclosed),theywillvibrateasair\npassesthroughthem;iftheyarefarapart,theywon’tvibrate. Soundsmadewiththe\nvoicedsound vocalfoldstogetherandvibratingarecalledvoiced;soundsmadewithoutthisvocal\nunvoicedsound cordvibrationarecalledunvoicedorvoiceless. Voicedsoundsinclude[b],[d],[g],\n[v],[z],andalltheEnglishvowels,amongothers. Unvoicedsoundsinclude[p],[t],\n[k],[f],[s],andothers.\nTheareaabovethetracheaiscalledthevocaltract;itconsistsoftheoraltract\nandthenasaltract. Aftertheairleavesthetrachea,itcanexitthebodythroughthe\nmouthorthenose. Mostsoundsaremadebyairpassingthroughthemouth. Sounds\nnasal made by air passing through the nose are called nasal sounds; nasal sounds (like\nEnglish[m],[n],and[ng])useboththeoralandnasaltractsasresonatingcavities.\nconsonant Phonesaredividedintotwomainclasses: consonantsandvowels. Bothkinds\nvowel ofsoundsareformedbythemotionofairthroughthemouth,throatornose. Con-\nsonantsaremadebyrestrictionorblockingoftheairflowinsomeway,andcanbe\nvoicedorunvoiced. Vowelshavelessobstruction,areusuallyvoiced,andaregen-\nerallylouderandlonger-lastingthanconsonants. Thetechnicaluseofthesetermsis\nmuchlikethecommonusage;[p],[b],[t],[d],[k],[g],[f],[v],[s],[z],[r],[l],etc.,\nare consonants; [aa], [ae], [ao], [ih], [aw], [ow], [uw], etc., are vowels. Semivow-\nels (such as [y] and [w]) have some of the properties of both; they are voiced like\nvowels,buttheyareshortandlesssyllabiclikeconsonants. H.2 • ARTICULATORYPHONETICS 5\nhasvoicedstopslike[b],[d],and[g]aswellasunvoicedstopslike[p],[t],and[k].\nStopsarealsocalledplosives.\nnasal Thenasalsounds[n],[m],and[ng]aremadebyloweringthevelumandallow-\ningairtopassintothenasalcavity.\nfricatives In fricatives, airflow is constricted but not cut off completely. The turbulent\nairflowthatresultsfromtheconstrictionproducesacharacteristic“hissing”sound.\nThe English labiodental fricatives [f] and [v] are produced by pressing the lower\nlip against the upper teeth, allowing a restricted airflow between the upper teeth.\nThedentalfricatives[th]and[dh]allowairtoflowaroundthetonguebetweenthe\nteeth. The alveolar fricatives [s] and [z] are produced with the tongue against the\nalveolarridge,forcingairovertheedgeoftheteeth. Inthepalato-alveolarfricatives\n[sh] and [zh], the tongue is at the back of the alveolar ridge, forcing air through a\ngrooveformedinthetongue. Thehigher-pitchedfricatives(inEnglish[s],[z],[sh]\nsibilants and[zh])arecalledsibilants. Stopsthatarefollowedimmediatelybyfricativesare\ncalledaffricates;theseincludeEnglish[ch](chicken)and[jh](giraffe).\napproximant Inapproximants,thetwoarticulatorsareclosetogetherbutnotcloseenoughto\ncauseturbulentairflow. InEnglish[y](yellow),thetonguemovesclosetotheroof\nofthemouthbutnotcloseenoughtocausetheturbulencethatwouldcharacterizea\nfricative. InEnglish[w](wood), thebackofthetonguecomesclosetothevelum.\nAmerican [r] can be formed in at least two ways; with just the tip of the tongue\nextendedandclosetothepalateorwiththewholetonguebunchedupnearthepalate.\n[l]isformedwiththetipofthetongueupagainstthealveolarridgeortheteeth,with\none or both sides of the tongue lowered to allow air to flow over it. [l] is called a\nlateralsoundbecauseofthedropinthesidesofthetongue.\ntap Ataporflap[dx]isaquickmotionofthetongueagainstthealveolarridge.The\nconsonantinthemiddleofthewordlotus([lowdxaxs])isatapinmostdialectsof\nAmericanEnglish;speakersofmanyU.K.dialectswouldusea[t]instead.\nVowels\nLikeconsonants, vowelscanbecharacterizedby thepositionofthearticulatorsas\nthey are made. The three most relevant parameters for vowels are what is called\nvowel height, which correlates roughly with the height of the highest part of the\ntongue, vowelfrontnessorbackness, indicatingwhetherthishighpointistoward\nthe front or back of the oral tract and whether the shape of the lips is rounded or\nnot. FigureH.4showsthepositionofthetonguefordifferentvowels.\ntongue palate\nclosed\nvelum\nbeet [iy] bat [ae] boot [uw]\nFigureH.4 TonguepositionsforEnglishhighfront[iy],lowfront[ae]andhighback[uw].\nIn the vowel [iy], for example, the highest point of the tongue is toward the\nfront of the mouth. In the vowel [uw], by contrast, the high-point of the tongue is\nlocatedtowardthebackofthemouth. Vowelsinwhichthetongueisraisedtoward\nFrontvowel the front are called front vowels; those in which the tongue is raised toward the H.3 • PROSODY 7\nσ σ σ\nOnset Rime Onset Rime Rime\nh Nucleus Coda g r Nucleus Coda Nucleus Coda\nae m iy n eh g z\nFigureH.6 Syllablestructureofham,green,eggs.σ=syllable.\nH.3 Prosody\nprosody Prosody is the study of the intonational and rhythmic aspects of language, and in\nparticular the use of F0, energy, and duration to convey pragmatic, affective, or\nconversation-interactional meanings.1 We’ll introduce these acoustic quantities in\ndetail in the next section when we turn to acoustic phonetics, but briefly we can\nthink of energy as the acoustic quality that we perceive as loudness, and F0 as the\nfrequency of the sound that is produced, the acoustic quality that we hear as the\npitch of an utterance. Prosody can be used to mark discourse structure, like the\ndifferencebetweenstatementsandquestions,orthewaythataconversationisstruc-\ntured. Prosodyisusedtomarkthesaliencyofaparticularwordorphrase. Prosody\nis heavily used for paralinguistic functions like conveying affective meanings like\nhappiness, surprise, or anger. And prosody plays an important role in managing\nturn-takinginconversation.\nH.3.1 ProsodicProminence: Accent,StressandSchwa\nprominence InanaturalutteranceofAmericanEnglish,somewordssoundmoreprominentthan\nothers, and certain syllables in these words are also more prominent than others.\nWhatwemeanbyprominenceisthatthesewordsorsyllablesareperceptuallymore\nsalient to the listener. Speakers make a word or syllable more salient in English\nbysayingitlouder, sayingitslower(soithasalongerduration), orbyvaryingF0\nduringtheword,makingithigherormorevariable.\npitchaccent Accent Werepresentprominenceviaalinguisticmarkercalledpitchaccent.Words\norsyllablesthatareprominentaresaidtobear(beassociatedwith)apitchaccent.\nThusthisutterancemightbepronouncedbyaccentingtheunderlinedwords:\n(H.1) I’malittlesurprisedtohearitcharacterizedashappy.\nLexicalStress Thesyllablesthatbearpitchaccentarecalledaccentedsyllables.\nNoteverysyllableofawordcanbeaccented: pitchaccenthastoberealizedonthe\nlexicalstress syllablethathaslexicalstress. Lexicalstressisapropertyoftheword’spronuncia-\ntionindictionaries;thesyllablethathaslexicalstressistheonethatwillbelouder\norlongerifthewordisaccented. Forexample,thewordsurprisedisstressedonits\nsecondsyllable,notitsfirst. (TrystressingtheothersyllablebysayingSURprised;\nhopefully that sounds wrong to you). Thus, if the word surprised receives a pitch\naccentinasentence,itisthesecondsyllablethatwillbestronger. Thefollowingex-\n1 Thewordisusedinadifferentbutrelatedwayinpoetry,tomeanthestudyofversemetricalstructure. 8 APPENDIXH • PHONETICS\nampleshowsunderlinedaccentedwordswiththestressedsyllablebearingtheaccent\n(thelouder,longersyllable)inboldface:\n(H.2) I’malittlesurprisedtohearitcharacterizedashappy.\nStress is marked in dictionaries. The CMU dictionary (CMU, 1993), for ex-\nample, marks vowels with 0 (unstressed) or 1 (stressed) as in entries for counter:\n[K AW1 N T ER0], or table: [T EY1 B AH0 L]. Difference in lexical stress can\naffectwordmeaning;thenouncontentispronounced[KAA1NTEH0NT],while\ntheadjectiveispronounced[KAA0NTEH1NT].\nReducedVowelsandSchwa Unstressedvowelscanbeweakenedevenfurtherto\nreducedvowel reducedvowels,themostcommonofwhichisschwa([ax]),asinthesecondvowel\nschwa of parakeet: [p ae r ax k iy t]. In a reduced vowel the articulatory gesture isn’t as\ncompleteasforafullvowel. Notallunstressedvowelsarereduced;anyvowel,and\ndiphthongsinparticular, canretainitsfullqualityeveninunstressedposition. For\nexample,thevowel[iy]canappearinstressedpositionasinthewordeat[iyt]orin\nunstressedpositionasinthewordcarry[kaeriy].\nprominence Insummary,thereisacontinuumofprosodicprominence,forwhichitisoften\nusefultorepresentlevelslikeaccented,stressed,fullvowel,andreducedvowel.\nH.3.2 ProsodicStructure\nSpokensentenceshaveprosodicstructure: somewordsseemtogroupnaturallyto-\ngether, while some words seem to have a noticeable break or disjuncture between\nprosodic them. Prosodic structure is often described in terms of prosodic phrasing, mean-\nphrasing\ning that an utterance has a prosodic phrase structure in a similar way to it having\na syntactic phrase structure. For example, the sentence I wanted to go to London,\nintonation but could only get tickets for France seems to have two main intonation phrases,\nphrase\ntheirboundaryoccurringatthecomma.Furthermore,inthefirstphrase,thereseems\nto be another set of lesser prosodic phrase boundaries (often called intermediate\nintermediate phrases) that split up the words as I wanted | to go | to London. These kinds of\nphrase\nintonation phrases are often correlated with syntactic structure constituents (Price\netal.1991,BennettandElfner2019).\nAutomatically predicting prosodic boundaries can be important for tasks like\nTTS.Modernapproachesusesequencemodelsthattakeeitherrawtextortextan-\nnotatedwithfeatureslikeparsetreesasinput,andmakeabreak/no-breakdecision\nateachwordboundary. Theycanbetrainedondatalabeledforprosodicstructure\nliketheBostonUniversityRadioNewsCorpus(Ostendorfetal.,1995).\nH.3.3 Tune\nTwo utterances with the same prominence and phrasing patterns can still differ\ntune prosodically by having different tunes. The tune of an utterance is the rise and\nfallofits F0overtime. A veryobviousexampleoftune isthedifferencebetween\nstatementsandyes-noquestionsinEnglish. Thesamewordscanbesaidwithafinal\nquestionrise F0risetoindicateayes-noquestion(calledaquestionrise):\nYou know what I mean ?\nfinalfall orafinaldropinF0(calledafinalfall)toindicateadeclarativeintonation: 22 APPENDIXH • PHONETICS\nspectrograph (Koenig et al., 1946), theoretical insights like the working out of the\nsource-filtertheoryandotherissuesinthemappingbetweenarticulationandacous-\ntics((Fant,1960),Stevensetal.1953,StevensandHouse1955,HeinzandStevens\n1961, Stevens and House 1961) the F1xF2 space of vowel formants (Peterson and\nBarney, 1952), the understanding of the phonetic nature of stress and the use of\nduration and intensity as cues (Fry, 1955), and a basic understanding of issues in\nphoneperception(MillerandNicely1955,Libermanetal.1952). Lehiste(1967)is\nacollectionofclassicpapersonacousticphonetics. Manyoftheseminalpapersof\nGunnarFanthavebeencollectedinFant(2004).\nExcellent textbooks on acoustic phonetics include Johnson (2003) and Lade-\nfoged (1996). Coleman (2005) includes an introduction to computational process-\ning of acoustics and speech from a linguistic perspective. Stevens (1998) lays out\nan influential theory of speech sound production. There are a number of software\npackagesforacousticphoneticanalysis.ProbablythemostwidelyusedoneisPraat\n(BoersmaandWeenink,2005).\nExercises\nH.1 FindthemistakesintheARPAbettranscriptionsofthefollowingwords:\na. “three”[dhri] d. “study”[stuhdi] g. “slight”[sliyt]\nb. “sing”[sihng] e. “though”[thow]\nc. “eyes”[ays] f. “planning”[pplaanihng]\nH.2 Ira Gershwin’s lyric for Let’s Call the Whole Thing Off talks about two pro-\nnunciations(each)ofthewords“tomato”,“potato”,and“either”. Transcribe\nintotheARPAbetbothpronunciationsofeachofthesethreewords.\nH.3 TranscribethefollowingwordsintheARPAbet:\n1. dark\n2. suit\n3. greasy\n4. wash\n5. water\nH.4 Takeawavefileofyourchoice. Someexamplesareonthetextbookwebsite.\nDownloadthePraatsoftware,anduseittotranscribethewavefilesattheword\nlevel and into ARPAbet phones, using Praat to help you play pieces of each\nwavefileandtolookatthewavefileandthespectrogram.\nH.5 RecordyourselfsayingfiveoftheEnglishvowels: [aa],[eh],[ae],[iy],[uw].\nFindF1andF2foreachofyourvowels. 22 APPENDIXH • PHONETICS\nspectrograph (Koenig et al., 1946), theoretical insights like the working out of the\nsource-filtertheoryandotherissuesinthemappingbetweenarticulationandacous-\ntics((Fant,1960),Stevensetal.1953,StevensandHouse1955,HeinzandStevens\n1961, Stevens and House 1961) the F1xF2 space of vowel formants (Peterson and\nBarney, 1952), the understanding of the phonetic nature of stress and the use of\nduration and intensity as cues (Fry, 1955), and a basic understanding of issues in\nphoneperception(MillerandNicely1955,Libermanetal.1952). Lehiste(1967)is\nacollectionofclassicpapersonacousticphonetics. Manyoftheseminalpapersof\nGunnarFanthavebeencollectedinFant(2004).\nExcellent textbooks on acoustic phonetics include Johnson (2003) and Lade-\nfoged (1996). Coleman (2005) includes an introduction to computational process-\ning of acoustics and speech from a linguistic perspective. Stevens (1998) lays out\nan influential theory of speech sound production. There are a number of software\npackagesforacousticphoneticanalysis.ProbablythemostwidelyusedoneisPraat\n(BoersmaandWeenink,2005).\nExercises\nH.1 FindthemistakesintheARPAbettranscriptionsofthefollowingwords:\na. “three”[dhri] d. “study”[stuhdi] g. “slight”[sliyt]\nb. “sing”[sihng] e. “though”[thow]\nc. “eyes”[ays] f. “planning”[pplaanihng]\nH.2 Ira Gershwin’s lyric for Let’s Call the Whole Thing Off talks about two pro-\nnunciations(each)ofthewords“tomato”,“potato”,and“either”. Transcribe\nintotheARPAbetbothpronunciationsofeachofthesethreewords.\nH.3 TranscribethefollowingwordsintheARPAbet:\n1. dark\n2. suit\n3. greasy\n4. wash\n5. water\nH.4 Takeawavefileofyourchoice. Someexamplesareonthetextbookwebsite.\nDownloadthePraatsoftware,anduseittotranscribethewavefilesattheword\nlevel and into ARPAbet phones, using Praat to help you play pieces of each\nwavefileandtolookatthewavefileandthespectrogram.\nH.5 RecordyourselfsayingfiveoftheEnglishvowels: [aa],[eh],[ae],[iy],[uw].\nFindF1andF2foreachofyourvowels."
    }
}