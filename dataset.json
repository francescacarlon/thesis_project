{
    "1": {
        "chapter": "17",
        "sections": "17.1",
        "topic": "English Word Classes",
        "original_category": "L",
        "original_text": "Parts of speech fall into two broad categories: closed class and open class. Closed classes are those with relatively fixed membership, such as prepositions [...]. By contrast, nouns and verbs are open classes [...]. Closed class words are generally function words like of, it, and, or you, which tend to be very short, occur frequently, and often have structuring uses in grammar. Four major open classes occur in the languages of the world: nouns [...], verbs, adjectives, and adverbs, [...]. Nouns are words for people, places, or things,  [...]. Verbs refer to actions and processes, including main verbs like draw, provide, and go. English verbs have inflections (non-third-person-singular (eat), third-personsingular (eats), progressive (eating), past participle (eaten)). [...]. Adjectives often describe properties or qualities of nouns, like color (white, black), age (old, young), and value (good, bad), [...]. Adverbs generally modify something. [...] A particle [...] is used in combination with a verb. Particles often have extended meanings that aren't quite the same as the prepositions they resemble, as in the particle 'over' in 'she turned the paper over'. A phrasal verb verb and a particle acting as a single unit is called a phrasal verb. The meaning of phrasal verbs is often non-compositional - not predictable from the individual meanings of the verb and the particle. [...]"
    },
    "2": {
        "chapter": "18",
        "sections": "18.1, 18.2",
        "topic": "Constituency and Context-Free Grammars",
        "original_category": "L",
        "original_text": "Syntactic constituency is the idea that groups of words can behave as single units, or constituents. Consider the noun phrase, a sequence of words surrounding at least one noun. [...] they can all appear in similar syntactic environments, for example, before a verb: 'three parties from Brooklyn arrive'. [...] A widely used formal system for modeling constituent structure in natural language is the context-free grammar (CFG). [...] A context-free grammar consists of a set of rules or productions, each of which expresses the ways that symbols of the language can be grouped and ordered together, and a lexicon of words and symbols. [...] an NP (or noun phrase) can be composed of either a ProperNoun or a determiner (Det) followed by a Nominal; [...]. Context-free rules can be hierarchically  embedded, [...]: Det → a, Det → the, Noun → flight. [...] Thus, a CFG can be used to generate a set of strings. This sequence of rule expansions is called a derivation of the string of words. It is common to represent a derivation by a parse tree (commonly shown inverted with the root at the top). [...]. A CFG like that of L0 defines a formal language. Sentences (strings of words) that can be derived by a grammar are in the formal language defined by that grammar, and are called grammatical sentences. Sentences that cannot be derived by a given formal grammar are not in the language defined by that grammar and are  referred to as ungrammatical. [...] In linguistics, the use of formal languages to model natural languages is called generative grammar [...]."
    },
    "3": {
        "chapter": "21",
        "sections": "21.1, 21.2",
        "topic": "Semantic Roles and Diathesis Alternations",
        "original_category": "L",
        "original_text": "Consider the meanings of the arguments Sasha, Pat, the window, and the door in these two sentences. Sasha broke the window. Pat opened the door. The subjects Sasha and Pat, what we might call the breaker of the window breaking event and the opener of the door-opening event, have something in common. They are both volitional actors, often animate, and they have direct causal responsibility for their events. Thematic roles are a way to capture this semantic commonality between breakers and openers. We say that the subjects of both these verbs are agents. Thus, AGENT is the thematic role that represents an abstract idea such as volitional causation. Similarly, the direct objects of both these verbs, the BrokenThing and OpenedThing, are both prototypically inanimate objects that are affected in some way by the action. The semantic role for these participants is theme. [...] Semantic roles thus help generalize over different surface realizations of predicate arguments. For example, while the AGENT is often realized as the subject of the sentence, in other cases the THEME can be the subject. [...] John (AGENT) broke the window (THEME). John (AGENT) broke the window (THEME) with a rock (INSTRUMENT). The rock (INSTRUMENT) broke the window (THEME). The window (THEME) broke. The window (THEME) was broken by John (AGENT). These examples suggest that break has (at least) the possible arguments AGENT, THEME, and INSTRUMENT. [...] These multiple argument structure realizations [...] are called verb alternations or diathesis alternations."
    },
    "4": {
        "chapter": "G",
        "sections": "G.1, G.2",
        "topic": "Word Senses and Relations Between Senses",
        "original_category": "L",
        "original_text": "A sense (or word sense) is a discrete representation of one aspect of the meaning of a word. [...] In context, it's easy to see the different meanings: mouse1: a mouse controlling a computer system in 1968. mouse2: a quiet animal like a mouse. bank1: a bank can hold the investments in a custodial account. bank2: as agriculture burgeons on the east bank, the river...[...] we need to consider the alternative ways that dictionaries and thesauruses offer for defining senses. One is based on the fact that dictionaries or thesauruses give textual definitions for each sense called glosses. [...] bank: 1. financial institution that accepts deposits and channels the money into lending activities 2. sloping land (especially the slope beside a body of water). [...] when two senses of two different words (lemmas) are identical, or nearly identical, we say the two senses are synonyms [...] couch/sofa vomit/throw up filbert/hazelnut car/automobile. [...] antonyms are words with an opposite meaning, like: long/short big/little fast/slow cold/hot dark/light. [...] [a] hyponym of [a] word if the [other word] is more specific, denoting a subclass of the other. For example, car is a hyponym of vehicle, dog is a hyponym of animal, [...]. Conversely, we say that vehicle is a hypernym of car, and animal is a hypernym of dog. [...] meronymy, the part-whole relation. [...] wheel is a meronym of car, and car is a holonym of wheel."
    },
    "5": {
        "chapter": "H",
        "sections": "H.1, H.2",
        "topic": "Phonetics",
        "original_category": "L",
        "original_text": "We'll represent the pronunciation of a word as a string of phones, which are speech sounds, each represented with symbols adapted from the Roman alphabet. The standard phonetic representation for transcribing the world's languages is the International Phonetic Alphabet (IPA), [...] the mapping between the letters of English orthography and phones is relatively opaque; a single letter can represent very different sounds in different contexts. [...] Many other languages, for example, Spanish, are much more transparent in their sound-orthography mapping than English. Articulatory phonetics is the study of how these phones are produced as the various articulatory phonetics organs in the mouth, throat, and nose modify the airflow from the lungs. [...] we can group [consonants] into classes by their point of maximum restriction, their place of articulation. [...] bilabial: [...] [p], [b], [m]. labiodental [v] and [f] [...]. dental: [...] [th] of thing and the [dh] of though. [...] alveolar: [s], [z], [t], [d]. [...] velar: [k], [g]. [...] Consonants are also distinguished by how the restriction in airflow is made [...]. This feature is called the manner of articulation [...]. voiced stops [b], [d], and [g],unvoiced stops [p], [t], and [k]. nasal sounds [n], [m], and [ng]. labiodental fricatives [f] and [v]. alveolar fricatives [s] and [z]. affricates [ch] and [jh]. approximants [y] and [w]."
    },
    "6": {
        "chapter": "7",
        "sections": "7.3",
        "topic": "Feed-Forward Neural Networks (FFNNs)",
        "original_category": "CS",
        "original_text": "A feedforward network is a multilayer network in which the units are connected with no cycles; the outputs from units in each layer are passed to units in the next higher layer, and no outputs are passed back to lower layers. [...] Simple feedforward networks have three kinds of nodes: input units, hidden units, and output units. [...] The input layer x is a vector of simple scalar values. [...] The core of the neural network is the hidden layer h formed of hidden units hi, each of which is a neural unit [...], taking a weighted sum of its inputs and then applying a non-linearity. In the standard architecture, each layer is fully-connected, meaning that each unit in each layer takes as input the outputs from all the units in the previous layer, and there is a link between every pair of units from two adjacent layers. [...] Recall that a single hidden unit has as parameters a weight vector and a bias. We represent the parameters for the entire hidden layer by combining the weight vector and bias for each unit i into a single weight matrix W and a single bias vector b for the whole layer. [...] the hidden layer computation for a feedforward network can be done very efficiently with simple matrix operations. In fact, the computation only has three steps: multiplying the weight matrix by the input vector x, adding the bias vector b, and applying the activation function g (such as the sigmoid, tanh, or ReLU activation function defined above). The output of the hidden layer, the vector h, is thus the following (for this example we'll use the sigmoid function σ as our activation function): h = σ(Wx+b). [...] The role of the output layer is to take this new representation h and compute a final output. This output could be a real valued number, but in many cases the goal of the network is to make some sort of classification decision, [...]. If we are doing a binary task like sentiment classification, we might have a single output node, and its scalar value y is the probability of positive versus negative sentiment. If we are doing multinomial classification, such as assigning a part-ofspeech tag, we might have one output node for each potential part-of-speech, whose output value is the probability of that part-of-speech, and the values of all the output nodes must sum to one. The output layer is thus a vector y that gives a probability distribution across the output node. [...] [...] what we need for  classification is a vector of probabilities. There is a convenient function for normalizing a vector of real values, by which we mean converting it to a vector that encodes a probability distribution (all the numbers lie softmax between 0 and 1 and sum to 1): the softmax function. [...] So a neural network is like multinomial logistic regression, but (a) with many layers, [...]; (b) with those intermediate layers having many possible activation functions (tanh, ReLU, sigmoid) [...]; (c) [...], the prior layers of the network induce the feature representations themselves. Here are the final equations for a feedforward network with a single hidden layer, which takes an input vector x, outputs a probability distribution y, and is parameterized by weight matrices W and U and a bias vector b: h = σ(Wx+b) z = Uh y = softmax(z). [...]"
    },
    "7": {
        "chapter": "7a",
        "sections": "7.5",
        "topic": " Training FFNNs (Loss Function and Gradient)",
        "original_category": "CS",
        "original_text": "A feedforward neural net is an instance of supervised machine learning in which we know the correct output y for each observation x. What the system produces is ˆy, the system's estimate of the true y. The goal of the training procedure is to learn parameters W[i] and b[i] for each layer i that make ˆy for each training observation as close as possible to the true y. [...] First, we'll need a loss function that models the distance between the system output and the gold output, and it's common to use the loss function used for logistic regression, the cross-entropy loss. Second, to find the parameters that minimize this loss function, we'll use the gradient descent optimization algorithm [...]. Third, gradient descent requires knowing the gradient of the loss function, the vector that contains the partial derivative of the loss function with respect to each of the parameters. [...] for neural networks, with millions of parameters in many layers, it's much harder to see how to compute the partial derivative of some weight in layer 1 when the loss is attached to some much later layer. How do we partial out the loss over all those intermediate layers? The answer is the algorithm called error backpropagation or backward differentiation. [Cross-entropy loss]: The true label y is then a vector with K elements, each corresponding to a class, with yc = 1 if the correct class is c, with all other elements of y being 0. Recall that a vector like this, with one value equal to 1 and the rest 0, is called a one-hot vector. And our classifier will produce an estimate vector with K elements yˆ, each element yˆk of which represents the estimated probability p(yk = 1|x). [...] the cross-entropy loss is simply the negative log of the output probability corresponding to the correct class, [...]. Plugging in the softmax formula and with K the number of classes: LCE(yˆ,y) = -log*(exp(zc)/sum of exp(zj)) (where c is the correct class). [...] Computing the gradient requires the partial derivative of the loss function with respect to each parameter. [...] The solution to computing this gradient is an algorithm called error backpropagation."
    },
    "8": {
        "chapter": "7b",
        "sections": "7.5",
        "topic": "Backward Pass",
        "original_category": "CS",
        "original_text": "The importance of the computation graph comes from the backward pass, which is used to compute the derivatives that we'll need for the weight update. [...] Backwards differentiation makes use of the chain rule [...]. Suppose we are computing the derivative of a composite function f(x) = u(v(x)). The derivative of f(x) is the derivative of u(x) with respect to v(x) times the derivative of v(x) with respect to x. [...] The chain rule extends to more than two functions. If computing the derivative of a composite function f(x) = u(v(w(x))), the derivative of f(x) is: df/dx = du/dv·dv/dw·dw/dx. The intuition of backward differentiation is to pass gradients back from the final node to all the nodes in the graph. [...] Each node takes an upstream gradient that is passed in from its parent node to the right, and for each of its inputs computes a local gradient (the gradient of its output with respect to its input), and uses the chain rule to multiply these two to compute a downstream gradient to be passed on to the next earlier node. Let's now compute the 3 derivatives we need. Since in the computation graph L = ce, we can directly compute the derivative ∂L/∂c: ∂L/∂c = e. For the other two, we'll need to use the chain rule: ∂L/∂a=∂L/∂e*∂e/∂a [and] ∂L/∂b=∂L/∂e*∂e/∂d*∂d/∂b."
    },
    "9": {
        "chapter": "8",
        "sections": "8.1",
        "topic": "Recurrent Neural Networks (RNNs)",
        "original_category": "CS",
        "original_text": "A recurrent neural network (RNN) is any network that contains a cycle within its network connections, meaning that the value of some unit is directly, or indirectly, dependent on its own earlier outputs as an input. [...] As with ordinary feedforward networks, an input vector representing the current input, xt, is multiplied by a weight matrix and then passed through a non-linear activation function to compute the values for a layer of hidden units. This hidden layer is then used to calculate a corresponding output, yt. [...] The key difference from a feedforward network lies in the recurrent link [...] [which] augments the input to the computation at the hidden layer with the value of the hidden layer from the preceding point in time. The hidden layer from the previous time step provides a form of memory, or context, that encodes earlier processing and informs the decisions to be made at later points in time. [...] the context embodied in the previous hidden layer can include information extending back to the beginning of the sequence. Adding this temporal dimension makes RNNs appear to be more complex than non-recurrent architectures. [...] we're still performing the standard feedforward calculation [...]. The most significant change lies in the new set of weights, U, that connect the hidden layer from the previous time step to the current hidden layer. [...] the [tailored] backpropagation algorithm [...] [is] referred to as backpropagation through time [...]."
    },
    "10": {
        "chapter": "8a",
        "sections": "8.5",
        "topic": "Long Short-Term Memory (LSTMs)",
        "original_category": "CS",
        "original_text": "[...] it is quite difficult to train RNNs for tasks that require a network to make use of information distant from the current point of processing. [...] the information encoded in hidden states tends to be fairly local, more relevant to the most recent parts of the input sequence and recent decisions. [...] the gradients are eventually driven to zero, a situation called the vanishing gradients problem. To address these issues, more complex network architectures have been designed to explicitly manage the task of maintaining relevant context over time, by enabling the network to learn to forget information that is no longer needed and to remember information required for decisions still to come. The most commonly used such extension to RNNs is the long short-term memory (LSTM) network. LSTMs [...] remove information no longer needed from the context, and adding information likely to be needed for later decision making. [...] LSTMs accomplish this by first adding an explicit context layer to the architecture [...], and control the flow of information into and out of the units [...]. These gates are implemented through the use of additional weights that operate sequentially on the input, and previous hidden layer, and previous context layers. [...] The purpose of [the forget gate] is to delete information from the context that is no longer needed. [...] the add gate to select the information to add to the current context. [...] The final gate is the output gate which is used to decide what information is required for the current hidden state. [...]"
    },
    "11": {
        "chapter": "4",
        "sections": "4.1, 4.2",
        "topic": "Naive Bayes Classifiers",
        "original_category": "CL",
        "original_text": "2 CHAPTER4 • NAIVEBAYES,TEXTCLASSIFICATION,ANDSENTIMENT\nFinally, one of the oldest tasks in text classification is assigning a library sub-\nject category or topic label to a text. Deciding whether a research paper concerns\nepidemiologyorinstead,perhaps,embryology,isanimportantcomponentofinfor-\nmationretrieval.Varioussetsofsubjectcategoriesexist,suchastheMeSH(Medical\nSubjectHeadings) thesaurus. Infact,aswewillsee,subjectcategoryclassification\nisthetaskforwhichthenaiveBayesalgorithmwasinventedin1961Maron(1961).\nClassification is essential for tasks below the level of the document as well.\nWe’vealreadyseenperioddisambiguation(decidingifaperiodistheendofasen-\ntence or part of a word), and word tokenization (deciding if a character should be\na word boundary). Even language modeling can be viewed as classification: each\nwordcanbethoughtofasaclass,andsopredictingthenextwordisclassifyingthe\ncontext-so-farintoaclassforeachnextword. Apart-of-speechtagger(Chapter17)\nclassifieseachoccurrenceofawordinasentenceas,e.g.,anounoraverb.\nThe goal of classification is to take a single observation, extract some useful\nfeatures, and thereby classify the observation into one of a set of discrete classes.\nOne method for classifying text is to use rules handwritten by humans. Handwrit-\ntenrule-basedclassifierscanbecomponentsofstate-of-the-artsystemsinlanguage\nprocessing. Butrulescanbefragile,assituationsordatachangeovertime,andfor\nsometaskshumansaren’tnecessarilygoodatcomingupwiththerules.\nThe most common way of doing text classification in language processing is\nsupervised\nmachine insteadviasupervisedmachinelearning,thesubjectofthischapter. Insupervised\nlearning\nlearning,wehaveadatasetofinputobservations,eachassociatedwithsomecorrect\noutput (a ‘supervision signal’). The goal of the algorithm is to learn how to map\nfromanewobservationtoacorrectoutput.\nFormally, the task of supervised classification is to take an input x and a fixed\nset of output classes Y ={y ,y ,...,y } and return a predicted class y∈Y. For\n1 2 M\ntext classification, we’ll sometimes talk about c (for “class”) instead of y as our\noutput variable, and d (for “document”) instead of x as our input variable. In the\nsupervisedsituationwehaveatrainingsetofNdocumentsthathaveeachbeenhand-\nlabeledwithaclass: {(d ,c ),....,(d ,c )}. Ourgoalistolearnaclassifierthatis\n1 1 N N\ncapable of mapping from a new document d to its correct class c∈C, whereC is\nsomesetofusefuldocumentclasses.Aprobabilisticclassifieradditionallywilltell\nus the probability of the observation being in the class. This full distribution over\nthe classes can be useful information for downstream decisions; avoiding making\ndiscretedecisionsearlyoncanbeusefulwhencombiningsystems.\nMany kinds of machine learning algorithms are used to build classifiers. This\nchapter introduces naive Bayes; the following one introduces logistic regression.\nTheseexemplifytwowaysofdoingclassification. Generativeclassifierslikenaive\nBayes build a model of how a class could generate some input data. Given an ob-\nservation, theyreturntheclassmostlikelytohavegeneratedtheobservation. Dis-\ncriminativeclassifierslikelogisticregressioninsteadlearnwhatfeaturesfromthe\ninputaremostusefultodiscriminatebetweenthedifferentpossibleclasses. While\ndiscriminative systems are often more accurate and hence more commonly used,\ngenerativeclassifiersstillhavearole.\n4.1 Naive Bayes Classifiers\nnaiveBayes In this section we introduce the multinomial naive Bayes classifier, so called be-\nclassifier\ncause it is a Bayesian classifier that makes a simplifying (naive) assumption about 4.1 • NAIVEBAYESCLASSIFIERS 3\nhowthefeaturesinteract.\nTheintuitionoftheclassifierisshowninFig.4.1. Werepresentatextdocument\nbagofwords as if it were a bag of words, that is, an unordered set of words with their position\nignored,keepingonlytheirfrequencyinthedocument. Intheexampleinthefigure,\ninsteadofrepresentingthewordorderinallthephraseslike“Ilovethismovie”and\n“I would recommend it”, we simply note that the word I occurred 5 times in the\nentireexcerpt,thewordit6times,thewordslove,recommend,andmovieonce,and\nsoon.\nit 6\nI 5\nI love this movie! It's sweet, the 4\nbut with satirical humor. The fairy always lovetoit to 3\ndialogue is great and the it whimsical it I and 3\nadventure scenes are fun... and seen are anyone seen 2\nfriend\nIt manages to be whimsical happy dialogue yet 1\nand romantic while laughing adventure recommend would 1\nat the conventions of the whosweet of msa ot vir ieical it whimsical 1\nfairy tale genre. I would it I butto romantic I times 1\nrecommend it to just about several yet sweet 1\nhumor\nanyone. I've seen it several again it the satirical 1\ntimes, and I'm always happy th toe sces ne ee sn I would adventure 1\nto see it again whenever I the themanages genre 1\nfun times\nhave a friend who hasn't I and and fairy 1\nabout\nseen it yet! whenever while humor 1\nhave\nconventions have 1\nwith great 1\n… …\nFigure4.1 IntuitionofthemultinomialnaiveBayesclassifierappliedtoamoviereview. Thepositionofthe\nwordsisignored(thebag-of-wordsassumption)andwemakeuseofthefrequencyofeachword.\nNaiveBayesisaprobabilisticclassifier, meaningthatforadocumentd, outof\nallclassesc∈C theclassifierreturnstheclasscˆwhichhasthemaximumposterior\nˆ probabilitygiventhedocument. InEq.4.1weusethehatnotationˆ tomean“our\nargmax estimateofthecorrectclass”,andweuseargmaxtomeananoperationthatselects\nthe argument (in this case the class c) that maximizes a function (in this case the\nprobabilityP(c|d).\ncˆ = argmaxP(c|d) (4.1)\nc∈C\nBayesian This idea of Bayesian inference has been known since the work of Bayes (1763),\ninference\nand was first applied to text classification by Mosteller and Wallace (1964). The\nintuition of Bayesian classification is to use Bayes’ rule to transform Eq. 4.1 into\nother probabilities that have some useful properties. Bayes’ rule is presented in\nEq. 4.2; it gives us a way to break down any conditional probability P(x|y) into\nthreeotherprobabilities:\nP(y|x)P(x)\nP(x|y)= (4.2)\nP(y) 4.1 • NAIVEBAYESCLASSIFIERS 3\nhowthefeaturesinteract.\nTheintuitionoftheclassifierisshowninFig.4.1. Werepresentatextdocument\nbagofwords as if it were a bag of words, that is, an unordered set of words with their position\nignored,keepingonlytheirfrequencyinthedocument. Intheexampleinthefigure,\ninsteadofrepresentingthewordorderinallthephraseslike“Ilovethismovie”and\n“I would recommend it”, we simply note that the word I occurred 5 times in the\nentireexcerpt,thewordit6times,thewordslove,recommend,andmovieonce,and\nsoon.\nit 6\nI 5\nI love this movie! It's sweet, the 4\nbut with satirical humor. The fairy always lovetoit to 3\ndialogue is great and the it whimsical it I and 3\nadventure scenes are fun... and seen are anyone seen 2\nfriend\nIt manages to be whimsical happy dialogue yet 1\nand romantic while laughing adventure recommend would 1\nat the conventions of the whosweet of msa ot vir ieical it whimsical 1\nfairy tale genre. I would it I butto romantic I times 1\nrecommend it to just about several yet sweet 1\nhumor\nanyone. I've seen it several again it the satirical 1\ntimes, and I'm always happy th toe sces ne ee sn I would adventure 1\nto see it again whenever I the themanages genre 1\nfun times\nhave a friend who hasn't I and and fairy 1\nabout\nseen it yet! whenever while humor 1\nhave\nconventions have 1\nwith great 1\n… …\nFigure4.1 IntuitionofthemultinomialnaiveBayesclassifierappliedtoamoviereview. Thepositionofthe\nwordsisignored(thebag-of-wordsassumption)andwemakeuseofthefrequencyofeachword.\nNaiveBayesisaprobabilisticclassifier, meaningthatforadocumentd, outof\nallclassesc∈C theclassifierreturnstheclasscˆwhichhasthemaximumposterior\nˆ probabilitygiventhedocument. InEq.4.1weusethehatnotationˆ tomean“our\nargmax estimateofthecorrectclass”,andweuseargmaxtomeananoperationthatselects\nthe argument (in this case the class c) that maximizes a function (in this case the\nprobabilityP(c|d).\ncˆ = argmaxP(c|d) (4.1)\nc∈C\nBayesian This idea of Bayesian inference has been known since the work of Bayes (1763),\ninference\nand was first applied to text classification by Mosteller and Wallace (1964). The\nintuition of Bayesian classification is to use Bayes’ rule to transform Eq. 4.1 into\nother probabilities that have some useful properties. Bayes’ rule is presented in\nEq. 4.2; it gives us a way to break down any conditional probability P(x|y) into\nthreeotherprobabilities:\nP(y|x)P(x)\nP(x|y)= (4.2)\nP(y) 4.2 • TRAININGTHENAIVEBAYESCLASSIFIER 5\nToapplythenaiveBayesclassifiertotext,wewilluseeachwordinthedocuments\nasafeature,assuggestedabove,andweconsidereachofthewordsinthedocument\nbywalkinganindexthrougheverywordpositioninthedocument:\npositions ← allwordpositionsintestdocument\n(cid:89)\nc NB = argmaxP(c) P(w i|c) (4.9)\nc∈C\ni∈positions\nNaiveBayescalculations, likecalculationsforlanguagemodeling, aredoneinlog\nspace, to avoid underflow and increase speed. Thus Eq. 4.9 is generally instead\nexpressed1as\n(cid:88)\nc NB = argmaxlogP(c)+ logP(w i|c) (4.10)\nc∈C\ni∈positions\nByconsideringfeaturesinlogspace,Eq.4.10computesthepredictedclassasalin-\nearfunctionofinputfeatures. Classifiersthatusealinearcombinationoftheinputs\nto make a classification decision —like naive Bayes and also logistic regression—\nlinear arecalledlinearclassifiers.\nclassifiers\n4.2 Training the Naive Bayes Classifier\nHowcanwelearntheprobabilitiesP(c)andP(f|c)? Let’sfirstconsiderthemaxi-\ni\nmumlikelihoodestimate. We’llsimplyusethefrequenciesinthedata. Fortheclass\npriorP(c)weaskwhatpercentageofthedocumentsinourtrainingsetareineach\nclass c. Let N be the number of documents in our training data with class c and\nc\nN bethetotalnumberofdocuments. Then:\ndoc\nN\nPˆ(c)= c (4.11)\nN\ndoc\nTolearntheprobabilityP(f|c),we’llassumeafeatureisjusttheexistenceofaword\ni\nin the document’s bag of words, and so we’ll want P(w|c), which we compute as\ni\nthefractionoftimesthewordw appearsamongallwordsinalldocumentsoftopic\ni\nc. Wefirstconcatenatealldocumentswithcategorycintoonebig“categoryc”text.\nThenweusethefrequencyofw inthisconcatenateddocumenttogiveamaximum\ni\nlikelihoodestimateoftheprobability:\ncount(w,c)\nPˆ(w i|c) = (cid:80) i (4.12)\ncount(w,c)\nw∈V\nHerethevocabularyVconsistsoftheunionofallthewordtypesinallclasses,not\njustthewordsinoneclassc.\nThere is a problem, however, with maximum likelihood training. Imagine we\naretryingtoestimatethelikelihoodoftheword“fantastic”givenclasspositive,but\nsupposetherearenotrainingdocumentsthatbothcontaintheword“fantastic”and\nare classified as positive. Perhaps the word “fantastic” happens to occur (sarcasti-\ncally?) intheclass negative. Insuchacasetheprobabilityforthisfeaturewillbe\nzero:\ncount(“fantastic”,positive)\nPˆ(“fantastic”|positive) = (cid:80) =0 (4.13)\ncount(w,positive)\nw∈V\n1 Inpracticethroughoutthisbook,we’lluselogtomeannaturallog(ln)whenthebaseisnotspecified. EXERCISES 21\n2014,Droretal.2017).\nFeatureselectionisamethodofremovingfeaturesthatareunlikelytogeneralize\nwell.Featuresaregenerallyrankedbyhowinformativetheyareabouttheclassifica-\ninformation tiondecision. Averycommonmetric,informationgain,tellsushowmanybitsof\ngain\ninformationthepresenceofthewordgivesusforguessingtheclass. Otherfeature\nselection metrics include χ2, pointwise mutual information, and GINI index; see\nYangandPedersen(1997)foracomparisonandGuyonandElisseeff(2003)foran\nintroductiontofeatureselection.\nExercises\n4.1 Assume the following likelihoods for each word being part of a positive or\nnegativemoviereview,andequalpriorprobabilitiesforeachclass.\npos neg\nI 0.09 0.16\nalways 0.07 0.06\nlike 0.29 0.06\nforeign 0.04 0.15\nfilms 0.08 0.11\nWhat class will Naive bayes assign to the sentence “I always like foreign\nfilms.”?\n4.2 Given the following short movie reviews, each labeled with a genre, either\ncomedyoraction:\n1. fun,couple,love,love comedy\n2. fast,furious,shoot action\n3. couple,fly,fast,fun,fun comedy\n4. furious,shoot,shoot,fun action\n5. fly,fast,shoot,love action\nandanewdocumentD:\nfast,couple,shoot,fly\ncomputethemostlikelyclassforD.AssumeanaiveBayesclassifieranduse\nadd-1smoothingforthelikelihoods.\n4.3 Traintwomodels, multinomialnaiveBayesandbinarizednaiveBayes, both\nwith add-1 smoothing, on the following document counts for key sentiment\nwords,withpositiveornegativeclassassignedasnoted.\ndoc “good” “poor” “great” (class)\nd1. 3 0 3 pos\nd2. 0 1 2 pos\nd3. 1 3 0 neg\nd4. 1 5 2 neg\nd5. 0 2 0 neg\nUsebothnaiveBayesmodelstoassignaclass(posorneg)tothissentence:\nAgood,goodplotandgreatcharacters,butpooracting.\nRecallfrompage6thatwithnaiveBayestextclassification,wesimplyignore\n(throwout)anywordthatneveroccurredinthetrainingdocument. (Wedon’t\nthrowoutwordsthatappearinsomeclassesbutnotothers; that’swhatadd-\nonesmoothingisfor.) Dothetwomodelsagreeordisagree? EXERCISES 21\n2014,Droretal.2017).\nFeatureselectionisamethodofremovingfeaturesthatareunlikelytogeneralize\nwell.Featuresaregenerallyrankedbyhowinformativetheyareabouttheclassifica-\ninformation tiondecision. Averycommonmetric,informationgain,tellsushowmanybitsof\ngain\ninformationthepresenceofthewordgivesusforguessingtheclass. Otherfeature\nselection metrics include χ2, pointwise mutual information, and GINI index; see\nYangandPedersen(1997)foracomparisonandGuyonandElisseeff(2003)foran\nintroductiontofeatureselection.\nExercises\n4.1 Assume the following likelihoods for each word being part of a positive or\nnegativemoviereview,andequalpriorprobabilitiesforeachclass.\npos neg\nI 0.09 0.16\nalways 0.07 0.06\nlike 0.29 0.06\nforeign 0.04 0.15\nfilms 0.08 0.11\nWhat class will Naive bayes assign to the sentence “I always like foreign\nfilms.”?\n4.2 Given the following short movie reviews, each labeled with a genre, either\ncomedyoraction:\n1. fun,couple,love,love comedy\n2. fast,furious,shoot action\n3. couple,fly,fast,fun,fun comedy\n4. furious,shoot,shoot,fun action\n5. fly,fast,shoot,love action\nandanewdocumentD:\nfast,couple,shoot,fly\ncomputethemostlikelyclassforD.AssumeanaiveBayesclassifieranduse\nadd-1smoothingforthelikelihoods.\n4.3 Traintwomodels, multinomialnaiveBayesandbinarizednaiveBayes, both\nwith add-1 smoothing, on the following document counts for key sentiment\nwords,withpositiveornegativeclassassignedasnoted.\ndoc “good” “poor” “great” (class)\nd1. 3 0 3 pos\nd2. 0 1 2 pos\nd3. 1 3 0 neg\nd4. 1 5 2 neg\nd5. 0 2 0 neg\nUsebothnaiveBayesmodelstoassignaclass(posorneg)tothissentence:\nAgood,goodplotandgreatcharacters,butpooracting.\nRecallfrompage6thatwithnaiveBayestextclassification,wesimplyignore\n(throwout)anywordthatneveroccurredinthetrainingdocument. (Wedon’t\nthrowoutwordsthatappearinsomeclassesbutnotothers; that’swhatadd-\nonesmoothingisfor.) Dothetwomodelsagreeordisagree? 22 Chapter4 • NaiveBayes,TextClassification,andSentiment\nAggarwal,C.C.andC.Zhai.2012. Asurveyoftextclassi- Heckerman,D.,E.Horvitz,M.Sahami,andS.T.Dumais.\nficationalgorithms. InC.C.AggarwalandC.Zhai,eds, 1998.Abayesianapproachtofilteringjunke-mail.AAAI-\nMiningtextdata,163–222.Springer. 98WorkshoponLearningforTextCategorization.\nBayes,T.1763. AnEssayTowardSolvingaProbleminthe Hu,M.andB.Liu.2004.Miningandsummarizingcustomer\nDoctrineofChances,volume53.ReprintedinFacsimiles reviews.KDD.\nofTwoPapersbyBayes,HafnerPublishing,1963. Hutchinson, B., V. Prabhakaran, E. Denton, K. Webster,\nBerg-Kirkpatrick, T., D.Burkett, andD.Klein.2012. An Y. Zhong, and S. Denuyl. 2020. Social biases in NLP\nempiricalinvestigationofstatisticalsignificanceinNLP. modelsasbarriersforpersonswithdisabilities.ACL.\nEMNLP. Jaech,A.,G.Mulcaire,S.Hathi,M.Ostendorf,andN.A.\nBisani,M.andH.Ney.2004. Bootstrapestimatesforconfi- Smith.2016.Hierarchicalcharacter-wordmodelsforlan-\ndenceintervalsinASRperformanceevaluation.ICASSP. guageidentification. ACLWorkshoponNLPforSocial\nMedia.\nBishop,C.M.2006.Patternrecognitionandmachinelearn-\ning.Springer. Jauhiainen, T., M. Lui, M. Zampieri, T. Baldwin, and\nK.Linde´n.2019. Automaticlanguageidentificationin\nBlodgett,S.L.,S.Barocas,H.Daume´III,andH.Wallach.\ntexts:Asurvey.JAIR,65(1):675–682.\n2020. Language(technology)ispower:Acriticalsurvey\nof“bias”inNLP.ACL. Jurgens,D.,Y.Tsvetkov,andD.Jurafsky.2017. Incorpo-\nratingdialectalvariabilityforsociallyequitablelanguage\nBlodgett,S.L.,L.Green,andB.O’Connor.2016. Demo-\nidentification.ACL.\ngraphicdialectalvariationinsocialmedia: Acasestudy\nofAfrican-AmericanEnglish.EMNLP. Kiritchenko, S. and S. M. Mohammad. 2018. Examining\ngenderandracebiasintwohundredsentimentanalysis\nBorges,J.L.1964.Theanalyticallanguageofjohnwilkins.\nsystems.*SEM.\nIn Other inquisitions 1937–1952. University of Texas\nPress.Trans.RuthL.C.Simms. Liu,B.andL.Zhang.2012.Asurveyofopinionminingand\nsentimentanalysis. InC.C.AggarwalandC.Zhai,eds,\nCaliskan,A.,J.J.Bryson,andA.Narayanan.2017.Seman-\nMiningtextdata,415–464.Springer.\nticsderivedautomaticallyfromlanguagecorporacontain\nhuman-likebiases.Science,356(6334):183–186. Lui,M.andT.Baldwin.2011. Cross-domainfeatureselec-\ntionforlanguageidentification.IJCNLP.\nChinchor,N.,L.Hirschman,andD.L.Lewis.1993. Eval-\nuatingMessageUnderstandingsystems: Ananalysisof Lui,M.andT.Baldwin.2012.langid.py:Anoff-the-shelf\nthethirdMessageUnderstandingConference. Computa- languageidentificationtool.ACL.\ntionalLinguistics,19(3):409–449. Manning,C.D.,P.Raghavan,andH.Schu¨tze.2008. Intro-\nCrawford, K. 2017. The trouble with bias. Keynote at ductiontoInformationRetrieval.Cambridge.\nNeurIPS. Maron,M.E.1961. Automaticindexing: anexperimental\nDavidson,T.,D.Bhattacharya,andI.Weber.2019. Racial inquiry.JournaloftheACM,8(3):404–417.\nbias in hate speech and abusive language detection McCallum,A.andK.Nigam.1998. Acomparisonofevent\ndatasets.ThirdWorkshoponAbusiveLanguageOnline. modelsfornaivebayestextclassification.AAAI/ICML-98\nDiasOliva,T.,D.Antonialli,andA.Gomes.2021.Fighting WorkshoponLearningforTextCategorization.\nhatespeech,silencingdragqueens?artificialintelligence Metsis, V., I. Androutsopoulos, and G. Paliouras. 2006.\nin content moderation and risks to lgbtq voices online. Spam filtering with naive bayes-which naive bayes?\nSexuality&Culture,25:700–732. CEAS.\nDixon,L.,J.Li,J.Sorensen,N.Thain,andL.Vasserman. Minsky,M.1961. Stepstowardartificialintelligence. Pro-\n2018. Measuringandmitigatingunintendedbiasintext ceedingsoftheIRE,49(1):8–30.\nclassification.2018AAAI/ACMConferenceonAI,Ethics,\nMitchell,M.,S.Wu,A.Zaldivar,P.Barnes,L.Vasserman,\nandSociety.\nB.Hutchinson,E.Spitzer,I.D.Raji,andT.Gebru.2019.\nDror,R.,G.Baumer,M.Bogomolov,andR.Reichart.2017. Modelcardsformodelreporting.ACMFAccT.\nReplicability analysis for natural language processing:\nMosteller,F.andD.L.Wallace.1963. Inferenceinanau-\nTestingsignificancewithmultipledatasets.TACL,5:471–\nthorshipproblem:Acomparativestudyofdiscrimination\n–486.\nmethodsappliedtotheauthorshipofthedisputedfeder-\nDror, R., L. Peled-Cohen, S. Shlomov, and R. Reichart. alistpapers.JournaloftheAmericanStatisticalAssocia-\n2020. Statistical Significance Testing for Natural Lan- tion,58(302):275–309.\nguage Processing, volume 45 of Synthesis Lectures on\nMosteller,F.andD.L.Wallace.1964. InferenceandDis-\nHumanLanguageTechnologies.Morgan&Claypool.\nputedAuthorship:TheFederalist.Springer-Verlag.1984\nEfron,B.andR.J.Tibshirani.1993. Anintroductiontothe 2ndedition:AppliedBayesianandClassicalInference.\nbootstrap.CRCpress.\nMurphy,K.P.2012. Machinelearning:Aprobabilisticper-\nGillick,L.andS.J.Cox.1989.Somestatisticalissuesinthe spective.MITPress.\ncomparisonofspeechrecognitionalgorithms.ICASSP.\nNoreen,E.W.1989.ComputerIntensiveMethodsforTesting\nGuyon,I.andA.Elisseeff.2003.Anintroductiontovariable Hypothesis.Wiley.\nandfeatureselection.JMLR,3:1157–1182.\nPang,B.andL.Lee.2008. Opinionminingandsentiment\nHastie,T.,R.J.Tibshirani,andJ.H.Friedman.2001. The analysis.Foundationsandtrendsininformationretrieval,\nElementsofStatisticalLearning.Springer. 2(1-2):1–135."
    },
    "12": {
        "chapter": "6",
        "sections": "6.2",
        "topic": "Vector Semantics",
        "original_category": "CL",
        "original_text": "6.2 • VECTORSEMANTICS 5\nValence Arousal Dominance\ncourageous 8.05 5.5 7.38\nmusic 7.67 5.57 6.5\nheartbreak 2.45 5.65 3.58\ncub 6.71 3.95 4.24\nOsgood et al. (1957) noticed that in using these 3 numbers to represent the\nmeaning of a word, the model was representing each word as a point in a three-\ndimensional space, a vector whose three dimensions corresponded to the word’s\nratingonthethreescales. Thisrevolutionaryideathatwordmeaningcouldberep-\nresented as a point in space (e.g., that part of the meaning of heartbreak can be\nrepresentedasthepoint[2.45,5.65,3.58])wasthefirstexpressionofthevectorse-\nmanticsmodelsthatweintroducenext.\n6.2 Vector Semantics\nvector Vector semantics is the standard way to represent word meaning in NLP, helping\nsemantics\nusmodelmanyoftheaspectsofwordmeaningwesawintheprevioussection. The\nroots of the model lie in the 1950s when two big ideas converged: Osgood’s 1957\nidea mentioned above to use a point in three-dimensional space to represent the\nconnotationofaword,andtheproposalbylinguistslikeJoos(1950),Harris(1954),\nand Firth (1957) to define the meaning of a word by its distribution in language\nuse, meaning its neighboring words or grammatical environments. Their idea was\nthattwowordsthatoccurinverysimilardistributions(whoseneighboringwordsare\nsimilar)havesimilarmeanings.\nForexample,supposeyoudidn’tknowthemeaningofthewordongchoi(are-\ncentborrowingfromCantonese)butyouseeitinthefollowingcontexts:\n(6.1) Ongchoiisdelicioussauteedwithgarlic.\n(6.2) Ongchoiissuperboverrice.\n(6.3) ...ongchoileaveswithsaltysauces...\nAndsupposethatyouhadseenmanyofthesecontextwordsinothercontexts:\n(6.4) ...spinachsauteedwithgarlicoverrice...\n(6.5) ...chardstemsandleavesaredelicious...\n(6.6) ...collardgreensandothersaltyleafygreens\nThe fact that ongchoi occurs with words like rice and garlic and delicious and\nsalty,asdowordslikespinach,chard,andcollardgreensmightsuggestthatongchoi\nis a leafy green similar to these other leafy greens.1 We can do the same thing\ncomputationallybyjustcountingwordsinthecontextofongchoi.\nTheideaofvectorsemanticsistorepresentawordasapointinamultidimen-\nsional semantic space that is derived (in ways we’ll see) from the distributions of\nembeddings word neighbors. Vectors for representing words are called embeddings (although\nthe term is sometimes more strictly applied only to dense vectors like word2vec\n(Section 6.8), rather than sparse tf-idf or PPMI vectors (Section 6.3-Section 6.6)).\nTheword“embedding”derivesfromitsmathematicalsenseasamappingfromone\nspace or structure to another, although the meaning has shifted; see the end of the\nchapter.\n1 It’sinfactIpomoeaaquatica,arelativeofmorningglorysometimescalledwaterspinachinEnglish. 6.2 • VECTORSEMANTICS 5\nValence Arousal Dominance\ncourageous 8.05 5.5 7.38\nmusic 7.67 5.57 6.5\nheartbreak 2.45 5.65 3.58\ncub 6.71 3.95 4.24\nOsgood et al. (1957) noticed that in using these 3 numbers to represent the\nmeaning of a word, the model was representing each word as a point in a three-\ndimensional space, a vector whose three dimensions corresponded to the word’s\nratingonthethreescales. Thisrevolutionaryideathatwordmeaningcouldberep-\nresented as a point in space (e.g., that part of the meaning of heartbreak can be\nrepresentedasthepoint[2.45,5.65,3.58])wasthefirstexpressionofthevectorse-\nmanticsmodelsthatweintroducenext.\n6.2 Vector Semantics\nvector Vector semantics is the standard way to represent word meaning in NLP, helping\nsemantics\nusmodelmanyoftheaspectsofwordmeaningwesawintheprevioussection. The\nroots of the model lie in the 1950s when two big ideas converged: Osgood’s 1957\nidea mentioned above to use a point in three-dimensional space to represent the\nconnotationofaword,andtheproposalbylinguistslikeJoos(1950),Harris(1954),\nand Firth (1957) to define the meaning of a word by its distribution in language\nuse, meaning its neighboring words or grammatical environments. Their idea was\nthattwowordsthatoccurinverysimilardistributions(whoseneighboringwordsare\nsimilar)havesimilarmeanings.\nForexample,supposeyoudidn’tknowthemeaningofthewordongchoi(are-\ncentborrowingfromCantonese)butyouseeitinthefollowingcontexts:\n(6.1) Ongchoiisdelicioussauteedwithgarlic.\n(6.2) Ongchoiissuperboverrice.\n(6.3) ...ongchoileaveswithsaltysauces...\nAndsupposethatyouhadseenmanyofthesecontextwordsinothercontexts:\n(6.4) ...spinachsauteedwithgarlicoverrice...\n(6.5) ...chardstemsandleavesaredelicious...\n(6.6) ...collardgreensandothersaltyleafygreens\nThe fact that ongchoi occurs with words like rice and garlic and delicious and\nsalty,asdowordslikespinach,chard,andcollardgreensmightsuggestthatongchoi\nis a leafy green similar to these other leafy greens.1 We can do the same thing\ncomputationallybyjustcountingwordsinthecontextofongchoi.\nTheideaofvectorsemanticsistorepresentawordasapointinamultidimen-\nsional semantic space that is derived (in ways we’ll see) from the distributions of\nembeddings word neighbors. Vectors for representing words are called embeddings (although\nthe term is sometimes more strictly applied only to dense vectors like word2vec\n(Section 6.8), rather than sparse tf-idf or PPMI vectors (Section 6.3-Section 6.6)).\nTheword“embedding”derivesfromitsmathematicalsenseasamappingfromone\nspace or structure to another, although the meaning has shifted; see the end of the\nchapter.\n1 It’sinfactIpomoeaaquatica,arelativeofmorningglorysometimescalledwaterspinachinEnglish. 6.3 • WORDSANDVECTORS 7\naparticularword(definedbytherow)occursinaparticulardocument(definedby\nthecolumn). Thusfoolappeared58timesinTwelfthNight.\nAsYouLikeIt TwelfthNight JuliusCaesar HenryV\nbattle 1 0 7 13\ngood 114 80 62 89\nfool 36 58 1 4\nwit 20 15 2 3\nFigure6.2 Theterm-documentmatrixforfourwordsinfourShakespeareplays.Eachcell\ncontainsthenumberoftimesthe(row)wordoccursinthe(column)document.\nThe term-document matrix of Fig. 6.2 was first defined as part of the vector\nvectorspace space model of information retrieval (Salton, 1971). In this model, a document is\nmodel\nrepresentedasacountvector,acolumninFig.6.3.\nvector Toreviewsomebasiclinearalgebra, avectoris, atheart, justalistorarrayof\nnumbers. SoAsYouLikeItisrepresentedasthelist[1,114,36,20](thefirstcolumn\nvector in Fig.6.3) and Julius Caesar is representedas the list [7,62,1,2](the third\nvectorspace column vector). A vector space is a collection of vectors, and is characterized by\ndimension its dimension. Vectors in a 3-dimensional vector space have an element for each\ndimensionofthespace. Wewilllooselyrefertoavectorina4-dimensionalspace\nasa4-dimensionalvector,withoneelementalongeachdimension. Intheexample\ninFig.6.3,we’vechosentomakethedocumentvectorsofdimension4,justsothey\nfit on the page; in real term-document matrices, the document vectors would have\ndimensionality|V|,thevocabularysize.\nTheorderingofthenumbersinavectorspaceindicatesthedifferentdimensions\nonwhichdocumentsvary. Thefirstdimensionforboththesevectorscorrespondsto\nthe number of times the word battle occurs, and we can compare each dimension,\nnotingforexamplethatthevectorsforAsYouLikeItandTwelfthNighthavesimilar\nvalues(1and0,respectively)forthefirstdimension.\nAsYouLikeIt TwelfthNight JuliusCaesar HenryV\nbattle 1 0 7 13\ngood 114 80 62 89\nfool 36 58 1 4\nwit 20 15 2 3\nFigure6.3 Theterm-documentmatrixforfourwordsinfourShakespeareplays. Thered\nboxesshowthateachdocumentisrepresentedasacolumnvectoroflengthfour.\nWecanthinkofthevectorforadocumentasapointin|V|-dimensionalspace;\nthusthedocumentsinFig.6.3arepointsin4-dimensionalspace.Since4-dimensional\nspacesarehardtovisualize,Fig.6.4showsavisualizationintwodimensions;we’ve\narbitrarilychosenthedimensionscorrespondingtothewordsbattleandfool.\nTerm-document matrices were originally defined as a means of finding similar\ndocumentsforthetaskofdocumentinformationretrieval. Twodocumentsthatare\nsimilar will tend to have similar words, and if two documents have similar words\ntheir column vectors will tend to be similar. The vectors for the comedies As You\nLikeIt[1,114,36,20]andTwelfthNight[0,80,58,15]lookalotmorelikeeachother\n(more fools and wit than battles) than they look like Julius Caesar [7,62,1,2] or\nHenry V [13,89,4,3]. This is clear with the raw numbers; in the first dimension\n(battle)thecomedieshavelownumbersandtheothershavehighnumbers,andwe\ncan see it visually in Fig. 6.4; we’ll see very shortly how to quantify this intuition\nmoreformally. 6.3 • WORDSANDVECTORS 7\naparticularword(definedbytherow)occursinaparticulardocument(definedby\nthecolumn). Thusfoolappeared58timesinTwelfthNight.\nAsYouLikeIt TwelfthNight JuliusCaesar HenryV\nbattle 1 0 7 13\ngood 114 80 62 89\nfool 36 58 1 4\nwit 20 15 2 3\nFigure6.2 Theterm-documentmatrixforfourwordsinfourShakespeareplays.Eachcell\ncontainsthenumberoftimesthe(row)wordoccursinthe(column)document.\nThe term-document matrix of Fig. 6.2 was first defined as part of the vector\nvectorspace space model of information retrieval (Salton, 1971). In this model, a document is\nmodel\nrepresentedasacountvector,acolumninFig.6.3.\nvector Toreviewsomebasiclinearalgebra, avectoris, atheart, justalistorarrayof\nnumbers. SoAsYouLikeItisrepresentedasthelist[1,114,36,20](thefirstcolumn\nvector in Fig.6.3) and Julius Caesar is representedas the list [7,62,1,2](the third\nvectorspace column vector). A vector space is a collection of vectors, and is characterized by\ndimension its dimension. Vectors in a 3-dimensional vector space have an element for each\ndimensionofthespace. Wewilllooselyrefertoavectorina4-dimensionalspace\nasa4-dimensionalvector,withoneelementalongeachdimension. Intheexample\ninFig.6.3,we’vechosentomakethedocumentvectorsofdimension4,justsothey\nfit on the page; in real term-document matrices, the document vectors would have\ndimensionality|V|,thevocabularysize.\nTheorderingofthenumbersinavectorspaceindicatesthedifferentdimensions\nonwhichdocumentsvary. Thefirstdimensionforboththesevectorscorrespondsto\nthe number of times the word battle occurs, and we can compare each dimension,\nnotingforexamplethatthevectorsforAsYouLikeItandTwelfthNighthavesimilar\nvalues(1and0,respectively)forthefirstdimension.\nAsYouLikeIt TwelfthNight JuliusCaesar HenryV\nbattle 1 0 7 13\ngood 114 80 62 89\nfool 36 58 1 4\nwit 20 15 2 3\nFigure6.3 Theterm-documentmatrixforfourwordsinfourShakespeareplays. Thered\nboxesshowthateachdocumentisrepresentedasacolumnvectoroflengthfour.\nWecanthinkofthevectorforadocumentasapointin|V|-dimensionalspace;\nthusthedocumentsinFig.6.3arepointsin4-dimensionalspace.Since4-dimensional\nspacesarehardtovisualize,Fig.6.4showsavisualizationintwodimensions;we’ve\narbitrarilychosenthedimensionscorrespondingtothewordsbattleandfool.\nTerm-document matrices were originally defined as a means of finding similar\ndocumentsforthetaskofdocumentinformationretrieval. Twodocumentsthatare\nsimilar will tend to have similar words, and if two documents have similar words\ntheir column vectors will tend to be similar. The vectors for the comedies As You\nLikeIt[1,114,36,20]andTwelfthNight[0,80,58,15]lookalotmorelikeeachother\n(more fools and wit than battles) than they look like Julius Caesar [7,62,1,2] or\nHenry V [13,89,4,3]. This is clear with the raw numbers; in the first dimension\n(battle)thecomedieshavelownumbersandtheothershavehighnumbers,andwe\ncan see it visually in Fig. 6.4; we’ll see very shortly how to quantify this intuition\nmoreformally. 10 CHAPTER6 • VECTORSEMANTICSANDEMBEDDINGS\ninthetrainingcorpus; keepingwordsafteraboutthemostfrequent50,000orsois\ngenerallynothelpful). Sincemostofthesenumbersarezerothesearesparsevector\nrepresentations;thereareefficientalgorithmsforstoringandcomputingwithsparse\nmatrices.\nNowthatwehavesomeintuitions,let’smoveontoexaminethedetailsofcom-\nputingwordsimilarity. Afterwardswe’lldiscussmethodsforweightingcells.\n6.4 Cosine for measuring similarity\nTo measure similarity between two target words v and w, we need a metric that\ntakestwovectors(ofthesamedimensionality,eitherbothwithwordsasdimensions,\nhenceoflength|V|,orbothwithdocumentsasdimensions,oflength|D|)andgives\nameasureoftheirsimilarity.Byfarthemostcommonsimilaritymetricisthecosine\noftheanglebetweenthevectors.\nThecosine—likemostmeasuresforvectorsimilarityusedinNLP—isbasedon\ndotproduct thedotproductoperatorfromlinearalgebra,alsocalledtheinnerproduct:\ninnerproduct\nN\n(cid:88)\ndotproduct(v,w)=v·w= v iw i=v 1w 1+v 2w 2+...+v Nw N (6.7)\ni=1\nThedotproductactsasasimilaritymetricbecauseitwilltendtobehighjustwhen\nthetwovectorshavelargevaluesinthesamedimensions. Alternatively,vectorsthat\nhavezerosindifferentdimensions—orthogonalvectors—willhaveadotproductof\n0,representingtheirstrongdissimilarity.\nThis raw dot product, however, has a problem as a similarity metric: it favors\nvectorlength longvectors. Thevectorlengthisdefinedas\n(cid:118)\n(cid:117) N\n(cid:117)(cid:88)\n|v| = (cid:116) v2 (6.8)\ni\ni=1\nThedotproductishigherifavectorislonger,withhighervaluesineachdimension.\nMore frequent words have longer vectors, since they tend to co-occur with more\nwordsandhavehigherco-occurrencevalueswitheachofthem.Therawdotproduct\nthuswillbehigherforfrequentwords. Butthisisaproblem;we’dlikeasimilarity\nmetricthattellsushowsimilartwowordsareregardlessoftheirfrequency.\nWe modify the dot product to normalize for the vector length by dividing the\ndotproductbythelengthsofeachofthetwovectors. Thisnormalizeddotproduct\nturnsouttobethesameasthecosineoftheanglebetweenthetwovectors,following\nfromthedefinitionofthedotproductbetweentwovectorsaandb:\na·b = |a||b|cosθ\na·b\n= cosθ (6.9)\n|a||b|\ncosine Thecosinesimilaritymetricbetweentwovectorsvandwthuscanbecomputedas: 6.5 • TF-IDF:WEIGHINGTERMSINTHEVECTOR 13\nWeemphasizediscriminativewordslikeRomeoviatheinversedocumentfre-\nidf quencyoridftermweight(SparckJones,1972). Theidfisdefinedusingthefrac-\ntion N/df, where N is the total number of documents in the collection, and df is\nt t\nthenumberofdocumentsinwhichtermt occurs. Thefewerdocumentsinwhicha\ntermoccurs,thehigherthisweight. Thelowestweightof1isassignedtotermsthat\noccurinallthedocuments. It’susuallyclearwhatcountsasadocument: inShake-\nspearewewoulduseaplay; whenprocessingacollectionofencyclopediaarticles\nlikeWikipedia,thedocumentisaWikipediapage;inprocessingnewspaperarticles,\nthedocumentisasinglearticle. Occasionallyyourcorpusmightnothaveappropri-\natedocumentdivisionsandyoumightneedtobreakupthecorpusintodocuments\nyourselfforthepurposesofcomputingidf.\nBecause of the large number of documents in many collections, this measure\ntoo is usually squashed with a log function. The resulting definition for inverse\ndocumentfrequency(idf)isthus\n(cid:18) (cid:19)\nN\nidf t = log 10 df (6.13)\nt\nHere are some idf values for some words in the Shakespeare corpus, (along with\nthedocumentfrequencydfvaluesonwhichtheyarebased)rangingfromextremely\ninformativewordswhichoccurinonlyoneplaylikeRomeo,tothosethatoccurina\nfewlikesaladorFalstaff,tothosewhichareverycommonlikefoolorsocommon\nastobecompletelynon-discriminativesincetheyoccurinall37playslikegoodor\nsweet.3\nWord df idf\nRomeo 1 1.57\nsalad 2 1.27\nFalstaff 4 0.967\nforest 12 0.489\nbattle 21 0.246\nwit 34 0.037\nfool 36 0.012\ngood 37 0\nsweet 37 0\ntf-idf The tf-idf weighted value w t,d for word t in document d thus combines term\nfrequencytf (definedeitherbyEq.6.11orbyEq.6.12)withidffromEq.6.13:\nt,d\nw t,d =tf t,d×idf t (6.14)\nFig.6.9appliestf-idfweightingtotheShakespeareterm-documentmatrixinFig.6.2,\nusing the tf equation Eq. 6.12. Note that the tf-idf values for the dimension corre-\nspondingtothewordgoodhavenowallbecome0;sincethiswordappearsinevery\ndocument,thetf-idfweightingleadsittobeignored.Similarly,thewordfool,which\nappearsin36outofthe37plays,hasamuchlowerweight.\nThe tf-idf weighting is the way for weighting co-occurrence matrices in infor-\nmation retrieval, but also plays a role in many other aspects of natural language\nprocessing. It’salsoagreatbaseline,thesimplethingtotryfirst. We’lllookatother\nweightingslikePPMI(PositivePointwiseMutualInformation)inSection6.6.\n3 SweetwasoneofShakespeare’sfavoriteadjectives, afactprobablyrelatedtotheincreaseduseof\nsugarinEuropeanrecipesaroundtheturnofthe16thcentury(Jurafsky,2014,p.175). 14 CHAPTER6 • VECTORSEMANTICSANDEMBEDDINGS\nAsYouLikeIt TwelfthNight JuliusCaesar HenryV\nbattle 0.246 0 0.454 0.520\ngood 0 0 0 0\nfool 0.030 0.033 0.0012 0.0019\nwit 0.085 0.081 0.048 0.054\nFigure6.9 Aportionofthetf-idfweightedterm-documentmatrixforfourwordsinShake-\nspeare plays, showing a selection of 4 plays, using counts from Fig. 6.2. For example the\n0.085valueforwitinAsYouLikeItistheproductoftf=1+log (20)=2.301andidf=.037.\n10\nNotethattheidfweightinghaseliminatedtheimportanceoftheubiquitouswordgoodand\nvastlyreducedtheimpactofthealmost-ubiquitouswordfool.\n6.6 Pointwise Mutual Information (PMI)\nAnalternativeweightingfunctiontotf-idf,PPMI(positivepointwisemutualinfor-\nmation),isusedforterm-term-matrices,whenthevectordimensionscorrespondto\nwordsratherthandocuments.PPMIdrawsontheintuitionthatthebestwaytoweigh\ntheassociationbetweentwowordsistoaskhowmuchmorethetwowordsco-occur\ninourcorpusthanwewouldhaveaprioriexpectedthemtoappearbychance.\npointwise\nmutual\nPointwisemutualinformation(Fano,1961)4isoneofthemostimportantcon-\ninformation\nceptsinNLP.Itisameasureofhowoftentwoeventsxandyoccur,comparedwith\nwhatwewouldexpectiftheywereindependent:\nP(x,y)\nI(x,y)=log (6.16)\n2P(x)P(y)\nThepointwisemutualinformationbetweenatargetwordwandacontextword\nc(ChurchandHanks1989,ChurchandHanks1990)isthendefinedas:\nP(w,c)\nPMI(w,c)=log (6.17)\n2P(w)P(c)\nThe numerator tells us how often we observed the two words together (assuming\nwe compute probability by using the MLE). The denominator tells us how often\nwewouldexpectthetwowordstoco-occurassumingtheyeachoccurredindepen-\ndently; recall that the probability of two independent events both occurring is just\nthe product of the probabilities of the two events. Thus, the ratio gives us an esti-\nmateofhowmuchmorethetwowordsco-occurthanweexpectbychance. PMIis\nausefultoolwheneverweneedtofindwordsthatarestronglyassociated.\nPMI values range from negative to positive infinity. But negative PMI values\n(which imply things are co-occurring less often than we would expect by chance)\ntend to be unreliable unless our corpora are enormous. To distinguish whether\ntwowordswhoseindividualprobabilityiseach10−6 occurtogetherlessoftenthan\nchance, we would need to be certain that the probability of the two occurring to-\ngetherissignificantlylessthan10−12,andthiskindofgranularitywouldrequirean\nenormous corpus. Furthermore it’s not clear whether it’s even possible to evaluate\nsuch scores of ‘unrelatedness’ with human judgments. For this reason it is more\n4 PMIisbasedonthemutualinformationbetweentworandomvariablesXandY,definedas:\n(cid:88)(cid:88) P(x,y)\nI(X,Y)= P(x,y)log (6.15)\n2P(x)P(y)\nx y\nInaconfusionofterminology,Fanousedthephrasemutualinformationtorefertowhatwenowcall\npointwisemutualinformationandthephraseexpectationofthemutualinformationforwhatwenowcall\nmutualinformation"
    },
    "13": {
        "chapter": "6a",
        "sections": "6.4",
        "topic": "Cosine Similarity",
        "original_category": "CL",
        "original_text": "6.2 • VECTORSEMANTICS 5\nValence Arousal Dominance\ncourageous 8.05 5.5 7.38\nmusic 7.67 5.57 6.5\nheartbreak 2.45 5.65 3.58\ncub 6.71 3.95 4.24\nOsgood et al. (1957) noticed that in using these 3 numbers to represent the\nmeaning of a word, the model was representing each word as a point in a three-\ndimensional space, a vector whose three dimensions corresponded to the word’s\nratingonthethreescales. Thisrevolutionaryideathatwordmeaningcouldberep-\nresented as a point in space (e.g., that part of the meaning of heartbreak can be\nrepresentedasthepoint[2.45,5.65,3.58])wasthefirstexpressionofthevectorse-\nmanticsmodelsthatweintroducenext.\n6.2 Vector Semantics\nvector Vector semantics is the standard way to represent word meaning in NLP, helping\nsemantics\nusmodelmanyoftheaspectsofwordmeaningwesawintheprevioussection. The\nroots of the model lie in the 1950s when two big ideas converged: Osgood’s 1957\nidea mentioned above to use a point in three-dimensional space to represent the\nconnotationofaword,andtheproposalbylinguistslikeJoos(1950),Harris(1954),\nand Firth (1957) to define the meaning of a word by its distribution in language\nuse, meaning its neighboring words or grammatical environments. Their idea was\nthattwowordsthatoccurinverysimilardistributions(whoseneighboringwordsare\nsimilar)havesimilarmeanings.\nForexample,supposeyoudidn’tknowthemeaningofthewordongchoi(are-\ncentborrowingfromCantonese)butyouseeitinthefollowingcontexts:\n(6.1) Ongchoiisdelicioussauteedwithgarlic.\n(6.2) Ongchoiissuperboverrice.\n(6.3) ...ongchoileaveswithsaltysauces...\nAndsupposethatyouhadseenmanyofthesecontextwordsinothercontexts:\n(6.4) ...spinachsauteedwithgarlicoverrice...\n(6.5) ...chardstemsandleavesaredelicious...\n(6.6) ...collardgreensandothersaltyleafygreens\nThe fact that ongchoi occurs with words like rice and garlic and delicious and\nsalty,asdowordslikespinach,chard,andcollardgreensmightsuggestthatongchoi\nis a leafy green similar to these other leafy greens.1 We can do the same thing\ncomputationallybyjustcountingwordsinthecontextofongchoi.\nTheideaofvectorsemanticsistorepresentawordasapointinamultidimen-\nsional semantic space that is derived (in ways we’ll see) from the distributions of\nembeddings word neighbors. Vectors for representing words are called embeddings (although\nthe term is sometimes more strictly applied only to dense vectors like word2vec\n(Section 6.8), rather than sparse tf-idf or PPMI vectors (Section 6.3-Section 6.6)).\nTheword“embedding”derivesfromitsmathematicalsenseasamappingfromone\nspace or structure to another, although the meaning has shifted; see the end of the\nchapter.\n1 It’sinfactIpomoeaaquatica,arelativeofmorningglorysometimescalledwaterspinachinEnglish. 6.3 • WORDSANDVECTORS 7\naparticularword(definedbytherow)occursinaparticulardocument(definedby\nthecolumn). Thusfoolappeared58timesinTwelfthNight.\nAsYouLikeIt TwelfthNight JuliusCaesar HenryV\nbattle 1 0 7 13\ngood 114 80 62 89\nfool 36 58 1 4\nwit 20 15 2 3\nFigure6.2 Theterm-documentmatrixforfourwordsinfourShakespeareplays.Eachcell\ncontainsthenumberoftimesthe(row)wordoccursinthe(column)document.\nThe term-document matrix of Fig. 6.2 was first defined as part of the vector\nvectorspace space model of information retrieval (Salton, 1971). In this model, a document is\nmodel\nrepresentedasacountvector,acolumninFig.6.3.\nvector Toreviewsomebasiclinearalgebra, avectoris, atheart, justalistorarrayof\nnumbers. SoAsYouLikeItisrepresentedasthelist[1,114,36,20](thefirstcolumn\nvector in Fig.6.3) and Julius Caesar is representedas the list [7,62,1,2](the third\nvectorspace column vector). A vector space is a collection of vectors, and is characterized by\ndimension its dimension. Vectors in a 3-dimensional vector space have an element for each\ndimensionofthespace. Wewilllooselyrefertoavectorina4-dimensionalspace\nasa4-dimensionalvector,withoneelementalongeachdimension. Intheexample\ninFig.6.3,we’vechosentomakethedocumentvectorsofdimension4,justsothey\nfit on the page; in real term-document matrices, the document vectors would have\ndimensionality|V|,thevocabularysize.\nTheorderingofthenumbersinavectorspaceindicatesthedifferentdimensions\nonwhichdocumentsvary. Thefirstdimensionforboththesevectorscorrespondsto\nthe number of times the word battle occurs, and we can compare each dimension,\nnotingforexamplethatthevectorsforAsYouLikeItandTwelfthNighthavesimilar\nvalues(1and0,respectively)forthefirstdimension.\nAsYouLikeIt TwelfthNight JuliusCaesar HenryV\nbattle 1 0 7 13\ngood 114 80 62 89\nfool 36 58 1 4\nwit 20 15 2 3\nFigure6.3 Theterm-documentmatrixforfourwordsinfourShakespeareplays. Thered\nboxesshowthateachdocumentisrepresentedasacolumnvectoroflengthfour.\nWecanthinkofthevectorforadocumentasapointin|V|-dimensionalspace;\nthusthedocumentsinFig.6.3arepointsin4-dimensionalspace.Since4-dimensional\nspacesarehardtovisualize,Fig.6.4showsavisualizationintwodimensions;we’ve\narbitrarilychosenthedimensionscorrespondingtothewordsbattleandfool.\nTerm-document matrices were originally defined as a means of finding similar\ndocumentsforthetaskofdocumentinformationretrieval. Twodocumentsthatare\nsimilar will tend to have similar words, and if two documents have similar words\ntheir column vectors will tend to be similar. The vectors for the comedies As You\nLikeIt[1,114,36,20]andTwelfthNight[0,80,58,15]lookalotmorelikeeachother\n(more fools and wit than battles) than they look like Julius Caesar [7,62,1,2] or\nHenry V [13,89,4,3]. This is clear with the raw numbers; in the first dimension\n(battle)thecomedieshavelownumbersandtheothershavehighnumbers,andwe\ncan see it visually in Fig. 6.4; we’ll see very shortly how to quantify this intuition\nmoreformally. 10 CHAPTER6 • VECTORSEMANTICSANDEMBEDDINGS\ninthetrainingcorpus; keepingwordsafteraboutthemostfrequent50,000orsois\ngenerallynothelpful). Sincemostofthesenumbersarezerothesearesparsevector\nrepresentations;thereareefficientalgorithmsforstoringandcomputingwithsparse\nmatrices.\nNowthatwehavesomeintuitions,let’smoveontoexaminethedetailsofcom-\nputingwordsimilarity. Afterwardswe’lldiscussmethodsforweightingcells.\n6.4 Cosine for measuring similarity\nTo measure similarity between two target words v and w, we need a metric that\ntakestwovectors(ofthesamedimensionality,eitherbothwithwordsasdimensions,\nhenceoflength|V|,orbothwithdocumentsasdimensions,oflength|D|)andgives\nameasureoftheirsimilarity.Byfarthemostcommonsimilaritymetricisthecosine\noftheanglebetweenthevectors.\nThecosine—likemostmeasuresforvectorsimilarityusedinNLP—isbasedon\ndotproduct thedotproductoperatorfromlinearalgebra,alsocalledtheinnerproduct:\ninnerproduct\nN\n(cid:88)\ndotproduct(v,w)=v·w= v iw i=v 1w 1+v 2w 2+...+v Nw N (6.7)\ni=1\nThedotproductactsasasimilaritymetricbecauseitwilltendtobehighjustwhen\nthetwovectorshavelargevaluesinthesamedimensions. Alternatively,vectorsthat\nhavezerosindifferentdimensions—orthogonalvectors—willhaveadotproductof\n0,representingtheirstrongdissimilarity.\nThis raw dot product, however, has a problem as a similarity metric: it favors\nvectorlength longvectors. Thevectorlengthisdefinedas\n(cid:118)\n(cid:117) N\n(cid:117)(cid:88)\n|v| = (cid:116) v2 (6.8)\ni\ni=1\nThedotproductishigherifavectorislonger,withhighervaluesineachdimension.\nMore frequent words have longer vectors, since they tend to co-occur with more\nwordsandhavehigherco-occurrencevalueswitheachofthem.Therawdotproduct\nthuswillbehigherforfrequentwords. Butthisisaproblem;we’dlikeasimilarity\nmetricthattellsushowsimilartwowordsareregardlessoftheirfrequency.\nWe modify the dot product to normalize for the vector length by dividing the\ndotproductbythelengthsofeachofthetwovectors. Thisnormalizeddotproduct\nturnsouttobethesameasthecosineoftheanglebetweenthetwovectors,following\nfromthedefinitionofthedotproductbetweentwovectorsaandb:\na·b = |a||b|cosθ\na·b\n= cosθ (6.9)\n|a||b|\ncosine Thecosinesimilaritymetricbetweentwovectorsvandwthuscanbecomputedas:"
    },
    "14": {
        "chapter": "13",
        "sections": "13.2",
        "topic": "Machine Translation",
        "original_category": "CL",
        "original_text": "2 CHAPTER13 • MACHINETRANSLATION\ninmachinetranslation,thewordsofthetargetlanguagedon’tnecessarilyagreewith\nthewordsofthesourcelanguageinnumberororder. Considertranslatingthefol-\nlowingmade-upEnglishsentenceintoJapanese.\n(13.1) English: Hewrotealettertoafriend\nJapanese: tomodachinitegami-okaita\nfriend toletter wrote\nNote thatthe elementsof thesentences arein verydifferent placesin thedifferent\nlanguages. InEnglish,theverbisinthemiddleofthesentence,whileinJapanese,\ntheverbkaitacomesattheend. TheJapanesesentencedoesn’trequirethepronoun\nhe,whileEnglishdoes.\nSuchdifferencesbetweenlanguagescanbequitecomplex. Inthefollowingac-\ntualsentencefromtheUnitedNations,noticethemanychangesbetweentheChinese\nsentence (we’ve given in red a word-by-word gloss of the Chinese characters) and\nitsEnglishequivalentproducedbyhumantranslators.\n(13.2) 大会/GeneralAssembly在/on1982年/198212月/December10日/10通过\n了/adopted第37号/37th决议/resolution，核准了/approved第二\n次/second探索/exploration及/and和平peaceful利用/using外层空\n间/outerspace会议/conference的/of各项/various建议/suggestions。\nOn10December1982,theGeneralAssemblyadoptedresolution37in\nwhichitendorsedtherecommendationsoftheSecondUnitedNations\nConferenceontheExplorationandPeacefulUsesofOuterSpace.\nNote the many ways the English and Chinese differ. For example the order-\ningdiffers inmajorways; the Chineseorderof thenounphraseis “peacefulusing\nouterspaceconferenceofsuggestions”whiletheEnglishhas“suggestionsofthe...\nconference on peaceful use of outer space”). And the order differs in minor ways\n(the date is ordered differently). English requires the in many places that Chinese\ndoesn’t, and adds some details (like “in which” and “it”) that aren’t necessary in\nChinese. Chinese doesn’t grammatically mark plurality on nouns (unlike English,\nwhichhasthe“-s”in“recommendations”),andsotheChinesemustusethemodi-\nfier各项/varioustomakeitclearthatthereisnotjustonerecommendation. English\ncapitalizessomewordsbutnotothers. Encoder-decodernetworksareverysuccess-\nfulathandlingthesesortsofcomplicatedcasesofsequencemappings.\nWe’ll begin in the next section by considering the linguistic background about\nhowlanguagesvary,andtheimplicationsthisvariancehasforthetaskofMT.Then\nwe’llsketchoutthestandardalgorithm,givedetailsaboutthingslikeinputtokeniza-\ntion and creating training corpora of parallel sentences, give some more low-level\ndetailsabouttheencoder-decodernetwork,andfinallydiscusshowMTisevaluated,\nintroducingthesimplechrFmetric.\n13.1 Language Divergences and Typology\nThere are about 7,000 languages in the world. Some aspects of human language\nuniversal seemtobeuniversal,holdingtrueforeveryoneoftheselanguages,orarestatistical\nuniversals,holdingtrueformostoftheselanguages. Manyuniversalsarisefromthe\nfunctionalroleoflanguageasacommunicativesystembyhumans. Everylanguage,\nfor example, seems to have words for referring to people, for talking about eating\nand drinking, for being polite or not. There are also structural linguistic univer-\nsals;forexample,everylanguageseemstohavenounsandverbs(Chapter17),has 4 CHAPTER13 • MACHINETRANSLATION\ntoafriend,inwhichtheprepositiontoisfollowedbyitsargumentafriend. Arabic,\nwithaVSOorder,alsohastheverbbeforetheobjectandprepositions. Bycontrast,\nintheJapaneseexamplethatfollows,eachoftheseorderingsisreversed;theverbis\nprecededbyitsarguments,andthepostpositionfollowsitsargument.\n(13.3) English: Hewrotealettertoafriend\nJapanese: tomodachinitegami-okaita\nfriend toletter wrote\nArabic: katabtrisa¯lali s˙adq\nwrote letter tofriend\nOtherkindsoforderingpreferencesvaryidiosyncraticallyfromlanguagetolan-\nguage. InsomeSVOlanguages(likeEnglishandMandarin)adjectivestendtoap-\npearbeforenouns,whileinotherslanguageslikeSpanishandModernHebrew,ad-\njectivesappearafterthenoun:\n(13.4) Spanish bruja verde English green witch\n(a) (b)\nFigure13.2 Examples of other word order differences: (a) In German, adverbs occur in\ninitialpositionthatinEnglisharemorenaturallater,andtensedverbsoccurinsecondposi-\ntion. (b)InMandarin,prepositionphrasesexpressinggoalsoftenoccurpre-verbally,unlike\ninEnglish.\nFig. 13.2 shows examples of other word order differences. All of these word\norder differences between languages can cause problems for translation, requiring\nthesystemtodohugestructuralreorderingsasitgeneratestheoutput.\n13.1.2 LexicalDivergences\nOfcoursewealsoneedtotranslatetheindividualwordsfromonelanguagetoan-\nother. Foranytranslation,theappropriatewordcanvarydependingonthecontext.\nTheEnglishsource-languagewordbass,forexample,canappearinSpanishasthe\nfishlubinaorthemusicalinstrumentbajo. Germanusestwodistinctwordsforwhat\ninEnglishwouldbecalledawall: Wandforwallsinsideabuilding,andMauerfor\nwalls outside a building. Where English uses the word brother for any male sib-\nling, Chinese and many other languages have distinct words for older brother and\nyounger brother (Mandarin gege and didi, respectively). In all these cases, trans-\nlating bass, wall, or brother from English would require a kind of specialization,\ndisambiguating the different uses of a word. For this reason the fields of MT and\nWordSenseDisambiguation(AppendixG)arecloselylinked.\nSometimes one language places more grammatical constraints on word choice\nthananother. WesawabovethatEnglishmarksnounsforwhethertheyaresingular\norplural. Mandarindoesn’t. OrFrenchandSpanish, forexample, markgrammat-\nicalgenderonadjectives, soanEnglishtranslationintoFrenchrequiresspecifying\nadjectivegender.\nThewaythatlanguagesdifferinlexicallydividingupconceptualspacemaybe\nmorecomplexthanthisone-to-manytranslationproblem,leadingtomany-to-many 6 CHAPTER13 • MACHINETRANSLATION\nfusion atively clean boundaries, to fusion languages like Russian, in which a single affix\nmay conflate multiple morphemes, like -om in the word stolom (table-SG-INSTR-\nDECL1), which fuses the distinct morphological categories instrumental, singular,\nandfirstdeclension.\nTranslatingbetweenlanguageswithrichmorphologyrequiresdealingwithstruc-\nturebelowthewordlevel,andforthisreasonmodernsystemsgenerallyusesubword\nmodelslikethewordpieceorBPEmodelsofSection13.2.1.\n13.1.4 Referentialdensity\nFinally,languagesvaryalongatypologicaldimensionrelatedtothethingstheytend\ntoomit.Somelanguages,likeEnglish,requirethatweuseanexplicitpronounwhen\ntalkingaboutareferentthatisgiveninthediscourse. Inotherlanguages,however,\nwecansometimesomitpronounsaltogether,asthefollowingexamplefromSpanish\nshows1:\n(13.6) [Eljefe]idioconunlibro. 0/iMostro´ suhallazgoaundescifradorambulante.\n[Theboss]cameuponabook. [He]showedhisfindtoawanderingdecoder.\npro-drop Languagesthatcanomitpronounsarecalledpro-droplanguages. Evenamong\nthe pro-drop languages, there are marked differences in frequencies of omission.\nJapaneseandChinese,forexample,tendtoomitfarmorethandoesSpanish. This\ndimensionofvariationacrosslanguagesiscalledthedimensionofreferentialden-\nreferential sity. Wesaythatlanguagesthattendtousemorepronounsaremorereferentially\ndensity\ndensethanthosethatusemorezeros.Referentiallysparselanguages,likeChineseor\nJapanese,thatrequirethehearertodomoreinferentialworktorecoverantecedents\ncoldlanguage arealsocalledcoldlanguages. Languagesthataremoreexplicitandmakeiteasier\nhotlanguage forthehearerarecalledhotlanguages. Thetermshotandcoldareborrowedfrom\nMarshallMcLuhan’s1964distinctionbetweenhotmedialikemovies,whichfillin\nmanydetailsfortheviewer,versuscoldmedialikecomics,whichrequirethereader\ntodomoreinferentialworktofillouttherepresentation(Bickel,2003).\nTranslatingfromlanguageswithextensivepro-drop,likeChineseorJapanese,to\nnon-pro-droplanguageslikeEnglishcanbedifficultsincethemodelmustsomehow\nidentifyeachzeroandrecoverwhoorwhatisbeingtalkedaboutinordertoinsert\ntheproperpronoun.\n13.2 Machine Translation using Encoder-Decoder\nThestandardarchitectureforMTistheencoder-decodertransformerorsequence-\nto-sequence model, an architecture we saw for RNNs in Chapter 8. We’ll see the\ndetailsofhowtoapplythisarchitecturetotransformersinSection13.3,butfirstlet’s\ntalkabouttheoveralltask.\nMostmachinetranslationtasksmakethesimplificationthatwecantranslateeach\nsentenceindependently,sowe’lljustconsiderindividualsentencesfornow. Given\na sentence in a source language, the MT task is then to generate a corresponding\nsentence in a target language. For example, an MT system is given an English\nsentencelike\nThegreenwitcharrived\n1 Hereweusethe0/-notation;we’llintroducethisanddiscussthisissuefurtherinChapter23 13.2 • MACHINETRANSLATIONUSINGENCODER-DECODER 7\nandmusttranslateitintotheSpanishsentence:\nLlego´ labrujaverde\nMT uses supervised machine learning: at training time the system is given a\nlarge set of parallel sentences (each sentence in a source language matched with\na sentence in the target language), and learns to map source sentences into target\nsentences. Inpractice, ratherthanusingwords(asintheexampleabove), wesplit\nthesentencesintoasequenceofsubwordtokens(tokenscanbewords,orsubwords,\norindividualcharacters). Thesystemsarethentrainedtomaximizetheprobability\nof the sequence of tokens in the target language y 1,...,y m given the sequence of\ntokensinthesourcelanguagex 1,...,x n:\nP(y 1,...,y m x 1,...,x n) (13.7)\n|\nRatherthanusetheinputtokensdirectly,theencoder-decoderarchitecturecon-\nsists of two components, an encoder and a decoder. The encoder takes the input\nwordsx=[x 1,...,x n]andproducesanintermediatecontexth.Atdecodingtime,the\nsystemtakeshand,wordbyword,generatestheoutputy:\nh = encoder(x) (13.8)\ny t+1 = decoder(h,y 1,...,y t)) t [1,...,m] (13.9)\n∀ ∈\nInthenexttwosectionswe’lltalkaboutsubwordtokenization,andthenhowtoget\nparallel corpora for training, and then we’ll introduce the details of the encoder-\ndecoderarchitecture.\n13.2.1 Tokenization\nMachine translation systems use a vocabulary that is fixed in advance, and rather\nthan using space-separated words, this vocabulary is generated with subword to-\nkenization algorithms, like the BPE algorithm sketched in Chapter 2. A shared\nvocabularyisusedforthesourceandtargetlanguages,whichmakesiteasytocopy\ntokens(likenames)fromsourcetotarget. Usingsubwordtokenizationwithtokens\nsharedbetweenlanguagesmakesitnaturaltotranslatebetweenlanguageslikeEn-\nglishorHindithatusespacestoseparatewords,andlanguageslikeChineseorThai\nthatdon’t.\nWebuildthevocabularybyrunningasubwordtokenizationalgorithmonacor-\npusthatcontainsbothsourceandtargetlanguagedata.\nRatherthanthesimpleBPEalgorithmfromFig.??, modernsystemsoftenuse\nmorepowerfultokenizationalgorithms. Somesystems(likeBERT)useavariantof\nwordpiece BPEcalledthewordpiecealgorithm, whichinsteadofchoosingthemostfrequent\nsetoftokenstomerge,choosesmergesbasedonwhichonemostincreasesthelan-\nguagemodelprobabilityofthetokenization. Wordpiecesuseaspecialsymbolatthe\nbeginningofeachtoken;here’saresultingtokenizationfromtheGoogleMTsystem\n(Wuetal.,2016):\nwords: Jetmakersfeudoverseatwidthwithbigordersatstake\nwordpieces: Jet makers feud over seat width with big orders at stake\nThewordpiecealgorithmisgivenatrainingcorpusandadesiredvocabularysize\nV,andproceedsasfollows:\n1. Initializethewordpiecelexiconwithcharacters(forexampleasubsetofUni-\ncodecharacters,collapsingalltheremainingcharacterstoaspecialunknown\ncharactertoken). 8 CHAPTER13 • MACHINETRANSLATION\n2. RepeatuntilthereareVwordpieces:\n(a) Trainann-gramlanguagemodelonthetrainingcorpus,usingthecurrent\nsetofwordpieces.\n(b) Considerthesetofpossiblenewwordpiecesmadebyconcatenatingtwo\nwordpiecesfromthecurrentlexicon.Choosetheonenewwordpiecethat\nmostincreasesthelanguagemodelprobabilityofthetrainingcorpus.\nRecall that with BPE we had to specify the number of merges to perform; in\nwordpiece, by contrast, we specify the total vocabulary, which is a more intuitive\nparameter. Avocabularyof8Kto32Kwordpiecesiscommonlyused.\nAn even more commonly used tokenization algorithm is (somewhat ambigu-\nunigram ously)calledtheunigramalgorithm(Kudo,2018)orsometimestheSentencePiece\nSentencePiece algorithm, and is used in systems like ALBERT (Lan et al., 2020) and T5 (Raf-\nfel et al., 2020). (Because unigram is the default tokenization algorithm used in\nalibrarycalledSentencePiecethataddsausefulwrapperaroundtokenizationalgo-\nrithms(KudoandRichardson,2018),authorsoftensaytheyareusingSentencePiece\ntokenizationbutreallymeantheyareusingtheunigramalgorithm).\nInunigramtokenization,insteadofbuildingupavocabularybymergingtokens,\nwe start with a huge vocabulary of every individual unicode character plus all fre-\nquent sequences of characters (including all space-separated words, for languages\nwithspaces),anditerativelyremovesometokenstogettoadesiredfinalvocabulary\nsize. The algorithm is complex (involving suffix-trees for efficiently storing many\ntokens,andtheEMalgorithmforiterativelyassigningprobabilitiestotokens),sowe\ndon’tgiveithere,butseeKudo(2018)andKudoandRichardson(2018). Roughly\nspeaking the algorithm proceeds iteratively by estimating the probability of each\ntoken, tokenizing the input data using various tokenizations, then removing a per-\ncentageoftokensthatdon’toccurinhigh-probabilitytokenization,andtheniterates\nuntilthevocabularyhasbeenreduceddowntothedesirednumberoftokens.\nWhydoesunigramtokenizationworkbetterthanBPE?BPEtendstocreatelots\nofverysmallnon-meaningfultokens(becauseBPEcanonlycreatelargerwordsor\nmorphemes by merging characters one at a time), and it also tends to merge very\ncommon tokens, like the suffix ed, onto their neighbors. We can see from these\nexamples from Bostrom and Durrett (2020) that unigram tends to produce tokens\nthataremoresemanticallymeaningful:\nOriginal: corrupted Original: Completely preposterous suggestions\nBPE: cor rupted BPE: Comple t ely prep ost erous suggest ions\nUnigram: corrupt ed Unigram: Complete ly pre post er ous suggestion s\n13.2.2 CreatingtheTrainingdata\nparallelcorpus Machine translation models are trained on a parallel corpus, sometimes called a\nbitext, a text that appears in two (or more) languages. Large numbers of paral-\nEuroparl lel corpora are available. Some are governmental; the Europarl corpus (Koehn,\n2005),extractedfromtheproceedingsoftheEuropeanParliament,containsbetween\n400,000and2millionsentenceseachfrom21Europeanlanguages. TheUnitedNa-\ntionsParallelCorpuscontainsontheorderof10millionsentencesinthesixofficial\nlanguagesoftheUnitedNations(Arabic,Chinese,English,French,Russian,Span-\nish)Ziemskietal.(2016). Otherparallelcorporahavebeenmadefrommovieand\nTVsubtitles,liketheOpenSubtitlescorpus(LisonandTiedemann,2016),orfrom\ngeneralwebtext,liketheParaCrawlcorpusof223millionsentencepairsbetween\n23EUlanguagesandEnglishextractedfromtheCommonCrawlBan˜o´netal.(2020). 13.2 • MACHINETRANSLATIONUSINGENCODER-DECODER 9\nSentencealignment\nStandardtrainingcorporaforMTcomeasalignedpairsofsentences. Whencreat-\ningnewcorpora,forexampleforunderresourcedlanguagesornewdomains,these\nsentencealignmentsmustbecreated.Fig.13.4givesasamplehypotheticalsentence\nalignment.\nE1: “Good morning,\" said the little prince. F1: -Bonjour, dit le petit prince.\nE2: “Good morning,\" said the merchant. F2: -Bonjour, dit le marchand de pilules perfectionnées qui\napaisent la soif.\nE3: This was a merchant who sold pills that had\nF3: On en avale une par semaine et l'on n'éprouve plus le\nbeen perfected to quench thirst.\nbesoin de boire.\nE4: You just swallow one pill a week and you F4: -C’est une grosse économie de temps, dit le marchand.\nwon’t feel the need for anything to drink.\nE5: “They save a huge amount of time,\" said the merchant. F5: Les experts ont fait des calculs.\nE6: “Fifty−three minutes a week.\" F6: On épargne cinquante-trois minutes par semaine.\nE7: “If I had fifty−three minutes to spend?\" said the F7: “Moi, se dit le petit prince, si j'avais cinquante-trois minutes\nlittle prince to himself. à dépenser, je marcherais tout doucement vers une fontaine...\"\nE8: “I would take a stroll to a spring of fresh water”\nFigure13.4 A sample alignment between sentences in English and French, with sentences extracted from\nAntoinedeSaint-Exupery’sLePetitPrinceandahypotheticaltranslation. Sentencealignmenttakessentences\ne 1,...,en,and f 1,...,fn andfindsminimalsetsofsentencesthataretranslationsofeachother,includingsingle\nsentencemappingslike(e ,f ),(e ,f ),(e ,f ),(e ,f )aswellas2-1alignments(e /e ,f ),(e /e ,f ),andnull\n1 1 4 3 5 4 6 6 2 3 2 7 8 7\nalignments(f ).\n5\nGiventwodocumentsthataretranslationsofeachother,wegenerallyneedtwo\nstepstoproducesentencealignments:\n• acostfunctionthattakesaspanofsourcesentencesandaspanoftargetsen-\ntencesandreturnsascoremeasuringhowlikelythesespansaretobetransla-\ntions.\n• an alignment algorithm that takes these scores to find a good alignment be-\ntweenthedocuments.\nTo score the similarity of sentences across languages, we need to make use of\namultilingualembeddingspace,inwhichsentencesfromdifferentlanguagesare\nin the same embedding space (Artetxe and Schwenk, 2019). Given such a space,\ncosinesimilarityofsuchembeddingsprovidesanaturalscoringfunction(Schwenk,\n2018). ThompsonandKoehn(2019)givethefollowingcostfunctionbetweentwo\nsentencesorspansx,yfromthesourceandtargetdocumentsrespectively:\n(1 cos(x,y))nSents(x)nSents(y)\nc(x,y)= − (13.10)\nS s=11 −cos(x,y s)+ S s=11 −cos(x s,y)\nwherenSents()givesthe(cid:80)numberofsentences(cid:80)(thisbiasesthemetrictowardmany\nalignments of single sentences instead of aligning very large spans). The denom-\ninator helps to normalize the similarities, and so x 1,...,x S,y 1,...,y S, are randomly\nselectedsentencessampledfromtherespectivedocuments.\nUsually dynamic programming is used as the alignment algorithm (Gale and\nChurch, 1993), in a simple extension of the minimum edit distance algorithm we\nintroducedinChapter2.\nFinally,it’shelpfultodosomecorpuscleanupbyremovingnoisysentencepairs.\nThis can involve handwritten rules to remove low-precision pairs (for example re-\nmoving sentences that are too long, too short, have different URLs, or even pairs"
    },
    "15": {
        "chapter": "16",
        "sections": "16.2",
        "topic": "Automatic Speech Recognition (ASR)",
        "original_category": "CL",
        "original_text": "4 CHAPTER16 • AUTOMATICSPEECHRECOGNITIONANDTEXT-TO-SPEECH\ncorpus contains recordings of twenty different dinner parties in real homes, each\nwith four participants, and in three locations (kitchen, dining area, living room),\nrecorded both with distant room microphones and with body-worn mikes. The\nHKUST HKUST Mandarin Telephone Speech corpus has 1206 ten-minute telephone con-\nversationsbetweenspeakersofMandarinacrossChina,includingtranscriptsofthe\nconversations,whicharebetweeneitherfriendsorstrangers(Liuetal.,2006). The\nAISHELL-1 AISHELL-1corpuscontains170hoursofMandarinreadspeechofsentencestaken\nfrom various domains, read by different speakers mainly from northern China (Bu\netal.,2017).\nFigure16.1showstheroughpercentageofincorrectwords(theworderrorrate,\norWER,definedonpage16)fromstate-of-the-artsystemsonsomeofthesetasks.\nNote that the error rate on read speech (like the LibriSpeech audiobook corpus) is\naround2%;thisisasolvedtask,althoughthesenumberscomefromsystemsthatre-\nquireenormouscomputationalresources. Bycontrast,theerrorratefortranscribing\nconversationsbetweenhumansismuchhigher;5.8to11%fortheSwitchboardand\nCALLHOME corpora. The error rate is higher yet again for speakers of varieties\nlikeAfricanAmericanVernacularEnglish,andyetagainfordifficultconversational\ntasksliketranscriptionof4-speakerdinnerpartyspeech,whichcanhaveerrorrates\nas high as 81.3%. Character error rates (CER) are also much lower for read Man-\ndarinspeechthanfornaturalconversation.\nEnglishTasks WER%\nLibriSpeechaudiobooks960hourclean 1.4\nLibriSpeechaudiobooks960hourother 2.6\nSwitchboardtelephoneconversationsbetweenstrangers 5.8\nCALLHOMEtelephoneconversationsbetweenfamily 11.0\nSociolinguisticinterviews,CORAAL(AAL) 27.0\nCHiMe5dinnerpartieswithbody-wornmicrophones 47.9\nCHiMe5dinnerpartieswithdistantmicrophones 81.3\nChinese(Mandarin)Tasks CER%\nAISHELL-1Mandarinreadspeechcorpus 6.7\nHKUSTMandarinChinesetelephoneconversations 23.5\nFigure16.1 RoughWordErrorRates(WER=%ofwordsmisrecognized)reportedaround\n2020forASRonvariousAmericanEnglishrecognitiontasks,andcharactererrorrates(CER)\nfortwoChineserecognitiontasks.\n16.2 Feature Extraction for ASR: Log Mel Spectrum\nThefirststepinASRistotransformtheinputwaveformintoasequenceofacoustic\nfeaturevector feature vectors, each vector representing the information in a small time window\nof the signal. Let’s see how to convert a raw wavefile to the most commonly used\nfeatures,sequencesoflogmelspectrumvectors.Aspeechsignalprocessingcourse\nisrecommendedformoredetails.\n16.2.1 SamplingandQuantization\nThe input to a speech recognizer is a complex series of changes in air pressure.\nThese changes in air pressure obviously originate with the speaker and are caused 16.2 • FEATUREEXTRACTIONFORASR:LOGMELSPECTRUM 5\nbythespecificwaythatairpassesthroughtheglottisandouttheoralornasalcav-\nities. We represent sound waves by plotting the change in air pressure over time.\nOnemetaphorwhichsometimeshelpsinunderstandingthesegraphsisthatofaver-\ntical plate blocking the air pressure waves (perhaps in a microphone in front of a\nspeaker’smouth,ortheeardruminahearer’sear). Thegraphmeasurestheamount\nof compression or rarefaction (uncompression) of the air molecules at this plate.\nFigure16.2showsashortsegmentofawaveformtakenfromtheSwitchboardcorpus\noftelephonespeechofthevowel[iy]fromsomeonesaying“shejusthadababy”.\n0.02283\n0\n–0.01697\n0 0.03875\nTime (s)\nFigure16.2 Awaveformofaninstanceofthevowel[iy](thelastvowelintheword“baby”). They-axis\nshowsthelevelofairpressureaboveandbelownormalatmosphericpressure. Thex-axisshowstime. Notice\nthatthewaverepeatsregularly.\nThe first step in digitizing a sound wave like Fig. 16.2 is to convert the analog\nrepresentations(firstairpressureandthenanalogelectricsignalsinamicrophone)\nsampling intoadigitalsignal.Thisanalog-to-digitalconversionhastwosteps:samplingand\nquantization. Tosampleasignal,wemeasureitsamplitudeataparticulartime;the\nsamplingrateisthenumberofsamplestakenpersecond. Toaccuratelymeasurea\nwave,wemusthaveatleasttwosamplesineachcycle: onemeasuringthepositive\npart of the wave and one measuring the negative part. More than two samples per\ncycleincreasestheamplitudeaccuracy,butfewerthantwosamplescausesthefre-\nquencyofthewavetobecompletelymissed. Thus, themaximumfrequencywave\nthat can be measured is one whose frequency is half the sample rate (since every\ncycle needs two samples). This maximum frequency for a given sampling rate is\nNyquist calledtheNyquistfrequency. Mostinformationinhumanspeechisinfrequencies\nfrequency\nbelow 10,000 Hz; thus, a 20,000 Hz sampling rate would be necessary for com-\npleteaccuracy. Buttelephonespeechisfilteredbytheswitchingnetwork,andonly\nfrequencies less than 4,000 Hz are transmitted by telephones. Thus, an 8,000 Hz\nsampling rate is sufficient for telephone-bandwidth speech like the Switchboard\ncorpus,while16,000Hzsamplingisoftenusedformicrophonespeech.\nAlthoughusinghighersamplingratesproduceshigherASRaccuracy,wecan’t\ncombine different sampling rates for training and testing ASR systems. Thus if\nwearetestingonatelephonecorpuslikeSwitchboard(8KHzsampling),wemust\ndownsample our training corpus to 8 KHz. Similarly, if we are training on mul-\ntiple corpora and one of them includes telephone speech, we downsample all the\nwidebandcorporato8Khz.\nAmplitudemeasurementsarestoredasintegers,either8bit(valuesfrom-128–\n127)or16bit(valuesfrom-32768–32767).Thisprocessofrepresentingreal-valued\nquantization numbersasintegersiscalledquantization; allvaluesthatareclosertogetherthan\ntheminimumgranularity(thequantumsize)arerepresentedidentically. Wereferto\neachsampleattimeindexninthedigitized,quantizedwaveformasx[n].\nOnce data is quantized, it is stored in various formats. One parameter of these\nformats is the sample rate and sample size discussed above; telephone speech is\noften sampled at 8 kHz and stored as 8-bit samples, and microphone data is often\nsampledat16kHzandstoredas16-bitsamples.Anotherparameteristhenumberof 6 CHAPTER16 • AUTOMATICSPEECHRECOGNITIONANDTEXT-TO-SPEECH\nchannel channels.Forstereodataorfortwo-partyconversations,wecanstorebothchannels\ninthesamefileorwecanstoretheminseparatefiles.Afinalparameterisindividual\nsamplestorage—linearlyorcompressed.Onecommoncompressionformatusedfor\ntelephone speech is µ-law (often written u-law but still pronounced mu-law). The\nintuition of log compression algorithms like µ-law is that human hearing is more\nsensitive at small intensities than large ones; the log represents small values with\nmorefaithfulnessattheexpenseofmoreerroronlargevalues.Thelinear(unlogged)\nPCM valuesaregenerallyreferredtoaslinearPCMvalues(PCMstandsforpulsecode\nmodulation,butnevermindthat).Here’stheequationforcompressingalinearPCM\nsamplevaluexto8-bitµ-law,(whereµ=255for8bits):\nsgn(x)log(1+µ x)\nF(x) = | | 1 x 1 (16.1)\nlog(1+µ) − ≤ ≤\nThereareanumberofstandardfileformatsforstoringtheresultingdigitizedwave-\nfile,suchasMicrosoft’s.wavandApple’sAIFFallofwhichhavespecialheaders;\nsimpleheaderless“raw”filesarealsoused. Forexample,the.wavformatisasub-\nset of Microsoft’s RIFF format for multimedia files; RIFF is a general format that\ncanrepresentaseriesofnestedchunksofdataandcontrolinformation. Figure16.3\nshowsasimple.wavfilewithasingledatachunktogetherwithitsformatchunk.\nFigure16.3 Microsoftwavefileheaderformat,assumingsimplefilewithonechunk. Fol-\nlowingthis44-byteheaderwouldbethedatachunk.\n16.2.2 Windowing\nFrom the digitized, quantized representation of the waveform, we need to extract\nspectral features from a small window of speech that characterizes part of a par-\nticular phoneme. Inside this small window, we can roughly think of the signal as\nstationary stationary (that is, its statistical properties are constant within this region). (By\nnon-stationary contrast, in general, speech is a non-stationary signal, meaning that its statistical\npropertiesarenotconstantovertime). Weextractthisroughlystationaryportionof\nspeechbyusingawindowwhichisnon-zeroinsidearegionandzeroelsewhere,run-\nningthiswindowacrossthespeechsignalandmultiplyingitbytheinputwaveform\ntoproduceawindowedwaveform.\nframe The speech extracted from each window is called a frame. The windowing is\ncharacterized by three parameters: the window size or frame size of the window\nstride (its width in milliseconds), the frame stride, (also called shift or offset) between\nsuccessivewindows,andtheshapeofthewindow.\nTo extract the signal we multiply the value of the signal at time n, s[n] by the\nvalueofthewindowattimen,w[n]:\ny[n]=w[n]s[n] (16.2)\nrectangular The window shape sketched in Fig. 16.4 is rectangular; you can see the ex-\ntractedwindowedsignallooksjustliketheoriginalsignal. Therectangularwindow, 16.2 • FEATUREEXTRACTIONFORASR:LOGMELSPECTRUM 7\nWindow\n25 ms\nShift\nWindow\n10\nms 25 ms\nShift\nWindow\n10\nms 25 ms\nFigure16.4 Windowing,showinga25msrectangularwindowwitha10msstride.\nhowever,abruptlycutsoffthesignalatitsboundaries,whichcreatesproblemswhen\nwedoFourieranalysis. Forthisreason,foracousticfeaturecreationwemorecom-\nHamming monly use the Hamming window, which shrinks the values of the signal toward\nzero at the window boundaries, avoiding discontinuities. Figure 16.5 shows both;\ntheequationsareasfollows(assumingawindowthatisLframeslong):\n(cid:26)\n1 0 n L 1\nrectangular w[n] = ≤ ≤ − (16.3)\n0 otherwise\n(cid:26) 0.54 0.46cos(2πn) 0 n L 1\nHamming w[n] = − L ≤ ≤ − (16.4)\n0 otherwise\n0.4999\n0\n–0.5\n0 0.0475896\nTime (s)\nRectangular window Hamming window\n0.4999 0.4999\n0 0\n–0.5 –0.4826\n0.00455938 0.0256563 0.00455938 0.0256563\nTime (s) Time (s)\nFigure16.5 WindowingasinewavewiththerectangularorHammingwindows.\n16.2.3 DiscreteFourierTransform\nThenextstepistoextractspectralinformationforourwindowedsignal;weneedto\nknow how much energy the signal contains at different frequency bands. The tool 8 CHAPTER16 • AUTOMATICSPEECHRECOGNITIONANDTEXT-TO-SPEECH\nfor extracting spectral information for discrete frequency bands for a discrete-time\nDiscrete\nFourier (sampled)signalisthediscreteFouriertransformorDFT.\ntransf Do Frm T TheinputtotheDFTisawindowedsignalx[n]...x[m],andtheoutput,foreach\nof N discrete frequency bands, is a complex number X[k] representing the magni-\ntude and phase of that frequency component in the original signal. If we plot the\nmagnitude against the frequency, we can visualize the spectrum (see Appendix H\nfor more on spectra). For example, Fig. 16.6 shows a 25 ms Hamming-windowed\nportion of a signal and its spectrum as computed by a DFT (with some additional\nsmoothing).\n0.04414\n0\n–0.04121\n0.0141752 0.039295 0 8000\nTime (s) Frequency (Hz)\n)zH/Bd(\nlevel\nerusserp\ndnuoS\n20\n0\n–20\n(a) (b)\nFigure16.6 (a) A 25 ms Hamming-windowed portion of a signal from the vowel [iy]\nand (b)itsspectrumcomputedbyaDFT.\nWe do not introduce the mathematical details of the DFT here, except to note\nEuler’sformula thatFourieranalysisreliesonEuler’sformula,with jastheimaginaryunit:\nejθ =cosθ+jsinθ (16.5)\nAsabriefreminderforthosestudentswhohavealreadystudiedsignalprocessing,\ntheDFTisdefinedasfollows:\nN 1\nX[k]=(cid:88)− x[n]e−j2 Nπkn\n(16.6)\nn=0\nfastFourier AcommonlyusedalgorithmforcomputingtheDFTisthefastFouriertransform\ntransform\nFFT orFFT.ThisimplementationoftheDFTisveryefficientbutonlyworksforvalues\nofN thatarepowersof2.\n16.2.4 MelFilterBankandLog\nThe results of the FFT tell us the energy at each frequency band. Human hearing,\nhowever,isnotequallysensitiveatallfrequencybands;itislesssensitiveathigher\nfrequencies. This bias toward low frequencies helps human recognition, since in-\nformationinlowfrequencies(likeformants)iscrucialfordistinguishingvowelsor\nnasals, whileinformationinhighfrequencies(likestopburstsorfricativenoise)is\nless crucial for successful recognition. Modeling this human perceptual property\nimprovesspeechrecognitionperformanceinthesameway.\nWeimplementthisintuitionbycollectingenergies,notequallyateachfrequency\nmel band, butaccordingtothemelscale, anauditoryfrequencyscale. Amel(Stevens\netal.1937,StevensandVolkmann1940)isaunitofpitch. Pairsofsoundsthatare\nperceptuallyequidistantinpitchareseparatedbyanequalnumberofmels. Themel"
    } 
}