{
    "1": {
        "chapter": "17",
        "sections": "17.1",
        "topic": "English Word Classes",
        "original_category": "L",
        "original_text": "Parts of speech fall into two broad categories: closed class and open class. Closed classes are those with relatively fixed membership, such as prepositions [...]. By contrast, nouns and verbs are open classes [...]. Closed class words are generally function words like of, it, and, or you, which tend to be very short, occur frequently, and often have structuring uses in grammar. Four major open classes occur in the languages of the world: nouns [...], verbs, adjectives, and adverbs, [...]. Nouns are words for people, places, or things,  [...]. Verbs refer to actions and processes, including main verbs like draw, provide, and go. English verbs have inflections (non-third-person-singular (eat), third-personsingular (eats), progressive (eating), past participle (eaten)). [...]. Adjectives often describe properties or qualities of nouns, like color (white, black), age (old, young), and value (good, bad), [...]. Adverbs generally modify something. [...] A particle [...] is used in combination with a verb. Particles often have extended meanings that aren't quite the same as the prepositions they resemble, as in the particle 'over' in 'she turned the paper over'. A phrasal verb verb and a particle acting as a single unit is called a phrasal verb. The meaning of phrasal verbs is often non-compositional - not predictable from the individual meanings of the verb and the particle. [...]"
    },
    "2": {
        "chapter": "18",
        "sections": "18.1, 18.2",
        "topic": "Constituency and Context-Free Grammars",
        "original_category": "L",
        "original_text": "Syntactic constituency is the idea that groups of words can behave as single units, or constituents. Consider the noun phrase, a sequence of words surrounding at least one noun. [...] they can all appear in similar syntactic environments, for example, before a verb: 'three parties from Brooklyn arrive'. [...] A widely used formal system for modeling constituent structure in natural language is the context-free grammar (CFG). [...] A context-free grammar consists of a set of rules or productions, each of which expresses the ways that symbols of the language can be grouped and ordered together, and a lexicon of words and symbols. [...] an NP (or noun phrase) can be composed of either a ProperNoun or a determiner (Det) followed by a Nominal; [...]. Context-free rules can be hierarchically  embedded, [...]: Det → a, Det → the, Noun → flight. [...] Thus, a CFG can be used to generate a set of strings. This sequence of rule expansions is called a derivation of the string of words. It is common to represent a derivation by a parse tree (commonly shown inverted with the root at the top). [...]. A CFG like that of L0 defines a formal language. Sentences (strings of words) that can be derived by a grammar are in the formal language defined by that grammar, and are called grammatical sentences. Sentences that cannot be derived by a given formal grammar are not in the language defined by that grammar and are  referred to as ungrammatical. [...] In linguistics, the use of formal languages to model natural languages is called generative grammar [...]."
    },
    "3": {
        "chapter": "21",
        "sections": "21.1, 21.2",
        "topic": "Semantic Roles and Diathesis Alternations",
        "original_category": "L",
        "original_text": "Consider the meanings of the arguments Sasha, Pat, the window, and the door in these two sentences. Sasha broke the window. Pat opened the door. The subjects Sasha and Pat, what we might call the breaker of the window breaking event and the opener of the door-opening event, have something in common. They are both volitional actors, often animate, and they have direct causal responsibility for their events. Thematic roles are a way to capture this semantic commonality between breakers and openers. We say that the subjects of both these verbs are agents. Thus, AGENT is the thematic role that represents an abstract idea such as volitional causation. Similarly, the direct objects of both these verbs, the BrokenThing and OpenedThing, are both prototypically inanimate objects that are affected in some way by the action. The semantic role for these participants is theme. [...] Semantic roles thus help generalize over different surface realizations of predicate arguments. For example, while the AGENT is often realized as the subject of the sentence, in other cases the THEME can be the subject. [...] John (AGENT) broke the window (THEME). John (AGENT) broke the window (THEME) with a rock (INSTRUMENT). The rock (INSTRUMENT) broke the window (THEME). The window (THEME) broke. The window (THEME) was broken by John (AGENT). These examples suggest that break has (at least) the possible arguments AGENT, THEME, and INSTRUMENT. [...] These multiple argument structure realizations [...] are called verb alternations or diathesis alternations."
    },
    "4": {
        "chapter": "G",
        "sections": "G.1, G.2",
        "topic": "Word Senses and Relations Between Senses",
        "original_category": "L",
        "original_text": "A sense (or word sense) is a discrete representation of one aspect of the meaning of a word. [...] In context, it's easy to see the different meanings: mouse1: a mouse controlling a computer system in 1968. mouse2: a quiet animal like a mouse. bank1: a bank can hold the investments in a custodial account. bank2: as agriculture burgeons on the east bank, the river...[...] we need to consider the alternative ways that dictionaries and thesauruses offer for defining senses. One is based on the fact that dictionaries or thesauruses give textual definitions for each sense called glosses. [...] bank: 1. financial institution that accepts deposits and channels the money into lending activities 2. sloping land (especially the slope beside a body of water). [...] when two senses of two different words (lemmas) are identical, or nearly identical, we say the two senses are synonyms [...] couch/sofa vomit/throw up filbert/hazelnut car/automobile. [...] antonyms are words with an opposite meaning, like: long/short big/little fast/slow cold/hot dark/light. [...] [a] hyponym of [a] word if the [other word] is more specific, denoting a subclass of the other. For example, car is a hyponym of vehicle, dog is a hyponym of animal, [...]. Conversely, we say that vehicle is a hypernym of car, and animal is a hypernym of dog. [...] meronymy, the part-whole relation. [...] wheel is a meronym of car, and car is a holonym of wheel."
    },
    "5": {
        "chapter": "H",
        "sections": "H.1, H.2",
        "topic": "Phonetics",
        "original_category": "L",
        "original_text": "We'll represent the pronunciation of a word as a string of phones, which are speech sounds, each represented with symbols adapted from the Roman alphabet. The standard phonetic representation for transcribing the world's languages is the International Phonetic Alphabet (IPA), [...] the mapping between the letters of English orthography and phones is relatively opaque; a single letter can represent very different sounds in different contexts. [...] Many other languages, for example, Spanish, are much more transparent in their sound-orthography mapping than English. Articulatory phonetics is the study of how these phones are produced as the various articulatory phonetics organs in the mouth, throat, and nose modify the airflow from the lungs. [...] we can group [consonants] into classes by their point of maximum restriction, their place of articulation. [...] bilabial: [...] [p], [b], [m]. labiodental [v] and [f] [...]. dental: [...] [th] of thing and the [dh] of though. [...] alveolar: [s], [z], [t], [d]. [...] velar: [k], [g]. [...] Consonants are also distinguished by how the restriction in airflow is made [...]. This feature is called the manner of articulation [...]. voiced stops [b], [d], and [g],unvoiced stops [p], [t], and [k]. nasal sounds [n], [m], and [ng]. labiodental fricatives [f] and [v]. alveolar fricatives [s] and [z]. affricates [ch] and [jh]. approximants [y] and [w]."
    },
    "6": {
        "chapter": "7",
        "sections": "7.3",
        "topic": "Feed-Forward Neural Networks (FFNNs)",
        "original_category": "CS",
        "original_text": "2 CHAPTER7 • NEURALNETWORKS\n7.1 Units\nThebuildingblockofaneuralnetworkisasinglecomputationalunit. Aunittakes\na set of real valued numbers as input, performs some computation on them, and\nproducesanoutput.\nAtitsheart,aneuralunitistakingaweightedsumofitsinputs,withoneaddi-\nbiasterm tional term in the sum called a bias term. Given a set of inputs x 1...x n, a unit has\nasetofcorrespondingweightsw ...w andabiasb, sotheweightedsumzcanbe\n1 n\nrepresentedas:\n(cid:88)\nz=b+ wx (7.1)\ni i\ni\nOftenit’smoreconvenienttoexpressthisweightedsumusingvectornotation;recall\nvector from linear algebra that a vector is, at heart, just a list or array of numbers. Thus\nwe’lltalkaboutzintermsofaweightvectorw,ascalarbiasb,andaninputvector\nx,andwe’llreplacethesumwiththeconvenientdotproduct:\nz=w·x+b (7.2)\nAsdefinedinEq.7.2,zisjustarealvaluednumber.\nFinally, instead of using z, a linear function of x, as the output, neural units\napply a non-linear function f to z. We will refer to the output of this function as\nactivation the activation value for the unit, a. Since we are just modeling a single unit, the\nactivationforthenodeisinfactthefinaloutputofthenetwork,whichwe’llgenerally\ncally. Sothevalueyisdefinedas:\ny=a= f(z)\nWe’lldiscussthreepopularnon-linearfunctions f below(thesigmoid,thetanh,and\ntherectifiedlinearunitorReLU)butit’spedagogicallyconvenienttostartwiththe\nsigmoid sigmoidfunctionsincewesawitinChapter5:\n1\ny=σ(z)= (7.3)\n1+e−z\nThesigmoid(showninFig.7.1)hasanumberofadvantages;itmapstheoutput\ninto the range (0,1), which is useful in squashing outliers toward 0 or 1. And it’s\ndifferentiable,whichaswesawinSection??willbehandyforlearning.\nFigure7.1 The sigmoid function takes a real value and maps it to the range (0,1). It is\nnearlylineararound0butoutliervaluesgetsquashedtoward0or1.\nSubstitutingEq.7.2intoEq.7.3givesustheoutputofaneuralunit:\n1\ny=σ(w·x+b)= (7.4)\n1+exp(−(w·x+b)) 7.3 • FEEDFORWARDNEURALNETWORKS 7\nx 2 h 2\n1 1\n0 0\nx\n1 h\n0 1 0 1 2 1\na) The original x space b) The new (linearly separable) h space\nFigure7.7 The hidden layer forming a new representation of the input. (b) shows the\nrepresentationofthehiddenlayer,h,comparedtotheoriginalinputrepresentationxin(a).\nNotice that the input point [0, 1] has been collapsed with the input point [1, 0], making it\npossibletolinearlyseparatethepositiveandnegativecasesofXOR.AfterGoodfellowetal.\n(2016).\n7.3 Feedforward Neural Networks\nLet’snowwalkthroughaslightlymoreformalpresentationofthesimplestkindof\nfeedforward neuralnetwork,thefeedforwardnetwork. Afeedforwardnetworkisamultilayer\nnetwork\nnetworkinwhichtheunitsareconnectedwithnocycles; theoutputsfromunitsin\neach layer are passed to units in the next higher layer, and no outputs are passed\nback to lower layers. (In Chapter 8 we’ll introduce networks with cycles, called\nrecurrentneuralnetworks.)\nForhistoricalreasonsmultilayernetworks,especiallyfeedforwardnetworks,are\nmulti-layer sometimescalledmulti-layerperceptrons(orMLPs);thisisatechnicalmisnomer,\nperceptrons\nMLP since the units in modern multilayer networks aren’t perceptrons (perceptrons are\npurely linear, but modern networks are made up of units with non-linearities like\nsigmoids),butatsomepointthenamestuck.\nSimple feedforward networks have three kinds of nodes: input units, hidden\nunits,andoutputunits.\nFig.7.8showsapicture.Theinputlayerxisavectorofsimplescalarvaluesjust\naswesawinFig.7.2.\nhiddenlayer Thecoreoftheneuralnetworkisthehiddenlayerhformedofhiddenunitsh i,\neachofwhichisaneuralunitasdescribedinSection7.1,takingaweightedsumof\nitsinputsandthenapplyinganon-linearity. Inthestandardarchitecture,eachlayer\nfully-connected isfully-connected, meaningthateachunitineachlayertakesasinputtheoutputs\nfromalltheunitsinthepreviouslayer,andthereisalinkbetweeneverypairofunits\nfromtwoadjacentlayers. Thuseachhiddenunitsumsoveralltheinputunits.\nRecallthatasinglehiddenunithasasparametersaweightvectorandabias. We\nrepresenttheparametersfortheentirehiddenlayerbycombiningtheweightvector\nandbiasforeachunitiintoasingleweightmatrixWandasinglebiasvectorbfor\nthewholelayer(seeFig.7.8). EachelementW oftheweightmatrixWrepresents\nji\ntheweightoftheconnectionfromtheithinputunitx tothe jthhiddenunith .\ni j\nTheadvantageofusingasinglematrixW fortheweightsoftheentirelayeris\nthatnowthehiddenlayercomputationforafeedforwardnetworkcanbedonevery\nefficiently with simple matrix operations. In fact, the computation only has three 8 CHAPTER7 • NEURALNETWORKS\nx\n1\nx\n2\nx\nn\n0\n…\n…\nb\n+1\n…\nW U\ny\n1\nh\n1\nh\n2 y\n2\nh\n3\nh\nn\n1\ny\nn\n2\ninput layer hidden layer output layer\nFigure7.8 Asimple2-layerfeedforwardnetwork,withonehiddenlayer,oneoutputlayer,\nandoneinputlayer(theinputlayerisusuallynotcountedwhenenumeratinglayers).\nsteps: multiplyingtheweightmatrixbytheinputvectorx,addingthebiasvectorb,\nandapplyingtheactivationfunctiong(suchasthesigmoid,tanh,orReLUactivation\nfunctiondefinedabove).\nTheoutputofthehiddenlayer,thevectorh,isthusthefollowing(forthisexam-\nplewe’llusethesigmoidfunctionσ asouractivationfunction):\nh=σ(Wx+b) (7.8)\nNotice that we’re applying the σ function here to a vector, while in Eq. 7.3 it was\napplied to a scalar. We’re thus allowing σ(·), and indeed any activation function\ng(·),toapplytoavectorelement-wise,sog[z ,z ,z ]=[g(z ),g(z ),g(z )].\n1 2 3 1 2 3\nLet’sintroducesomeconstantstorepresentthedimensionalitiesofthesevectors\nand matrices. We’ll refer to the input layer as layer 0 of the network, and have n\n0\nrepresent the number of inputs, so x is a vector of real numbers of dimension n ,\n0\normoreformallyx∈Rn0, acolumnvectorofdimensionality[n 0,1]. Let’scallthe\nhiddenlayerlayer1andtheoutputlayerlayer2. Thehiddenlayerhasdimensional-\nityn 1,soh∈Rn1 andalsob∈Rn1 (sinceeachhiddenunitcantakeadifferentbias\nvalue). AndtheweightmatrixWhasdimensionalityW∈Rn1×n0,i.e. [n 1,n 0].\nTakeamomenttoconvinceyourselfthatthematrixmultiplicationinEq.7.8will\ncomputethevalueofeachh\nasσ(cid:0)(cid:80)n0\nW x +b\n(cid:1)\n.\nj i=1 ji i j\nAswesawinSection7.2,theresultingvalueh(forhiddenbutalsoforhypoth-\nesis) forms a representation of the input. The role of the output layer is to take\nthis new representation h and compute a final output. This output could be a real-\nvalued number, but in many cases the goal of the network is to make some sort of\nclassificationdecision,andsowewillfocusonthecaseofclassification.\nIfwearedoingabinarytasklikesentimentclassification,wemighthaveasin-\ngleoutputnode,anditsscalarvalueyistheprobabilityofpositiveversusnegative\nsentiment. If we are doing multinomial classification, such as assigning a part-of-\nspeechtag,wemighthaveoneoutputnodeforeachpotentialpart-of-speech,whose\noutputvalueistheprobabilityofthatpart-of-speech,andthevaluesofalltheoutput\nnodesmustsumtoone. Theoutputlayeristhusavectory thatgivesaprobability\ndistributionacrosstheoutputnodes.\nLet’sseehowthishappens. Likethehiddenlayer,theoutputlayerhasaweight\nmatrix(let’scallitU),butsomemodelsdon’tincludeabiasvectorbintheoutput 7.3 • FEEDFORWARDNEURALNETWORKS 9\nlayer, sowe’llsimplifybyeliminatingthebiasvectorinthisexample. Theweight\nmatrixismultipliedbyitsinputvector(h)toproducetheintermediateoutputz:\nz=Uh\nThere are n\n2\noutput nodes, so z∈Rn2, weight matrix U has dimensionality U∈\nRn2×n1,andelementU\nij\nistheweightfromunit jinthehiddenlayertounitiinthe\noutputlayer.\nHowever,zcan’tbetheoutputoftheclassifier,sinceit’savectorofreal-valued\nnumbers,whilewhatweneedforclassificationisavectorofprobabilities. Thereis\nnormalizing a convenient function for normalizing a vector of real values, by which we mean\nconvertingittoavectorthatencodesaprobabilitydistribution(allthenumberslie\nsoftmax between 0 and 1 and sum to 1): the softmax function that we saw on page ?? of\nChapter 5. More generally for any vector z of dimensionality d, the softmax is\ndefinedas:\nexp(z)\ni\nsoftmax(z i) =\n(cid:80)d\nexp(z )\n1≤i≤d (7.9)\nj=1 j\nThusforexamplegivenavector\nz=[0.6,1.1,−1.5,1.2,3.2,−1.1], (7.10)\nthesoftmaxfunctionwillnormalizeittoaprobabilitydistribution(shownrounded):\nsoftmax(z)=[0.055,0.090,0.0067,0.10,0.74,0.010] (7.11)\nYou may recall that we used softmax to create a probability distribution from a\nvectorofreal-valuednumbers(computedfromsummingweightstimesfeatures)in\nthemultinomialversionoflogisticregressioninChapter5.\nThat means we can think of a neural network classifier with one hidden layer\nasbuildingavectorhwhichisahiddenlayerrepresentationoftheinput,andthen\nrunning standard multinomial logistic regression on the features that the network\ndevelopsinh. Bycontrast,inChapter5thefeaturesweremainlydesignedbyhand\nvia feature templates. So a neural network is like multinomial logistic regression,\nbut(a)withmanylayers,sinceadeepneuralnetworkislikelayerafterlayeroflo-\ngisticregressionclassifiers;(b)withthoseintermediatelayershavingmanypossible\nactivation functions (tanh, ReLU, sigmoid) instead of just sigmoid (although we’ll\ncontinuetouseσ forconveniencetomeananyactivationfunction);(c)ratherthan\nformingthefeaturesbyfeaturetemplates,thepriorlayersofthenetworkinducethe\nfeaturerepresentationsthemselves.\nHerearethefinalequationsforafeedforwardnetworkwithasinglehiddenlayer,\nwhichtakesaninputvectorx,outputsaprobabilitydistributiony,andisparameter-\nizedbyweightmatricesWandUandabiasvectorb:\nh = σ(Wx+b)\nz = Uh\ny = softmax(z) (7.12)\nAnd just to remember the shapes of all our variables, x∈Rn0, h∈Rn1, b∈Rn1,\nW∈Rn1×n0,U∈Rn2×n1,andtheoutputvectory∈Rn2.We’llcallthisnetworka2-\nlayernetwork(wetraditionallydon’tcounttheinputlayerwhennumberinglayers,\nbutdocounttheoutputlayer).Sobythisterminologylogisticregressionisa1-layer\nnetwork. 10 CHAPTER7 • NEURALNETWORKS\n7.3.1 Moredetailsonfeedforwardnetworks\nLet’s now set up some notation to make it easier to talk about deeper networks of\ndepth more than 2. We’ll use superscripts in square brackets to mean layer num-\nbers, starting at 0 for the input layer. So W[1] will mean the weight matrix for the\n(first)hiddenlayer,andb[1] willmeanthebiasvectorforthe(first)hiddenlayer. n\nj\nwillmeanthenumberof unitsatlayer j. We’ll useg(·)tostandfortheactivation\nfunction, which will tend to be ReLU or tanh for intermediate layers and softmax\nforoutputlayers. We’llusea[i] tomeantheoutputfromlayeri,andz[i] tomeanthe\ncombinationofpreviouslayeroutput, weightsandbiasesW[i]a[i−1]+b[i]. The0th\nlayerisforinputs,sowe’llrefertotheinputsxmoregenerallyasa[0].\nThuswecanre-representour2-layernetfromEq.7.12asfollows:\nz[1] = W[1]a[0]+b[1]\na[1] = g[1](z[1])\nz[2] = W[2]a[1]+b[2]\na[2] = g[2](z[2])\nyˆ = a[2] (7.13)\nNotethatwiththisnotation,theequationsforthecomputationdoneateachlayerare\nthesame. Thealgorithmforcomputingtheforwardstepinann-layerfeedforward\nnetwork,giventheinputvectora[0]isthussimply:\nforiin1,...,n\nz[i] = W[i]a[i−1] + b[i]\na[i] = g[i](z[i])\nyˆ = a[n]\nIt’softenusefultohaveanameforthefinalsetofactivationsrightbeforethefinal\nsoftmax. So however many layers we have, we’ll generally call the unnormalized\nvaluesinthefinalvectorz[n],thevectorofscoresrightbeforethefinalsoftmax,the\nlogits logits(see(??).\nThe need for non-linear activation functions One of the reasons we use non-\nlinearactivationfunctionsforeachlayerinaneuralnetworkisthatifwedidnot,the\nresultingnetworkisexactlyequivalenttoasingle-layernetwork. Let’sseewhythis\nistrue. Imaginethefirsttwolayersofsuchanetworkofpurelylinearlayers:\nz[1] = W[1]x+b[1]\nz[2] = W[2]z[1]+b[2]\nWecanrewritethefunctionthatthenetworkiscomputingas:\nz[2] = W[2]z[1]+b[2]\n= W[2](W[1]x+b[1])+b[2]\n= W[2]W[1]x+W[2]b[1]+b[2]\n= W(cid:48)x+b(cid:48) (7.14)\nThisgeneralizestoanynumberoflayers.Sowithoutnon-linearactivationfunctions,\na multilayer network is just a notational variant of a single layer network with a\ndifferent set of weights, and we lose all the representational power of multilayer\nnetworks."
    },
    "7": {
        "chapter": "7a",
        "sections": "7.5",
        "topic": " Training FFNNs (Loss Function and Gradient)",
        "original_category": "CS",
        "original_text": "7.1 • UNITS 3\nFig.7.2showsafinalschematicofabasicneuralunit. Inthisexampletheunit\ntakes3inputvaluesx ,x ,andx ,andcomputesaweightedsum,multiplyingeach\n1 2 3\nvaluebyaweight(w ,w ,andw ,respectively),addsthemtoabiastermb,andthen\n1 2 3\npassestheresultingsumthroughasigmoidfunctiontoresultinanumberbetween0\nand1.\nx\n1 w 1\nx w 2 ∑ z σ a y\n2\nw\n3\nx b\n3\n+1\nFigure7.2 Aneuralunit,taking3inputsx ,x ,andx (andabiasbthatwerepresentasa\n1 2 3\nweightforaninputclampedat+1)andproducinganoutputy. Weincludesomeconvenient\nintermediatevariables: theoutputofthesummation,z,andtheoutputofthesigmoid,a. In\nthiscasetheoutputoftheunityisthesameasa,butindeepernetworkswe’llreserveyto\nmeanthefinaloutputoftheentirenetwork,leavingaastheactivationofanindividualnode.\nLet’swalkthroughanexamplejusttogetanintuition. Let’ssupposewehavea\nunitwiththefollowingweightvectorandbias:\nw = [0.2,0.3,0.9]\nb = 0.5\nWhatwouldthisunitdowiththefollowinginputvector:\nx = [0.5,0.6,0.1]\nTheresultingoutputywouldbe:\n1 1 1\ny=σ(w·x+b)= = = =.70\n1+e−(w·x+b) 1+e−(.5∗.2+.6∗.3+.1∗.9+.5) 1+e−0.87\nInpractice,thesigmoidisnotcommonlyusedasanactivationfunction. Afunction\ntanh thatisverysimilarbutalmostalwaysbetteristhetanhfunctionshowninFig.7.3a;\ntanhisavariantofthesigmoidthatrangesfrom-1to+1:\nez−e−z\ny=tanh(z)= (7.5)\nez+e−z\nThesimplestactivationfunction, andperhapsthemostcommonlyused, istherec-\nReLU tified linear unit, also called the ReLU, shown in Fig. 7.3b. It’s just the same as z\nwhenzispositive,and0otherwise:\ny=ReLU(z)=max(z,0) (7.6)\nTheseactivationfunctionshavedifferentpropertiesthatmakethemusefulfordiffer-\nentlanguageapplicationsornetworkarchitectures. Forexample,thetanhfunction\nhasthenicepropertiesofbeingsmoothlydifferentiableandmappingoutliervalues\ntowardthemean. Therectifierfunction,ontheotherhand,hasnicepropertiesthat 4 CHAPTER7 • NEURALNETWORKS\n(a) (b)\nFigure7.3 ThetanhandReLUactivationfunctions.\nresultfromitbeingveryclosetolinear. Inthesigmoidortanhfunctions,veryhigh\nsaturated valuesofzresultinvaluesofythataresaturated,i.e.,extremelycloseto1,andhave\nderivativesverycloseto0. Zeroderivativescauseproblemsforlearning,becauseas\nwe’ll see in Section 7.5, we’ll train networks by propagating an error signal back-\nwards, multiplying gradients (partial derivatives) from each layer of the network;\ngradientsthatarealmost0causetheerrorsignaltogetsmallerandsmalleruntilitis\nvanishing toosmalltobeusedfortraining,aproblemcalledthevanishinggradientproblem.\ngradient\nRectifiersdon’thavethisproblem,sincethederivativeofReLUforhighvaluesofz\nis1ratherthanverycloseto0.\n7.2 The XOR problem\nEarlyinthehistoryofneuralnetworksitwasrealizedthatthepowerofneuralnet-\nworks, as with the real neurons that inspired them, comes from combining these\nunitsintolargernetworks.\nOneofthemostcleverdemonstrationsoftheneedformulti-layernetworkswas\nthe proof by Minsky and Papert (1969) that a single neural unit cannot compute\nsomeverysimplefunctionsofitsinput. Considerthetaskofcomputingelementary\nlogical functions of two inputs, like AND, OR, and XOR. As a reminder, here are\nthetruthtablesforthosefunctions:\nAND OR XOR\nx1 x2 y x1 x2 y x1 x2 y\n0 0 0 0 0 0 0 0 0\n0 1 0 0 1 1 0 1 1\n1 0 0 1 0 1 1 0 1\n1 1 1 1 1 1 1 1 0\nperceptron Thisexamplewasfirstshownfortheperceptron,whichisaverysimpleneural\nunitthathasabinaryoutputanddoesnothaveanon-linearactivationfunction. The\noutputyofaperceptronis0or1,andiscomputedasfollows(usingthesameweight\nw,inputx,andbiasbasinEq.7.2):\n(cid:26)\n0, ifw·x+b≤0\ny= (7.7)\n1, ifw·x+b>0 14 CHAPTER7 • NEURALNETWORKS\ndistributionforanentiretestset:\n(cid:124)\nH = σ(XW +b)\n(cid:124)\nZ = HU\nYˆ = softmax(Z) (7.22)\nThe idea of using word2vec or GloVe embeddings as our input representation—\nandmoregenerallytheideaofrelyingonanotheralgorithmtohavealreadylearned\npretraining an embedding representation for our input words—is called pretraining. Using\npretrainedembeddingrepresentations,whethersimplestaticwordembeddingslike\nword2vec or the much more powerful contextual embeddings we’ll introduce in\nChapter 11, is one of the central ideas of deep learning. (It’s also possible, how-\never,totrainthewordembeddingsaspartofanNLPtask; we’lltalkabouthowto\ndothisinSection7.7inthecontextoftheneurallanguagemodelingtask.)\n7.5 Training Neural Nets\nAfeedforwardneuralnetisaninstanceofsupervisedmachinelearninginwhichwe\nknow the correct output y for each observation x. What the system produces, via\nEq.7.13,isyˆ,thesystem’sestimateofthetruey. Thegoalofthetrainingprocedure\nis to learn parameters W[i] and b[i] for each layer i that make yˆ for each training\nobservationascloseaspossibletothetruey.\nIngeneral,wedoallthisbydrawingonthemethodsweintroducedinChapter5\nforlogisticregression,sothereadershouldbecomfortablewiththatchapterbefore\nproceeding.\nFirst, we’ll need a loss function that models the distance between the system\noutputandthegoldoutput,andit’scommontousethelossfunctionusedforlogistic\nregression,thecross-entropyloss.\nSecond, to find the parameters that minimize this loss function, we’ll use the\ngradientdescentoptimizationalgorithmintroducedinChapter5.\nThird, gradientdescentrequiresknowingthegradientofthelossfunction, the\nvector that contains the partial derivative of the loss function with respect to each\nof the parameters. In logistic regression, for each observation we could directly\ncomputethederivativeofthelossfunctionwithrespecttoanindividualworb. But\nforneuralnetworks,withmillionsofparametersinmanylayers,it’smuchharderto\nsee how to compute the partial derivative of some weight in layer 1 when the loss\nisattachedtosomemuchlaterlayer. Howdowepartialoutthelossoverallthose\nintermediatelayers? Theansweristhealgorithmcallederrorbackpropagationor\nbackwarddifferentiation.\n7.5.1 Lossfunction\ncross-entropy Thecross-entropylossthatisusedinneuralnetworksisthesameonewesawfor\nloss\nlogistic regression. If the neural network is being used as a binary classifier, with\nthe sigmoid at the final layer, the loss function is the same logistic regression loss\nwesawinEq.??:\nL CE(yˆ,y)=−logp(y|x) = −[ylogyˆ+(1−y)log(1−yˆ)] (7.23)\nIfweareusingthenetworktoclassifyinto3ormoreclasses,thelossfunctionis\nexactlythesameasthelossformultinomialregressionthatwesawinChapter5on 7.5 • TRAININGNEURALNETS 15\npage??. Let’sbrieflysummarizetheexplanationhereforconvenience. First,when\nwehavemorethan2classeswe’llneedtorepresentbothy andyˆ asvectors. Let’s\nassume we’re doing hard classification, where only one class is the correct one.\nThe true label y is then a vector with K elements, each corresponding to a class,\nwithy =1ifthecorrectclassisc,withallotherelementsofybeing0. Recallthat\nc\navectorlikethis,withonevalueequalto1andtherest0,iscalledaone-hotvector.\nAndourclassifierwillproduceanestimatevectorwithK elementsyˆ,eachelement\nyˆ ofwhichrepresentstheestimatedprobability p(y =1|x).\nk k\nThelossfunctionforasingleexamplexisthenegativesumofthelogsoftheK\noutputclasses,eachweightedbytheirprobabilityy :\nk\nK\n(cid:88)\nL CE(yˆ,y)=− y klogyˆ k (7.24)\nk=1\nWecansimplifythisequationfurther;let’sfirstrewritetheequationusingthefunc-\ntion 1{} which evaluates to 1 if the condition in the brackets is true and to 0 oth-\nerwise. ThismakesitmoreobviousthatthetermsinthesuminEq.7.24willbe0\nexceptforthetermcorrespondingtothetrueclassforwhichy =1:\nk\nK\n(cid:88)\nL (yˆ,y) = − 1{y =1}logyˆ\nCE k k\nk=1\nInotherwords,thecross-entropylossissimplythenegativelogoftheoutputproba-\nbilitycorrespondingtothecorrectclass,andwethereforealsocallthisthenegative\nnegativelog loglikelihoodloss:\nlikelihoodloss\nL CE(yˆ,y) = −logyˆ c (wherecisthecorrectclass) (7.25)\nPlugginginthesoftmaxformulafromEq.7.9,andwithK thenumberofclasses:\nexp(z )\nc\nL CE(yˆ,y) = −log\n(cid:80)K\nexp(z )\n(wherecisthecorrectclass) (7.26)\nj=1 j\n7.5.2 ComputingtheGradient\nHow do we compute the gradient of this loss function? Computing the gradient\nrequires the partial derivative of the loss function with respect to each parameter.\nFor a network with one weight layer and sigmoid output (which is what logistic\nregressionis),wecouldsimplyusethederivativeofthelossthatweusedforlogistic\nregressioninEq.7.27(andderivedinSection??):\n∂L (yˆ,y)\nCE\n= (yˆ−y)x\nj\n∂w\nj\n= (σ(w·x+b)−y)x j (7.27)\nOrforanetworkwithoneweightlayerandsoftmaxoutput(=multinomiallogistic\nregression),wecouldusethederivativeofthesoftmaxlossfromEq.??,shownfor\naparticularweightw andinputx\nk i\n∂L (yˆ,y)\nCE = −(y −yˆ )x\nk k i\n∂w\nk,i\n= −(y −p(y =1|x))x\nk k i\n(cid:32) (cid:33)\nexp(w ·x+b )\n= − y k− (cid:80)K expk\n(w\n·x+k\nb )\nx i (7.28)\nj=1 j j 16 CHAPTER7 • NEURALNETWORKS\nButthesederivativesonlygivecorrectupdatesforoneweightlayer:thelastone!\nFordeepnetworks,computingthegradientsforeachweightismuchmorecomplex,\nsincewearecomputingthederivativewithrespecttoweightparametersthatappear\nall the way back in the very early layers of the network, even though the loss is\ncomputedonlyattheveryendofthenetwork.\nThesolutiontocomputingthisgradientisanalgorithmcallederrorbackprop-\nerrorback- agation or backprop (Rumelhart et al., 1986). While backprop was invented spe-\npropagation\nciallyforneuralnetworks, itturnsouttobethesameasamoregeneralprocedure\ncalled backward differentiation, which depends on the notion of computation\ngraphs. Let’sseehowthatworksinthenextsubsection.\n7.5.3 ComputationGraphs\nAcomputationgraphisarepresentationoftheprocessofcomputingamathematical\nexpression,inwhichthecomputationisbrokendownintoseparateoperations,each\nofwhichismodeledasanodeinagraph.\nConsidercomputingthefunctionL(a,b,c)=c(a+2b). Ifwemakeeachofthe\ncomponentadditionandmultiplicationoperationsexplicit,andaddnames(dande)\nfortheintermediateoutputs,theresultingseriesofcomputationsis:\nd = 2∗b\ne = a+d\nL = c∗e\nWe can now represent this as a graph, with nodes for each operation, and di-\nrected edges showing the outputs from each operation as the inputs to the next, as\nin Fig. 7.12. The simplest use of computation graphs is to compute the value of\nthefunctionwithsomegiveninputs. Inthefigure,we’veassumedtheinputsa=3,\nb=1,c=−2,andwe’veshowntheresultoftheforwardpasstocomputethere-\nsultL(3,1,−2)=−10. Intheforwardpassofacomputationgraph,weapplyeach\noperation left to right, passing the outputs of each computation as the input to the\nnextnode.\nforward pass\na=3\na\ne=5\ne=a+d\nd=2\nb=1\nb d = 2b L=ce L=-10\nc=-2\nc\nFigure7.12 ComputationgraphforthefunctionL(a,b,c)=c(a+2b),withvaluesforinput\nnodesa=3,b=1,c=−2,showingtheforwardpasscomputationofL.\n7.5.4 Backwarddifferentiationoncomputationgraphs\nThe importance of the computation graph comes from the backward pass, which\nis used to compute the derivatives that we’ll need for the weight update. In this\nexampleourgoalistocomputethederivativeoftheoutputfunctionLwithrespect 7.5 • TRAININGNEURALNETS 17\nto eachof the input variables, i.e., ∂L, ∂L, and ∂L. The derivative ∂L tells ushow\n∂a ∂b ∂c ∂a\nmuchasmallchangeinaaffectsL.\nchainrule Backwards differentiation makes use of the chain rule in calculus, so let’s re-\nmind ourselves of that. Suppose we are computing the derivative of a composite\nfunction f(x)=u(v(x)). Thederivativeof f(x)isthederivativeofu(x)withrespect\ntov(x)timesthederivativeofv(x)withrespecttox:\ndf du dv\n= · (7.29)\ndx dv dx\nThechainruleextendstomorethantwofunctions. Ifcomputingthederivativeofa\ncompositefunction f(x)=u(v(w(x))),thederivativeof f(x)is:\ndf du dv dw\n= · · (7.30)\ndx dv dw dx\nTheintuitionofbackwarddifferentiationistopassgradientsbackfromthefinal\nnodetoallthenodesinthegraph.Fig.7.13showspartofthebackwardcomputation\natonenodee. Eachnodetakesanupstreamgradientthatispassedinfromitsparent\nnodetotheright,andforeachofitsinputscomputesalocalgradient(thegradient\nofitsoutputwithrespecttoitsinput),andusesthechainruletomultiplythesetwo\ntocomputeadownstreamgradienttobepassedontothenextearliernode.\nd e\nd e L\n∂L ∂L ∂e ∂e ∂L\n=\n∂d ∂e ∂d ∂d ∂e\ndownstream local upstream\ngradient gradient gradient\nFigure7.13 Eachnode(likeehere)takesanupstreamgradient,multipliesitbythelocal\ngradient(thegradientofitsoutputwithrespecttoitsinput),andusesthechainruletocompute\na downstream gradient to be passed on to a prior node. A node may have multiple local\ngradientsifithasmultipleinputs.\nLet’s now compute the 3 derivatives we need. Since in the computation graph\nL=ce,wecandirectlycomputethederivative ∂L:\n∂c\n∂L\n=e (7.31)\n∂c\nFortheothertwo,we’llneedtousethechainrule:\n∂L ∂L∂e\n=\n∂a ∂e∂a\n∂L ∂L∂e∂d\n= (7.32)\n∂b ∂e∂d ∂b\nEq.7.32andEq.7.31thusrequirefiveintermediatederivatives: ∂L, ∂L, ∂e, ∂e,and\n∂e ∂c ∂a ∂d\n∂d,whichareasfollows(makinguseofthefactthatthederivativeofasumisthe\n∂b 7.5 • TRAININGNEURALNETS 19\nForthebackwardpasswe’llalsoneedtocomputethelossL. Thelossfunction\nforbinarysigmoidoutputfromEq.7.23is\nL CE(yˆ,y) = −[ylogyˆ+(1−y)log(1−yˆ)] (7.34)\nOuroutputyˆ=a[2],sowecanrephrasethisas\n(cid:104) (cid:105)\nL CE(a[2],y) = − yloga[2]+(1−y)log(1−a[2]) (7.35)\n[1]\nw\n11 *\n[1]\nw\n12 z[1] = a[1] =\n* 1 1\n+ ReLU\nx\n1\n*\n[1]\nb [2]\nx\n2\n1 w 1[ 12] z + = a[2] = σ L (a[2] ,y)\n*\n*\n[1] [2]\nw [1] [1] w\n21 * z 2 = a 2 = 12\n+ ReLU\n[1]\nw [2]\n22 b\n1\n[1]\nb\n2\nFigure7.15 Samplecomputationgraphforasimple2-layerneuralnet(=1hiddenlayer)withtwoinputunits\nand2hiddenunits. We’veadjustedthenotationabittoavoidlongequationsinthenodesbyjustmentioning\n[1]\nthefunctionthatisbeingcomputed,andtheresultingvariablename.Thusthe*totherightofnodew means\n11\nthatw[1] istobemultipliedbyx ,andthenodez[1]=+meansthatthevalueofz[1] iscomputedbysumming\n11 1\n[1]\nthethreenodesthatfeedintoit(thetwoproducts,andthebiastermb ).\ni\nThe weights that need updating (those for which we need to know the partial\nderivativeofthelossfunction)areshowninteal. Inordertodothebackwardpass,\nwe’llneedtoknowthederivativesofallthefunctionsinthegraph. Wealreadysaw\ninSection??thederivativeofthesigmoidσ:\ndσ(z)\n=σ(z)(1−σ(z)) (7.36)\ndz\nWe’ll also need the derivatives of each of the other activation functions. The\nderivativeoftanhis:\ndtanh(z)\n=1−tanh2(z) (7.37)\ndz\nThederivativeoftheReLUis2\n(cid:26)\ndReLU(z) 0 for z<0\n= (7.38)\ndz 1 for z≥0\n2 Thederivativeisactuallyundefinedatthepointz=0,butbyconventionwetreatitas1. 20 CHAPTER7 • NEURALNETWORKS\nWe’llgivethestartofthecomputation,computingthederivativeofthelossfunction\nLwithrespecttoz,or ∂L (andleavingtherestofthecomputationasanexercisefor\n∂z\nthereader). Bythechainrule:\n∂L ∂L ∂a[2]\n= (7.39)\n∂z ∂a[2] ∂z\nSolet’sfirstcompute ∂L ,takingthederivativeofEq.7.35,repeatedhere:\n∂a[2]\n(cid:104) (cid:105)\nL (a[2],y) = − yloga[2]+(1−y)log(1−a[2])\nCE\n(cid:32)(cid:32) (cid:33) (cid:33)\n∂L ∂log(a[2]) ∂log(1−a[2])\n= − y +(1−y)\n∂a[2] ∂a[2] ∂a[2]\n(cid:18)(cid:18) (cid:19) (cid:19)\n1 1\n= − y +(1−y) (−1)\na[2] 1−a[2]\n(cid:18) (cid:19)\ny y−1\n= − + (7.40)\na[2] 1−a[2]\nNext,bythederivativeofthesigmoid:\n∂a[2]\n=a[2](1−a[2])\n∂z\nFinally,wecanusethechainrule:\n∂L ∂L ∂a[2]\n=\n∂z ∂a[2] ∂z\n(cid:18) (cid:19)\ny y−1\n= − + a[2](1−a[2])\na[2] 1−a[2]\n= a[2]−y (7.41)\nContinuingthebackwardcomputationofthegradients(nextbypassingthegra-\n[2]\ndientsoverb andthetwoproductnodes,andsoon,backtoallthetealnodes),is\n1\nleftasanexerciseforthereader.\n7.5.5 Moredetailsonlearning\nOptimizationinneuralnetworksisanon-convexoptimizationproblem,morecom-\nplexthanforlogisticregression,andforthatandotherreasonstherearemanybest\npracticesforsuccessfullearning.\nForlogisticregressionwecaninitializegradientdescentwithalltheweightsand\nbiaseshavingthevalue0. Inneuralnetworks,bycontrast,weneedtoinitializethe\nweightswithsmallrandomnumbers. It’salsohelpfultonormalizetheinputvalues\ntohave0meanandunitvariance.\nVariousformsofregularizationareusedtopreventoverfitting. Oneofthemost\ndropout important is dropout: randomly dropping some units and their connections from\nthe network during training (Hinton et al. 2012, Srivastava et al. 2014). At each\niterationoftraining(wheneverweupdateparameters,i.e. eachmini-batchifweare\nusing mini-batch gradient descent), we repeatedly choose a probability p and for\neach unit we replace its output with zero with probability p (and renormalize the\nrestoftheoutputsfromthatlayer)."
    },
    "8": {
        "chapter": "7b",
        "sections": "7.5",
        "topic": "Backward Pass",
        "original_category": "CS",
        "original_text": "7.1 • UNITS 3\nFig.7.2showsafinalschematicofabasicneuralunit. Inthisexampletheunit\ntakes3inputvaluesx ,x ,andx ,andcomputesaweightedsum,multiplyingeach\n1 2 3\nvaluebyaweight(w ,w ,andw ,respectively),addsthemtoabiastermb,andthen\n1 2 3\npassestheresultingsumthroughasigmoidfunctiontoresultinanumberbetween0\nand1.\nx\n1 w 1\nx w 2 ∑ z σ a y\n2\nw\n3\nx b\n3\n+1\nFigure7.2 Aneuralunit,taking3inputsx ,x ,andx (andabiasbthatwerepresentasa\n1 2 3\nweightforaninputclampedat+1)andproducinganoutputy. Weincludesomeconvenient\nintermediatevariables: theoutputofthesummation,z,andtheoutputofthesigmoid,a. In\nthiscasetheoutputoftheunityisthesameasa,butindeepernetworkswe’llreserveyto\nmeanthefinaloutputoftheentirenetwork,leavingaastheactivationofanindividualnode.\nLet’swalkthroughanexamplejusttogetanintuition. Let’ssupposewehavea\nunitwiththefollowingweightvectorandbias:\nw = [0.2,0.3,0.9]\nb = 0.5\nWhatwouldthisunitdowiththefollowinginputvector:\nx = [0.5,0.6,0.1]\nTheresultingoutputywouldbe:\n1 1 1\ny=σ(w·x+b)= = = =.70\n1+e−(w·x+b) 1+e−(.5∗.2+.6∗.3+.1∗.9+.5) 1+e−0.87\nInpractice,thesigmoidisnotcommonlyusedasanactivationfunction. Afunction\ntanh thatisverysimilarbutalmostalwaysbetteristhetanhfunctionshowninFig.7.3a;\ntanhisavariantofthesigmoidthatrangesfrom-1to+1:\nez−e−z\ny=tanh(z)= (7.5)\nez+e−z\nThesimplestactivationfunction, andperhapsthemostcommonlyused, istherec-\nReLU tified linear unit, also called the ReLU, shown in Fig. 7.3b. It’s just the same as z\nwhenzispositive,and0otherwise:\ny=ReLU(z)=max(z,0) (7.6)\nTheseactivationfunctionshavedifferentpropertiesthatmakethemusefulfordiffer-\nentlanguageapplicationsornetworkarchitectures. Forexample,thetanhfunction\nhasthenicepropertiesofbeingsmoothlydifferentiableandmappingoutliervalues\ntowardthemean. Therectifierfunction,ontheotherhand,hasnicepropertiesthat 4 CHAPTER7 • NEURALNETWORKS\n(a) (b)\nFigure7.3 ThetanhandReLUactivationfunctions.\nresultfromitbeingveryclosetolinear. Inthesigmoidortanhfunctions,veryhigh\nsaturated valuesofzresultinvaluesofythataresaturated,i.e.,extremelycloseto1,andhave\nderivativesverycloseto0. Zeroderivativescauseproblemsforlearning,becauseas\nwe’ll see in Section 7.5, we’ll train networks by propagating an error signal back-\nwards, multiplying gradients (partial derivatives) from each layer of the network;\ngradientsthatarealmost0causetheerrorsignaltogetsmallerandsmalleruntilitis\nvanishing toosmalltobeusedfortraining,aproblemcalledthevanishinggradientproblem.\ngradient\nRectifiersdon’thavethisproblem,sincethederivativeofReLUforhighvaluesofz\nis1ratherthanverycloseto0.\n7.2 The XOR problem\nEarlyinthehistoryofneuralnetworksitwasrealizedthatthepowerofneuralnet-\nworks, as with the real neurons that inspired them, comes from combining these\nunitsintolargernetworks.\nOneofthemostcleverdemonstrationsoftheneedformulti-layernetworkswas\nthe proof by Minsky and Papert (1969) that a single neural unit cannot compute\nsomeverysimplefunctionsofitsinput. Considerthetaskofcomputingelementary\nlogical functions of two inputs, like AND, OR, and XOR. As a reminder, here are\nthetruthtablesforthosefunctions:\nAND OR XOR\nx1 x2 y x1 x2 y x1 x2 y\n0 0 0 0 0 0 0 0 0\n0 1 0 0 1 1 0 1 1\n1 0 0 1 0 1 1 0 1\n1 1 1 1 1 1 1 1 0\nperceptron Thisexamplewasfirstshownfortheperceptron,whichisaverysimpleneural\nunitthathasabinaryoutputanddoesnothaveanon-linearactivationfunction. The\noutputyofaperceptronis0or1,andiscomputedasfollows(usingthesameweight\nw,inputx,andbiasbasinEq.7.2):\n(cid:26)\n0, ifw·x+b≤0\ny= (7.7)\n1, ifw·x+b>0 14 CHAPTER7 • NEURALNETWORKS\ndistributionforanentiretestset:\n(cid:124)\nH = σ(XW +b)\n(cid:124)\nZ = HU\nYˆ = softmax(Z) (7.22)\nThe idea of using word2vec or GloVe embeddings as our input representation—\nandmoregenerallytheideaofrelyingonanotheralgorithmtohavealreadylearned\npretraining an embedding representation for our input words—is called pretraining. Using\npretrainedembeddingrepresentations,whethersimplestaticwordembeddingslike\nword2vec or the much more powerful contextual embeddings we’ll introduce in\nChapter 11, is one of the central ideas of deep learning. (It’s also possible, how-\never,totrainthewordembeddingsaspartofanNLPtask; we’lltalkabouthowto\ndothisinSection7.7inthecontextoftheneurallanguagemodelingtask.)\n7.5 Training Neural Nets\nAfeedforwardneuralnetisaninstanceofsupervisedmachinelearninginwhichwe\nknow the correct output y for each observation x. What the system produces, via\nEq.7.13,isyˆ,thesystem’sestimateofthetruey. Thegoalofthetrainingprocedure\nis to learn parameters W[i] and b[i] for each layer i that make yˆ for each training\nobservationascloseaspossibletothetruey.\nIngeneral,wedoallthisbydrawingonthemethodsweintroducedinChapter5\nforlogisticregression,sothereadershouldbecomfortablewiththatchapterbefore\nproceeding.\nFirst, we’ll need a loss function that models the distance between the system\noutputandthegoldoutput,andit’scommontousethelossfunctionusedforlogistic\nregression,thecross-entropyloss.\nSecond, to find the parameters that minimize this loss function, we’ll use the\ngradientdescentoptimizationalgorithmintroducedinChapter5.\nThird, gradientdescentrequiresknowingthegradientofthelossfunction, the\nvector that contains the partial derivative of the loss function with respect to each\nof the parameters. In logistic regression, for each observation we could directly\ncomputethederivativeofthelossfunctionwithrespecttoanindividualworb. But\nforneuralnetworks,withmillionsofparametersinmanylayers,it’smuchharderto\nsee how to compute the partial derivative of some weight in layer 1 when the loss\nisattachedtosomemuchlaterlayer. Howdowepartialoutthelossoverallthose\nintermediatelayers? Theansweristhealgorithmcallederrorbackpropagationor\nbackwarddifferentiation.\n7.5.1 Lossfunction\ncross-entropy Thecross-entropylossthatisusedinneuralnetworksisthesameonewesawfor\nloss\nlogistic regression. If the neural network is being used as a binary classifier, with\nthe sigmoid at the final layer, the loss function is the same logistic regression loss\nwesawinEq.??:\nL CE(yˆ,y)=−logp(y|x) = −[ylogyˆ+(1−y)log(1−yˆ)] (7.23)\nIfweareusingthenetworktoclassifyinto3ormoreclasses,thelossfunctionis\nexactlythesameasthelossformultinomialregressionthatwesawinChapter5on 7.5 • TRAININGNEURALNETS 15\npage??. Let’sbrieflysummarizetheexplanationhereforconvenience. First,when\nwehavemorethan2classeswe’llneedtorepresentbothy andyˆ asvectors. Let’s\nassume we’re doing hard classification, where only one class is the correct one.\nThe true label y is then a vector with K elements, each corresponding to a class,\nwithy =1ifthecorrectclassisc,withallotherelementsofybeing0. Recallthat\nc\navectorlikethis,withonevalueequalto1andtherest0,iscalledaone-hotvector.\nAndourclassifierwillproduceanestimatevectorwithK elementsyˆ,eachelement\nyˆ ofwhichrepresentstheestimatedprobability p(y =1|x).\nk k\nThelossfunctionforasingleexamplexisthenegativesumofthelogsoftheK\noutputclasses,eachweightedbytheirprobabilityy :\nk\nK\n(cid:88)\nL CE(yˆ,y)=− y klogyˆ k (7.24)\nk=1\nWecansimplifythisequationfurther;let’sfirstrewritetheequationusingthefunc-\ntion 1{} which evaluates to 1 if the condition in the brackets is true and to 0 oth-\nerwise. ThismakesitmoreobviousthatthetermsinthesuminEq.7.24willbe0\nexceptforthetermcorrespondingtothetrueclassforwhichy =1:\nk\nK\n(cid:88)\nL (yˆ,y) = − 1{y =1}logyˆ\nCE k k\nk=1\nInotherwords,thecross-entropylossissimplythenegativelogoftheoutputproba-\nbilitycorrespondingtothecorrectclass,andwethereforealsocallthisthenegative\nnegativelog loglikelihoodloss:\nlikelihoodloss\nL CE(yˆ,y) = −logyˆ c (wherecisthecorrectclass) (7.25)\nPlugginginthesoftmaxformulafromEq.7.9,andwithK thenumberofclasses:\nexp(z )\nc\nL CE(yˆ,y) = −log\n(cid:80)K\nexp(z )\n(wherecisthecorrectclass) (7.26)\nj=1 j\n7.5.2 ComputingtheGradient\nHow do we compute the gradient of this loss function? Computing the gradient\nrequires the partial derivative of the loss function with respect to each parameter.\nFor a network with one weight layer and sigmoid output (which is what logistic\nregressionis),wecouldsimplyusethederivativeofthelossthatweusedforlogistic\nregressioninEq.7.27(andderivedinSection??):\n∂L (yˆ,y)\nCE\n= (yˆ−y)x\nj\n∂w\nj\n= (σ(w·x+b)−y)x j (7.27)\nOrforanetworkwithoneweightlayerandsoftmaxoutput(=multinomiallogistic\nregression),wecouldusethederivativeofthesoftmaxlossfromEq.??,shownfor\naparticularweightw andinputx\nk i\n∂L (yˆ,y)\nCE = −(y −yˆ )x\nk k i\n∂w\nk,i\n= −(y −p(y =1|x))x\nk k i\n(cid:32) (cid:33)\nexp(w ·x+b )\n= − y k− (cid:80)K expk\n(w\n·x+k\nb )\nx i (7.28)\nj=1 j j 16 CHAPTER7 • NEURALNETWORKS\nButthesederivativesonlygivecorrectupdatesforoneweightlayer:thelastone!\nFordeepnetworks,computingthegradientsforeachweightismuchmorecomplex,\nsincewearecomputingthederivativewithrespecttoweightparametersthatappear\nall the way back in the very early layers of the network, even though the loss is\ncomputedonlyattheveryendofthenetwork.\nThesolutiontocomputingthisgradientisanalgorithmcallederrorbackprop-\nerrorback- agation or backprop (Rumelhart et al., 1986). While backprop was invented spe-\npropagation\nciallyforneuralnetworks, itturnsouttobethesameasamoregeneralprocedure\ncalled backward differentiation, which depends on the notion of computation\ngraphs. Let’sseehowthatworksinthenextsubsection.\n7.5.3 ComputationGraphs\nAcomputationgraphisarepresentationoftheprocessofcomputingamathematical\nexpression,inwhichthecomputationisbrokendownintoseparateoperations,each\nofwhichismodeledasanodeinagraph.\nConsidercomputingthefunctionL(a,b,c)=c(a+2b). Ifwemakeeachofthe\ncomponentadditionandmultiplicationoperationsexplicit,andaddnames(dande)\nfortheintermediateoutputs,theresultingseriesofcomputationsis:\nd = 2∗b\ne = a+d\nL = c∗e\nWe can now represent this as a graph, with nodes for each operation, and di-\nrected edges showing the outputs from each operation as the inputs to the next, as\nin Fig. 7.12. The simplest use of computation graphs is to compute the value of\nthefunctionwithsomegiveninputs. Inthefigure,we’veassumedtheinputsa=3,\nb=1,c=−2,andwe’veshowntheresultoftheforwardpasstocomputethere-\nsultL(3,1,−2)=−10. Intheforwardpassofacomputationgraph,weapplyeach\noperation left to right, passing the outputs of each computation as the input to the\nnextnode.\nforward pass\na=3\na\ne=5\ne=a+d\nd=2\nb=1\nb d = 2b L=ce L=-10\nc=-2\nc\nFigure7.12 ComputationgraphforthefunctionL(a,b,c)=c(a+2b),withvaluesforinput\nnodesa=3,b=1,c=−2,showingtheforwardpasscomputationofL.\n7.5.4 Backwarddifferentiationoncomputationgraphs\nThe importance of the computation graph comes from the backward pass, which\nis used to compute the derivatives that we’ll need for the weight update. In this\nexampleourgoalistocomputethederivativeoftheoutputfunctionLwithrespect 7.5 • TRAININGNEURALNETS 17\nto eachof the input variables, i.e., ∂L, ∂L, and ∂L. The derivative ∂L tells ushow\n∂a ∂b ∂c ∂a\nmuchasmallchangeinaaffectsL.\nchainrule Backwards differentiation makes use of the chain rule in calculus, so let’s re-\nmind ourselves of that. Suppose we are computing the derivative of a composite\nfunction f(x)=u(v(x)). Thederivativeof f(x)isthederivativeofu(x)withrespect\ntov(x)timesthederivativeofv(x)withrespecttox:\ndf du dv\n= · (7.29)\ndx dv dx\nThechainruleextendstomorethantwofunctions. Ifcomputingthederivativeofa\ncompositefunction f(x)=u(v(w(x))),thederivativeof f(x)is:\ndf du dv dw\n= · · (7.30)\ndx dv dw dx\nTheintuitionofbackwarddifferentiationistopassgradientsbackfromthefinal\nnodetoallthenodesinthegraph.Fig.7.13showspartofthebackwardcomputation\natonenodee. Eachnodetakesanupstreamgradientthatispassedinfromitsparent\nnodetotheright,andforeachofitsinputscomputesalocalgradient(thegradient\nofitsoutputwithrespecttoitsinput),andusesthechainruletomultiplythesetwo\ntocomputeadownstreamgradienttobepassedontothenextearliernode.\nd e\nd e L\n∂L ∂L ∂e ∂e ∂L\n=\n∂d ∂e ∂d ∂d ∂e\ndownstream local upstream\ngradient gradient gradient\nFigure7.13 Eachnode(likeehere)takesanupstreamgradient,multipliesitbythelocal\ngradient(thegradientofitsoutputwithrespecttoitsinput),andusesthechainruletocompute\na downstream gradient to be passed on to a prior node. A node may have multiple local\ngradientsifithasmultipleinputs.\nLet’s now compute the 3 derivatives we need. Since in the computation graph\nL=ce,wecandirectlycomputethederivative ∂L:\n∂c\n∂L\n=e (7.31)\n∂c\nFortheothertwo,we’llneedtousethechainrule:\n∂L ∂L∂e\n=\n∂a ∂e∂a\n∂L ∂L∂e∂d\n= (7.32)\n∂b ∂e∂d ∂b\nEq.7.32andEq.7.31thusrequirefiveintermediatederivatives: ∂L, ∂L, ∂e, ∂e,and\n∂e ∂c ∂a ∂d\n∂d,whichareasfollows(makinguseofthefactthatthederivativeofasumisthe\n∂b 7.5 • TRAININGNEURALNETS 19\nForthebackwardpasswe’llalsoneedtocomputethelossL. Thelossfunction\nforbinarysigmoidoutputfromEq.7.23is\nL CE(yˆ,y) = −[ylogyˆ+(1−y)log(1−yˆ)] (7.34)\nOuroutputyˆ=a[2],sowecanrephrasethisas\n(cid:104) (cid:105)\nL CE(a[2],y) = − yloga[2]+(1−y)log(1−a[2]) (7.35)\n[1]\nw\n11 *\n[1]\nw\n12 z[1] = a[1] =\n* 1 1\n+ ReLU\nx\n1\n*\n[1]\nb [2]\nx\n2\n1 w 1[ 12] z + = a[2] = σ L (a[2] ,y)\n*\n*\n[1] [2]\nw [1] [1] w\n21 * z 2 = a 2 = 12\n+ ReLU\n[1]\nw [2]\n22 b\n1\n[1]\nb\n2\nFigure7.15 Samplecomputationgraphforasimple2-layerneuralnet(=1hiddenlayer)withtwoinputunits\nand2hiddenunits. We’veadjustedthenotationabittoavoidlongequationsinthenodesbyjustmentioning\n[1]\nthefunctionthatisbeingcomputed,andtheresultingvariablename.Thusthe*totherightofnodew means\n11\nthatw[1] istobemultipliedbyx ,andthenodez[1]=+meansthatthevalueofz[1] iscomputedbysumming\n11 1\n[1]\nthethreenodesthatfeedintoit(thetwoproducts,andthebiastermb ).\ni\nThe weights that need updating (those for which we need to know the partial\nderivativeofthelossfunction)areshowninteal. Inordertodothebackwardpass,\nwe’llneedtoknowthederivativesofallthefunctionsinthegraph. Wealreadysaw\ninSection??thederivativeofthesigmoidσ:\ndσ(z)\n=σ(z)(1−σ(z)) (7.36)\ndz\nWe’ll also need the derivatives of each of the other activation functions. The\nderivativeoftanhis:\ndtanh(z)\n=1−tanh2(z) (7.37)\ndz\nThederivativeoftheReLUis2\n(cid:26)\ndReLU(z) 0 for z<0\n= (7.38)\ndz 1 for z≥0\n2 Thederivativeisactuallyundefinedatthepointz=0,butbyconventionwetreatitas1. 20 CHAPTER7 • NEURALNETWORKS\nWe’llgivethestartofthecomputation,computingthederivativeofthelossfunction\nLwithrespecttoz,or ∂L (andleavingtherestofthecomputationasanexercisefor\n∂z\nthereader). Bythechainrule:\n∂L ∂L ∂a[2]\n= (7.39)\n∂z ∂a[2] ∂z\nSolet’sfirstcompute ∂L ,takingthederivativeofEq.7.35,repeatedhere:\n∂a[2]\n(cid:104) (cid:105)\nL (a[2],y) = − yloga[2]+(1−y)log(1−a[2])\nCE\n(cid:32)(cid:32) (cid:33) (cid:33)\n∂L ∂log(a[2]) ∂log(1−a[2])\n= − y +(1−y)\n∂a[2] ∂a[2] ∂a[2]\n(cid:18)(cid:18) (cid:19) (cid:19)\n1 1\n= − y +(1−y) (−1)\na[2] 1−a[2]\n(cid:18) (cid:19)\ny y−1\n= − + (7.40)\na[2] 1−a[2]\nNext,bythederivativeofthesigmoid:\n∂a[2]\n=a[2](1−a[2])\n∂z\nFinally,wecanusethechainrule:\n∂L ∂L ∂a[2]\n=\n∂z ∂a[2] ∂z\n(cid:18) (cid:19)\ny y−1\n= − + a[2](1−a[2])\na[2] 1−a[2]\n= a[2]−y (7.41)\nContinuingthebackwardcomputationofthegradients(nextbypassingthegra-\n[2]\ndientsoverb andthetwoproductnodes,andsoon,backtoallthetealnodes),is\n1\nleftasanexerciseforthereader.\n7.5.5 Moredetailsonlearning\nOptimizationinneuralnetworksisanon-convexoptimizationproblem,morecom-\nplexthanforlogisticregression,andforthatandotherreasonstherearemanybest\npracticesforsuccessfullearning.\nForlogisticregressionwecaninitializegradientdescentwithalltheweightsand\nbiaseshavingthevalue0. Inneuralnetworks,bycontrast,weneedtoinitializethe\nweightswithsmallrandomnumbers. It’salsohelpfultonormalizetheinputvalues\ntohave0meanandunitvariance.\nVariousformsofregularizationareusedtopreventoverfitting. Oneofthemost\ndropout important is dropout: randomly dropping some units and their connections from\nthe network during training (Hinton et al. 2012, Srivastava et al. 2014). At each\niterationoftraining(wheneverweupdateparameters,i.e. eachmini-batchifweare\nusing mini-batch gradient descent), we repeatedly choose a probability p and for\neach unit we replace its output with zero with probability p (and renormalize the\nrestoftheoutputsfromthatlayer)."
    },
    "9": {
        "chapter": "8",
        "sections": "8.1",
        "topic": "Recurrent Neural Networks (RNNs)",
        "original_category": "CS",
        "original_text": "Speech and Language Processing. Daniel Jurafsky & James H. Martin. Copyright © 2024. All\nrights reserved. Draft of August 20, 2024.\nCHAPTER\n8 RNNs and LSTMs\nTimewillexplain.\nJaneAusten,Persuasion\nLanguageisaninherentlytemporalphenomenon. Spokenlanguageisasequenceof\nacousticeventsovertime,andwecomprehendandproducebothspokenandwritten\nlanguageasasequentialinputstream. Thetemporalnatureoflanguageisreflected\ninthemetaphorsweuse;wetalkoftheflowofconversations,newsfeeds,andtwitter\nstreams,allofwhichemphasizethatlanguageisasequencethatunfoldsintime.\nThis temporal nature is reflected in some language processing algorithms. For\nexample,theViterbialgorithmweintroducedforHMMpart-of-speechtaggingpro-\nceedsthroughtheinputawordatatime,carryingforwardinformationgleanedalong\ntheway. Butothermachinelearningapproaches,likethosewe’vestudiedforsenti-\nmentanalysisorothertextclassificationtasksdon’thavethistemporalnature–they\nassumesimultaneousaccesstoallaspectsoftheirinput.\nThe feedforward networks of Chapter 7 also assumed simultaneous access, al-\nthoughtheyalsohadasimplemodelfortime. Recallthatweappliedfeedforward\nnetworks to language modeling by having them look only at a fixed-size window\nofwords,andthenslidingthiswindowovertheinput,makingindependentpredic-\ntionsalongtheway. Thissliding-windowapproachisalsousedinthetransformer\narchitecturewewillintroduceinChapter9.\nThis chapter introduces a deep learning architecture that offers an alternative\nwayofrepresentingtime: recurrentneuralnetworks(RNNs),andtheirvariantslike\nLSTMs. RNNshaveamechanismthatdealsdirectlywiththesequentialnatureof\nlanguage,allowingthemtohandlethetemporalnatureoflanguagewithouttheuseof\narbitraryfixed-sizedwindows. Therecurrentnetworkoffersanewwaytorepresent\nthe prior context, in its recurrent connections, allowing the model’s decision to\ndependoninformationfromhundredsofwordsinthepast. We’llseehowtoapply\nthe model to the task of language modeling, to sequence modeling tasks like part-\nof-speechtagging,andtotextclassificationtaskslikesentimentanalysis.\n8.1 Recurrent Neural Networks\nA recurrent neural network (RNN) is any network that contains a cycle within its\nnetworkconnections, meaningthatthevalueofsomeunitisdirectly, orindirectly,\ndependent on its own earlier outputs as an input. While powerful, such networks\naredifficulttoreasonaboutandtotrain. However,withinthegeneralclassofrecur-\nrent networks there are constrained architectures that have proven to be extremely\neffectivewhenappliedtolanguage. Inthissection,weconsideraclassofrecurrent\nElman networksreferredtoasElmanNetworks(Elman,1990)orsimplerecurrentnet-\nNetworks 2 CHAPTER8 • RNNSANDLSTMS\nworks. Thesenetworksareusefulintheirownrightandserveasthebasisformore\ncomplexapproachesliketheLongShort-TermMemory(LSTM)networksdiscussed\nlaterinthischapter. InthischapterwhenweusethetermRNNwe’llbereferringto\nthesesimplermoreconstrainednetworks(althoughyouwilloftenseethetermRNN\ntomeananynetwithrecurrentpropertiesincludingLSTMs).\nx t h t y t\nFigure8.1 SimplerecurrentneuralnetworkafterElman(1990).Thehiddenlayerincludes\narecurrentconnectionaspartofitsinput. Thatis, theactivationvalueofthehiddenlayer\ndepends on the current input as well as the activation value of the hidden layer from the\nprevioustimestep.\nFig. 8.1 illustrates the structure of an RNN. As with ordinary feedforward net-\nworks, an input vector representing the current input, x, is multiplied by a weight\nt\nmatrixandthenpassedthroughanon-linearactivationfunctiontocomputetheval-\nues for a layer of hidden units. This hidden layer is then used to calculate a cor-\nrespondingoutput, y. Inadeparturefromourearlierwindow-basedapproach, se-\nt\nquences are processed by presenting one item at a time to the network. We’ll use\nsubscriptstorepresenttime,thusx willmeantheinputvectorxattimet. Thekey\nt\ndifferencefromafeedforwardnetworkliesintherecurrentlinkshowninthefigure\nwiththedashedline. Thislinkaugmentstheinputtothecomputationatthehidden\nlayerwiththevalueofthehiddenlayerfromtheprecedingpointintime.\nThe hidden layer from the previous time step provides a form of memory, or\ncontext, that encodes earlier processing and informs the decisions to be made at\nlater points in time. Critically, this approach does not impose a fixed-length limit\nonthispriorcontext;thecontextembodiedintheprevioushiddenlayercaninclude\ninformationextendingbacktothebeginningofthesequence.\nAddingthistemporaldimensionmakesRNNsappeartobemorecomplexthan\nnon-recurrent architectures. But in reality, they’re not all that different. Given an\ninputvectorandthevaluesforthehiddenlayerfromtheprevioustimestep, we’re\nstill performing the standard feedforward calculation introduced in Chapter 7. To\nsee this, consider Fig. 8.2 which clarifies the nature of the recurrence and how it\nfactorsintothecomputationatthehiddenlayer. Themostsignificantchangeliesin\nthenewsetofweights,U,thatconnectthehiddenlayerfromtheprevioustimestep\ntothecurrenthiddenlayer. Theseweightsdeterminehowthenetworkmakesuseof\npastcontextincalculatingtheoutputforthecurrentinput.Aswiththeotherweights\ninthenetwork,theseconnectionsaretrainedviabackpropagation.\n8.1.1 InferenceinRNNs\nForward inference (mapping a sequence of inputs to a sequence of outputs) in an\nRNNisnearlyidenticaltowhatwe’vealreadyseenwithfeedforwardnetworks. To\ncompute an output y for an input x, we need the activation value for the hidden\nt t\nlayerh. Tocalculatethis,wemultiplytheinputx withtheweightmatrixW,and\nt t\nthe hidden layer from the previous time step h with the weight matrix U. We\nt 1\nadd these values together and pass them through−a suitable activation function, g,\nto arrive at the activation value for the current hidden layer, h. Once we have the\nt 8.1 • RECURRENTNEURALNETWORKS 3\ny\nt\nV\nh\nt\n+\nU W\nh t-1 x t\nFigure8.2 Simplerecurrentneuralnetworkillustratedasafeedforwardnetwork.Thehid-\ndenlayerh\nt 1\nfromthepriortimestepismultipliedbyweightmatrixUandthenaddedto\nthefeedforw−ardcomponentfromthecurrenttimestep.\nvaluesforthehiddenlayer,weproceedwiththeusualcomputationtogeneratethe\noutputvector.\nh t = g(Uh t 1+Wx t) (8.1)\n−\ny t = f(Vh t) (8.2)\nLet’s refer to the input, hidden and output layer dimensions as d , d , and d\nin h out\nrespectively.Giventhis,ourthreeparametermatricesare:W Rdh×din,U Rdh×dh,\nandV Rdout ×dh. ∈ ∈\n∈\nWe compute y via a softmax computation that gives a probability distribution\nt\noverthepossibleoutputclasses.\ny t = softmax(Vh t) (8.3)\nThe fact that the computation at timet requires the value of the hidden layer from\ntimet 1mandatesanincrementalinferencealgorithmthatproceedsfromthestart\n−\nofthesequencetotheendasillustratedinFig.8.3. Thesequentialnatureofsimple\nrecurrentnetworkscanalsobeseenbyunrollingthenetworkintimeasisshownin\nFig. 8.4. In this figure, the various layers of units are copied for each time step to\nillustratethattheywillhavedifferingvaluesovertime. However,thevariousweight\nmatricesaresharedacrosstime.\nfunctionFORWARDRNN(x,network)returnsoutputsequencey\nh 0\n0\n←\nfori 1toLENGTH(x) do\n←\nh i g(Uh i 1 + Wx i)\ny i← f(Vh i−)\n←\nreturny\nFigure8.3 Forwardinferenceinasimplerecurrentnetwork.ThematricesU,VandWare\nsharedacrosstime,whilenewvaluesforhandyarecalculatedwitheachtimestep.\n8.1.2 Training\nAs with feedforward networks, we’ll use a training set, a loss function, and back-\npropagation to obtain the gradients needed to adjust the weights in these recurrent"
    },
    "10": {
        "chapter": "8a",
        "sections": "8.5",
        "topic": "Long Short-Term Memory (LSTMs)",
        "original_category": "CS",
        "original_text": "6 CHAPTER8 • RNNSANDLSTMS\n^\na) y t b)\n^\ny\nt\nU\nV\nh\nt\nh t-1 U h t-1 U h t\nW\nW W W\ne e e e e e\nt-2 t-1 t t-2 t-1 t\nFigure8.5 Simplified sketch of two LM architectures moving through a text, showing a\nschematiccontextofthreetokens:(a)afeedforwardneurallanguagemodelwhichhasafixed\ncontextinputtotheweightmatrixW,(b)anRNNlanguagemodel,inwhichthehiddenstate\nh t 1summarizesthepriorcontext.\n−\nis,attimet:\ne t = Ex t (8.4)\nh t = g(Uh t 1+We t) (8.5)\n−\nˆy t = softmax(Vh t) (8.6)\nWhenwedolanguagemodelingwithRNNs(andwe’llseethisagaininChapter9\nwith transformers), it’s convenient to make the assumption that the embedding di-\nmension d and the hidden dimension d are the same. So we’ll just call both of\ne h\nthesethemodeldimensiond. SotheembeddingmatrixEisofshape[d V ],and\n×| |\nx isaone-hotvectorofshape[V 1]. Theproducte isthusofshape[d 1]. W\nt t\n| |× ×\nand U are of shape [d d], so h is also of shape [d 1]. V is of shape [V d],\nt\n× × | |×\nso the result of Vh is a vector of shape [V 1]. This vector can be thought of as\n| |×\nasetofscoresoverthevocabularygiventheevidenceprovidedinh. Passingthese\nscoresthroughthesoftmaxnormalizesthescoresintoaprobabilitydistribution.The\nprobabilitythataparticularwordkinthevocabularyisthenextwordisrepresented\nbyˆy[k],thekthcomponentofˆy:\nt t\nP(w t+1=kw 1,...,w t) = ˆy t[k] (8.7)\n|\nTheprobabilityofanentiresequenceisjusttheproductoftheprobabilitiesofeach\niteminthesequence,wherewe’lluseˆy[w]tomeantheprobabilityofthetrueword\ni i\nw attimestepi.\ni\nn\nP(w 1:n) = P(w i w 1:i 1) (8.8)\n| −\ni=1\n(cid:89)\nn\n= ˆy i[w i] (8.9)\ni=1\n(cid:89)\n8.2.2 TraininganRNNlanguagemodel\nself-supervision To train an RNN as a language model, we use the same self-supervision (or self-\ntraining) algorithm we saw in Section ??: we take a corpus of text as training 8.5 • THELSTM 13\nateachpointintime. Hereweuseeitherthesemicolon”;”ortheequivalentsymbol\ntomeanvectorconcatenation:\n⊕\nh = [hf ; hb]\nt t t\n= hf hb (8.18)\nt t\n⊕\nFig. 8.11 illustrates such a bidirectional network that concatenates the outputs of\nthe forward and backward pass. Other simple ways to combine the forward and\nbackward contexts include element-wise addition or multiplication. The output at\neachstepintimethuscapturesinformationtotheleftandtotherightofthecurrent\ninput. Insequencelabelingapplications,theseconcatenatedoutputscanserveasthe\nbasisforalocallabelingdecision.\ny y y y\n1 2 3 n\nconcatenated\noutputs\nRNN 2\nRNN 1\nx 1 x 2 x 3 x n\nFigure8.11 AbidirectionalRNN.Separatemodelsaretrainedintheforwardandbackward\ndirections, with the output of each model at each time point concatenated to represent the\nbidirectionalstateatthattimepoint.\nBidirectionalRNNshavealsoproventobequiteeffectiveforsequenceclassifi-\ncation. RecallfromFig.8.8thatforsequenceclassificationweusedthefinalhidden\nstate of the RNN as the input to a subsequent feedforward classifier. A difficulty\nwith this approach is that the final state naturally reflects more information about\nthe end of the sentence than its beginning. Bidirectional RNNs provide a simple\nsolutiontothisproblem;asshowninFig.8.12,wesimplycombinethefinalhidden\nstates from the forward and backward passes (for example by concatenation) and\nusethatasinputforfollow-onprocessing.\n8.5 The LSTM\nInpractice,itisquitedifficulttotrainRNNsfortasksthatrequireanetworktomake\nuseofinformationdistantfromthecurrentpointofprocessing. Despitehavingac-\ncesstotheentireprecedingsequence,theinformationencodedinhiddenstatestends\nto be fairly local, more relevant to the most recent parts of the input sequence and\nrecentdecisions. Yetdistantinformationiscriticaltomanylanguageapplications.\nConsiderthefollowingexampleinthecontextoflanguagemodeling. 8.5 • THELSTM 15\ncision making. The key to solving both problems is to learn how to manage this\ncontextratherthanhard-codingastrategyintothearchitecture. LSTMsaccomplish\nthis by first adding an explicit context layer to the architecture (in addition to the\nusual recurrent hidden layer), and through the use of specialized neural units that\nmake use of gates to control the flow of information into and out of the units that\ncomprisethenetworklayers. Thesegatesareimplementedthroughtheuseofaddi-\ntionalweightsthatoperatesequentiallyontheinput,andprevioushiddenlayer,and\npreviouscontextlayers.\nThegatesinanLSTMshareacommondesignpattern;eachconsistsofafeed-\nforward layer, followed by a sigmoid activation function, followed by a pointwise\nmultiplicationwiththelayerbeinggated.Thechoiceofthesigmoidastheactivation\nfunctionarisesfromitstendencytopushitsoutputstoeither0or1. Combiningthis\nwithapointwisemultiplicationhasaneffectsimilartothatofabinarymask. Values\ninthelayerbeinggatedthatalignwithvaluesnear1inthemaskarepassedthrough\nnearlyunchanged;valuescorrespondingtolowervaluesareessentiallyerased.\nforgetgate The first gate we’ll consider is the forget gate. The purpose of this gate is\nto delete information from the context that is no longer needed. The forget gate\ncomputes a weighted sum of the previous state’s hidden layer and the current in-\nput and passes that through a sigmoid. This mask is then multiplied element-wise\nby the context vector to remove the information from context that is no longer re-\nquired. Element-wisemultiplicationoftwovectors(representedbytheoperator ,\n(cid:12)\nandsometimescalledtheHadamardproduct)isthevectorofthesamedimension\nasthetwoinputvectors,whereeachelementiistheproductofelementiinthetwo\ninputvectors:\nf t = σ(U fh t 1+W fx t) (8.20)\n−\nk t = c t 1 f t (8.21)\n− (cid:12)\nThenexttaskistocomputetheactualinformationweneedtoextractfromtheprevi-\noushiddenstateandcurrentinputs—thesamebasiccomputationwe’vebeenusing\nforallourrecurrentnetworks.\ng t = tanh(U gh t 1+W gx t) (8.22)\n−\naddgate Next,wegeneratethemaskfortheaddgatetoselecttheinformationtoaddtothe\ncurrentcontext.\ni t = σ(U ih t 1+W ix t) (8.23)\n−\nj t = g t i t (8.24)\n(cid:12)\nNext,weaddthistothemodifiedcontextvectortogetournewcontextvector.\nc t =j t+k t (8.25)\noutputgate Thefinalgatewe’lluseistheoutputgatewhichisusedtodecidewhatinforma-\ntionisrequiredforthecurrenthiddenstate(asopposedtowhatinformationneeds\ntobepreservedforfuturedecisions).\no t = σ(U oh t 1+W ox t) (8.26)\n−\nh t = o t tanh(c t) (8.27)\n(cid:12)\nFig. 8.13 illustrates the complete computation for a single LSTM unit. Given the 16 CHAPTER8 • RNNSANDLSTMS\n+\nc t-1\nct c t\nh t-1 ht-1\ntanh\nht h t\nx t xt\n+ σ\ntanh\nσ\nσ\n+\n+\n+\nf\ng\ni\no\n⦿\n⦿ ⦿\nct-1\nLSTM\nFigure8.13 AsingleLSTMunitdisplayedasacomputationgraph. Theinputstoeachunitconsistsofthe\ncurrentinput,x,theprevioushiddenstate,h t 1,andthepreviouscontext,c t 1. Theoutputsareanewhidden\nstate,ht andanupdatedcontext,ct. − −\nappropriate weights for the various gates, an LSTM accepts as input the context\nlayer, and hidden layer from the previous time step, along with the current input\nvector. Itthengeneratesupdatedcontextandhiddenvectorsasoutput.\nItisthehiddenstate,h,thatprovidestheoutputfortheLSTMateachtimestep.\nt\nThisoutputcanbeusedastheinputtosubsequentlayersinastackedRNN,oratthe\nfinallayerofanetworkh canbeusedtoprovidethefinaloutputoftheLSTM.\nt\n8.5.1 GatedUnits,LayersandNetworks\nTheneuralunitsusedinLSTMsareobviouslymuchmorecomplexthanthoseused\ninbasicfeedforwardnetworks. Fortunately,thiscomplexityisencapsulatedwithin\nthebasicprocessingunits, allowingustomaintainmodularityandtoeasilyexper-\niment with different architectures. To see this, consider Fig. 8.14 which illustrates\ntheinputsandoutputsassociatedwitheachkindofunit.\nAtthefarleft,(a)isthebasicfeedforwardunitwhereasinglesetofweightsand\nasingleactivationfunctiondetermineitsoutput,andwhenarrangedinalayerthere\nare no connections among the units in the layer. Next, (b) represents the unit in a\nsimplerecurrentnetwork. Nowtherearetwoinputsandanadditionalsetofweights\ntogowithit. However,thereisstillasingleactivationfunctionandoutput.\nThe increased complexity of the LSTM units is encapsulated within the unit\nitself.TheonlyadditionalexternalcomplexityfortheLSTMoverthebasicrecurrent\nunit(b)isthepresenceoftheadditionalcontextvectorasaninputandoutput.\nThismodularityiskeytothepowerandwidespreadapplicabilityofLSTMunits.\nLSTMunits(orothervarieties,likeGRUs)canbesubstitutedintoanyofthenetwork\narchitectures described in Section 8.4. And, as with simple RNNs, multi-layered\nnetworksmakinguseofgatedunitscanbeunrolledintodeepfeedforwardnetworks\nandtrainedintheusualfashionwithbackpropagation.Inpractice,therefore,LSTMs\nratherthanRNNshavebecomethestandardunitforanymodernsystemthatmakes\nuseofrecurrentnetworks."
    },
    "11": {
        "chapter": "4",
        "sections": "4.1, 4.2",
        "topic": "Naive Bayes Classifiers",
        "original_category": "CL",
        "original_text": "2 CHAPTER4 • NAIVEBAYES,TEXTCLASSIFICATION,ANDSENTIMENT\nFinally, one of the oldest tasks in text classification is assigning a library sub-\nject category or topic label to a text. Deciding whether a research paper concerns\nepidemiologyorinstead,perhaps,embryology,isanimportantcomponentofinfor-\nmationretrieval.Varioussetsofsubjectcategoriesexist,suchastheMeSH(Medical\nSubjectHeadings) thesaurus. Infact,aswewillsee,subjectcategoryclassification\nisthetaskforwhichthenaiveBayesalgorithmwasinventedin1961Maron(1961).\nClassification is essential for tasks below the level of the document as well.\nWe’vealreadyseenperioddisambiguation(decidingifaperiodistheendofasen-\ntence or part of a word), and word tokenization (deciding if a character should be\na word boundary). Even language modeling can be viewed as classification: each\nwordcanbethoughtofasaclass,andsopredictingthenextwordisclassifyingthe\ncontext-so-farintoaclassforeachnextword. Apart-of-speechtagger(Chapter17)\nclassifieseachoccurrenceofawordinasentenceas,e.g.,anounoraverb.\nThe goal of classification is to take a single observation, extract some useful\nfeatures, and thereby classify the observation into one of a set of discrete classes.\nOne method for classifying text is to use rules handwritten by humans. Handwrit-\ntenrule-basedclassifierscanbecomponentsofstate-of-the-artsystemsinlanguage\nprocessing. Butrulescanbefragile,assituationsordatachangeovertime,andfor\nsometaskshumansaren’tnecessarilygoodatcomingupwiththerules.\nThe most common way of doing text classification in language processing is\nsupervised\nmachine insteadviasupervisedmachinelearning,thesubjectofthischapter. Insupervised\nlearning\nlearning,wehaveadatasetofinputobservations,eachassociatedwithsomecorrect\noutput (a ‘supervision signal’). The goal of the algorithm is to learn how to map\nfromanewobservationtoacorrectoutput.\nFormally, the task of supervised classification is to take an input x and a fixed\nset of output classes Y ={y ,y ,...,y } and return a predicted class y∈Y. For\n1 2 M\ntext classification, we’ll sometimes talk about c (for “class”) instead of y as our\noutput variable, and d (for “document”) instead of x as our input variable. In the\nsupervisedsituationwehaveatrainingsetofNdocumentsthathaveeachbeenhand-\nlabeledwithaclass: {(d ,c ),....,(d ,c )}. Ourgoalistolearnaclassifierthatis\n1 1 N N\ncapable of mapping from a new document d to its correct class c∈C, whereC is\nsomesetofusefuldocumentclasses.Aprobabilisticclassifieradditionallywilltell\nus the probability of the observation being in the class. This full distribution over\nthe classes can be useful information for downstream decisions; avoiding making\ndiscretedecisionsearlyoncanbeusefulwhencombiningsystems.\nMany kinds of machine learning algorithms are used to build classifiers. This\nchapter introduces naive Bayes; the following one introduces logistic regression.\nTheseexemplifytwowaysofdoingclassification. Generativeclassifierslikenaive\nBayes build a model of how a class could generate some input data. Given an ob-\nservation, theyreturntheclassmostlikelytohavegeneratedtheobservation. Dis-\ncriminativeclassifierslikelogisticregressioninsteadlearnwhatfeaturesfromthe\ninputaremostusefultodiscriminatebetweenthedifferentpossibleclasses. While\ndiscriminative systems are often more accurate and hence more commonly used,\ngenerativeclassifiersstillhavearole.\n4.1 Naive Bayes Classifiers\nnaiveBayes In this section we introduce the multinomial naive Bayes classifier, so called be-\nclassifier\ncause it is a Bayesian classifier that makes a simplifying (naive) assumption about 4.1 • NAIVEBAYESCLASSIFIERS 3\nhowthefeaturesinteract.\nTheintuitionoftheclassifierisshowninFig.4.1. Werepresentatextdocument\nbagofwords as if it were a bag of words, that is, an unordered set of words with their position\nignored,keepingonlytheirfrequencyinthedocument. Intheexampleinthefigure,\ninsteadofrepresentingthewordorderinallthephraseslike“Ilovethismovie”and\n“I would recommend it”, we simply note that the word I occurred 5 times in the\nentireexcerpt,thewordit6times,thewordslove,recommend,andmovieonce,and\nsoon.\nit 6\nI 5\nI love this movie! It's sweet, the 4\nbut with satirical humor. The fairy always lovetoit to 3\ndialogue is great and the it whimsical it I and 3\nadventure scenes are fun... and seen are anyone seen 2\nfriend\nIt manages to be whimsical happy dialogue yet 1\nand romantic while laughing adventure recommend would 1\nat the conventions of the whosweet of msa ot vir ieical it whimsical 1\nfairy tale genre. I would it I butto romantic I times 1\nrecommend it to just about several yet sweet 1\nhumor\nanyone. I've seen it several again it the satirical 1\ntimes, and I'm always happy th toe sces ne ee sn I would adventure 1\nto see it again whenever I the themanages genre 1\nfun times\nhave a friend who hasn't I and and fairy 1\nabout\nseen it yet! whenever while humor 1\nhave\nconventions have 1\nwith great 1\n… …\nFigure4.1 IntuitionofthemultinomialnaiveBayesclassifierappliedtoamoviereview. Thepositionofthe\nwordsisignored(thebag-of-wordsassumption)andwemakeuseofthefrequencyofeachword.\nNaiveBayesisaprobabilisticclassifier, meaningthatforadocumentd, outof\nallclassesc∈C theclassifierreturnstheclasscˆwhichhasthemaximumposterior\nˆ probabilitygiventhedocument. InEq.4.1weusethehatnotationˆ tomean“our\nargmax estimateofthecorrectclass”,andweuseargmaxtomeananoperationthatselects\nthe argument (in this case the class c) that maximizes a function (in this case the\nprobabilityP(c|d).\ncˆ = argmaxP(c|d) (4.1)\nc∈C\nBayesian This idea of Bayesian inference has been known since the work of Bayes (1763),\ninference\nand was first applied to text classification by Mosteller and Wallace (1964). The\nintuition of Bayesian classification is to use Bayes’ rule to transform Eq. 4.1 into\nother probabilities that have some useful properties. Bayes’ rule is presented in\nEq. 4.2; it gives us a way to break down any conditional probability P(x|y) into\nthreeotherprobabilities:\nP(y|x)P(x)\nP(x|y)= (4.2)\nP(y) 4.1 • NAIVEBAYESCLASSIFIERS 3\nhowthefeaturesinteract.\nTheintuitionoftheclassifierisshowninFig.4.1. Werepresentatextdocument\nbagofwords as if it were a bag of words, that is, an unordered set of words with their position\nignored,keepingonlytheirfrequencyinthedocument. Intheexampleinthefigure,\ninsteadofrepresentingthewordorderinallthephraseslike“Ilovethismovie”and\n“I would recommend it”, we simply note that the word I occurred 5 times in the\nentireexcerpt,thewordit6times,thewordslove,recommend,andmovieonce,and\nsoon.\nit 6\nI 5\nI love this movie! It's sweet, the 4\nbut with satirical humor. The fairy always lovetoit to 3\ndialogue is great and the it whimsical it I and 3\nadventure scenes are fun... and seen are anyone seen 2\nfriend\nIt manages to be whimsical happy dialogue yet 1\nand romantic while laughing adventure recommend would 1\nat the conventions of the whosweet of msa ot vir ieical it whimsical 1\nfairy tale genre. I would it I butto romantic I times 1\nrecommend it to just about several yet sweet 1\nhumor\nanyone. I've seen it several again it the satirical 1\ntimes, and I'm always happy th toe sces ne ee sn I would adventure 1\nto see it again whenever I the themanages genre 1\nfun times\nhave a friend who hasn't I and and fairy 1\nabout\nseen it yet! whenever while humor 1\nhave\nconventions have 1\nwith great 1\n… …\nFigure4.1 IntuitionofthemultinomialnaiveBayesclassifierappliedtoamoviereview. Thepositionofthe\nwordsisignored(thebag-of-wordsassumption)andwemakeuseofthefrequencyofeachword.\nNaiveBayesisaprobabilisticclassifier, meaningthatforadocumentd, outof\nallclassesc∈C theclassifierreturnstheclasscˆwhichhasthemaximumposterior\nˆ probabilitygiventhedocument. InEq.4.1weusethehatnotationˆ tomean“our\nargmax estimateofthecorrectclass”,andweuseargmaxtomeananoperationthatselects\nthe argument (in this case the class c) that maximizes a function (in this case the\nprobabilityP(c|d).\ncˆ = argmaxP(c|d) (4.1)\nc∈C\nBayesian This idea of Bayesian inference has been known since the work of Bayes (1763),\ninference\nand was first applied to text classification by Mosteller and Wallace (1964). The\nintuition of Bayesian classification is to use Bayes’ rule to transform Eq. 4.1 into\nother probabilities that have some useful properties. Bayes’ rule is presented in\nEq. 4.2; it gives us a way to break down any conditional probability P(x|y) into\nthreeotherprobabilities:\nP(y|x)P(x)\nP(x|y)= (4.2)\nP(y) 4.2 • TRAININGTHENAIVEBAYESCLASSIFIER 5\nToapplythenaiveBayesclassifiertotext,wewilluseeachwordinthedocuments\nasafeature,assuggestedabove,andweconsidereachofthewordsinthedocument\nbywalkinganindexthrougheverywordpositioninthedocument:\npositions ← allwordpositionsintestdocument\n(cid:89)\nc NB = argmaxP(c) P(w i|c) (4.9)\nc∈C\ni∈positions\nNaiveBayescalculations, likecalculationsforlanguagemodeling, aredoneinlog\nspace, to avoid underflow and increase speed. Thus Eq. 4.9 is generally instead\nexpressed1as\n(cid:88)\nc NB = argmaxlogP(c)+ logP(w i|c) (4.10)\nc∈C\ni∈positions\nByconsideringfeaturesinlogspace,Eq.4.10computesthepredictedclassasalin-\nearfunctionofinputfeatures. Classifiersthatusealinearcombinationoftheinputs\nto make a classification decision —like naive Bayes and also logistic regression—\nlinear arecalledlinearclassifiers.\nclassifiers\n4.2 Training the Naive Bayes Classifier\nHowcanwelearntheprobabilitiesP(c)andP(f|c)? Let’sfirstconsiderthemaxi-\ni\nmumlikelihoodestimate. We’llsimplyusethefrequenciesinthedata. Fortheclass\npriorP(c)weaskwhatpercentageofthedocumentsinourtrainingsetareineach\nclass c. Let N be the number of documents in our training data with class c and\nc\nN bethetotalnumberofdocuments. Then:\ndoc\nN\nPˆ(c)= c (4.11)\nN\ndoc\nTolearntheprobabilityP(f|c),we’llassumeafeatureisjusttheexistenceofaword\ni\nin the document’s bag of words, and so we’ll want P(w|c), which we compute as\ni\nthefractionoftimesthewordw appearsamongallwordsinalldocumentsoftopic\ni\nc. Wefirstconcatenatealldocumentswithcategorycintoonebig“categoryc”text.\nThenweusethefrequencyofw inthisconcatenateddocumenttogiveamaximum\ni\nlikelihoodestimateoftheprobability:\ncount(w,c)\nPˆ(w i|c) = (cid:80) i (4.12)\ncount(w,c)\nw∈V\nHerethevocabularyVconsistsoftheunionofallthewordtypesinallclasses,not\njustthewordsinoneclassc.\nThere is a problem, however, with maximum likelihood training. Imagine we\naretryingtoestimatethelikelihoodoftheword“fantastic”givenclasspositive,but\nsupposetherearenotrainingdocumentsthatbothcontaintheword“fantastic”and\nare classified as positive. Perhaps the word “fantastic” happens to occur (sarcasti-\ncally?) intheclass negative. Insuchacasetheprobabilityforthisfeaturewillbe\nzero:\ncount(“fantastic”,positive)\nPˆ(“fantastic”|positive) = (cid:80) =0 (4.13)\ncount(w,positive)\nw∈V\n1 Inpracticethroughoutthisbook,we’lluselogtomeannaturallog(ln)whenthebaseisnotspecified. EXERCISES 21\n2014,Droretal.2017).\nFeatureselectionisamethodofremovingfeaturesthatareunlikelytogeneralize\nwell.Featuresaregenerallyrankedbyhowinformativetheyareabouttheclassifica-\ninformation tiondecision. Averycommonmetric,informationgain,tellsushowmanybitsof\ngain\ninformationthepresenceofthewordgivesusforguessingtheclass. Otherfeature\nselection metrics include χ2, pointwise mutual information, and GINI index; see\nYangandPedersen(1997)foracomparisonandGuyonandElisseeff(2003)foran\nintroductiontofeatureselection.\nExercises\n4.1 Assume the following likelihoods for each word being part of a positive or\nnegativemoviereview,andequalpriorprobabilitiesforeachclass.\npos neg\nI 0.09 0.16\nalways 0.07 0.06\nlike 0.29 0.06\nforeign 0.04 0.15\nfilms 0.08 0.11\nWhat class will Naive bayes assign to the sentence “I always like foreign\nfilms.”?\n4.2 Given the following short movie reviews, each labeled with a genre, either\ncomedyoraction:\n1. fun,couple,love,love comedy\n2. fast,furious,shoot action\n3. couple,fly,fast,fun,fun comedy\n4. furious,shoot,shoot,fun action\n5. fly,fast,shoot,love action\nandanewdocumentD:\nfast,couple,shoot,fly\ncomputethemostlikelyclassforD.AssumeanaiveBayesclassifieranduse\nadd-1smoothingforthelikelihoods.\n4.3 Traintwomodels, multinomialnaiveBayesandbinarizednaiveBayes, both\nwith add-1 smoothing, on the following document counts for key sentiment\nwords,withpositiveornegativeclassassignedasnoted.\ndoc “good” “poor” “great” (class)\nd1. 3 0 3 pos\nd2. 0 1 2 pos\nd3. 1 3 0 neg\nd4. 1 5 2 neg\nd5. 0 2 0 neg\nUsebothnaiveBayesmodelstoassignaclass(posorneg)tothissentence:\nAgood,goodplotandgreatcharacters,butpooracting.\nRecallfrompage6thatwithnaiveBayestextclassification,wesimplyignore\n(throwout)anywordthatneveroccurredinthetrainingdocument. (Wedon’t\nthrowoutwordsthatappearinsomeclassesbutnotothers; that’swhatadd-\nonesmoothingisfor.) Dothetwomodelsagreeordisagree? EXERCISES 21\n2014,Droretal.2017).\nFeatureselectionisamethodofremovingfeaturesthatareunlikelytogeneralize\nwell.Featuresaregenerallyrankedbyhowinformativetheyareabouttheclassifica-\ninformation tiondecision. Averycommonmetric,informationgain,tellsushowmanybitsof\ngain\ninformationthepresenceofthewordgivesusforguessingtheclass. Otherfeature\nselection metrics include χ2, pointwise mutual information, and GINI index; see\nYangandPedersen(1997)foracomparisonandGuyonandElisseeff(2003)foran\nintroductiontofeatureselection.\nExercises\n4.1 Assume the following likelihoods for each word being part of a positive or\nnegativemoviereview,andequalpriorprobabilitiesforeachclass.\npos neg\nI 0.09 0.16\nalways 0.07 0.06\nlike 0.29 0.06\nforeign 0.04 0.15\nfilms 0.08 0.11\nWhat class will Naive bayes assign to the sentence “I always like foreign\nfilms.”?\n4.2 Given the following short movie reviews, each labeled with a genre, either\ncomedyoraction:\n1. fun,couple,love,love comedy\n2. fast,furious,shoot action\n3. couple,fly,fast,fun,fun comedy\n4. furious,shoot,shoot,fun action\n5. fly,fast,shoot,love action\nandanewdocumentD:\nfast,couple,shoot,fly\ncomputethemostlikelyclassforD.AssumeanaiveBayesclassifieranduse\nadd-1smoothingforthelikelihoods.\n4.3 Traintwomodels, multinomialnaiveBayesandbinarizednaiveBayes, both\nwith add-1 smoothing, on the following document counts for key sentiment\nwords,withpositiveornegativeclassassignedasnoted.\ndoc “good” “poor” “great” (class)\nd1. 3 0 3 pos\nd2. 0 1 2 pos\nd3. 1 3 0 neg\nd4. 1 5 2 neg\nd5. 0 2 0 neg\nUsebothnaiveBayesmodelstoassignaclass(posorneg)tothissentence:\nAgood,goodplotandgreatcharacters,butpooracting.\nRecallfrompage6thatwithnaiveBayestextclassification,wesimplyignore\n(throwout)anywordthatneveroccurredinthetrainingdocument. (Wedon’t\nthrowoutwordsthatappearinsomeclassesbutnotothers; that’swhatadd-\nonesmoothingisfor.) Dothetwomodelsagreeordisagree? 22 Chapter4 • NaiveBayes,TextClassification,andSentiment\nAggarwal,C.C.andC.Zhai.2012. Asurveyoftextclassi- Heckerman,D.,E.Horvitz,M.Sahami,andS.T.Dumais.\nficationalgorithms. InC.C.AggarwalandC.Zhai,eds, 1998.Abayesianapproachtofilteringjunke-mail.AAAI-\nMiningtextdata,163–222.Springer. 98WorkshoponLearningforTextCategorization.\nBayes,T.1763. AnEssayTowardSolvingaProbleminthe Hu,M.andB.Liu.2004.Miningandsummarizingcustomer\nDoctrineofChances,volume53.ReprintedinFacsimiles reviews.KDD.\nofTwoPapersbyBayes,HafnerPublishing,1963. Hutchinson, B., V. Prabhakaran, E. Denton, K. Webster,\nBerg-Kirkpatrick, T., D.Burkett, andD.Klein.2012. An Y. Zhong, and S. Denuyl. 2020. Social biases in NLP\nempiricalinvestigationofstatisticalsignificanceinNLP. modelsasbarriersforpersonswithdisabilities.ACL.\nEMNLP. Jaech,A.,G.Mulcaire,S.Hathi,M.Ostendorf,andN.A.\nBisani,M.andH.Ney.2004. Bootstrapestimatesforconfi- Smith.2016.Hierarchicalcharacter-wordmodelsforlan-\ndenceintervalsinASRperformanceevaluation.ICASSP. guageidentification. ACLWorkshoponNLPforSocial\nMedia.\nBishop,C.M.2006.Patternrecognitionandmachinelearn-\ning.Springer. Jauhiainen, T., M. Lui, M. Zampieri, T. Baldwin, and\nK.Linde´n.2019. Automaticlanguageidentificationin\nBlodgett,S.L.,S.Barocas,H.Daume´III,andH.Wallach.\ntexts:Asurvey.JAIR,65(1):675–682.\n2020. Language(technology)ispower:Acriticalsurvey\nof“bias”inNLP.ACL. Jurgens,D.,Y.Tsvetkov,andD.Jurafsky.2017. Incorpo-\nratingdialectalvariabilityforsociallyequitablelanguage\nBlodgett,S.L.,L.Green,andB.O’Connor.2016. Demo-\nidentification.ACL.\ngraphicdialectalvariationinsocialmedia: Acasestudy\nofAfrican-AmericanEnglish.EMNLP. Kiritchenko, S. and S. M. Mohammad. 2018. Examining\ngenderandracebiasintwohundredsentimentanalysis\nBorges,J.L.1964.Theanalyticallanguageofjohnwilkins.\nsystems.*SEM.\nIn Other inquisitions 1937–1952. University of Texas\nPress.Trans.RuthL.C.Simms. Liu,B.andL.Zhang.2012.Asurveyofopinionminingand\nsentimentanalysis. InC.C.AggarwalandC.Zhai,eds,\nCaliskan,A.,J.J.Bryson,andA.Narayanan.2017.Seman-\nMiningtextdata,415–464.Springer.\nticsderivedautomaticallyfromlanguagecorporacontain\nhuman-likebiases.Science,356(6334):183–186. Lui,M.andT.Baldwin.2011. Cross-domainfeatureselec-\ntionforlanguageidentification.IJCNLP.\nChinchor,N.,L.Hirschman,andD.L.Lewis.1993. Eval-\nuatingMessageUnderstandingsystems: Ananalysisof Lui,M.andT.Baldwin.2012.langid.py:Anoff-the-shelf\nthethirdMessageUnderstandingConference. Computa- languageidentificationtool.ACL.\ntionalLinguistics,19(3):409–449. Manning,C.D.,P.Raghavan,andH.Schu¨tze.2008. Intro-\nCrawford, K. 2017. The trouble with bias. Keynote at ductiontoInformationRetrieval.Cambridge.\nNeurIPS. Maron,M.E.1961. Automaticindexing: anexperimental\nDavidson,T.,D.Bhattacharya,andI.Weber.2019. Racial inquiry.JournaloftheACM,8(3):404–417.\nbias in hate speech and abusive language detection McCallum,A.andK.Nigam.1998. Acomparisonofevent\ndatasets.ThirdWorkshoponAbusiveLanguageOnline. modelsfornaivebayestextclassification.AAAI/ICML-98\nDiasOliva,T.,D.Antonialli,andA.Gomes.2021.Fighting WorkshoponLearningforTextCategorization.\nhatespeech,silencingdragqueens?artificialintelligence Metsis, V., I. Androutsopoulos, and G. Paliouras. 2006.\nin content moderation and risks to lgbtq voices online. Spam filtering with naive bayes-which naive bayes?\nSexuality&Culture,25:700–732. CEAS.\nDixon,L.,J.Li,J.Sorensen,N.Thain,andL.Vasserman. Minsky,M.1961. Stepstowardartificialintelligence. Pro-\n2018. Measuringandmitigatingunintendedbiasintext ceedingsoftheIRE,49(1):8–30.\nclassification.2018AAAI/ACMConferenceonAI,Ethics,\nMitchell,M.,S.Wu,A.Zaldivar,P.Barnes,L.Vasserman,\nandSociety.\nB.Hutchinson,E.Spitzer,I.D.Raji,andT.Gebru.2019.\nDror,R.,G.Baumer,M.Bogomolov,andR.Reichart.2017. Modelcardsformodelreporting.ACMFAccT.\nReplicability analysis for natural language processing:\nMosteller,F.andD.L.Wallace.1963. Inferenceinanau-\nTestingsignificancewithmultipledatasets.TACL,5:471–\nthorshipproblem:Acomparativestudyofdiscrimination\n–486.\nmethodsappliedtotheauthorshipofthedisputedfeder-\nDror, R., L. Peled-Cohen, S. Shlomov, and R. Reichart. alistpapers.JournaloftheAmericanStatisticalAssocia-\n2020. Statistical Significance Testing for Natural Lan- tion,58(302):275–309.\nguage Processing, volume 45 of Synthesis Lectures on\nMosteller,F.andD.L.Wallace.1964. InferenceandDis-\nHumanLanguageTechnologies.Morgan&Claypool.\nputedAuthorship:TheFederalist.Springer-Verlag.1984\nEfron,B.andR.J.Tibshirani.1993. Anintroductiontothe 2ndedition:AppliedBayesianandClassicalInference.\nbootstrap.CRCpress.\nMurphy,K.P.2012. Machinelearning:Aprobabilisticper-\nGillick,L.andS.J.Cox.1989.Somestatisticalissuesinthe spective.MITPress.\ncomparisonofspeechrecognitionalgorithms.ICASSP.\nNoreen,E.W.1989.ComputerIntensiveMethodsforTesting\nGuyon,I.andA.Elisseeff.2003.Anintroductiontovariable Hypothesis.Wiley.\nandfeatureselection.JMLR,3:1157–1182.\nPang,B.andL.Lee.2008. Opinionminingandsentiment\nHastie,T.,R.J.Tibshirani,andJ.H.Friedman.2001. The analysis.Foundationsandtrendsininformationretrieval,\nElementsofStatisticalLearning.Springer. 2(1-2):1–135."
    },
    "12": {
        "chapter": "6",
        "sections": "6.2",
        "topic": "Vector Semantics",
        "original_category": "CL",
        "original_text": "6.2 • VECTORSEMANTICS 5\nValence Arousal Dominance\ncourageous 8.05 5.5 7.38\nmusic 7.67 5.57 6.5\nheartbreak 2.45 5.65 3.58\ncub 6.71 3.95 4.24\nOsgood et al. (1957) noticed that in using these 3 numbers to represent the\nmeaning of a word, the model was representing each word as a point in a three-\ndimensional space, a vector whose three dimensions corresponded to the word’s\nratingonthethreescales. Thisrevolutionaryideathatwordmeaningcouldberep-\nresented as a point in space (e.g., that part of the meaning of heartbreak can be\nrepresentedasthepoint[2.45,5.65,3.58])wasthefirstexpressionofthevectorse-\nmanticsmodelsthatweintroducenext.\n6.2 Vector Semantics\nvector Vector semantics is the standard way to represent word meaning in NLP, helping\nsemantics\nusmodelmanyoftheaspectsofwordmeaningwesawintheprevioussection. The\nroots of the model lie in the 1950s when two big ideas converged: Osgood’s 1957\nidea mentioned above to use a point in three-dimensional space to represent the\nconnotationofaword,andtheproposalbylinguistslikeJoos(1950),Harris(1954),\nand Firth (1957) to define the meaning of a word by its distribution in language\nuse, meaning its neighboring words or grammatical environments. Their idea was\nthattwowordsthatoccurinverysimilardistributions(whoseneighboringwordsare\nsimilar)havesimilarmeanings.\nForexample,supposeyoudidn’tknowthemeaningofthewordongchoi(are-\ncentborrowingfromCantonese)butyouseeitinthefollowingcontexts:\n(6.1) Ongchoiisdelicioussauteedwithgarlic.\n(6.2) Ongchoiissuperboverrice.\n(6.3) ...ongchoileaveswithsaltysauces...\nAndsupposethatyouhadseenmanyofthesecontextwordsinothercontexts:\n(6.4) ...spinachsauteedwithgarlicoverrice...\n(6.5) ...chardstemsandleavesaredelicious...\n(6.6) ...collardgreensandothersaltyleafygreens\nThe fact that ongchoi occurs with words like rice and garlic and delicious and\nsalty,asdowordslikespinach,chard,andcollardgreensmightsuggestthatongchoi\nis a leafy green similar to these other leafy greens.1 We can do the same thing\ncomputationallybyjustcountingwordsinthecontextofongchoi.\nTheideaofvectorsemanticsistorepresentawordasapointinamultidimen-\nsional semantic space that is derived (in ways we’ll see) from the distributions of\nembeddings word neighbors. Vectors for representing words are called embeddings (although\nthe term is sometimes more strictly applied only to dense vectors like word2vec\n(Section 6.8), rather than sparse tf-idf or PPMI vectors (Section 6.3-Section 6.6)).\nTheword“embedding”derivesfromitsmathematicalsenseasamappingfromone\nspace or structure to another, although the meaning has shifted; see the end of the\nchapter.\n1 It’sinfactIpomoeaaquatica,arelativeofmorningglorysometimescalledwaterspinachinEnglish. 6.2 • VECTORSEMANTICS 5\nValence Arousal Dominance\ncourageous 8.05 5.5 7.38\nmusic 7.67 5.57 6.5\nheartbreak 2.45 5.65 3.58\ncub 6.71 3.95 4.24\nOsgood et al. (1957) noticed that in using these 3 numbers to represent the\nmeaning of a word, the model was representing each word as a point in a three-\ndimensional space, a vector whose three dimensions corresponded to the word’s\nratingonthethreescales. Thisrevolutionaryideathatwordmeaningcouldberep-\nresented as a point in space (e.g., that part of the meaning of heartbreak can be\nrepresentedasthepoint[2.45,5.65,3.58])wasthefirstexpressionofthevectorse-\nmanticsmodelsthatweintroducenext.\n6.2 Vector Semantics\nvector Vector semantics is the standard way to represent word meaning in NLP, helping\nsemantics\nusmodelmanyoftheaspectsofwordmeaningwesawintheprevioussection. The\nroots of the model lie in the 1950s when two big ideas converged: Osgood’s 1957\nidea mentioned above to use a point in three-dimensional space to represent the\nconnotationofaword,andtheproposalbylinguistslikeJoos(1950),Harris(1954),\nand Firth (1957) to define the meaning of a word by its distribution in language\nuse, meaning its neighboring words or grammatical environments. Their idea was\nthattwowordsthatoccurinverysimilardistributions(whoseneighboringwordsare\nsimilar)havesimilarmeanings.\nForexample,supposeyoudidn’tknowthemeaningofthewordongchoi(are-\ncentborrowingfromCantonese)butyouseeitinthefollowingcontexts:\n(6.1) Ongchoiisdelicioussauteedwithgarlic.\n(6.2) Ongchoiissuperboverrice.\n(6.3) ...ongchoileaveswithsaltysauces...\nAndsupposethatyouhadseenmanyofthesecontextwordsinothercontexts:\n(6.4) ...spinachsauteedwithgarlicoverrice...\n(6.5) ...chardstemsandleavesaredelicious...\n(6.6) ...collardgreensandothersaltyleafygreens\nThe fact that ongchoi occurs with words like rice and garlic and delicious and\nsalty,asdowordslikespinach,chard,andcollardgreensmightsuggestthatongchoi\nis a leafy green similar to these other leafy greens.1 We can do the same thing\ncomputationallybyjustcountingwordsinthecontextofongchoi.\nTheideaofvectorsemanticsistorepresentawordasapointinamultidimen-\nsional semantic space that is derived (in ways we’ll see) from the distributions of\nembeddings word neighbors. Vectors for representing words are called embeddings (although\nthe term is sometimes more strictly applied only to dense vectors like word2vec\n(Section 6.8), rather than sparse tf-idf or PPMI vectors (Section 6.3-Section 6.6)).\nTheword“embedding”derivesfromitsmathematicalsenseasamappingfromone\nspace or structure to another, although the meaning has shifted; see the end of the\nchapter.\n1 It’sinfactIpomoeaaquatica,arelativeofmorningglorysometimescalledwaterspinachinEnglish. 6.3 • WORDSANDVECTORS 7\naparticularword(definedbytherow)occursinaparticulardocument(definedby\nthecolumn). Thusfoolappeared58timesinTwelfthNight.\nAsYouLikeIt TwelfthNight JuliusCaesar HenryV\nbattle 1 0 7 13\ngood 114 80 62 89\nfool 36 58 1 4\nwit 20 15 2 3\nFigure6.2 Theterm-documentmatrixforfourwordsinfourShakespeareplays.Eachcell\ncontainsthenumberoftimesthe(row)wordoccursinthe(column)document.\nThe term-document matrix of Fig. 6.2 was first defined as part of the vector\nvectorspace space model of information retrieval (Salton, 1971). In this model, a document is\nmodel\nrepresentedasacountvector,acolumninFig.6.3.\nvector Toreviewsomebasiclinearalgebra, avectoris, atheart, justalistorarrayof\nnumbers. SoAsYouLikeItisrepresentedasthelist[1,114,36,20](thefirstcolumn\nvector in Fig.6.3) and Julius Caesar is representedas the list [7,62,1,2](the third\nvectorspace column vector). A vector space is a collection of vectors, and is characterized by\ndimension its dimension. Vectors in a 3-dimensional vector space have an element for each\ndimensionofthespace. Wewilllooselyrefertoavectorina4-dimensionalspace\nasa4-dimensionalvector,withoneelementalongeachdimension. Intheexample\ninFig.6.3,we’vechosentomakethedocumentvectorsofdimension4,justsothey\nfit on the page; in real term-document matrices, the document vectors would have\ndimensionality|V|,thevocabularysize.\nTheorderingofthenumbersinavectorspaceindicatesthedifferentdimensions\nonwhichdocumentsvary. Thefirstdimensionforboththesevectorscorrespondsto\nthe number of times the word battle occurs, and we can compare each dimension,\nnotingforexamplethatthevectorsforAsYouLikeItandTwelfthNighthavesimilar\nvalues(1and0,respectively)forthefirstdimension.\nAsYouLikeIt TwelfthNight JuliusCaesar HenryV\nbattle 1 0 7 13\ngood 114 80 62 89\nfool 36 58 1 4\nwit 20 15 2 3\nFigure6.3 Theterm-documentmatrixforfourwordsinfourShakespeareplays. Thered\nboxesshowthateachdocumentisrepresentedasacolumnvectoroflengthfour.\nWecanthinkofthevectorforadocumentasapointin|V|-dimensionalspace;\nthusthedocumentsinFig.6.3arepointsin4-dimensionalspace.Since4-dimensional\nspacesarehardtovisualize,Fig.6.4showsavisualizationintwodimensions;we’ve\narbitrarilychosenthedimensionscorrespondingtothewordsbattleandfool.\nTerm-document matrices were originally defined as a means of finding similar\ndocumentsforthetaskofdocumentinformationretrieval. Twodocumentsthatare\nsimilar will tend to have similar words, and if two documents have similar words\ntheir column vectors will tend to be similar. The vectors for the comedies As You\nLikeIt[1,114,36,20]andTwelfthNight[0,80,58,15]lookalotmorelikeeachother\n(more fools and wit than battles) than they look like Julius Caesar [7,62,1,2] or\nHenry V [13,89,4,3]. This is clear with the raw numbers; in the first dimension\n(battle)thecomedieshavelownumbersandtheothershavehighnumbers,andwe\ncan see it visually in Fig. 6.4; we’ll see very shortly how to quantify this intuition\nmoreformally. 6.3 • WORDSANDVECTORS 7\naparticularword(definedbytherow)occursinaparticulardocument(definedby\nthecolumn). Thusfoolappeared58timesinTwelfthNight.\nAsYouLikeIt TwelfthNight JuliusCaesar HenryV\nbattle 1 0 7 13\ngood 114 80 62 89\nfool 36 58 1 4\nwit 20 15 2 3\nFigure6.2 Theterm-documentmatrixforfourwordsinfourShakespeareplays.Eachcell\ncontainsthenumberoftimesthe(row)wordoccursinthe(column)document.\nThe term-document matrix of Fig. 6.2 was first defined as part of the vector\nvectorspace space model of information retrieval (Salton, 1971). In this model, a document is\nmodel\nrepresentedasacountvector,acolumninFig.6.3.\nvector Toreviewsomebasiclinearalgebra, avectoris, atheart, justalistorarrayof\nnumbers. SoAsYouLikeItisrepresentedasthelist[1,114,36,20](thefirstcolumn\nvector in Fig.6.3) and Julius Caesar is representedas the list [7,62,1,2](the third\nvectorspace column vector). A vector space is a collection of vectors, and is characterized by\ndimension its dimension. Vectors in a 3-dimensional vector space have an element for each\ndimensionofthespace. Wewilllooselyrefertoavectorina4-dimensionalspace\nasa4-dimensionalvector,withoneelementalongeachdimension. Intheexample\ninFig.6.3,we’vechosentomakethedocumentvectorsofdimension4,justsothey\nfit on the page; in real term-document matrices, the document vectors would have\ndimensionality|V|,thevocabularysize.\nTheorderingofthenumbersinavectorspaceindicatesthedifferentdimensions\nonwhichdocumentsvary. Thefirstdimensionforboththesevectorscorrespondsto\nthe number of times the word battle occurs, and we can compare each dimension,\nnotingforexamplethatthevectorsforAsYouLikeItandTwelfthNighthavesimilar\nvalues(1and0,respectively)forthefirstdimension.\nAsYouLikeIt TwelfthNight JuliusCaesar HenryV\nbattle 1 0 7 13\ngood 114 80 62 89\nfool 36 58 1 4\nwit 20 15 2 3\nFigure6.3 Theterm-documentmatrixforfourwordsinfourShakespeareplays. Thered\nboxesshowthateachdocumentisrepresentedasacolumnvectoroflengthfour.\nWecanthinkofthevectorforadocumentasapointin|V|-dimensionalspace;\nthusthedocumentsinFig.6.3arepointsin4-dimensionalspace.Since4-dimensional\nspacesarehardtovisualize,Fig.6.4showsavisualizationintwodimensions;we’ve\narbitrarilychosenthedimensionscorrespondingtothewordsbattleandfool.\nTerm-document matrices were originally defined as a means of finding similar\ndocumentsforthetaskofdocumentinformationretrieval. Twodocumentsthatare\nsimilar will tend to have similar words, and if two documents have similar words\ntheir column vectors will tend to be similar. The vectors for the comedies As You\nLikeIt[1,114,36,20]andTwelfthNight[0,80,58,15]lookalotmorelikeeachother\n(more fools and wit than battles) than they look like Julius Caesar [7,62,1,2] or\nHenry V [13,89,4,3]. This is clear with the raw numbers; in the first dimension\n(battle)thecomedieshavelownumbersandtheothershavehighnumbers,andwe\ncan see it visually in Fig. 6.4; we’ll see very shortly how to quantify this intuition\nmoreformally. 10 CHAPTER6 • VECTORSEMANTICSANDEMBEDDINGS\ninthetrainingcorpus; keepingwordsafteraboutthemostfrequent50,000orsois\ngenerallynothelpful). Sincemostofthesenumbersarezerothesearesparsevector\nrepresentations;thereareefficientalgorithmsforstoringandcomputingwithsparse\nmatrices.\nNowthatwehavesomeintuitions,let’smoveontoexaminethedetailsofcom-\nputingwordsimilarity. Afterwardswe’lldiscussmethodsforweightingcells.\n6.4 Cosine for measuring similarity\nTo measure similarity between two target words v and w, we need a metric that\ntakestwovectors(ofthesamedimensionality,eitherbothwithwordsasdimensions,\nhenceoflength|V|,orbothwithdocumentsasdimensions,oflength|D|)andgives\nameasureoftheirsimilarity.Byfarthemostcommonsimilaritymetricisthecosine\noftheanglebetweenthevectors.\nThecosine—likemostmeasuresforvectorsimilarityusedinNLP—isbasedon\ndotproduct thedotproductoperatorfromlinearalgebra,alsocalledtheinnerproduct:\ninnerproduct\nN\n(cid:88)\ndotproduct(v,w)=v·w= v iw i=v 1w 1+v 2w 2+...+v Nw N (6.7)\ni=1\nThedotproductactsasasimilaritymetricbecauseitwilltendtobehighjustwhen\nthetwovectorshavelargevaluesinthesamedimensions. Alternatively,vectorsthat\nhavezerosindifferentdimensions—orthogonalvectors—willhaveadotproductof\n0,representingtheirstrongdissimilarity.\nThis raw dot product, however, has a problem as a similarity metric: it favors\nvectorlength longvectors. Thevectorlengthisdefinedas\n(cid:118)\n(cid:117) N\n(cid:117)(cid:88)\n|v| = (cid:116) v2 (6.8)\ni\ni=1\nThedotproductishigherifavectorislonger,withhighervaluesineachdimension.\nMore frequent words have longer vectors, since they tend to co-occur with more\nwordsandhavehigherco-occurrencevalueswitheachofthem.Therawdotproduct\nthuswillbehigherforfrequentwords. Butthisisaproblem;we’dlikeasimilarity\nmetricthattellsushowsimilartwowordsareregardlessoftheirfrequency.\nWe modify the dot product to normalize for the vector length by dividing the\ndotproductbythelengthsofeachofthetwovectors. Thisnormalizeddotproduct\nturnsouttobethesameasthecosineoftheanglebetweenthetwovectors,following\nfromthedefinitionofthedotproductbetweentwovectorsaandb:\na·b = |a||b|cosθ\na·b\n= cosθ (6.9)\n|a||b|\ncosine Thecosinesimilaritymetricbetweentwovectorsvandwthuscanbecomputedas: 6.5 • TF-IDF:WEIGHINGTERMSINTHEVECTOR 13\nWeemphasizediscriminativewordslikeRomeoviatheinversedocumentfre-\nidf quencyoridftermweight(SparckJones,1972). Theidfisdefinedusingthefrac-\ntion N/df, where N is the total number of documents in the collection, and df is\nt t\nthenumberofdocumentsinwhichtermt occurs. Thefewerdocumentsinwhicha\ntermoccurs,thehigherthisweight. Thelowestweightof1isassignedtotermsthat\noccurinallthedocuments. It’susuallyclearwhatcountsasadocument: inShake-\nspearewewoulduseaplay; whenprocessingacollectionofencyclopediaarticles\nlikeWikipedia,thedocumentisaWikipediapage;inprocessingnewspaperarticles,\nthedocumentisasinglearticle. Occasionallyyourcorpusmightnothaveappropri-\natedocumentdivisionsandyoumightneedtobreakupthecorpusintodocuments\nyourselfforthepurposesofcomputingidf.\nBecause of the large number of documents in many collections, this measure\ntoo is usually squashed with a log function. The resulting definition for inverse\ndocumentfrequency(idf)isthus\n(cid:18) (cid:19)\nN\nidf t = log 10 df (6.13)\nt\nHere are some idf values for some words in the Shakespeare corpus, (along with\nthedocumentfrequencydfvaluesonwhichtheyarebased)rangingfromextremely\ninformativewordswhichoccurinonlyoneplaylikeRomeo,tothosethatoccurina\nfewlikesaladorFalstaff,tothosewhichareverycommonlikefoolorsocommon\nastobecompletelynon-discriminativesincetheyoccurinall37playslikegoodor\nsweet.3\nWord df idf\nRomeo 1 1.57\nsalad 2 1.27\nFalstaff 4 0.967\nforest 12 0.489\nbattle 21 0.246\nwit 34 0.037\nfool 36 0.012\ngood 37 0\nsweet 37 0\ntf-idf The tf-idf weighted value w t,d for word t in document d thus combines term\nfrequencytf (definedeitherbyEq.6.11orbyEq.6.12)withidffromEq.6.13:\nt,d\nw t,d =tf t,d×idf t (6.14)\nFig.6.9appliestf-idfweightingtotheShakespeareterm-documentmatrixinFig.6.2,\nusing the tf equation Eq. 6.12. Note that the tf-idf values for the dimension corre-\nspondingtothewordgoodhavenowallbecome0;sincethiswordappearsinevery\ndocument,thetf-idfweightingleadsittobeignored.Similarly,thewordfool,which\nappearsin36outofthe37plays,hasamuchlowerweight.\nThe tf-idf weighting is the way for weighting co-occurrence matrices in infor-\nmation retrieval, but also plays a role in many other aspects of natural language\nprocessing. It’salsoagreatbaseline,thesimplethingtotryfirst. We’lllookatother\nweightingslikePPMI(PositivePointwiseMutualInformation)inSection6.6.\n3 SweetwasoneofShakespeare’sfavoriteadjectives, afactprobablyrelatedtotheincreaseduseof\nsugarinEuropeanrecipesaroundtheturnofthe16thcentury(Jurafsky,2014,p.175). 14 CHAPTER6 • VECTORSEMANTICSANDEMBEDDINGS\nAsYouLikeIt TwelfthNight JuliusCaesar HenryV\nbattle 0.246 0 0.454 0.520\ngood 0 0 0 0\nfool 0.030 0.033 0.0012 0.0019\nwit 0.085 0.081 0.048 0.054\nFigure6.9 Aportionofthetf-idfweightedterm-documentmatrixforfourwordsinShake-\nspeare plays, showing a selection of 4 plays, using counts from Fig. 6.2. For example the\n0.085valueforwitinAsYouLikeItistheproductoftf=1+log (20)=2.301andidf=.037.\n10\nNotethattheidfweightinghaseliminatedtheimportanceoftheubiquitouswordgoodand\nvastlyreducedtheimpactofthealmost-ubiquitouswordfool.\n6.6 Pointwise Mutual Information (PMI)\nAnalternativeweightingfunctiontotf-idf,PPMI(positivepointwisemutualinfor-\nmation),isusedforterm-term-matrices,whenthevectordimensionscorrespondto\nwordsratherthandocuments.PPMIdrawsontheintuitionthatthebestwaytoweigh\ntheassociationbetweentwowordsistoaskhowmuchmorethetwowordsco-occur\ninourcorpusthanwewouldhaveaprioriexpectedthemtoappearbychance.\npointwise\nmutual\nPointwisemutualinformation(Fano,1961)4isoneofthemostimportantcon-\ninformation\nceptsinNLP.Itisameasureofhowoftentwoeventsxandyoccur,comparedwith\nwhatwewouldexpectiftheywereindependent:\nP(x,y)\nI(x,y)=log (6.16)\n2P(x)P(y)\nThepointwisemutualinformationbetweenatargetwordwandacontextword\nc(ChurchandHanks1989,ChurchandHanks1990)isthendefinedas:\nP(w,c)\nPMI(w,c)=log (6.17)\n2P(w)P(c)\nThe numerator tells us how often we observed the two words together (assuming\nwe compute probability by using the MLE). The denominator tells us how often\nwewouldexpectthetwowordstoco-occurassumingtheyeachoccurredindepen-\ndently; recall that the probability of two independent events both occurring is just\nthe product of the probabilities of the two events. Thus, the ratio gives us an esti-\nmateofhowmuchmorethetwowordsco-occurthanweexpectbychance. PMIis\nausefultoolwheneverweneedtofindwordsthatarestronglyassociated.\nPMI values range from negative to positive infinity. But negative PMI values\n(which imply things are co-occurring less often than we would expect by chance)\ntend to be unreliable unless our corpora are enormous. To distinguish whether\ntwowordswhoseindividualprobabilityiseach10−6 occurtogetherlessoftenthan\nchance, we would need to be certain that the probability of the two occurring to-\ngetherissignificantlylessthan10−12,andthiskindofgranularitywouldrequirean\nenormous corpus. Furthermore it’s not clear whether it’s even possible to evaluate\nsuch scores of ‘unrelatedness’ with human judgments. For this reason it is more\n4 PMIisbasedonthemutualinformationbetweentworandomvariablesXandY,definedas:\n(cid:88)(cid:88) P(x,y)\nI(X,Y)= P(x,y)log (6.15)\n2P(x)P(y)\nx y\nInaconfusionofterminology,Fanousedthephrasemutualinformationtorefertowhatwenowcall\npointwisemutualinformationandthephraseexpectationofthemutualinformationforwhatwenowcall\nmutualinformation"
    },
    "13": {
        "chapter": "6a",
        "sections": "6.4",
        "topic": "Cosine Similarity",
        "original_category": "CL",
        "original_text": "6.2 • VECTORSEMANTICS 5\nValence Arousal Dominance\ncourageous 8.05 5.5 7.38\nmusic 7.67 5.57 6.5\nheartbreak 2.45 5.65 3.58\ncub 6.71 3.95 4.24\nOsgood et al. (1957) noticed that in using these 3 numbers to represent the\nmeaning of a word, the model was representing each word as a point in a three-\ndimensional space, a vector whose three dimensions corresponded to the word’s\nratingonthethreescales. Thisrevolutionaryideathatwordmeaningcouldberep-\nresented as a point in space (e.g., that part of the meaning of heartbreak can be\nrepresentedasthepoint[2.45,5.65,3.58])wasthefirstexpressionofthevectorse-\nmanticsmodelsthatweintroducenext.\n6.2 Vector Semantics\nvector Vector semantics is the standard way to represent word meaning in NLP, helping\nsemantics\nusmodelmanyoftheaspectsofwordmeaningwesawintheprevioussection. The\nroots of the model lie in the 1950s when two big ideas converged: Osgood’s 1957\nidea mentioned above to use a point in three-dimensional space to represent the\nconnotationofaword,andtheproposalbylinguistslikeJoos(1950),Harris(1954),\nand Firth (1957) to define the meaning of a word by its distribution in language\nuse, meaning its neighboring words or grammatical environments. Their idea was\nthattwowordsthatoccurinverysimilardistributions(whoseneighboringwordsare\nsimilar)havesimilarmeanings.\nForexample,supposeyoudidn’tknowthemeaningofthewordongchoi(are-\ncentborrowingfromCantonese)butyouseeitinthefollowingcontexts:\n(6.1) Ongchoiisdelicioussauteedwithgarlic.\n(6.2) Ongchoiissuperboverrice.\n(6.3) ...ongchoileaveswithsaltysauces...\nAndsupposethatyouhadseenmanyofthesecontextwordsinothercontexts:\n(6.4) ...spinachsauteedwithgarlicoverrice...\n(6.5) ...chardstemsandleavesaredelicious...\n(6.6) ...collardgreensandothersaltyleafygreens\nThe fact that ongchoi occurs with words like rice and garlic and delicious and\nsalty,asdowordslikespinach,chard,andcollardgreensmightsuggestthatongchoi\nis a leafy green similar to these other leafy greens.1 We can do the same thing\ncomputationallybyjustcountingwordsinthecontextofongchoi.\nTheideaofvectorsemanticsistorepresentawordasapointinamultidimen-\nsional semantic space that is derived (in ways we’ll see) from the distributions of\nembeddings word neighbors. Vectors for representing words are called embeddings (although\nthe term is sometimes more strictly applied only to dense vectors like word2vec\n(Section 6.8), rather than sparse tf-idf or PPMI vectors (Section 6.3-Section 6.6)).\nTheword“embedding”derivesfromitsmathematicalsenseasamappingfromone\nspace or structure to another, although the meaning has shifted; see the end of the\nchapter.\n1 It’sinfactIpomoeaaquatica,arelativeofmorningglorysometimescalledwaterspinachinEnglish. 6.3 • WORDSANDVECTORS 7\naparticularword(definedbytherow)occursinaparticulardocument(definedby\nthecolumn). Thusfoolappeared58timesinTwelfthNight.\nAsYouLikeIt TwelfthNight JuliusCaesar HenryV\nbattle 1 0 7 13\ngood 114 80 62 89\nfool 36 58 1 4\nwit 20 15 2 3\nFigure6.2 Theterm-documentmatrixforfourwordsinfourShakespeareplays.Eachcell\ncontainsthenumberoftimesthe(row)wordoccursinthe(column)document.\nThe term-document matrix of Fig. 6.2 was first defined as part of the vector\nvectorspace space model of information retrieval (Salton, 1971). In this model, a document is\nmodel\nrepresentedasacountvector,acolumninFig.6.3.\nvector Toreviewsomebasiclinearalgebra, avectoris, atheart, justalistorarrayof\nnumbers. SoAsYouLikeItisrepresentedasthelist[1,114,36,20](thefirstcolumn\nvector in Fig.6.3) and Julius Caesar is representedas the list [7,62,1,2](the third\nvectorspace column vector). A vector space is a collection of vectors, and is characterized by\ndimension its dimension. Vectors in a 3-dimensional vector space have an element for each\ndimensionofthespace. Wewilllooselyrefertoavectorina4-dimensionalspace\nasa4-dimensionalvector,withoneelementalongeachdimension. Intheexample\ninFig.6.3,we’vechosentomakethedocumentvectorsofdimension4,justsothey\nfit on the page; in real term-document matrices, the document vectors would have\ndimensionality|V|,thevocabularysize.\nTheorderingofthenumbersinavectorspaceindicatesthedifferentdimensions\nonwhichdocumentsvary. Thefirstdimensionforboththesevectorscorrespondsto\nthe number of times the word battle occurs, and we can compare each dimension,\nnotingforexamplethatthevectorsforAsYouLikeItandTwelfthNighthavesimilar\nvalues(1and0,respectively)forthefirstdimension.\nAsYouLikeIt TwelfthNight JuliusCaesar HenryV\nbattle 1 0 7 13\ngood 114 80 62 89\nfool 36 58 1 4\nwit 20 15 2 3\nFigure6.3 Theterm-documentmatrixforfourwordsinfourShakespeareplays. Thered\nboxesshowthateachdocumentisrepresentedasacolumnvectoroflengthfour.\nWecanthinkofthevectorforadocumentasapointin|V|-dimensionalspace;\nthusthedocumentsinFig.6.3arepointsin4-dimensionalspace.Since4-dimensional\nspacesarehardtovisualize,Fig.6.4showsavisualizationintwodimensions;we’ve\narbitrarilychosenthedimensionscorrespondingtothewordsbattleandfool.\nTerm-document matrices were originally defined as a means of finding similar\ndocumentsforthetaskofdocumentinformationretrieval. Twodocumentsthatare\nsimilar will tend to have similar words, and if two documents have similar words\ntheir column vectors will tend to be similar. The vectors for the comedies As You\nLikeIt[1,114,36,20]andTwelfthNight[0,80,58,15]lookalotmorelikeeachother\n(more fools and wit than battles) than they look like Julius Caesar [7,62,1,2] or\nHenry V [13,89,4,3]. This is clear with the raw numbers; in the first dimension\n(battle)thecomedieshavelownumbersandtheothershavehighnumbers,andwe\ncan see it visually in Fig. 6.4; we’ll see very shortly how to quantify this intuition\nmoreformally. 10 CHAPTER6 • VECTORSEMANTICSANDEMBEDDINGS\ninthetrainingcorpus; keepingwordsafteraboutthemostfrequent50,000orsois\ngenerallynothelpful). Sincemostofthesenumbersarezerothesearesparsevector\nrepresentations;thereareefficientalgorithmsforstoringandcomputingwithsparse\nmatrices.\nNowthatwehavesomeintuitions,let’smoveontoexaminethedetailsofcom-\nputingwordsimilarity. Afterwardswe’lldiscussmethodsforweightingcells.\n6.4 Cosine for measuring similarity\nTo measure similarity between two target words v and w, we need a metric that\ntakestwovectors(ofthesamedimensionality,eitherbothwithwordsasdimensions,\nhenceoflength|V|,orbothwithdocumentsasdimensions,oflength|D|)andgives\nameasureoftheirsimilarity.Byfarthemostcommonsimilaritymetricisthecosine\noftheanglebetweenthevectors.\nThecosine—likemostmeasuresforvectorsimilarityusedinNLP—isbasedon\ndotproduct thedotproductoperatorfromlinearalgebra,alsocalledtheinnerproduct:\ninnerproduct\nN\n(cid:88)\ndotproduct(v,w)=v·w= v iw i=v 1w 1+v 2w 2+...+v Nw N (6.7)\ni=1\nThedotproductactsasasimilaritymetricbecauseitwilltendtobehighjustwhen\nthetwovectorshavelargevaluesinthesamedimensions. Alternatively,vectorsthat\nhavezerosindifferentdimensions—orthogonalvectors—willhaveadotproductof\n0,representingtheirstrongdissimilarity.\nThis raw dot product, however, has a problem as a similarity metric: it favors\nvectorlength longvectors. Thevectorlengthisdefinedas\n(cid:118)\n(cid:117) N\n(cid:117)(cid:88)\n|v| = (cid:116) v2 (6.8)\ni\ni=1\nThedotproductishigherifavectorislonger,withhighervaluesineachdimension.\nMore frequent words have longer vectors, since they tend to co-occur with more\nwordsandhavehigherco-occurrencevalueswitheachofthem.Therawdotproduct\nthuswillbehigherforfrequentwords. Butthisisaproblem;we’dlikeasimilarity\nmetricthattellsushowsimilartwowordsareregardlessoftheirfrequency.\nWe modify the dot product to normalize for the vector length by dividing the\ndotproductbythelengthsofeachofthetwovectors. Thisnormalizeddotproduct\nturnsouttobethesameasthecosineoftheanglebetweenthetwovectors,following\nfromthedefinitionofthedotproductbetweentwovectorsaandb:\na·b = |a||b|cosθ\na·b\n= cosθ (6.9)\n|a||b|\ncosine Thecosinesimilaritymetricbetweentwovectorsvandwthuscanbecomputedas:"
    },
    "14": {
        "chapter": "13",
        "sections": "13.2",
        "topic": "Machine Translation",
        "original_category": "CL",
        "original_text": "2 CHAPTER13 • MACHINETRANSLATION\ninmachinetranslation,thewordsofthetargetlanguagedon’tnecessarilyagreewith\nthewordsofthesourcelanguageinnumberororder. Considertranslatingthefol-\nlowingmade-upEnglishsentenceintoJapanese.\n(13.1) English: Hewrotealettertoafriend\nJapanese: tomodachinitegami-okaita\nfriend toletter wrote\nNote thatthe elementsof thesentences arein verydifferent placesin thedifferent\nlanguages. InEnglish,theverbisinthemiddleofthesentence,whileinJapanese,\ntheverbkaitacomesattheend. TheJapanesesentencedoesn’trequirethepronoun\nhe,whileEnglishdoes.\nSuchdifferencesbetweenlanguagescanbequitecomplex. Inthefollowingac-\ntualsentencefromtheUnitedNations,noticethemanychangesbetweentheChinese\nsentence (we’ve given in red a word-by-word gloss of the Chinese characters) and\nitsEnglishequivalentproducedbyhumantranslators.\n(13.2) 大会/GeneralAssembly在/on1982年/198212月/December10日/10通过\n了/adopted第37号/37th决议/resolution，核准了/approved第二\n次/second探索/exploration及/and和平peaceful利用/using外层空\n间/outerspace会议/conference的/of各项/various建议/suggestions。\nOn10December1982,theGeneralAssemblyadoptedresolution37in\nwhichitendorsedtherecommendationsoftheSecondUnitedNations\nConferenceontheExplorationandPeacefulUsesofOuterSpace.\nNote the many ways the English and Chinese differ. For example the order-\ningdiffers inmajorways; the Chineseorderof thenounphraseis “peacefulusing\nouterspaceconferenceofsuggestions”whiletheEnglishhas“suggestionsofthe...\nconference on peaceful use of outer space”). And the order differs in minor ways\n(the date is ordered differently). English requires the in many places that Chinese\ndoesn’t, and adds some details (like “in which” and “it”) that aren’t necessary in\nChinese. Chinese doesn’t grammatically mark plurality on nouns (unlike English,\nwhichhasthe“-s”in“recommendations”),andsotheChinesemustusethemodi-\nfier各项/varioustomakeitclearthatthereisnotjustonerecommendation. English\ncapitalizessomewordsbutnotothers. Encoder-decodernetworksareverysuccess-\nfulathandlingthesesortsofcomplicatedcasesofsequencemappings.\nWe’ll begin in the next section by considering the linguistic background about\nhowlanguagesvary,andtheimplicationsthisvariancehasforthetaskofMT.Then\nwe’llsketchoutthestandardalgorithm,givedetailsaboutthingslikeinputtokeniza-\ntion and creating training corpora of parallel sentences, give some more low-level\ndetailsabouttheencoder-decodernetwork,andfinallydiscusshowMTisevaluated,\nintroducingthesimplechrFmetric.\n13.1 Language Divergences and Typology\nThere are about 7,000 languages in the world. Some aspects of human language\nuniversal seemtobeuniversal,holdingtrueforeveryoneoftheselanguages,orarestatistical\nuniversals,holdingtrueformostoftheselanguages. Manyuniversalsarisefromthe\nfunctionalroleoflanguageasacommunicativesystembyhumans. Everylanguage,\nfor example, seems to have words for referring to people, for talking about eating\nand drinking, for being polite or not. There are also structural linguistic univer-\nsals;forexample,everylanguageseemstohavenounsandverbs(Chapter17),has 4 CHAPTER13 • MACHINETRANSLATION\ntoafriend,inwhichtheprepositiontoisfollowedbyitsargumentafriend. Arabic,\nwithaVSOorder,alsohastheverbbeforetheobjectandprepositions. Bycontrast,\nintheJapaneseexamplethatfollows,eachoftheseorderingsisreversed;theverbis\nprecededbyitsarguments,andthepostpositionfollowsitsargument.\n(13.3) English: Hewrotealettertoafriend\nJapanese: tomodachinitegami-okaita\nfriend toletter wrote\nArabic: katabtrisa¯lali s˙adq\nwrote letter tofriend\nOtherkindsoforderingpreferencesvaryidiosyncraticallyfromlanguagetolan-\nguage. InsomeSVOlanguages(likeEnglishandMandarin)adjectivestendtoap-\npearbeforenouns,whileinotherslanguageslikeSpanishandModernHebrew,ad-\njectivesappearafterthenoun:\n(13.4) Spanish bruja verde English green witch\n(a) (b)\nFigure13.2 Examples of other word order differences: (a) In German, adverbs occur in\ninitialpositionthatinEnglisharemorenaturallater,andtensedverbsoccurinsecondposi-\ntion. (b)InMandarin,prepositionphrasesexpressinggoalsoftenoccurpre-verbally,unlike\ninEnglish.\nFig. 13.2 shows examples of other word order differences. All of these word\norder differences between languages can cause problems for translation, requiring\nthesystemtodohugestructuralreorderingsasitgeneratestheoutput.\n13.1.2 LexicalDivergences\nOfcoursewealsoneedtotranslatetheindividualwordsfromonelanguagetoan-\nother. Foranytranslation,theappropriatewordcanvarydependingonthecontext.\nTheEnglishsource-languagewordbass,forexample,canappearinSpanishasthe\nfishlubinaorthemusicalinstrumentbajo. Germanusestwodistinctwordsforwhat\ninEnglishwouldbecalledawall: Wandforwallsinsideabuilding,andMauerfor\nwalls outside a building. Where English uses the word brother for any male sib-\nling, Chinese and many other languages have distinct words for older brother and\nyounger brother (Mandarin gege and didi, respectively). In all these cases, trans-\nlating bass, wall, or brother from English would require a kind of specialization,\ndisambiguating the different uses of a word. For this reason the fields of MT and\nWordSenseDisambiguation(AppendixG)arecloselylinked.\nSometimes one language places more grammatical constraints on word choice\nthananother. WesawabovethatEnglishmarksnounsforwhethertheyaresingular\norplural. Mandarindoesn’t. OrFrenchandSpanish, forexample, markgrammat-\nicalgenderonadjectives, soanEnglishtranslationintoFrenchrequiresspecifying\nadjectivegender.\nThewaythatlanguagesdifferinlexicallydividingupconceptualspacemaybe\nmorecomplexthanthisone-to-manytranslationproblem,leadingtomany-to-many 6 CHAPTER13 • MACHINETRANSLATION\nfusion atively clean boundaries, to fusion languages like Russian, in which a single affix\nmay conflate multiple morphemes, like -om in the word stolom (table-SG-INSTR-\nDECL1), which fuses the distinct morphological categories instrumental, singular,\nandfirstdeclension.\nTranslatingbetweenlanguageswithrichmorphologyrequiresdealingwithstruc-\nturebelowthewordlevel,andforthisreasonmodernsystemsgenerallyusesubword\nmodelslikethewordpieceorBPEmodelsofSection13.2.1.\n13.1.4 Referentialdensity\nFinally,languagesvaryalongatypologicaldimensionrelatedtothethingstheytend\ntoomit.Somelanguages,likeEnglish,requirethatweuseanexplicitpronounwhen\ntalkingaboutareferentthatisgiveninthediscourse. Inotherlanguages,however,\nwecansometimesomitpronounsaltogether,asthefollowingexamplefromSpanish\nshows1:\n(13.6) [Eljefe]idioconunlibro. 0/iMostro´ suhallazgoaundescifradorambulante.\n[Theboss]cameuponabook. [He]showedhisfindtoawanderingdecoder.\npro-drop Languagesthatcanomitpronounsarecalledpro-droplanguages. Evenamong\nthe pro-drop languages, there are marked differences in frequencies of omission.\nJapaneseandChinese,forexample,tendtoomitfarmorethandoesSpanish. This\ndimensionofvariationacrosslanguagesiscalledthedimensionofreferentialden-\nreferential sity. Wesaythatlanguagesthattendtousemorepronounsaremorereferentially\ndensity\ndensethanthosethatusemorezeros.Referentiallysparselanguages,likeChineseor\nJapanese,thatrequirethehearertodomoreinferentialworktorecoverantecedents\ncoldlanguage arealsocalledcoldlanguages. Languagesthataremoreexplicitandmakeiteasier\nhotlanguage forthehearerarecalledhotlanguages. Thetermshotandcoldareborrowedfrom\nMarshallMcLuhan’s1964distinctionbetweenhotmedialikemovies,whichfillin\nmanydetailsfortheviewer,versuscoldmedialikecomics,whichrequirethereader\ntodomoreinferentialworktofillouttherepresentation(Bickel,2003).\nTranslatingfromlanguageswithextensivepro-drop,likeChineseorJapanese,to\nnon-pro-droplanguageslikeEnglishcanbedifficultsincethemodelmustsomehow\nidentifyeachzeroandrecoverwhoorwhatisbeingtalkedaboutinordertoinsert\ntheproperpronoun.\n13.2 Machine Translation using Encoder-Decoder\nThestandardarchitectureforMTistheencoder-decodertransformerorsequence-\nto-sequence model, an architecture we saw for RNNs in Chapter 8. We’ll see the\ndetailsofhowtoapplythisarchitecturetotransformersinSection13.3,butfirstlet’s\ntalkabouttheoveralltask.\nMostmachinetranslationtasksmakethesimplificationthatwecantranslateeach\nsentenceindependently,sowe’lljustconsiderindividualsentencesfornow. Given\na sentence in a source language, the MT task is then to generate a corresponding\nsentence in a target language. For example, an MT system is given an English\nsentencelike\nThegreenwitcharrived\n1 Hereweusethe0/-notation;we’llintroducethisanddiscussthisissuefurtherinChapter23 13.2 • MACHINETRANSLATIONUSINGENCODER-DECODER 7\nandmusttranslateitintotheSpanishsentence:\nLlego´ labrujaverde\nMT uses supervised machine learning: at training time the system is given a\nlarge set of parallel sentences (each sentence in a source language matched with\na sentence in the target language), and learns to map source sentences into target\nsentences. Inpractice, ratherthanusingwords(asintheexampleabove), wesplit\nthesentencesintoasequenceofsubwordtokens(tokenscanbewords,orsubwords,\norindividualcharacters). Thesystemsarethentrainedtomaximizetheprobability\nof the sequence of tokens in the target language y 1,...,y m given the sequence of\ntokensinthesourcelanguagex 1,...,x n:\nP(y 1,...,y m x 1,...,x n) (13.7)\n|\nRatherthanusetheinputtokensdirectly,theencoder-decoderarchitecturecon-\nsists of two components, an encoder and a decoder. The encoder takes the input\nwordsx=[x 1,...,x n]andproducesanintermediatecontexth.Atdecodingtime,the\nsystemtakeshand,wordbyword,generatestheoutputy:\nh = encoder(x) (13.8)\ny t+1 = decoder(h,y 1,...,y t)) t [1,...,m] (13.9)\n∀ ∈\nInthenexttwosectionswe’lltalkaboutsubwordtokenization,andthenhowtoget\nparallel corpora for training, and then we’ll introduce the details of the encoder-\ndecoderarchitecture.\n13.2.1 Tokenization\nMachine translation systems use a vocabulary that is fixed in advance, and rather\nthan using space-separated words, this vocabulary is generated with subword to-\nkenization algorithms, like the BPE algorithm sketched in Chapter 2. A shared\nvocabularyisusedforthesourceandtargetlanguages,whichmakesiteasytocopy\ntokens(likenames)fromsourcetotarget. Usingsubwordtokenizationwithtokens\nsharedbetweenlanguagesmakesitnaturaltotranslatebetweenlanguageslikeEn-\nglishorHindithatusespacestoseparatewords,andlanguageslikeChineseorThai\nthatdon’t.\nWebuildthevocabularybyrunningasubwordtokenizationalgorithmonacor-\npusthatcontainsbothsourceandtargetlanguagedata.\nRatherthanthesimpleBPEalgorithmfromFig.??, modernsystemsoftenuse\nmorepowerfultokenizationalgorithms. Somesystems(likeBERT)useavariantof\nwordpiece BPEcalledthewordpiecealgorithm, whichinsteadofchoosingthemostfrequent\nsetoftokenstomerge,choosesmergesbasedonwhichonemostincreasesthelan-\nguagemodelprobabilityofthetokenization. Wordpiecesuseaspecialsymbolatthe\nbeginningofeachtoken;here’saresultingtokenizationfromtheGoogleMTsystem\n(Wuetal.,2016):\nwords: Jetmakersfeudoverseatwidthwithbigordersatstake\nwordpieces: Jet makers feud over seat width with big orders at stake\nThewordpiecealgorithmisgivenatrainingcorpusandadesiredvocabularysize\nV,andproceedsasfollows:\n1. Initializethewordpiecelexiconwithcharacters(forexampleasubsetofUni-\ncodecharacters,collapsingalltheremainingcharacterstoaspecialunknown\ncharactertoken). 8 CHAPTER13 • MACHINETRANSLATION\n2. RepeatuntilthereareVwordpieces:\n(a) Trainann-gramlanguagemodelonthetrainingcorpus,usingthecurrent\nsetofwordpieces.\n(b) Considerthesetofpossiblenewwordpiecesmadebyconcatenatingtwo\nwordpiecesfromthecurrentlexicon.Choosetheonenewwordpiecethat\nmostincreasesthelanguagemodelprobabilityofthetrainingcorpus.\nRecall that with BPE we had to specify the number of merges to perform; in\nwordpiece, by contrast, we specify the total vocabulary, which is a more intuitive\nparameter. Avocabularyof8Kto32Kwordpiecesiscommonlyused.\nAn even more commonly used tokenization algorithm is (somewhat ambigu-\nunigram ously)calledtheunigramalgorithm(Kudo,2018)orsometimestheSentencePiece\nSentencePiece algorithm, and is used in systems like ALBERT (Lan et al., 2020) and T5 (Raf-\nfel et al., 2020). (Because unigram is the default tokenization algorithm used in\nalibrarycalledSentencePiecethataddsausefulwrapperaroundtokenizationalgo-\nrithms(KudoandRichardson,2018),authorsoftensaytheyareusingSentencePiece\ntokenizationbutreallymeantheyareusingtheunigramalgorithm).\nInunigramtokenization,insteadofbuildingupavocabularybymergingtokens,\nwe start with a huge vocabulary of every individual unicode character plus all fre-\nquent sequences of characters (including all space-separated words, for languages\nwithspaces),anditerativelyremovesometokenstogettoadesiredfinalvocabulary\nsize. The algorithm is complex (involving suffix-trees for efficiently storing many\ntokens,andtheEMalgorithmforiterativelyassigningprobabilitiestotokens),sowe\ndon’tgiveithere,butseeKudo(2018)andKudoandRichardson(2018). Roughly\nspeaking the algorithm proceeds iteratively by estimating the probability of each\ntoken, tokenizing the input data using various tokenizations, then removing a per-\ncentageoftokensthatdon’toccurinhigh-probabilitytokenization,andtheniterates\nuntilthevocabularyhasbeenreduceddowntothedesirednumberoftokens.\nWhydoesunigramtokenizationworkbetterthanBPE?BPEtendstocreatelots\nofverysmallnon-meaningfultokens(becauseBPEcanonlycreatelargerwordsor\nmorphemes by merging characters one at a time), and it also tends to merge very\ncommon tokens, like the suffix ed, onto their neighbors. We can see from these\nexamples from Bostrom and Durrett (2020) that unigram tends to produce tokens\nthataremoresemanticallymeaningful:\nOriginal: corrupted Original: Completely preposterous suggestions\nBPE: cor rupted BPE: Comple t ely prep ost erous suggest ions\nUnigram: corrupt ed Unigram: Complete ly pre post er ous suggestion s\n13.2.2 CreatingtheTrainingdata\nparallelcorpus Machine translation models are trained on a parallel corpus, sometimes called a\nbitext, a text that appears in two (or more) languages. Large numbers of paral-\nEuroparl lel corpora are available. Some are governmental; the Europarl corpus (Koehn,\n2005),extractedfromtheproceedingsoftheEuropeanParliament,containsbetween\n400,000and2millionsentenceseachfrom21Europeanlanguages. TheUnitedNa-\ntionsParallelCorpuscontainsontheorderof10millionsentencesinthesixofficial\nlanguagesoftheUnitedNations(Arabic,Chinese,English,French,Russian,Span-\nish)Ziemskietal.(2016). Otherparallelcorporahavebeenmadefrommovieand\nTVsubtitles,liketheOpenSubtitlescorpus(LisonandTiedemann,2016),orfrom\ngeneralwebtext,liketheParaCrawlcorpusof223millionsentencepairsbetween\n23EUlanguagesandEnglishextractedfromtheCommonCrawlBan˜o´netal.(2020). 13.2 • MACHINETRANSLATIONUSINGENCODER-DECODER 9\nSentencealignment\nStandardtrainingcorporaforMTcomeasalignedpairsofsentences. Whencreat-\ningnewcorpora,forexampleforunderresourcedlanguagesornewdomains,these\nsentencealignmentsmustbecreated.Fig.13.4givesasamplehypotheticalsentence\nalignment.\nE1: “Good morning,\" said the little prince. F1: -Bonjour, dit le petit prince.\nE2: “Good morning,\" said the merchant. F2: -Bonjour, dit le marchand de pilules perfectionnées qui\napaisent la soif.\nE3: This was a merchant who sold pills that had\nF3: On en avale une par semaine et l'on n'éprouve plus le\nbeen perfected to quench thirst.\nbesoin de boire.\nE4: You just swallow one pill a week and you F4: -C’est une grosse économie de temps, dit le marchand.\nwon’t feel the need for anything to drink.\nE5: “They save a huge amount of time,\" said the merchant. F5: Les experts ont fait des calculs.\nE6: “Fifty−three minutes a week.\" F6: On épargne cinquante-trois minutes par semaine.\nE7: “If I had fifty−three minutes to spend?\" said the F7: “Moi, se dit le petit prince, si j'avais cinquante-trois minutes\nlittle prince to himself. à dépenser, je marcherais tout doucement vers une fontaine...\"\nE8: “I would take a stroll to a spring of fresh water”\nFigure13.4 A sample alignment between sentences in English and French, with sentences extracted from\nAntoinedeSaint-Exupery’sLePetitPrinceandahypotheticaltranslation. Sentencealignmenttakessentences\ne 1,...,en,and f 1,...,fn andfindsminimalsetsofsentencesthataretranslationsofeachother,includingsingle\nsentencemappingslike(e ,f ),(e ,f ),(e ,f ),(e ,f )aswellas2-1alignments(e /e ,f ),(e /e ,f ),andnull\n1 1 4 3 5 4 6 6 2 3 2 7 8 7\nalignments(f ).\n5\nGiventwodocumentsthataretranslationsofeachother,wegenerallyneedtwo\nstepstoproducesentencealignments:\n• acostfunctionthattakesaspanofsourcesentencesandaspanoftargetsen-\ntencesandreturnsascoremeasuringhowlikelythesespansaretobetransla-\ntions.\n• an alignment algorithm that takes these scores to find a good alignment be-\ntweenthedocuments.\nTo score the similarity of sentences across languages, we need to make use of\namultilingualembeddingspace,inwhichsentencesfromdifferentlanguagesare\nin the same embedding space (Artetxe and Schwenk, 2019). Given such a space,\ncosinesimilarityofsuchembeddingsprovidesanaturalscoringfunction(Schwenk,\n2018). ThompsonandKoehn(2019)givethefollowingcostfunctionbetweentwo\nsentencesorspansx,yfromthesourceandtargetdocumentsrespectively:\n(1 cos(x,y))nSents(x)nSents(y)\nc(x,y)= − (13.10)\nS s=11 −cos(x,y s)+ S s=11 −cos(x s,y)\nwherenSents()givesthe(cid:80)numberofsentences(cid:80)(thisbiasesthemetrictowardmany\nalignments of single sentences instead of aligning very large spans). The denom-\ninator helps to normalize the similarities, and so x 1,...,x S,y 1,...,y S, are randomly\nselectedsentencessampledfromtherespectivedocuments.\nUsually dynamic programming is used as the alignment algorithm (Gale and\nChurch, 1993), in a simple extension of the minimum edit distance algorithm we\nintroducedinChapter2.\nFinally,it’shelpfultodosomecorpuscleanupbyremovingnoisysentencepairs.\nThis can involve handwritten rules to remove low-precision pairs (for example re-\nmoving sentences that are too long, too short, have different URLs, or even pairs"
    },
    "15": {
        "chapter": "16",
        "sections": "16.2",
        "topic": "Automatic Speech Recognition (ASR)",
        "original_category": "CL",
        "original_text": "4 CHAPTER16 • AUTOMATICSPEECHRECOGNITIONANDTEXT-TO-SPEECH\ncorpus contains recordings of twenty different dinner parties in real homes, each\nwith four participants, and in three locations (kitchen, dining area, living room),\nrecorded both with distant room microphones and with body-worn mikes. The\nHKUST HKUST Mandarin Telephone Speech corpus has 1206 ten-minute telephone con-\nversationsbetweenspeakersofMandarinacrossChina,includingtranscriptsofthe\nconversations,whicharebetweeneitherfriendsorstrangers(Liuetal.,2006). The\nAISHELL-1 AISHELL-1corpuscontains170hoursofMandarinreadspeechofsentencestaken\nfrom various domains, read by different speakers mainly from northern China (Bu\netal.,2017).\nFigure16.1showstheroughpercentageofincorrectwords(theworderrorrate,\norWER,definedonpage16)fromstate-of-the-artsystemsonsomeofthesetasks.\nNote that the error rate on read speech (like the LibriSpeech audiobook corpus) is\naround2%;thisisasolvedtask,althoughthesenumberscomefromsystemsthatre-\nquireenormouscomputationalresources. Bycontrast,theerrorratefortranscribing\nconversationsbetweenhumansismuchhigher;5.8to11%fortheSwitchboardand\nCALLHOME corpora. The error rate is higher yet again for speakers of varieties\nlikeAfricanAmericanVernacularEnglish,andyetagainfordifficultconversational\ntasksliketranscriptionof4-speakerdinnerpartyspeech,whichcanhaveerrorrates\nas high as 81.3%. Character error rates (CER) are also much lower for read Man-\ndarinspeechthanfornaturalconversation.\nEnglishTasks WER%\nLibriSpeechaudiobooks960hourclean 1.4\nLibriSpeechaudiobooks960hourother 2.6\nSwitchboardtelephoneconversationsbetweenstrangers 5.8\nCALLHOMEtelephoneconversationsbetweenfamily 11.0\nSociolinguisticinterviews,CORAAL(AAL) 27.0\nCHiMe5dinnerpartieswithbody-wornmicrophones 47.9\nCHiMe5dinnerpartieswithdistantmicrophones 81.3\nChinese(Mandarin)Tasks CER%\nAISHELL-1Mandarinreadspeechcorpus 6.7\nHKUSTMandarinChinesetelephoneconversations 23.5\nFigure16.1 RoughWordErrorRates(WER=%ofwordsmisrecognized)reportedaround\n2020forASRonvariousAmericanEnglishrecognitiontasks,andcharactererrorrates(CER)\nfortwoChineserecognitiontasks.\n16.2 Feature Extraction for ASR: Log Mel Spectrum\nThefirststepinASRistotransformtheinputwaveformintoasequenceofacoustic\nfeaturevector feature vectors, each vector representing the information in a small time window\nof the signal. Let’s see how to convert a raw wavefile to the most commonly used\nfeatures,sequencesoflogmelspectrumvectors.Aspeechsignalprocessingcourse\nisrecommendedformoredetails.\n16.2.1 SamplingandQuantization\nThe input to a speech recognizer is a complex series of changes in air pressure.\nThese changes in air pressure obviously originate with the speaker and are caused 16.2 • FEATUREEXTRACTIONFORASR:LOGMELSPECTRUM 5\nbythespecificwaythatairpassesthroughtheglottisandouttheoralornasalcav-\nities. We represent sound waves by plotting the change in air pressure over time.\nOnemetaphorwhichsometimeshelpsinunderstandingthesegraphsisthatofaver-\ntical plate blocking the air pressure waves (perhaps in a microphone in front of a\nspeaker’smouth,ortheeardruminahearer’sear). Thegraphmeasurestheamount\nof compression or rarefaction (uncompression) of the air molecules at this plate.\nFigure16.2showsashortsegmentofawaveformtakenfromtheSwitchboardcorpus\noftelephonespeechofthevowel[iy]fromsomeonesaying“shejusthadababy”.\n0.02283\n0\n–0.01697\n0 0.03875\nTime (s)\nFigure16.2 Awaveformofaninstanceofthevowel[iy](thelastvowelintheword“baby”). They-axis\nshowsthelevelofairpressureaboveandbelownormalatmosphericpressure. Thex-axisshowstime. Notice\nthatthewaverepeatsregularly.\nThe first step in digitizing a sound wave like Fig. 16.2 is to convert the analog\nrepresentations(firstairpressureandthenanalogelectricsignalsinamicrophone)\nsampling intoadigitalsignal.Thisanalog-to-digitalconversionhastwosteps:samplingand\nquantization. Tosampleasignal,wemeasureitsamplitudeataparticulartime;the\nsamplingrateisthenumberofsamplestakenpersecond. Toaccuratelymeasurea\nwave,wemusthaveatleasttwosamplesineachcycle: onemeasuringthepositive\npart of the wave and one measuring the negative part. More than two samples per\ncycleincreasestheamplitudeaccuracy,butfewerthantwosamplescausesthefre-\nquencyofthewavetobecompletelymissed. Thus, themaximumfrequencywave\nthat can be measured is one whose frequency is half the sample rate (since every\ncycle needs two samples). This maximum frequency for a given sampling rate is\nNyquist calledtheNyquistfrequency. Mostinformationinhumanspeechisinfrequencies\nfrequency\nbelow 10,000 Hz; thus, a 20,000 Hz sampling rate would be necessary for com-\npleteaccuracy. Buttelephonespeechisfilteredbytheswitchingnetwork,andonly\nfrequencies less than 4,000 Hz are transmitted by telephones. Thus, an 8,000 Hz\nsampling rate is sufficient for telephone-bandwidth speech like the Switchboard\ncorpus,while16,000Hzsamplingisoftenusedformicrophonespeech.\nAlthoughusinghighersamplingratesproduceshigherASRaccuracy,wecan’t\ncombine different sampling rates for training and testing ASR systems. Thus if\nwearetestingonatelephonecorpuslikeSwitchboard(8KHzsampling),wemust\ndownsample our training corpus to 8 KHz. Similarly, if we are training on mul-\ntiple corpora and one of them includes telephone speech, we downsample all the\nwidebandcorporato8Khz.\nAmplitudemeasurementsarestoredasintegers,either8bit(valuesfrom-128–\n127)or16bit(valuesfrom-32768–32767).Thisprocessofrepresentingreal-valued\nquantization numbersasintegersiscalledquantization; allvaluesthatareclosertogetherthan\ntheminimumgranularity(thequantumsize)arerepresentedidentically. Wereferto\neachsampleattimeindexninthedigitized,quantizedwaveformasx[n].\nOnce data is quantized, it is stored in various formats. One parameter of these\nformats is the sample rate and sample size discussed above; telephone speech is\noften sampled at 8 kHz and stored as 8-bit samples, and microphone data is often\nsampledat16kHzandstoredas16-bitsamples.Anotherparameteristhenumberof 6 CHAPTER16 • AUTOMATICSPEECHRECOGNITIONANDTEXT-TO-SPEECH\nchannel channels.Forstereodataorfortwo-partyconversations,wecanstorebothchannels\ninthesamefileorwecanstoretheminseparatefiles.Afinalparameterisindividual\nsamplestorage—linearlyorcompressed.Onecommoncompressionformatusedfor\ntelephone speech is µ-law (often written u-law but still pronounced mu-law). The\nintuition of log compression algorithms like µ-law is that human hearing is more\nsensitive at small intensities than large ones; the log represents small values with\nmorefaithfulnessattheexpenseofmoreerroronlargevalues.Thelinear(unlogged)\nPCM valuesaregenerallyreferredtoaslinearPCMvalues(PCMstandsforpulsecode\nmodulation,butnevermindthat).Here’stheequationforcompressingalinearPCM\nsamplevaluexto8-bitµ-law,(whereµ=255for8bits):\nsgn(x)log(1+µ x)\nF(x) = | | 1 x 1 (16.1)\nlog(1+µ) − ≤ ≤\nThereareanumberofstandardfileformatsforstoringtheresultingdigitizedwave-\nfile,suchasMicrosoft’s.wavandApple’sAIFFallofwhichhavespecialheaders;\nsimpleheaderless“raw”filesarealsoused. Forexample,the.wavformatisasub-\nset of Microsoft’s RIFF format for multimedia files; RIFF is a general format that\ncanrepresentaseriesofnestedchunksofdataandcontrolinformation. Figure16.3\nshowsasimple.wavfilewithasingledatachunktogetherwithitsformatchunk.\nFigure16.3 Microsoftwavefileheaderformat,assumingsimplefilewithonechunk. Fol-\nlowingthis44-byteheaderwouldbethedatachunk.\n16.2.2 Windowing\nFrom the digitized, quantized representation of the waveform, we need to extract\nspectral features from a small window of speech that characterizes part of a par-\nticular phoneme. Inside this small window, we can roughly think of the signal as\nstationary stationary (that is, its statistical properties are constant within this region). (By\nnon-stationary contrast, in general, speech is a non-stationary signal, meaning that its statistical\npropertiesarenotconstantovertime). Weextractthisroughlystationaryportionof\nspeechbyusingawindowwhichisnon-zeroinsidearegionandzeroelsewhere,run-\nningthiswindowacrossthespeechsignalandmultiplyingitbytheinputwaveform\ntoproduceawindowedwaveform.\nframe The speech extracted from each window is called a frame. The windowing is\ncharacterized by three parameters: the window size or frame size of the window\nstride (its width in milliseconds), the frame stride, (also called shift or offset) between\nsuccessivewindows,andtheshapeofthewindow.\nTo extract the signal we multiply the value of the signal at time n, s[n] by the\nvalueofthewindowattimen,w[n]:\ny[n]=w[n]s[n] (16.2)\nrectangular The window shape sketched in Fig. 16.4 is rectangular; you can see the ex-\ntractedwindowedsignallooksjustliketheoriginalsignal. Therectangularwindow, 16.2 • FEATUREEXTRACTIONFORASR:LOGMELSPECTRUM 7\nWindow\n25 ms\nShift\nWindow\n10\nms 25 ms\nShift\nWindow\n10\nms 25 ms\nFigure16.4 Windowing,showinga25msrectangularwindowwitha10msstride.\nhowever,abruptlycutsoffthesignalatitsboundaries,whichcreatesproblemswhen\nwedoFourieranalysis. Forthisreason,foracousticfeaturecreationwemorecom-\nHamming monly use the Hamming window, which shrinks the values of the signal toward\nzero at the window boundaries, avoiding discontinuities. Figure 16.5 shows both;\ntheequationsareasfollows(assumingawindowthatisLframeslong):\n(cid:26)\n1 0 n L 1\nrectangular w[n] = ≤ ≤ − (16.3)\n0 otherwise\n(cid:26) 0.54 0.46cos(2πn) 0 n L 1\nHamming w[n] = − L ≤ ≤ − (16.4)\n0 otherwise\n0.4999\n0\n–0.5\n0 0.0475896\nTime (s)\nRectangular window Hamming window\n0.4999 0.4999\n0 0\n–0.5 –0.4826\n0.00455938 0.0256563 0.00455938 0.0256563\nTime (s) Time (s)\nFigure16.5 WindowingasinewavewiththerectangularorHammingwindows.\n16.2.3 DiscreteFourierTransform\nThenextstepistoextractspectralinformationforourwindowedsignal;weneedto\nknow how much energy the signal contains at different frequency bands. The tool 8 CHAPTER16 • AUTOMATICSPEECHRECOGNITIONANDTEXT-TO-SPEECH\nfor extracting spectral information for discrete frequency bands for a discrete-time\nDiscrete\nFourier (sampled)signalisthediscreteFouriertransformorDFT.\ntransf Do Frm T TheinputtotheDFTisawindowedsignalx[n]...x[m],andtheoutput,foreach\nof N discrete frequency bands, is a complex number X[k] representing the magni-\ntude and phase of that frequency component in the original signal. If we plot the\nmagnitude against the frequency, we can visualize the spectrum (see Appendix H\nfor more on spectra). For example, Fig. 16.6 shows a 25 ms Hamming-windowed\nportion of a signal and its spectrum as computed by a DFT (with some additional\nsmoothing).\n0.04414\n0\n–0.04121\n0.0141752 0.039295 0 8000\nTime (s) Frequency (Hz)\n)zH/Bd(\nlevel\nerusserp\ndnuoS\n20\n0\n–20\n(a) (b)\nFigure16.6 (a) A 25 ms Hamming-windowed portion of a signal from the vowel [iy]\nand (b)itsspectrumcomputedbyaDFT.\nWe do not introduce the mathematical details of the DFT here, except to note\nEuler’sformula thatFourieranalysisreliesonEuler’sformula,with jastheimaginaryunit:\nejθ =cosθ+jsinθ (16.5)\nAsabriefreminderforthosestudentswhohavealreadystudiedsignalprocessing,\ntheDFTisdefinedasfollows:\nN 1\nX[k]=(cid:88)− x[n]e−j2 Nπkn\n(16.6)\nn=0\nfastFourier AcommonlyusedalgorithmforcomputingtheDFTisthefastFouriertransform\ntransform\nFFT orFFT.ThisimplementationoftheDFTisveryefficientbutonlyworksforvalues\nofN thatarepowersof2.\n16.2.4 MelFilterBankandLog\nThe results of the FFT tell us the energy at each frequency band. Human hearing,\nhowever,isnotequallysensitiveatallfrequencybands;itislesssensitiveathigher\nfrequencies. This bias toward low frequencies helps human recognition, since in-\nformationinlowfrequencies(likeformants)iscrucialfordistinguishingvowelsor\nnasals, whileinformationinhighfrequencies(likestopburstsorfricativenoise)is\nless crucial for successful recognition. Modeling this human perceptual property\nimprovesspeechrecognitionperformanceinthesameway.\nWeimplementthisintuitionbycollectingenergies,notequallyateachfrequency\nmel band, butaccordingtothemelscale, anauditoryfrequencyscale. Amel(Stevens\netal.1937,StevensandVolkmann1940)isaunitofpitch. Pairsofsoundsthatare\nperceptuallyequidistantinpitchareseparatedbyanequalnumberofmels. Themel"
    } 
}