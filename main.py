from benchmark_creation import create_benchmark
from prompts import PROMPT_FUNCTIONS

# âœ… Define available LLMs and prompts
LLM_MODELS = {
    # "gpt4o": "gpt4o"
    # "claude" : "claude",
    #"deepseek" : "deepseek",
    # "llama" : "llama",
    "mistral" : "mistral"
}


def main():
    print("\nğŸš€ Starting automatic benchmark generation for all LLMs and prompts...\n")

    for llm_model in LLM_MODELS.values():  # âœ… Only keep values (model names)
        for prompt_function_name in PROMPT_FUNCTIONS.values():  # âœ… Only keep function names
            print(f"\nğŸ“ Running benchmark for:")
            print(f"   ğŸ§  LLM Model: {llm_model}")
            print(f"   âœï¸  Prompt Function: {prompt_function_name}")

            # âœ… Call the benchmark function for each LLM and each prompt
            create_benchmark(llm_model, prompt_function_name)

    print("\nâœ… All benchmarks have been generated successfully!")

if __name__ == "__main__":
    main()
