from benchmark_creation import create_benchmark
from prompts import PROMPT_FUNCTIONS

# ‚úÖ Define available LLMs and prompts
LLM_MODELS = {
    # "gpt4o": "gpt4o"
    # "claude" : "claude",
    # "deepseek" : "deepseek"
    # "mistral" : "mistral"
    # "llama" : "llama"    
}


def main():
    print("\nüöÄ Starting automatic benchmark generation for all LLMs and prompts...\n")

    # TEST: ONLY N PROMPT 

    llm_model = "llama"
    prompt_function_name = PROMPT_FUNCTIONS[1]  # Choose prompt n. to test

    print(f"\nüìù Running benchmark for:")
    print(f"   üß† LLM Model: {llm_model}")
    print(f"   ‚úèÔ∏è  Prompt Function: {prompt_function_name}")

    # ‚úÖ Call the benchmark function for each LLM and each prompt
    create_benchmark(llm_model, prompt_function_name, target_key="12") # max_entries=15) #,  # max_entries=from 1 to n. OR # target_key=only that key

    # WHOLE BENCHMARK
    # for llm_model in LLM_MODELS.values():  # ‚úÖ Only keep values (model names)
    #     for prompt_function_name in PROMPT_FUNCTIONS.values():  # ‚úÖ Only keep function names
# 
    #         print(f"\nüìù Running benchmark for:")
    #         print(f"   üß† LLM Model: {llm_model}")
    #         print(f"   ‚úèÔ∏è  Prompt Function: {prompt_function_name}")

    # ‚úÖ Call the benchmark function for each LLM and each prompt
            # create_benchmark(llm_model, prompt_function_name) # WHOLE BENCHMARK

    print("\n‚úÖ All benchmarks have been generated successfully!")

if __name__ == "__main__":
    main()
